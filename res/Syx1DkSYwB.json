{"notes": [{"id": "Syx1DkSYwB", "original": "B1lglca_vr", "number": 1751, "cdate": 1569439575019, "ddate": null, "tcdate": 1569439575019, "tmdate": 1583912027983, "tddate": null, "forum": "Syx1DkSYwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "RMDYTxVXKY", "original": null, "number": 1, "cdate": 1576798731552, "ddate": null, "tcdate": 1576798731552, "tmdate": 1576800904909, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Congratulations on getting your paper accepted to ICLR. Please make sure to incorporate the reviewers' suggestions for the final version.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708975, "tmdate": 1576800257543, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Decision"}}}, {"id": "S1g8clOTFH", "original": null, "number": 2, "cdate": 1571811470288, "ddate": null, "tcdate": 1571811470288, "tmdate": 1574311118989, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper aims at improving the computational cost of variance reduction methods while preserving their benefits regarding the fast provable convergence. The existing variance reduction based methods suffer from higher per-iteration gradient query complexity as compared to the vanilla mini-batch SGD, which limits their utility in many practical settings. This paper notices that, for many models, as the training progresses the gradient vectors start exhibiting structure in the sense that only a small number of coordinates have large magnitude. Based on this observation, the paper proposes a modified variance reduction method (by modifying the SpiderBoost method), where a 'memory vector' keeps track of the coordinates of the gradient vectors with large variance. Let $d$ be the size of the model parameter. During each iteration, one computes the gradient for $k_1$ coordinates with the highest variance (according to the memory vector) and an additional $k_2$ random coordinates. \n\nThe paper shows that in the worst case, the proposed method has the same gradient query complexity as the SpiderBoost variance reduction method. Assuming that the proposed method can track the sparsity of the gradient vector, the proposed method achieves a gradient query complexity which is $O(\\sqrt{(k_1 + k_2)/d})$ times that of the SpiderBoost method. The paper demonstrates the gradient query complexity improvement over the SpiderBoost method on MNIST and CIFAR-10 data set.\n\nThe paper presents novel results by utilizing the ideas from the field of communication-efficient distributed optimization. As far as the reviewer can tell, the results in the paper are correct. That said, there is quite a bit of room for improvement in terms of the writing of the paper. \n\nThe paper appears to have way too many typos. For example, \n\nIn Section 2.1:\n- Why is $k$ introduced?\n- $S$ denotes a random subset with size $k$ ---> $k_2$?\n- drawn from the set ${\\ell : |y_{\\ell}| < |y_{(k)}|}$ ----> ${\\ell : |x_{\\ell}| < |x_{(k_2)}|}$?\n- $rtop(x, y) = (0, 12, 0, 0, 1)$ --> $rtop(x, y) = (0, 16, 0, 0, 1)$\nIn Lemma 1: \n - while defining $top_{-k_1}(x, y)$, \".... if |x_{\\ell}| >= |x_{(k_1)}|\" ----> \".... if |x_{\\ell}| <= |x_{(k_1)}|\"?\nIn Section 2.2:\n - What are $g_0, g_1,..., g_{L-1}$? Shouldn't these be $\\phi_0, \\phi_1,..., \\phi_{L-1}$?\nIn A1: \n - right after (5), what is $\\tilde{x}_0$ in the definition of $\\Delta_f$?\n\nThe authors may also consider making the empirical evaluation more comprehensive by considering tasks from the NLP domain, e.g., language modeling. This would further help asses the utility of the proposed method.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575644453763, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Reviewers"], "noninvitees": [], "tcdate": 1570237732815, "tmdate": 1575644453777, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Review"}}}, {"id": "Bylt-T73oH", "original": null, "number": 7, "cdate": 1573825793477, "ddate": null, "tcdate": 1573825793477, "tmdate": 1573825830064, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment", "content": {"title": "Summary of Updates to Second Revision", "comment": "Our most recent submission includes the following notable changes:\n\n1. We have added a natural language processing experiment in response to Reviewer #1. This is a generative LSTM language model which involved additional algorithmic design decisions to deal with intermediate states at different time scales. The model is trained on the Penn Treebank corpus. This is compared to an SGD baseline with constant learning rate.\n\n2. We have added a sparse matrix factorization experiment in response to Reviewer #2. This is a Bayesian Personalized Ranking model train on the 100k MovieLens dataset. We implement a learning rate schedule for our algorithm, which we consider a fair analogue to an exponential decay learning rate schedule for SGD. The SGD baseline for this experiment uses an exponential decay learning rate schedule.\n\n3. We have updated our related work, and added a subsection which more clearly defines our contributions.\n\nWe were planning to include an experiment to demonstrate an implementation of sparse back propagation for wall-clock time comparison in response to Reviewer #3, but we ran out of time.\n\nWe'd like to thank the reviewers, area chairs, and anyone else involved in the decision making process for their time and consideration."}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx1DkSYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1751/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1751/Authors|ICLR.cc/2020/Conference/Paper1751/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151416, "tmdate": 1576860535085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment"}}}, {"id": "ByeDcCHjiS", "original": null, "number": 6, "cdate": 1573768846701, "ddate": null, "tcdate": 1573768846701, "tmdate": 1573768846701, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "H1eEA5y_sr", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment", "content": {"title": "Response", "comment": "I do agree that this paper has some theoretical contributions related to sparsity for general nonconvex problems. \n\n- Please add experiments on general non-convex example to show the advantages of your proposed methods with others (not just only with SpiderBoost -- I believe that in term of numerical experiments, SpiderBoost is not the best one). \n- Please revise your related work properly. \n- Please highlight your contributions in the introduction part. \n \nI will update my score accordingly. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx1DkSYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1751/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1751/Authors|ICLR.cc/2020/Conference/Paper1751/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151416, "tmdate": 1576860535085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment"}}}, {"id": "Hyexcsydor", "original": null, "number": 4, "cdate": 1573546888070, "ddate": null, "tcdate": 1573546888070, "tmdate": 1573546888070, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "S1gBwSJRtB", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment", "content": {"title": "Reply to Reviewer #3", "comment": "Thank you for your thoughtful feedback. Please find below our responses to your comments.\n\n- queries/N and wallclock time: We are currently looking into adding additional experiments to address this question.\n\n- Confusion about rtop operator: We apologize for the typos here and thank you for providing a more intuitive explanation. We updated the paper to address confusion about the definitions of $k$, $k_1$, and $k_2$. We also updated definitions and examples for corrections and enhanced clarity. We added a more intuitive explanation of what the rtop operator does before formally introducing the operator.\n\n- Linearity of rtop Operator: By linearity we mean that rtop(x, y+y') = rtop(x, y) + rtop(x, y') for a fixed S and hence E[rtop(x, y+y')] = E[rtop(x, y)]+E[rtop(x,y')]. We added a sentence clarifying this.\n\n- Theoretical Motivation of Hyperparameters: Based on Theorems 1 and 2, Our experiments are carried out by specifying the fraction $k/d$ of gradient coordinates we want used for variance reduction. We then set set $k_1 = k // 2$, where $//$ is integer division, and $k_2 = k - k_1$. In our experiments, we set $k/d = 0.1$. We set $B = c b$ for a small constant $c$ (in our experiments we use $c=10$). We then set $m = B/b$.\n\n- Impact of non-uniform minibatch sampling: Non-uniform minibatch sampling has no negative effect on sparsity because it only changes the probability that each entry is sampled and not the sparsity pattern."}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx1DkSYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1751/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1751/Authors|ICLR.cc/2020/Conference/Paper1751/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151416, "tmdate": 1576860535085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment"}}}, {"id": "H1eEA5y_sr", "original": null, "number": 3, "cdate": 1573546699994, "ddate": null, "tcdate": 1573546699994, "tmdate": 1573546699994, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "rJx0y3HZYS", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment", "content": {"title": "Reply to Review #2", "comment": "Thank you for your feedback. Indeed, some implementations of SVRG-style variance reduction methods are shown to be ineffective for training deep neural networks (Defazio et al. 2018). We have reproduced these results and acknowledge these findings in our paper with the appropriate citations. The covariance between the gradient snapshot computed every $m$ iterations and the gradient computed in the inner loop decreases rapidly for very large deep learning models. Our algorithm reduces the cost of computing variance reduction terms, which reduces the number of inner loop iterations required to see the benefits of the variance reducing computations. Furthermore, by using sparsity, our algorithm naturally reduces the overall potential variance of these methods. This potential variance occurs when the aforementioned covariance approaches zero. Our algorithm is only beneficial when there is sparsity structure in the gradients of the objective, and we experimentally show that such sparsity structure exists in the gradients of some deep neural networks.\n\nWe view this work as taking a step toward making SVRG/SCSG-style variance reduction methods practical. These methods provide a unique way to estimate gradient sparsity by taking advantage of the full/large-batch gradient computed at the beginning of each outer-loop. In other words, the correction term in SVRG/SCSG-style algorithms can be used for two purposes at the same time without extra overhead: Variance reduction and estimation of gradient sparsity. By contrast, this cannot be done with SGD-style algorithms without introducing significant computational overhead.\n\nWhile this work can be viewed as addressing some of the observations made by Defazio et al., we plan to directly address them in future work. Lei et al. (2017) comment that the mechanism used by SVRG/SCSG-style variance reduction methods to accelerate gradient-based methods is qualitatively different than momentum (in SGD-momentum/Adam) and adaptive stepsizes (in AdaGrad/Adam). We believe that, if properly combined with existing state-of-the-art techniques, SVRG/SCSG-style variance reduction methods have the potential to further reduce stochastic variance.\n\n- Experiments: To concretely address your feedback about our experiments, we will add at least one additional non-convex problem which exhibits gradient sparsity.\n\n- Figures in Log-scale: Thank you for the suggestion. We updated our figures to log-scale. \n\n- Connection With SARAH: Thank you for pointing out our missing references. We added them into our references and discussed them in Section 2. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx1DkSYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1751/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1751/Authors|ICLR.cc/2020/Conference/Paper1751/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151416, "tmdate": 1576860535085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment"}}}, {"id": "B1ldttyujr", "original": null, "number": 2, "cdate": 1573546367683, "ddate": null, "tcdate": 1573546367683, "tmdate": 1573546367683, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "S1g8clOTFH", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment", "content": {"title": "Reply to Review #1", "comment": "Thank you for your thoughtful feedback. Please find below our responses to your comments.\n\n- Typos in Section 2.1: We apologized for the typos. In the new draft, we updated definitions and examples for corrections and enhanced clarity.\n\n- Section 2.2 Definition of Activation Functions: This is now simplified to use only $\\phi$ instead of introducing $g$.\n\n- A1: what is $\\overline{x}_0$ in $\\Delta f$: $\\tilde{x}_{0}$ should be $x_{0}$. We correct it in the updated draft.\n\n- We are currently looking into adding additional experiments to address your suggestion to broaden the variety of experiments."}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx1DkSYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1751/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1751/Authors|ICLR.cc/2020/Conference/Paper1751/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151416, "tmdate": 1576860535085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment"}}}, {"id": "S1xl3O1uiB", "original": null, "number": 1, "cdate": 1573546151916, "ddate": null, "tcdate": 1573546151916, "tmdate": 1573546151916, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment", "content": {"title": "Thank You", "comment": "We thank all reviewers for their thoughtful comments. We also thank them for pointing out typos and potential issues on clarity. We will revise our paper twice before discussions end. The first revision, which we have already submitted, addresses all the issues noted in our response to each reviewer. Our second and final revision adds experiments based on the suggestions of each reviewer."}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx1DkSYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1751/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1751/Authors|ICLR.cc/2020/Conference/Paper1751/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151416, "tmdate": 1576860535085, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Authors", "ICLR.cc/2020/Conference/Paper1751/Reviewers", "ICLR.cc/2020/Conference/Paper1751/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Comment"}}}, {"id": "rJx0y3HZYS", "original": null, "number": 1, "cdate": 1571015653528, "ddate": null, "tcdate": 1571015653528, "tmdate": 1572972428245, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: \nThe author(s) provide a method which combines some property of SCGS method and SpiderBoost. Theoretical results are provided and achieve the state-of-the-art complexity, which match the one of SpiderBoost. Numerical experiments show some advantage compared to SpiderBoost on some deep neural network architecture for some standard datasets MNIST, SVHN, and CIFAR-10. \n\nComments: \n\n1) It is true that variance reduction methods achieve the state-of-the-art complexity theory for finding first order stationary point of general nonconvex optimization problems. However, it is well-known that variance reduction methods are not very efficient for training deep neural networks. All of the experiments in this paper are focusing on deep learning problems. If the author(s) would like to show good performance, I would suggest to compare the algorithms with the state-of-the-art algorithms in Deep Learning such as Adam, SGD-Momentum. Showing some improvement over SpiderBoost for deep learning problems would have low impact. \n\n2) I would suggest the author(s) to switch directions to focus on general nonconvex problems, that is, to find some different examples on general non-convex optimization problems rather than for deep learning problems. In other words, to find examples which show that your algorithm has more advantage than SGD-type algorithms, SVRG-type. \n\n3) I would also suggest the author(s) to plot all figures in log-scale in order to see in more detail performance. \n\n4) According to my knowledge, SpiderBoost is an alternative way of re-writing the SARAH algorithm [1, 2] with some small modification, that is a variant of SARAH. Therefore, the SARAH algorithm should be highly related to this paper and need to be discussed and mentioned more clearly. \n\n[1] Nguyen et al 2017a, \u201cSARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient\u201d.\n[2] Nguyen et al 2017b, \u201cStochastic Recursive Gradient Algorithm for Nonconvex Optimization\u201d. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575644453763, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Reviewers"], "noninvitees": [], "tcdate": 1570237732815, "tmdate": 1575644453777, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Review"}}}, {"id": "S1gBwSJRtB", "original": null, "number": 3, "cdate": 1571841372842, "ddate": null, "tcdate": 1571841372842, "tmdate": 1572972428166, "tddate": null, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "invitation": "ICLR.cc/2020/Conference/Paper1751/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper introduces a sparse variant to SpiderBoost which reduces the complexity cost of updating gradient estimates by way of sparse updates. The authors prove that this variant incurs a negligible increase in worst case complexities as soon as certain assumptions are satisfied, and that when their algorithm captures sparsity correctly, they improve upon SpiderBoost's complexity.\n\nThis paper is clearly, and the experiments support the theoretical contributions. \n\nIn Figure 1, you report results as a function of gradient queries/N. Given Theorem 2, I assume that the graphs would look similar as a function of wall-clock time; can you confirm this?\n\nRecommendation: Accept. \n\nMinor comments and questions for the author:\n- I am slightly confused by the introduction of the rtop operator. Specifically, \n  1) What is the relation between k1, k2, and k?\n  2) You write that S is a random subset of size k. Should this be k2?\n  3) In your first example, should we have rtop(x,y) = (0, 16, 0, 0, 1), since for \\ell = 2, y_\\ell = 4, d-k1 = 4, k2=1? Am I missing something?\n  More generally, my understanding is that the rtop(x,y) operator randomly sparsifies y based on x, which essentially provides indication of where sparsity would be least harmful; when not sparsifying, rtop applies a rescaling that guarantees unbiased estimates. If this is correct, I would recommend making that intuition more clear early on in the paper, in order to improve upon the clarity of the paper.\n\n- You state that rtop is linear in y; since rtop depends on the random variable S, is the claim that E[rtop(x, y+y')] = E[rtop(x, y)]+E[rtop(x,y')] (which follows from unbiasedness)?\n\n- For your experiments, could you discuss how your choice of hyperparameters relates to the constraints in Theorem 1 and 2? \n\n- I believe Table 1 would be more impactful if it also included the initial entropy ratios at the beginning of training, rather than reporting those values below.\n\n- Other variance reduction techniques for minibatching focus on choosing the minibatches themselves with non-uniform sampling. Under such a sampling mechanism, do you foresee any complications to using SpiderBoost with Sparse Gradients, eg., decrease in overall sparsity?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1751/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1751/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction With Sparse Gradients", "authors": ["Melih Elibol", "Lihua Lei", "Michael I. Jordan"], "authorids": ["elibol@cs.berkeley.edu", "lihualei@stanford.edu", "jordan@cs.berkeley.edu"], "keywords": ["optimization", "variance reduction", "machine learning", "deep neural networks"], "TL;DR": "We use sparsity to improve the computational complexity of variance reduction methods.", "abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "pdf": "/pdf/1a035539b658876ec2f0ce5aae15a1c8f99c2d07.pdf", "code": "http://s000.tinyupload.com/index.php?file_id=39477384063585848544", "paperhash": "elibol|variance_reduction_with_sparse_gradients", "_bibtex": "@inproceedings{\nElibol2020Variance,\ntitle={Variance Reduction With Sparse Gradients},\nauthor={Melih Elibol and Lihua Lei and Michael I. Jordan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx1DkSYwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f754c4de82f0ba8c1491413a5443c63f996ec7ba.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx1DkSYwB", "replyto": "Syx1DkSYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1751/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575644453763, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1751/Reviewers"], "noninvitees": [], "tcdate": 1570237732815, "tmdate": 1575644453777, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1751/-/Official_Review"}}}], "count": 11}