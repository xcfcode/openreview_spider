{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028579129, "tcdate": 1490028579129, "number": 1, "id": "r1iQuFaie", "invitation": "ICLR.cc/2017/workshop/-/paper68/acceptance", "forum": "By1eEXVFg", "replyto": "By1eEXVFg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present two classes of attacks on the VAE-GAN architecture and demonstrate them against networks trained on MNIST, SVHN, and CelebA. Our first attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our second attack moves beyond relying on the standard loss for computing the gradient and directly optimizes against differences in source and target latent representations. We additionally present an interesting visualization, which gives insight into how adversarial examples appear in generative models.", "pdf": "/pdf/e3016daecab4db609e96fa0bc4b44e70d96f9600.pdf", "TL;DR": "Exploration of adversarial examples against latent space generative models on multiple datasets.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Deep learning"], "conflicts": ["nus.edu.sg", "google.com", "cs.berkeley.edu"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028579755, "id": "ICLR.cc/2017/workshop/-/paper68/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "By1eEXVFg", "replyto": "By1eEXVFg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028579755}}}, {"tddate": null, "tmdate": 1489193713559, "tcdate": 1489193713559, "number": 2, "id": "Hktxjaxix", "invitation": "ICLR.cc/2017/workshop/-/paper68/official/review", "forum": "By1eEXVFg", "replyto": "By1eEXVFg", "signatures": ["ICLR.cc/2017/workshop/paper68/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper68/AnonReviewer2"], "content": {"title": "Interesting exploration", "rating": "6: Marginally above acceptance threshold", "review": "The work explores whether one can perturb the input of a VAE (or VAE-GAN) imperceptibly in such a way that the reconstruction resembles a target sample. The paper shows two ways to do so.\nIt is not particularly surprising that an inference network in a VAE is susceptible to adversarial examples (in fact the opposite would be more interesting). It is after all a neural network with only a bit of noise added to it, and the susceptibility of such networks to adversarial attacks was established before. But it is nevertheless a finding worth reporting.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present two classes of attacks on the VAE-GAN architecture and demonstrate them against networks trained on MNIST, SVHN, and CelebA. Our first attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our second attack moves beyond relying on the standard loss for computing the gradient and directly optimizes against differences in source and target latent representations. We additionally present an interesting visualization, which gives insight into how adversarial examples appear in generative models.", "pdf": "/pdf/e3016daecab4db609e96fa0bc4b44e70d96f9600.pdf", "TL;DR": "Exploration of adversarial examples against latent space generative models on multiple datasets.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Deep learning"], "conflicts": ["nus.edu.sg", "google.com", "cs.berkeley.edu"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489193714415, "id": "ICLR.cc/2017/workshop/-/paper68/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper68/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper68/AnonReviewer1", "ICLR.cc/2017/workshop/paper68/AnonReviewer2"], "reply": {"forum": "By1eEXVFg", "replyto": "By1eEXVFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper68/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper68/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489193714415}}}, {"tddate": null, "tmdate": 1488996643925, "tcdate": 1488996643925, "number": 1, "id": "ryhXtpp9e", "invitation": "ICLR.cc/2017/workshop/-/paper68/public/comment", "forum": "By1eEXVFg", "replyto": "BJn3uHP9g", "signatures": ["~Jernej_Kos1"], "readers": ["everyone"], "writers": ["~Jernej_Kos1"], "content": {"title": "Updates", "comment": "Thank you for your comments!\n\nWe agree that the previous description of the attack was too abstract. We have now fixed it to include a specific example involving exchanging a compressed image between two parties. The new paragraph is as follows:\n\n===\nSpecifically, we consider an attack where the latent representation is used as a form of compression when transmitting an image between two parties. The attacker\u2019s goal is to convince the sender to transmit an image of the attacker\u2019s choosing to the receiver, but the attacker has no direct control over the bytes sent between the two parties. The sender believes that the receiver will reconstruct the same image that he sees, but if the attack is successful, the receiver will in fact reconstruct an image chosen by the attacker. \n===\n\nBased on your comments, we have now also improved both Figures 1 and 2 to show everything in a single place. The figures now show the original image, the adversarial examples for both methods and reconstructions of original images and adversarial examples."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present two classes of attacks on the VAE-GAN architecture and demonstrate them against networks trained on MNIST, SVHN, and CelebA. Our first attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our second attack moves beyond relying on the standard loss for computing the gradient and directly optimizes against differences in source and target latent representations. We additionally present an interesting visualization, which gives insight into how adversarial examples appear in generative models.", "pdf": "/pdf/e3016daecab4db609e96fa0bc4b44e70d96f9600.pdf", "TL;DR": "Exploration of adversarial examples against latent space generative models on multiple datasets.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Deep learning"], "conflicts": ["nus.edu.sg", "google.com", "cs.berkeley.edu"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487315943439, "tcdate": 1487315943439, "id": "ICLR.cc/2017/workshop/-/paper68/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper68/reviewers"], "reply": {"forum": "By1eEXVFg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487315943439}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1488996438029, "tcdate": 1487315942723, "number": 68, "id": "By1eEXVFg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "By1eEXVFg", "original": "SJk01vogl", "signatures": ["~Jernej_Kos1"], "readers": ["everyone"], "content": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present two classes of attacks on the VAE-GAN architecture and demonstrate them against networks trained on MNIST, SVHN, and CelebA. Our first attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our second attack moves beyond relying on the standard loss for computing the gradient and directly optimizes against differences in source and target latent representations. We additionally present an interesting visualization, which gives insight into how adversarial examples appear in generative models.", "pdf": "/pdf/e3016daecab4db609e96fa0bc4b44e70d96f9600.pdf", "TL;DR": "Exploration of adversarial examples against latent space generative models on multiple datasets.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Deep learning"], "conflicts": ["nus.edu.sg", "google.com", "cs.berkeley.edu"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1484385866165, "tcdate": 1478352838680, "number": 556, "id": "SJk01vogl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJk01vogl", "signatures": ["~Jernej_Kos1"], "readers": ["everyone"], "content": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1488570548327, "tcdate": 1488570548327, "number": 1, "id": "BJn3uHP9g", "invitation": "ICLR.cc/2017/workshop/-/paper68/official/review", "forum": "By1eEXVFg", "replyto": "By1eEXVFg", "signatures": ["ICLR.cc/2017/workshop/paper68/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper68/AnonReviewer1"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "CONTRIBUTIONS\n\nThis paper builds upon recent work on adversarial examples -- which shows that one can maliciously craft an input to a classifier using various approaches and have the classifier recognize this input as any desired class -- and extends it to the realm of deep generative models.\n\nIt claims two main contributions:\n- It shows the existence of two attack vectors on encode/decode-type generative models and proposes a method to perform each of them.\n- It provides a new visualization tool which gives \"[...] insight into how adversarial examples appear in generative models.\"\n\nNOVELTY, CLARITY, SIGNIFICANCE, QUALITY\n\nThe idea of the existence of adversarial examples in generative models is new to me, and I think it is very relevant to the field.\n\nI find the definition of adversarial examples for generative models proposed in the introduction confusing.\n\nThe sentence \"Specifically, if the person doing the encoding step is separated from the person doing the decoding step, the attacker may be able to cause the encoding party to believe they have encoded a particular message for the decoding party, but in reality they have encoded a different message of the attacker\u2019s choice.\" is vague and does not give a concrete picture of what attack scenario the authors have in mind.\n\nSimilarly, the sentence \"Our results show that these attack methods are effective and VAE and VAE-GAN can be easily fooled.\" does not explain what exactly VAEs and VAE-GANs would be fooled into doing.\n\nFortunately, the \"Methods\" section offers clarification: in the L_VAE case we are looking for a perturbation of a source input such that its reconstruction matches that of a target input, whereas in the latent attack case we are looking for a perturbation of a source input such that its latent representation is close to that of a target input. In both cases we want to keep the adversarial input as close to the source input as possible.\n\nI find Figures 1 and 2 difficult to parse. It appears to me that only the reconstructions are shown; in the absence of the original images and their respective perturbations I can't really assess how close to the original the perturbations are and therefore how successful the attack is. Looking at Figures 5 and 6 in the appendix makes me think that the perturbations are indeed pretty close to the original, but I think having the original, its reconstruction, the perturbation and its reconstruction side by side would go a long way towards making the results look more convincing.\n\nOverall I feel like this work shows great potential, but the lack of clarity gets in the way of the results. I would be inclined to recommend acceptance if the introduction and figure presentation were reworked to improve clarity.\n\nPROS (+), CONS (-)\n\n+ Subject is relevant and novel\n+ Some of the results, especially in the appendix, show good potential\n- Confusing introduction\n- Figures are not very well explained and contextualized", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present two classes of attacks on the VAE-GAN architecture and demonstrate them against networks trained on MNIST, SVHN, and CelebA. Our first attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our second attack moves beyond relying on the standard loss for computing the gradient and directly optimizes against differences in source and target latent representations. We additionally present an interesting visualization, which gives insight into how adversarial examples appear in generative models.", "pdf": "/pdf/e3016daecab4db609e96fa0bc4b44e70d96f9600.pdf", "TL;DR": "Exploration of adversarial examples against latent space generative models on multiple datasets.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Deep learning"], "conflicts": ["nus.edu.sg", "google.com", "cs.berkeley.edu"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489193714415, "id": "ICLR.cc/2017/workshop/-/paper68/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper68/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper68/AnonReviewer1", "ICLR.cc/2017/workshop/paper68/AnonReviewer2"], "reply": {"forum": "By1eEXVFg", "replyto": "By1eEXVFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper68/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper68/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489193714415}}}], "count": 5}