{"notes": [{"id": "B1gKiN7luV", "original": "Ske005LFvN", "number": 27, "cdate": 1553114272549, "ddate": null, "tcdate": 1553114272549, "tmdate": 1571428743912, "tddate": null, "forum": "B1gKiN7luV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Split Batch Normalization: Improving Semi-Supervised Learning under Domain Shift", "authors": ["Micha\u0142 Zaj\u0105c", "Konrad Zolna", "Stanis\u0142aw Jastrz\u0119bski"], "authorids": ["emzajac@gmail.com", "konrad.zolna@gmail.com", "staszek.jastrzebski@gmail.com"], "keywords": ["semi-supervised learning", "domain shift", "image classification", "deep neural networks"], "abstract": "Recent work has shown that using unlabeled data in semi-supervised learning is not always beneficial and can even hurt generalization, especially when there is a class mismatch between the unlabeled and labeled examples. We investigate this phenomenon for image classification and many other forms of domain shifts (e.g. salt-and-pepper noise). Our main contribution is showing how to benefit from additional unlabeled data that comes from a shifted distribution in batch-normalized neural networks. We achieve it by simply using separate batch normalization statistics for unlabeled examples. Due to its simplicity, we recommend it as a standard practice.", "pdf": "/pdf/f8a6fa49e28560488119e520f06cc97c2d7ed0f9.pdf", "paperhash": "zajc|split_batch_normalization_improving_semisupervised_learning_under_domain_shift"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "r1xyFC0tYE", "original": null, "number": 1, "cdate": 1554800246551, "ddate": null, "tcdate": 1554800246551, "tmdate": 1555511880783, "tddate": null, "forum": "B1gKiN7luV", "replyto": "B1gKiN7luV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper27/Official_Review", "content": {"title": "An worthwhile observation that has its place in the workshop", "review": "This work proposes an approach to tackle the domain adaptation problem in semi-supervised learning, based on a decoupling of the computation of the batch statistics in the batch normalization layers.\n\nIn a setting where the unlabeled data does not follow the supervised data distribution, semi-supervised learning techniques can lead to a degradation of performance with respect to a purely supervised setting. In this work, it is shown that computing the batch normalization statistics separately for the unsupervised and for the supervised data can alleviate the domain shift and lead to improved semi-supervision.\n\nI have a few questions and remarks:\n\na) The introduction mentions the problem of the domain shift for the unlabeled data. I would add that it is unclear how one could benefit from unlabeled samples in the general case if those samples are completely out-of-domain: after all, the core idea of semi-supervised learning is to grasp a better prior on the data domain. I can see that the network can still learn information e.g. when the inputs share the same modality (RGB data) or has an overlap of the classes. Overall, I would make this clearer in the introduction what one expects from semi-supervised learning in an out-of-domain setting. One thing is that semi-supervision should not degrade performance w.r.t. a purely supervised setting, which can happen with current semi-supervised algorithms.\n\nb) I would also experiment with random noise unrelated to the supervised data distribution to see the limits of the approach, and study a case of extreme domain mismatch. In such setting, one would hope to match the purely supervised baseline performance. I expect a batch-norm adaptation to be insufficient for this.\n\nc) I assume that at test-time, the batch norm statistics computed on the supervised set are used; I would make this clear in the document.\n\nI think that adapting batch norm is sufficient in the experiments done but probably not a universal remedy to domain shift in semi-supervised learning, which could be shown with extreme distribution. In general, extra experiments could also show a more progressive evaluation of different shifts, between same-domain unsupervised data, and fully out-of-domain unsupervised data.\n\nI think however that this idea raises some valid points and introduces an easy fix that can be enough in some cases; moreover split-BN can stimulate new ideas related to domain shift and out-of-domain unsupervised learning. Therefore I believe this paper has its place in the workshop.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Split Batch Normalization: Improving Semi-Supervised Learning under Domain Shift", "authors": ["Micha\u0142 Zaj\u0105c", "Konrad Zolna", "Stanis\u0142aw Jastrz\u0119bski"], "authorids": ["emzajac@gmail.com", "konrad.zolna@gmail.com", "staszek.jastrzebski@gmail.com"], "keywords": ["semi-supervised learning", "domain shift", "image classification", "deep neural networks"], "abstract": "Recent work has shown that using unlabeled data in semi-supervised learning is not always beneficial and can even hurt generalization, especially when there is a class mismatch between the unlabeled and labeled examples. We investigate this phenomenon for image classification and many other forms of domain shifts (e.g. salt-and-pepper noise). Our main contribution is showing how to benefit from additional unlabeled data that comes from a shifted distribution in batch-normalized neural networks. We achieve it by simply using separate batch normalization statistics for unlabeled examples. Due to its simplicity, we recommend it as a standard practice.", "pdf": "/pdf/f8a6fa49e28560488119e520f06cc97c2d7ed0f9.pdf", "paperhash": "zajc|split_batch_normalization_improving_semisupervised_learning_under_domain_shift"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper27/Official_Review", "cdate": 1553713417692, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "B1gKiN7luV", "replyto": "B1gKiN7luV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713417692, "tmdate": 1555511815429, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper27/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "S1e-YHZ2tE", "original": null, "number": 2, "cdate": 1554941305000, "ddate": null, "tcdate": 1554941305000, "tmdate": 1555511878010, "tddate": null, "forum": "B1gKiN7luV", "replyto": "B1gKiN7luV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper27/Official_Review", "content": {"title": "Simple idea which makes sense and works well in practice", "review": "Summary:\n\nThe authors argue that distribution shift can be detrimental when doing semi-supervised learning. As a simple fix, they propose to not share batchnorm statistics between labeled and unlabeled data. They show consistent improvement for the case where little unlabeled data contains examples for the classes of which labels are present.\n\nNovelty:\n\nThe idea to have separate batchnorm parameters seems natural and also seems to work for the problem described here. However, conditional batchnorm is a well-known technique in general so the overall novelty is limited.\n\nRating:\n\nThe overall empirical results are consistent and testing on common perturbations is very insightful. I think the paper is not outstanding, but a simple and valuable contribution to the workshop.", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Split Batch Normalization: Improving Semi-Supervised Learning under Domain Shift", "authors": ["Micha\u0142 Zaj\u0105c", "Konrad Zolna", "Stanis\u0142aw Jastrz\u0119bski"], "authorids": ["emzajac@gmail.com", "konrad.zolna@gmail.com", "staszek.jastrzebski@gmail.com"], "keywords": ["semi-supervised learning", "domain shift", "image classification", "deep neural networks"], "abstract": "Recent work has shown that using unlabeled data in semi-supervised learning is not always beneficial and can even hurt generalization, especially when there is a class mismatch between the unlabeled and labeled examples. We investigate this phenomenon for image classification and many other forms of domain shifts (e.g. salt-and-pepper noise). Our main contribution is showing how to benefit from additional unlabeled data that comes from a shifted distribution in batch-normalized neural networks. We achieve it by simply using separate batch normalization statistics for unlabeled examples. Due to its simplicity, we recommend it as a standard practice.", "pdf": "/pdf/f8a6fa49e28560488119e520f06cc97c2d7ed0f9.pdf", "paperhash": "zajc|split_batch_normalization_improving_semisupervised_learning_under_domain_shift"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper27/Official_Review", "cdate": 1553713417692, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "B1gKiN7luV", "replyto": "B1gKiN7luV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713417692, "tmdate": 1555511815429, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper27/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "ryxuvOt6YN", "original": null, "number": 3, "cdate": 1555040352133, "ddate": null, "tcdate": 1555040352133, "tmdate": 1555511877160, "tddate": null, "forum": "B1gKiN7luV", "replyto": "B1gKiN7luV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper27/Official_Review", "content": {"title": "Hypothesis clearly defined and tested", "review": "This paper improves performance in two modern semi-supervised learning (SSL) models which utilize batch-norm in cases where the models train on unlabeled data from a different distribution than labeled data.  Their simple technique consists of calculating separate statistics for the unlabeled and labeled data in batch-norm.  \n\nThe paper, which appears to be largely motivated by Section 4.4 of Oliver et al, flushes out the class mismatch problem presented in the aforementioned paper and also tests performance under domain shift. The choices for domain shift perturbations seem reasonable, if not totally realistic. \n\nAlthough the paper clearly demonstrates improved performance in models with batch-norm, I think that the discussion presented in Section 3.3  warrants additional investigation in future work. \n\nAll in all, the paper's hypothesis was clearly defined and tested with thorough experiments and explanation. In addition, the problem definition and proposed solution, though fairly narrow, fits neatly into the workshop format. \n\nEditing comment: \n- \u201cwhich we select 20 randomly classes as the supervised dataset.\u201d", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Split Batch Normalization: Improving Semi-Supervised Learning under Domain Shift", "authors": ["Micha\u0142 Zaj\u0105c", "Konrad Zolna", "Stanis\u0142aw Jastrz\u0119bski"], "authorids": ["emzajac@gmail.com", "konrad.zolna@gmail.com", "staszek.jastrzebski@gmail.com"], "keywords": ["semi-supervised learning", "domain shift", "image classification", "deep neural networks"], "abstract": "Recent work has shown that using unlabeled data in semi-supervised learning is not always beneficial and can even hurt generalization, especially when there is a class mismatch between the unlabeled and labeled examples. We investigate this phenomenon for image classification and many other forms of domain shifts (e.g. salt-and-pepper noise). Our main contribution is showing how to benefit from additional unlabeled data that comes from a shifted distribution in batch-normalized neural networks. We achieve it by simply using separate batch normalization statistics for unlabeled examples. Due to its simplicity, we recommend it as a standard practice.", "pdf": "/pdf/f8a6fa49e28560488119e520f06cc97c2d7ed0f9.pdf", "paperhash": "zajc|split_batch_normalization_improving_semisupervised_learning_under_domain_shift"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper27/Official_Review", "cdate": 1553713417692, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "B1gKiN7luV", "replyto": "B1gKiN7luV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper27/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713417692, "tmdate": 1555511815429, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper27/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "HkxRbBv0YV", "original": null, "number": 1, "cdate": 1555096838398, "ddate": null, "tcdate": 1555096838398, "tmdate": 1555510982537, "tddate": null, "forum": "B1gKiN7luV", "replyto": "B1gKiN7luV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper27/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Split Batch Normalization: Improving Semi-Supervised Learning under Domain Shift", "authors": ["Micha\u0142 Zaj\u0105c", "Konrad Zolna", "Stanis\u0142aw Jastrz\u0119bski"], "authorids": ["emzajac@gmail.com", "konrad.zolna@gmail.com", "staszek.jastrzebski@gmail.com"], "keywords": ["semi-supervised learning", "domain shift", "image classification", "deep neural networks"], "abstract": "Recent work has shown that using unlabeled data in semi-supervised learning is not always beneficial and can even hurt generalization, especially when there is a class mismatch between the unlabeled and labeled examples. We investigate this phenomenon for image classification and many other forms of domain shifts (e.g. salt-and-pepper noise). Our main contribution is showing how to benefit from additional unlabeled data that comes from a shifted distribution in batch-normalized neural networks. We achieve it by simply using separate batch normalization statistics for unlabeled examples. Due to its simplicity, we recommend it as a standard practice.", "pdf": "/pdf/f8a6fa49e28560488119e520f06cc97c2d7ed0f9.pdf", "paperhash": "zajc|split_batch_normalization_improving_semisupervised_learning_under_domain_shift"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper27/Decision", "cdate": 1554736066635, "reply": {"forum": "B1gKiN7luV", "replyto": "B1gKiN7luV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736066635, "tmdate": 1555510972037, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 5}