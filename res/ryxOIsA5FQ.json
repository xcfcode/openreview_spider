{"notes": [{"id": "ryxOIsA5FQ", "original": "H1xErXE5Ym", "number": 186, "cdate": 1538087759552, "ddate": null, "tcdate": 1538087759552, "tmdate": 1545355419262, "tddate": null, "forum": "ryxOIsA5FQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Stacking for Transfer Learning", "abstract": "In machine learning tasks, overtting frequently crops up when the number of samples of target domain is insuf\ufb01cient, for the generalization ability of the classi\ufb01er is poor in this circumstance. To solve this problem, transfer learning utilizes the knowledge of similar domains to improve the robustness of the learner. The main idea of existing transfer learning algorithms is to reduce the dierence between domains by sample selection or domain adaptation. However, no matter what transfer learning algorithm we use, the difference always exists and the hybrid training of source and target data leads to reducing \ufb01tting capability of the learner on target domain. Moreover, when the relatedness between domains is too low, negative transfer is more likely to occur. To tackle the problem, we proposed a two-phase transfer learning architecture based on ensemble learning, which uses the existing transfer learning algorithms to train the weak learners in the \ufb01rst stage, and uses the predictions of target data to train the \ufb01nal learner in the second stage. Under this architecture, the \ufb01tting capability and generalization capability can be guaranteed at the same time. We evaluated the proposed method on public datasets, which demonstrates the effectiveness and robustness of our proposed method.", "keywords": ["data diversi\ufb01cation", "domain adaptation", "transfer learning", "stacked generalization"], "authorids": ["pyk3350266@163.com"], "authors": ["Peng Yuankai"], "TL;DR": "How to use stacked generalization to improve the performance of existing transfer learning algorithms when limited labeled data is available.", "pdf": "/pdf/6ebec4ecc69570439f20ee3e3a16433e0efd3721.pdf", "paperhash": "yuankai|stacking_for_transfer_learning", "_bibtex": "@misc{\nyuankai2019stacking,\ntitle={Stacking for Transfer Learning},\nauthor={Peng Yuankai},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxOIsA5FQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SyxVaYC4JE", "original": null, "number": 1, "cdate": 1543985596046, "ddate": null, "tcdate": 1543985596046, "tmdate": 1545354495899, "tddate": null, "forum": "ryxOIsA5FQ", "replyto": "ryxOIsA5FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper186/Meta_Review", "content": {"metareview": "This work proposes a method for both instance and feature based transfer learning. \nThe reviewers agree that the approach in current form lacks sufficient technical novelty for publication. The paper would benefit from experiments on larger datasets and with more analysis into the different aspects of the proposed model. \n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Insufficient Novelty"}, "signatures": ["ICLR.cc/2019/Conference/Paper186/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper186/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacking for Transfer Learning", "abstract": "In machine learning tasks, overtting frequently crops up when the number of samples of target domain is insuf\ufb01cient, for the generalization ability of the classi\ufb01er is poor in this circumstance. To solve this problem, transfer learning utilizes the knowledge of similar domains to improve the robustness of the learner. The main idea of existing transfer learning algorithms is to reduce the dierence between domains by sample selection or domain adaptation. However, no matter what transfer learning algorithm we use, the difference always exists and the hybrid training of source and target data leads to reducing \ufb01tting capability of the learner on target domain. Moreover, when the relatedness between domains is too low, negative transfer is more likely to occur. To tackle the problem, we proposed a two-phase transfer learning architecture based on ensemble learning, which uses the existing transfer learning algorithms to train the weak learners in the \ufb01rst stage, and uses the predictions of target data to train the \ufb01nal learner in the second stage. Under this architecture, the \ufb01tting capability and generalization capability can be guaranteed at the same time. We evaluated the proposed method on public datasets, which demonstrates the effectiveness and robustness of our proposed method.", "keywords": ["data diversi\ufb01cation", "domain adaptation", "transfer learning", "stacked generalization"], "authorids": ["pyk3350266@163.com"], "authors": ["Peng Yuankai"], "TL;DR": "How to use stacked generalization to improve the performance of existing transfer learning algorithms when limited labeled data is available.", "pdf": "/pdf/6ebec4ecc69570439f20ee3e3a16433e0efd3721.pdf", "paperhash": "yuankai|stacking_for_transfer_learning", "_bibtex": "@misc{\nyuankai2019stacking,\ntitle={Stacking for Transfer Learning},\nauthor={Peng Yuankai},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxOIsA5FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper186/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353305045, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxOIsA5FQ", "replyto": "ryxOIsA5FQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper186/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper186/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper186/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353305045}}}, {"id": "BJx9irP76X", "original": null, "number": 3, "cdate": 1541793185777, "ddate": null, "tcdate": 1541793185777, "tmdate": 1541793185777, "tddate": null, "forum": "ryxOIsA5FQ", "replyto": "ryxOIsA5FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper186/Official_Review", "content": {"title": "Recommend Reject as the contribution is incremental ", "review": "This paper proposed to solve the instance-based transfer learning and feature-based transfer learning by stacking with a two-phase training strategy. The source data and target data are hybrid together first to train weak learners, and then the ensembled super learner is utilized to get the final prediction. Details for the stacking process are provided. Experimental results on MNIST-USPS, COIL, and Office-Caltech datasets show the proposed method can boost the performance, compared to TrAdaboost. \n\nPros:\nThe paper proposes to using stacking or ensembling to solve the domain adaptation problem, which shows some insight for further domain adaptation research.\n\nCons:\n1. One of the main issues of this paper is the lack of novelty. The framework is incremented from the previous domain adaptation method such as TrAdaboost or BDA. For feature-based transfer learning, Equation (7)(8)(9) directly from the previous method. \n2. Some arguments in this paper are not solid. For example, in the abstract,   the authors claim that under the two-stage training architecture, the fitting capability and generalization capability can be guaranteed at the same time. However, this is not well-justified in the following literature. Another example is \"the settings of \\lamda and N should be taken into consideration, if \\lambda is too large, the performance of each learner can't be guaranteed, if \\lambda is too small, training data can't be diversified enough\" (page 7line 9~11)\n3. This paper is weakened by the experimental part. Firstly, only TraDaboost method is used as a baseline. The paper can be largely improved by comparing with the state-of-the-art ensembling method for domain adaptation, for example:\nSelf-ensembling for visual domain adaptation, Geoff French,  ICLR 2018.\nSecondly, the datasets used in this paper is small-scale and biased. It would be exciting to see how the proposed method will perform on the state-of-the-art large-scale domain adaptation dataset, for example, Office-Home dataset, Syn2Real dataset. \n\n Others:\n1. Some terminologies used in this paper are confusing: (1) the h_t and c are not defined in Equation (2). in Algorithm 2, how to construct kernel matrix K_t using k_t?   \n2. The written of this paper can be largely improved. Some sentences are grammarly mistaken. Typos examples: \nAbstract line 1: overtting -> overfitting\nSection 2.1, we use TraAdaboost -> We use TrAdaboost\n3. The citation style used in this paper is not correct.\n\nProblems:\n1. In section 2.2, what's the difference between the kernel matrix K with the unbiased estimate of MK-MMD (proposed by Gretton, NIPS 2012, also used in Deep adaptation network, Long, et al. ICML2015)?", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper186/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacking for Transfer Learning", "abstract": "In machine learning tasks, overtting frequently crops up when the number of samples of target domain is insuf\ufb01cient, for the generalization ability of the classi\ufb01er is poor in this circumstance. To solve this problem, transfer learning utilizes the knowledge of similar domains to improve the robustness of the learner. The main idea of existing transfer learning algorithms is to reduce the dierence between domains by sample selection or domain adaptation. However, no matter what transfer learning algorithm we use, the difference always exists and the hybrid training of source and target data leads to reducing \ufb01tting capability of the learner on target domain. Moreover, when the relatedness between domains is too low, negative transfer is more likely to occur. To tackle the problem, we proposed a two-phase transfer learning architecture based on ensemble learning, which uses the existing transfer learning algorithms to train the weak learners in the \ufb01rst stage, and uses the predictions of target data to train the \ufb01nal learner in the second stage. Under this architecture, the \ufb01tting capability and generalization capability can be guaranteed at the same time. We evaluated the proposed method on public datasets, which demonstrates the effectiveness and robustness of our proposed method.", "keywords": ["data diversi\ufb01cation", "domain adaptation", "transfer learning", "stacked generalization"], "authorids": ["pyk3350266@163.com"], "authors": ["Peng Yuankai"], "TL;DR": "How to use stacked generalization to improve the performance of existing transfer learning algorithms when limited labeled data is available.", "pdf": "/pdf/6ebec4ecc69570439f20ee3e3a16433e0efd3721.pdf", "paperhash": "yuankai|stacking_for_transfer_learning", "_bibtex": "@misc{\nyuankai2019stacking,\ntitle={Stacking for Transfer Learning},\nauthor={Peng Yuankai},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxOIsA5FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper186/Official_Review", "cdate": 1542234519551, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxOIsA5FQ", "replyto": "ryxOIsA5FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper186/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335667478, "tmdate": 1552335667478, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper186/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkeGG_6OhX", "original": null, "number": 2, "cdate": 1541097481760, "ddate": null, "tcdate": 1541097481760, "tmdate": 1541534212640, "tddate": null, "forum": "ryxOIsA5FQ", "replyto": "ryxOIsA5FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper186/Official_Review", "content": {"title": "Recommend rejection of the paper due to the limited contributions and preliminary experiments.", "review": "The authors proposed a stacking method for both instance-based and feature-based transfer learning based on a two-phase strategy. It first introduced some self-defined parameters to diversify the data or the model, and then adopted some existing transfer learning to train the model.\n\nStrength:\n1) A stacking method for both instance-based and feature-based transfer learning.\n\nWeakness:\n1) Incremental contributions and limited novelty.\n2) Some claims are not well supported.\n3) Experimental results are preliminary.\n\nThe technical contribution of this work is limited. The main difference between the proposed instance-based stacking method and TrAdaboost are twofold: 1) using a self-defined threshold to select a subset of source samples for training; 2) using stacking instead of TrAdaboost to get the final output. The improvements upon TrAdaboost are marginal. Also, for the proposed feature-based stacking method, it just used the different kernel parameter values to diversify the model. The novelty is trivial. \n\nSeveral claims in the paper are not well discussed and/or evidence-supported, such as\uff1a\n1) \u201cWhen the similarity between domains is low, the final estimator can still achieve good performance on target training set.\u201d \n2) \u201cWhen source domain is not related enough, stacking performs better.\u201d\n3) \u201cWe reduce the risk of negative transfer in a simple and effective way without a similarity measure.\u201d\nMore discussions should be given, for example, how to measure the domain similarity and how to reduce the risk of negative transfer. Also, there are no experimental results to support the claims.\n\nFor the evaluation, it is inappropriate to choose Randomforest as the weak leaners since Randomforest is an ensemble method. It is better to give some explanation on how the data distribution and similarity between domains change with the kernel parameters in Figure 5.\n\nFor the algorithm comparison, only TrAdaboost is used as the baseline to compare with the proposed instance-based stacking method. The results could be more convincing if some recent ensemble-based transfer learning methods are included for comparison. For the evaluation of the proposed feature-based stacking method, the authors should at least compare their method with BDA since BDA is used as its base algorithm.\n\nSome symbols used in equations are not defined, such as h_t and c in Equation (2).\n\nThe paper needs a careful proofreading to correct the grammar errors and typos, such as:\n1) Line 1 of page 7: moreover -> Moreover?\n2) Line 1 of Abstract: overtting -> overfitting?\n\nIn summary, the paper has to make significant improvements before it can meet the bar of ICLR.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper186/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacking for Transfer Learning", "abstract": "In machine learning tasks, overtting frequently crops up when the number of samples of target domain is insuf\ufb01cient, for the generalization ability of the classi\ufb01er is poor in this circumstance. To solve this problem, transfer learning utilizes the knowledge of similar domains to improve the robustness of the learner. The main idea of existing transfer learning algorithms is to reduce the dierence between domains by sample selection or domain adaptation. However, no matter what transfer learning algorithm we use, the difference always exists and the hybrid training of source and target data leads to reducing \ufb01tting capability of the learner on target domain. Moreover, when the relatedness between domains is too low, negative transfer is more likely to occur. To tackle the problem, we proposed a two-phase transfer learning architecture based on ensemble learning, which uses the existing transfer learning algorithms to train the weak learners in the \ufb01rst stage, and uses the predictions of target data to train the \ufb01nal learner in the second stage. Under this architecture, the \ufb01tting capability and generalization capability can be guaranteed at the same time. We evaluated the proposed method on public datasets, which demonstrates the effectiveness and robustness of our proposed method.", "keywords": ["data diversi\ufb01cation", "domain adaptation", "transfer learning", "stacked generalization"], "authorids": ["pyk3350266@163.com"], "authors": ["Peng Yuankai"], "TL;DR": "How to use stacked generalization to improve the performance of existing transfer learning algorithms when limited labeled data is available.", "pdf": "/pdf/6ebec4ecc69570439f20ee3e3a16433e0efd3721.pdf", "paperhash": "yuankai|stacking_for_transfer_learning", "_bibtex": "@misc{\nyuankai2019stacking,\ntitle={Stacking for Transfer Learning},\nauthor={Peng Yuankai},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxOIsA5FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper186/Official_Review", "cdate": 1542234519551, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxOIsA5FQ", "replyto": "ryxOIsA5FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper186/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335667478, "tmdate": 1552335667478, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper186/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkeeJ9ku3Q", "original": null, "number": 1, "cdate": 1541040600172, "ddate": null, "tcdate": 1541040600172, "tmdate": 1541534212434, "tddate": null, "forum": "ryxOIsA5FQ", "replyto": "ryxOIsA5FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper186/Official_Review", "content": {"title": "No technical contributions, a lot of typos and grammar errors", "review": "In this paper, the authors proposed to learn a stacked classifier on top of the outputs of well-known transfer learning models for transfer learning. The authors claimed that their proposed solution can avoid negative transfer.\n\nTechnically, there are no contributions. The proposed solution is a straight-forward A+B, where both A and B are well-known. Specifically, in the proposed solution, different well-known transfer learning models are used as the 1st level classifiers to generate intermediate outputs, then a stacked classifier is trained with the intermediate outputs as its inputs. Stacking techniques are also well-known in ensemble learning. Therefore, I do not see any new technical ideas. \n\nMoreover, the proposed solution cannot really avoid negative transfer. If two domains are indeed very different, the performance of the basic transfer learning models would be very bad, e.g., worse than random guess. In this case building a stacked classifier cannot help to boost the final performance. \n\nThe datasets used to conduct experiments are all of toy sizes.\n\nThere are a lot of typos and grammar errors. The format of citations in the main text are incorrect. \n\nIn summary, the quality of this paper is far below the standard of top conferences.", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper186/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacking for Transfer Learning", "abstract": "In machine learning tasks, overtting frequently crops up when the number of samples of target domain is insuf\ufb01cient, for the generalization ability of the classi\ufb01er is poor in this circumstance. To solve this problem, transfer learning utilizes the knowledge of similar domains to improve the robustness of the learner. The main idea of existing transfer learning algorithms is to reduce the dierence between domains by sample selection or domain adaptation. However, no matter what transfer learning algorithm we use, the difference always exists and the hybrid training of source and target data leads to reducing \ufb01tting capability of the learner on target domain. Moreover, when the relatedness between domains is too low, negative transfer is more likely to occur. To tackle the problem, we proposed a two-phase transfer learning architecture based on ensemble learning, which uses the existing transfer learning algorithms to train the weak learners in the \ufb01rst stage, and uses the predictions of target data to train the \ufb01nal learner in the second stage. Under this architecture, the \ufb01tting capability and generalization capability can be guaranteed at the same time. We evaluated the proposed method on public datasets, which demonstrates the effectiveness and robustness of our proposed method.", "keywords": ["data diversi\ufb01cation", "domain adaptation", "transfer learning", "stacked generalization"], "authorids": ["pyk3350266@163.com"], "authors": ["Peng Yuankai"], "TL;DR": "How to use stacked generalization to improve the performance of existing transfer learning algorithms when limited labeled data is available.", "pdf": "/pdf/6ebec4ecc69570439f20ee3e3a16433e0efd3721.pdf", "paperhash": "yuankai|stacking_for_transfer_learning", "_bibtex": "@misc{\nyuankai2019stacking,\ntitle={Stacking for Transfer Learning},\nauthor={Peng Yuankai},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxOIsA5FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper186/Official_Review", "cdate": 1542234519551, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxOIsA5FQ", "replyto": "ryxOIsA5FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper186/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335667478, "tmdate": 1552335667478, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper186/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}