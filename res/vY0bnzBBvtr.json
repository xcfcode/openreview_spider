{"notes": [{"id": "vY0bnzBBvtr", "original": "IJw-vSLym4e", "number": 2775, "cdate": 1601308307769, "ddate": null, "tcdate": 1601308307769, "tmdate": 1614985642628, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "uvNdr51rdc_", "original": null, "number": 1, "cdate": 1610040519068, "ddate": null, "tcdate": 1610040519068, "tmdate": 1610474127557, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper explores the performance of Q-learning in the presence of either one-sided feedback or full feedback. Such feedbacks play an important role in improving the resulting regret bounds, which are (almost) not affected by the dimension of the state and action space. The motivation of such feedback settings stems from problems like inventory control. However, the assumptions underlying the theory herein are often quite strong, which might limit the applicability of the theory. The dependency on the length per episode H can also be improved.\u00a0 \u00a0"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040519054, "tmdate": 1610474127542, "id": "ICLR.cc/2021/Conference/Paper2775/-/Decision"}}}, {"id": "4oy8fg55ypa", "original": null, "number": 1, "cdate": 1603701249639, "ddate": null, "tcdate": 1603701249639, "tmdate": 1606266583753, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Official_Review", "content": {"title": "An interesting extension of Q-learning algorithms to rich feedback settings", "review": "After rebuttal:\n\nMy main concerns are addressed, and I changed my score to 5 accordingly.\n\n------\nMotivated by OR problems, this paper extends Q-learning algorithm to one-sided-feedback and full-feedback settings. With additional assumptions, this paper proves a $\\sqrt{T}$-regret bound with no dependence on the size of state/action spaces. Both proposed algorithms are theoretically and empirically shown to be more efficient than general Q-learning algorithms.\n\nThe novelty of this paper lies in applying reinforcement learning algorithms to the inventory control problem. It is interesting to see how can Q-learning algorithm be customized to problems with more structure. Technically, it is also novel to use elimination based algorithms in tabular reinforcement learning. However, the assumptions of this paper is a bit too strong, which is the major weakness of this paper. For example, it is assumed that the next state $x_{h+1}(\\cdot)$ is increasing in $y_h$, and the feasible action set is also monotonic, which means that the state and action spaces must be 1-dimensional. It is also assumed that both the transition and the reward depend only on action and environment randomness. But in the example (e.g. Backlogged model), the reward is a function of $x_h,y_h$. As a result, I'm not totally convinced that the assumptions in this paper is general and realistic.\n\nAfter spending considerable amount of time, I still have some concerns about the technical soundness. To be more specific:\n1. In Line 13 of Alg. 1, how is trajectory simulated? To be more specific, how does the algorithm choose action $y$ for step $h+1,\\cdots,\\tau_{h}^{k}(x,y)$ and how is the next state generated?\n2. Proof of Lemma 7 (page 3 of appendix): in the inequality before Eq. (12), how is the term $\\tilde{r}^i-\\tilde{r}^*$ bounded? If I understand correctly, $\\tilde{r}^i$ is the cumulative reward of the trajectory generated by the algorithm, and $\\tilde{r}^*$ is the reward generated by the optimal policy. When the trajectory is longer than 1 time step, the actions of the two trajectories on step $h+1$ may be different when the algorithm has not converged to optimal.\n\nAdditional comments:\n1. Since the state space is infinite and continuous, I'm wondering how function approximation RL algorithms behave in this setting.\n2. It is mentioned in Sec. 6 that the state space $x_h\\in \\mathbb{R}$ is continuous, but how to execute the HQL algorithm for continuous state space? Although the regret has no dependence on the size of state and action space, the time and space complexity do.\n3. In the bandit setting, the elimination algorithm has instance dependent regret (i.e., sum_i (1/Delta_i) log(T)). I'm wondering whether HQL algorithm has similar guarantee?\n4. Why is lost-sales model one-sided? What can the RL agent observe in this setting? If the agent can observe $D_h$, isn't it the case where the agent can compute the reward function for all possible $y$? If the agent can only observe min(y, D), how can the agent infer min(y-1,D)?\n\nIn summary, my main concerns are about the assumptions and technical soundness. Therefore, I would not recommend acceptance for the paper at this point.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2775/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2775/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088865, "tmdate": 1606915805472, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2775/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2775/-/Official_Review"}}}, {"id": "b50Eie_Gwck", "original": null, "number": 5, "cdate": 1606246394177, "ddate": null, "tcdate": 1606246394177, "tmdate": 1606246394177, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "4oy8fg55ypa", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Official_Comment", "content": {"title": "Difference of r's are bounded by Lemma 10", "comment": "Thank you for taking the time and effort to review our work and for your detailed comments.\n\nFor example,  I have x=10 units of product right now at step h, and this is more than max of the running set $A_h^k$={1,2, ..., 7}, so I order no new inventory y=10. A demand of 2 units occurred, now I have 8 units of product left. Let's say this is still more than max of the running set $A_{h+1}^k$={1,2, ..., 5}, so I order no new inventory. Another demand of 3 units occurred, now I have 5 units left. Let's say this is back in the running set $A_{h+2}^k$={1,2, ..., 9}, so $\\tau_h^k(10,10)=h+2$. Then I can simulate what my state would be at time h+1, if I took action $y_h=1$, then $x_{h+1}=(y_h-2)^+=0$, and then if I took action $y_{h+1}=5$, then $x_{h+2}=(y_{h+1}-3)^+=2$. If I took action $y_h=7$, then $x_{h+1}=(y_h-2)^+=5$, and then if I took action $y_{h+1}=5$, then $x_{h+2}=(y_{h+1}-3)^+=2$...\n\nThe two r's are w.h.p equal, because by Lemma 10, our action and the optimal action from h to $tau$ are both to choose the closest feasible action to the running set. Since in this inequality, the optimal policy and we start from the same state, they have the same feasible set and take the same actions, thus having the same cumulative reward until $\\tau$ (back in the running set). We will add in more explanation for this inequality in the proof.\n\nFor the inventory control problem, most literature considers continuous action space. We can discretize the action space without incurring a larger regret bound. Indeed the time complexity depends on S and A. We are more focused on their effect on the regret of the algorithms. The runtime is not a big bottleneck for our applications.\n\nThe instance dependent regret would indeed be interesting to investigate.\n\nLost-sales model is one-sided model. we can observe min(y, D), but not the actual D. Therefore, we can know min(y-1, D), but not min(y+1, D). Eg, y=3, D=6, and we only know that y=3, min(y, D)=3, then we know min(y-1, D) must be 2. But we don't know what min(y+1, D) is. If the unobserved D=3, then we would still have min(y+1, D)=3, but if D>=4, then we have  min(y+1, D)=4 instead."}, "signatures": ["ICLR.cc/2021/Conference/Paper2775/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vY0bnzBBvtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2775/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2775/Authors|ICLR.cc/2021/Conference/Paper2775/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844587, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2775/-/Official_Comment"}}}, {"id": "lTLC0jgtOUR", "original": null, "number": 4, "cdate": 1606207982720, "ddate": null, "tcdate": 1606207982720, "tmdate": 1606207982720, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "pCrX_TCzYsL", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Official_Comment", "content": {"title": "The runtime is not a big bottleneck for our applications.", "comment": "Thank you for taking the time and effort to review our work and for your detailed comments. \n\nIndeed the time complexity depends on S and A. We are more focused on their effect on the regret of the algorithms. The runtime is not a big bottleneck for our applications.\n\nWe will try to save space in Section 5 for a more detailed Section 4 Main Results.\n\nWe will add in more experiments, especially experiments with larger H."}, "signatures": ["ICLR.cc/2021/Conference/Paper2775/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vY0bnzBBvtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2775/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2775/Authors|ICLR.cc/2021/Conference/Paper2775/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844587, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2775/-/Official_Comment"}}}, {"id": "MSZ6g28FOGD", "original": null, "number": 3, "cdate": 1606207294199, "ddate": null, "tcdate": 1606207294199, "tmdate": 1606207294199, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "gk99AyMi7z", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Official_Comment", "content": {"title": "One-sided feedback does not give a straightforward way to adapt Jin et al. 2018 without extra techniques", "comment": "Thank you for taking the time and effort to review our work and for your detailed comments. \n\nIndeed, adapting the proofs in Jin et al. 2018 can provide a proof for FQL, but this is not true for HQL. Having one-sided feedback does not give a straightforward way to adapt prove the result for HQL using proof in Jin et al.  We have to use different techniques such as shortfall decomposition (Lemma 4) to prove the result for HQL.  \n\nWe will highlight the assumptions and reorganize the paper, including putting Example Applications before the formulation.\n\nWe will try to save space in Section 5 for a more detailed Section 4 Main Results.\n\nWe will add in more experiments, especially experiments with larger H.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2775/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vY0bnzBBvtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2775/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2775/Authors|ICLR.cc/2021/Conference/Paper2775/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844587, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2775/-/Official_Comment"}}}, {"id": "VAKQkzl9MiT", "original": null, "number": 2, "cdate": 1606206399728, "ddate": null, "tcdate": 1606206399728, "tmdate": 1606206458526, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "bZFihBqOOUi", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Official_Comment", "content": {"title": "FQL and HQL have different dependences on H because of different proof techniques.", "comment": "Thank you for taking the time and effort to review our work and for your detailed comments. FQL has the same dependence on H as prior works, because adapting the proofs in Jin et. al. 2018 can provide a proof for FQL, but this is not true for HQL. Very different techniques such as shortfall decomposition (Lemma 4) are needed to prove the result for HQL. This leads to an extra factor of H. \n\nWe will add in more experiments, especially experiments with larger H. \n\nWe will place the example applications before the model description."}, "signatures": ["ICLR.cc/2021/Conference/Paper2775/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vY0bnzBBvtr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2775/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2775/Authors|ICLR.cc/2021/Conference/Paper2775/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844587, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2775/-/Official_Comment"}}}, {"id": "pCrX_TCzYsL", "original": null, "number": 2, "cdate": 1603935386504, "ddate": null, "tcdate": 1603935386504, "tmdate": 1605024134022, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Official_Review", "content": {"title": "Review for \"Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings\"", "review": "This paper proposes Q-learning based algorithms called Elimination-Based Half-Q-Learning (HQL) and Full-Q-Learning (FQL). In the one-sided-feedback setting, the proposed algorithm improves the regret bounds over existing methods in terms of the dependency on the size of state-action space. Numerical experiments are provided to show the performance of the algorithm.\n\nOverall, I vote for rejecting. The detailed comments are as follows:\n\nPros:\n- By incorporating domain-specific structures into Q-learning algorithms, the author developed new algorithms tailored to one-sided-feedback/full-feedback models. The algorithm improves the regret bound in terms of the dependency on the state-action space.\n\nCons:\n- Although the algorithm improves the regret bounds with respect to the state-action space, the time complexity grows linearly with S and A (in Table 1). Therefore, I'm skeptical of the claim that \"the algorithms are barely hampered by even infinitely large state-action sets\".\n\n- The exposition in Section 4 could be improved. In the current version, there are only statements of two theorems. I think the authors should spend more space in Section 4 instead of Section 5. It would be helpful if authors can provide interpretation of the theorems and detailed comparison with existing results. And it would be nicer to provide examples that satisfy the assumptions made in Section 2.\n\n- I felt that the numerical experiment is not so convincing. For example, the episode length seems to be too small (H <= 5). In addition, since authors claim that the algorithms scale well to large state-action sets, it would be better to conduct the numerical experiment in that regime to show the efficiency of the algorithm.\n\n- The writing quality could be improved. There are several grammar mistakes and typos.", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2775/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2775/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088865, "tmdate": 1606915805472, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2775/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2775/-/Official_Review"}}}, {"id": "bZFihBqOOUi", "original": null, "number": 3, "cdate": 1604069672777, "ddate": null, "tcdate": 1604069672777, "tmdate": 1605024133958, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Official_Review", "content": {"title": "The formulation is interesting but the evaluation could be improved. ", "review": "This paper proposes two algorithms, Half-Q-Learning and Full-Q-Learning for the classic inventory control problem. It establishes cardinality-independent regret bonds for the two algorithms in terms of length of episode and horizon. Numerical results are presented to demonstrate the effectiveness of the algorithms. \n\nPlus\n- The paper is well written and the ideas are explained clearly. The proof flow is also explained clearly. \n- The inventory control problem in consideration is important. Exploiting the feedback structure in this problem is interesting. \n\nMinus\n- The regret performance w.r.t. H and running time are worse than prior works. \n- The experimental section could be improved, e.g., compare the algorithms with larger H values and running times. \n\nDetailed comments \n- The lowercase letter \u201ck\u201d above Eq. (1) should be \u201cK\u201d. \n- The reviewer would suggest the authors to place the inventory control problem before the formulation. This way, it is easier to see what assumptions are reasonable and satisfied by the target application. \n- It will be interesting if the authors could discuss why FQL and HQL are able to eliminate the SA factor from the regret but results in a larger regret w.r.t. H. Intuitively, the former is due to the feedback structure and a larger computational cost. But the latter is less clear and it would be nice to provide some intuition. \n- The experimental setting appears to be quite simplistic. The reviewer would suggest experimenting more scenarios to evaluation the performance. \n- The experiments use a fairly small H. The reviewer wonders what happens if the H value is larger? From the comparison results with Aggregate QL and QL-UCB in Table 1, it seems that FQL and HQL should perform worse than QL-UCB. It would also be interesting to compare the running time of the algorithms. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2775/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2775/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088865, "tmdate": 1606915805472, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2775/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2775/-/Official_Review"}}}, {"id": "gk99AyMi7z", "original": null, "number": 4, "cdate": 1604274168223, "ddate": null, "tcdate": 1604274168223, "tmdate": 1605024133891, "tddate": null, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "invitation": "ICLR.cc/2021/Conference/Paper2775/-/Official_Review", "content": {"title": "Review", "review": "##################################################################################\n\nSummary:\n\n\nThis paper is motivated by using reinforcement learning (RL) methods in inventory control. In particular, it customizes Q-learning for a special one-sided feedback/full-feedback setting. This combination of two areas is intriguing. The main contribution of the paper is a new algorithm leveraging the model structure, so that the regret no longer depends on the size of state and action space. \n\n\n##################################################################################\n\nReasons for score: \n\n \nI like the idea of introducing RL methods to classical operations research problems. The application of Q-learning in inventory control seems promising. My major concern is about the clarity of the paper and the novelty of theoretical analysis (See cons below). I think the paper is not suitable for ICLR in this current version.\n\n\n##################################################################################\n\nStrengths:\n\n\n- The one-sided feedback/full-feedback model is of high practical value. It is an abstraction and generalization of inventory control and applies to many other famous problems.\n- The proposed HQL and FQL algorithms are novel and well-motivated by the one-sided-feedback/full-feedback assumption.\n- The theoretical and empirical comparison with existing Q-leaning methods shows the benefits of utilizing model structures.\n\n\n##################################################################################\n\nWeakness:\n\n- The theoretical analysis is similar to Jin et al. (2018). It is not evident to me what new challenges are in the proof. Besides, the authors use Azuma-Hoeffding's inequality instead of Bernstein one so that the dependence on H is very loose.\n- In numerical experiments, the results would be more convincing if the authors could take a longer length of episode (currently H = 1, 3 or 5) and compare with traditional inventory control algorithms in the same setting.  \n- The writing needs significant improvement.\n\t* The model assumptions should be highlighted. The one-sided-feedback/full-feedback models are unconventional. It would help readers to better comprehend problem set-up if subsections 2.1 & 2.2 are written in a more organized way.\n\t* Section 4: Main Results lacks necessary discussion.\n\t* Section 5: Overview of Proof is lengthy and there is neither an evident clue nor an outline. Some technical results are more suitable to appear in appendix.\n\t* In the reviewer's humble opinion, Section 6: Example Applications could be placed before model description, so that the motivations are clearer at first glimpse.\n\n\n##################################################################################\n\nTypo: \n* Algorithm 1, Q-function update step, the subscript of value function $V$\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2775/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2775/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings", "authorids": ["~Xiao-Yue_Gong1", "~David_Simchi-Levi2"], "authors": ["Xiao-Yue Gong", "David Simchi-Levi"], "keywords": ["Q-learning", "episodic MDP", "full-feedback", "one-sided-feedback", "inventory control", "inventory"], "abstract": "Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$ regret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.", "one-sentence_summary": "We propose a new Q-learning algorithm that is provably more efficient for the one-sided/full feedback settings than existing Q-learning algorithms, showing the potential for adapting reinforcement learning to more varied structures of problems", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gong|provably_more_efficient_qlearning_in_the_onesidedfeedbackfullfeedback_settings", "supplementary_material": "/attachment/876a5d5e4a013aeffd58d529129d780376a6bb2f.zip", "pdf": "/pdf/c33e873651d5c90d20e74adf8740b63a1d04ebbd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LH-IineYU", "_bibtex": "@misc{\ngong2021provably,\ntitle={Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings},\nauthor={Xiao-Yue Gong and David Simchi-Levi},\nyear={2021},\nurl={https://openreview.net/forum?id=vY0bnzBBvtr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vY0bnzBBvtr", "replyto": "vY0bnzBBvtr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2775/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088865, "tmdate": 1606915805472, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2775/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2775/-/Official_Review"}}}], "count": 10}