{"notes": [{"id": "S1xA7ILFOV", "original": "ByeQgEM_vV", "number": 7, "cdate": 1553716773555, "ddate": null, "tcdate": 1553716773555, "tmdate": 1571693837338, "tddate": null, "forum": "S1xA7ILFOV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "On Scalable and Efficient Computation of Large Scale Optimal Transport", "authors": ["Yujia Xie", "Minshuo Chen", "Haoming Jiang", "Tuo Zhao", "Hongyuan Zha"], "authorids": ["xie.yujia000@gmail.com", "mchen393@gatech.edu", "jianghm@gatech.edu", "tuo.zhao@isye.gatech.edu", "zha@cc.gatech.edu"], "keywords": ["Scalable optimal transport", "generative model", "neural ODE"], "TL;DR": "Use GAN-based method to scalably solve optimal transport", "abstract": "Optimal Transport (OT) naturally arises in many machine learning applications, where we need to handle cross-modality data from multiple sources. Yet the heavy computational burden limits its wide-spread uses. To address the scalability issue, we propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms. We also show that we can recover the density of the optimal transport plan using neural ordinary differential equations. Numerical experiments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently sample from the optimal transport plan, which benefits downstream applications such as domain adaptation.", "pdf": "/pdf/793823c8960db4ddee0f9a6fd2b06416a0405561.pdf", "paperhash": "xie|on_scalable_and_efficient_computation_of_large_scale_optimal_transport"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "ByeQl_FmqN", "original": null, "number": 2, "cdate": 1555433451388, "ddate": null, "tcdate": 1555433451388, "tmdate": 1556906133457, "tddate": null, "forum": "S1xA7ILFOV", "replyto": "S1xA7ILFOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper7/Official_Review", "content": {"title": "A clever combination of many ideas in optimal transport and neural density estimation, with surprisingly rigorous experiments", "review": "This was submitted as a workshop paper but I think with just a little more detail it would be a strong contender for a regular conference paper. The authors show how a combination of primal-dual estimation for Lagrangian problems with neural ODEs for density estimation can be used to solve the regularized optimal transport problem on high dimensional spaces. They show with numerous experiments on challenging domain-to-domain problems that the joint probability distribution they learn can be used for meaningful joint generation tasks, such as pix2pix-like style transfer in the image domain. Overall this paper was a pleasure to read - clearly motivated, tackling an important problem and using state-of-the-art methods to achieve it. I would have appreciated a little more discussion of the stability of gradient ascent/descent for solving the Lagrangian multiplier formulation of their objective, as I have found these kinds of problems very hard to work with in a stochastic domain, but overall the paper was compelling and timely.", "rating": "5: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Scalable and Efficient Computation of Large Scale Optimal Transport", "authors": ["Yujia Xie", "Minshuo Chen", "Haoming Jiang", "Tuo Zhao", "Hongyuan Zha"], "authorids": ["xie.yujia000@gmail.com", "mchen393@gatech.edu", "jianghm@gatech.edu", "tuo.zhao@isye.gatech.edu", "zha@cc.gatech.edu"], "keywords": ["Scalable optimal transport", "generative model", "neural ODE"], "TL;DR": "Use GAN-based method to scalably solve optimal transport", "abstract": "Optimal Transport (OT) naturally arises in many machine learning applications, where we need to handle cross-modality data from multiple sources. Yet the heavy computational burden limits its wide-spread uses. To address the scalability issue, we propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms. We also show that we can recover the density of the optimal transport plan using neural ordinary differential equations. Numerical experiments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently sample from the optimal transport plan, which benefits downstream applications such as domain adaptation.", "pdf": "/pdf/793823c8960db4ddee0f9a6fd2b06416a0405561.pdf", "paperhash": "xie|on_scalable_and_efficient_computation_of_large_scale_optimal_transport"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper7/Official_Review", "cdate": 1554234180601, "reply": {"forum": "S1xA7ILFOV", "replyto": "S1xA7ILFOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234180601, "tmdate": 1556906084424, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "rygv3L3f5E", "original": null, "number": 1, "cdate": 1555379886995, "ddate": null, "tcdate": 1555379886995, "tmdate": 1556906133242, "tddate": null, "forum": "S1xA7ILFOV", "replyto": "S1xA7ILFOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper7/Official_Review", "content": {"title": "Unclear if using neural nets to approximate lagrangians is a good idea.", "review": "1. The paper makes a variety of unsupported claims to justify their approach. Mainly this has to do with assuming that reducing the problem to a neural-net parametrized minimax game is inherently a good idea.\n   - The paper assumes that GAN-stype minimax problems are easier to solve than having neural nets parametrize the dual. This is a claim that requires substantial elaboration, since it's widely acknowledged that GANs are hard to train, and one would rather have a non-minimax optimization objective than a minimax one.\n   - The paper also claims that because neural nets have good inductive biases for tasks such as object recognition, they would serve as good Lagrange multipliers. I'm not sure where this claim comes from. Lagrange multipliers simply need to diverge to indicate constraint violations. These claims about statistical learnability dont seem relevant to the problem.\n   - 'direct estimation of probability distribution is not always convenient' seems backwards. Implicit generative models are a necessary evil when you're trying to generate from certain types of neural networks, not that they're more convenient than using MLE. \n   - Citing the universal approximation theorem for DNNs is vacuous in this setting considering that you could just solve the original nonparametric problem.. the claim should be that neural nets provide lower approximation error for the amount of required computation. \n\n2. There should be substantially more work analyzing the effect of the Lipschitz constraint on the multipliers.\n   - Currently, the Lipschitz constraint comes out of nowhere. If the Lagrange multipliers are unbounded, then you've failed at the minimax objective, and the resulting solution is undefined in the primal. You have an optimal transport map that fails to satisfy the marginal constraints, so it does you no good to add arbitrary restrictions to the Lagrangian. \n   - If you want to continue to go down this road - you should really characterize what this Lipschitz constraint does to the objective. Equation (9) is a good start, but its not terribly clear what happens here, because you now have two metrics: one implied by the cost (c) and another one from the Lipschitz constraint. I think you'd be in better shape if you just reformulate the entire paper in terms of W_1 with a fixed distance metric and define the Lipschitz constant there.\n   - Finally, I conjecture (I think you might be able to do this by examining the subgradient structure of W_1 near X ...) that if you're in W_1 with eta > 1, then you would actually be solving the original problem. That would be a pretty interesting result, and would actually justify much of the paper.\n\n3. Baselines comparisons seem lacking.\n   - A natural approach is just to apply a neural ODE to the pushforward formulation from Monge. Yes, this isn't always feasible, but the proposed SPOT procedure is heuristic to start with. Does this approach not work?\n   - The generation experiments don't make sense from the narrative of the paper. If you've actually solved the OT problem, the generated images are trivial (i.e. recovered from the original training set) because you match the marginals. If you're generating new images, then you've failed to solve the OT marginal constraints. Either way this seems problematic.\n   - The paper is motivated in terms of continuous OT, but the experiments all operate on discrete, empirical distributions (plus generation experiments, but see my comments above.. the fact that you generalize just means you didn't solve the original OT problem.. this is a negative, not a plus). This is not only a conceptual gap in the paper, it means that the authors should have set up appropriate comparisons to entropically regularized / subsampled OT algorithms which are much more principled and mature.\n\n4. Consider reframing the paper\n   - The paper has some interesting results, about generating paired samples, and the importance of doing such tasks and so on. However, the paper is currently motivated as approximating the underlying OT objective. From this latter motivation, the paper is very lacking - there's many conceptual holes about whether the lagrangian constraints are holding, or if this is a good idea, or if you've set up the appropriate baselines. \n", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Scalable and Efficient Computation of Large Scale Optimal Transport", "authors": ["Yujia Xie", "Minshuo Chen", "Haoming Jiang", "Tuo Zhao", "Hongyuan Zha"], "authorids": ["xie.yujia000@gmail.com", "mchen393@gatech.edu", "jianghm@gatech.edu", "tuo.zhao@isye.gatech.edu", "zha@cc.gatech.edu"], "keywords": ["Scalable optimal transport", "generative model", "neural ODE"], "TL;DR": "Use GAN-based method to scalably solve optimal transport", "abstract": "Optimal Transport (OT) naturally arises in many machine learning applications, where we need to handle cross-modality data from multiple sources. Yet the heavy computational burden limits its wide-spread uses. To address the scalability issue, we propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms. We also show that we can recover the density of the optimal transport plan using neural ordinary differential equations. Numerical experiments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently sample from the optimal transport plan, which benefits downstream applications such as domain adaptation.", "pdf": "/pdf/793823c8960db4ddee0f9a6fd2b06416a0405561.pdf", "paperhash": "xie|on_scalable_and_efficient_computation_of_large_scale_optimal_transport"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper7/Official_Review", "cdate": 1554234180601, "reply": {"forum": "S1xA7ILFOV", "replyto": "S1xA7ILFOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234180601, "tmdate": 1556906084424, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper7/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "B1gb4VOPq4", "original": null, "number": 1, "cdate": 1555690536937, "ddate": null, "tcdate": 1555690536937, "tmdate": 1556906132993, "tddate": null, "forum": "S1xA7ILFOV", "replyto": "S1xA7ILFOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper7/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept", "comment": "Accepted"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Scalable and Efficient Computation of Large Scale Optimal Transport", "authors": ["Yujia Xie", "Minshuo Chen", "Haoming Jiang", "Tuo Zhao", "Hongyuan Zha"], "authorids": ["xie.yujia000@gmail.com", "mchen393@gatech.edu", "jianghm@gatech.edu", "tuo.zhao@isye.gatech.edu", "zha@cc.gatech.edu"], "keywords": ["Scalable optimal transport", "generative model", "neural ODE"], "TL;DR": "Use GAN-based method to scalably solve optimal transport", "abstract": "Optimal Transport (OT) naturally arises in many machine learning applications, where we need to handle cross-modality data from multiple sources. Yet the heavy computational burden limits its wide-spread uses. To address the scalability issue, we propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms. We also show that we can recover the density of the optimal transport plan using neural ordinary differential equations. Numerical experiments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently sample from the optimal transport plan, which benefits downstream applications such as domain adaptation.", "pdf": "/pdf/793823c8960db4ddee0f9a6fd2b06416a0405561.pdf", "paperhash": "xie|on_scalable_and_efficient_computation_of_large_scale_optimal_transport"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper7/Decision", "cdate": 1555612281316, "reply": {"forum": "S1xA7ILFOV", "replyto": "S1xA7ILFOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1555612281316, "tmdate": 1556906095367, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}