{"notes": [{"id": "HJgCcCNtwH", "original": "SylNxktuDB", "number": 1300, "cdate": 1569439381594, "ddate": null, "tcdate": 1569439381594, "tmdate": 1577168289415, "tddate": null, "forum": "HJgCcCNtwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "YPYO9DUiwz", "original": null, "number": 1, "cdate": 1576798719757, "ddate": null, "tcdate": 1576798719757, "tmdate": 1576800916777, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Decision", "content": {"decision": "Reject", "comment": "This work proposes new initialization and layer topologies for training a priori sparse networks. Reviewers agreed that the direction is interesting and that the paper is well written. Additionally the theory presented on the toy matrix reconstruction task helped motivate the proposed approach. However, it is also necessary to validate the new approach by comparing with existing sparsity literature on standard benchmarks. I recommend resubmitting with the additional experiments suggested by the reviewers.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727163, "tmdate": 1576800279395, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Decision"}}}, {"id": "HyeuHcRxcH", "original": null, "number": 2, "cdate": 1572035135880, "ddate": null, "tcdate": 1572035135880, "tmdate": 1574113437872, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper proposes a sparse cascade architecture that is a multiplication of several sparse matrices. Then the paper provides several considerations about the connectivity scheme. Finally, the paper proposes a specific connectivity pattern that outperforms other ones.\n\nI am not exactly in this area, but the paper is frequently confusing and hard to read. If I understand correctly, Sec 5 proposes a method to evaluate topologies. It seems that this method is strongly tied to the proposed cascade architecture, therefore cannot compare cascade vs non-cascade. The choice of the matrix reconstruction problem looks arbitrary. The paper finishes very abruptly.\n\nAnother consideration is that evaluating on random matrices doesn't demonstrate the ability to solve real world problems. There are no experimentation on real benchmarks or comparison to prior work on sparse networks. It should be possible to compare to methods that sparsify networks after training as well as to methods that enforce sparsity during/before training.\n\nOther:\n- it is strange to call a method by the first name of the researcher (Xavier)\n- there are some grammatical mistakes and problems with articles\n\nEDIT:\nAfter the rebuttal period paper still has weak experimentation. My score stays the same.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575226975225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Reviewers"], "noninvitees": [], "tcdate": 1570237739381, "tmdate": 1575226975236, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Review"}}}, {"id": "BJlH9wIhiB", "original": null, "number": 6, "cdate": 1573836685313, "ddate": null, "tcdate": 1573836685313, "tmdate": 1573836685313, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "SJlvcB8hir", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment", "content": {"title": "Response", "comment": "I didn't mean that the paper specifically suggests replacing spatial operations.  I was using spatial operations as an example of a case where it is obvious that the criterion of uniform controllability is undesirable - it is certainly not the only case.  It seems like any problem where there is structure to be exploited would have a more efficient representation with non-uniform controllability.  The chosen problem of random matrix reconstruction has no structure, so the problem and solution seem well matched, but I'm not convinced that this is generally representative.\n\nI look forward to seeing results on CIFAR-10 and ImageNet."}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCcCNtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1300/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1300/Authors|ICLR.cc/2020/Conference/Paper1300/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158119, "tmdate": 1576860531544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment"}}}, {"id": "BJlWGUInir", "original": null, "number": 5, "cdate": 1573836297233, "ddate": null, "tcdate": 1573836297233, "tmdate": 1573836297233, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "rkev_8yLtr", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment", "content": {"title": "Review #3 response", "comment": "We thank the reviewer for the extensive review and very insightful comments. We hope to address all of the \nconcerns in this response and the next iteration of the document. \n\nConcerning whether L2 reconstruction loss is a good proxy of the final accuracy, we agree with the reviewer\nthat this is an unanswered question. As of now, we only have empirical evidence that decreasing L2 \nreconstruction loss leads to better network performance. In the updated version of the paper, we will add \nseveral benchmarks (CIFAR10, ImageNet, etc.) and compare the L2 reconstruction loss of a given set of networks \nwith the accuracy on real problems. In the future we hope to have theoretical evidence supporting this claim. \n\nConcerning L0 norm, we initially used it for measuring the reconstruction quality of networks trained with L1\nloss, where the network will either perfectly reconstruct certain values (loss of 0), or have some error (loss\nof 1, independent of actual error). However, we discovered that L0 measure leads to topologies that will \nreconstruct a number of matrix elements perfectly, yet the topology will underperform on real tasks. This is \ndue to the fact that L0 metric does not try to 'balance out' the reconstruction quality over the whole matrix,\nbut instead is satisfied with greedily reconstructing some subset of elements. In the revision, we will clear\nup why L0 loss is not a good proxy for real world performance of a sparse topology.\n\nConcerning the question on whether the number and width of layers needs to be predefined: our work mainly aims\nto 'retrofit' existing neural network architectures. As such, the types of layers and layer width are already \nprovided by the network designer, and our work only attempts to break down the already specified linear layers.\nThe depth of the cascade does however have to be manually selected, but as it depends on the parameter budget \nthe user has provided, it can be calculated from the layer width and number of parameters. We now understand\nthat we should clear up these points raised by the reviewer.\n\nConcerning training with L1 loss, what we mean is that the networks are trained on the matrix reconstruction \ntask using L1 loss, but are finally evaluated using L0 loss. We will clear up this point in the updated version\nof the document.\n\nFinally, we agree with the reviewer on the experimental points, and will update the paper with experimental \nresults on CIFAR10, ImageNet, and more. For the second point, we will update the paper with more experimental\nevidence. We plan to add an experiment using neuroevolution, where we will compare evolved topologies to our \npredicted ones, on both matrix reconstruction and real world tasks.\n\nConcerning other compact architectures the reviewer mentioned (MobileNetv2, ShuffleNet and ShuffleNetv2, \nSqueezeNet, and ERFNet), we plan to apply our method to these architectures and evaluate whether further memory reductions can be gained, and at what cost to accuracy. \n\nAgain, we thank the reviewer for the time and effort put into helping us better this paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCcCNtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1300/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1300/Authors|ICLR.cc/2020/Conference/Paper1300/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158119, "tmdate": 1576860531544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment"}}}, {"id": "rJxGJL8njB", "original": null, "number": 4, "cdate": 1573836249973, "ddate": null, "tcdate": 1573836249973, "tmdate": 1573836249973, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "HyeuHcRxcH", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment", "content": {"title": "Review #2 response", "comment": "We thank the reviewer for their effort and comments.\n\nWe understand that the motivation for using matrix reconstruction as a proxy for evaluating network\nperformance is somewhat unclear. In future iterations of the paper we will strengthen our argument for\nwhy using a matrix reconstruction task is an appropriate measure for evaluating sparse topologies.\n\nAlso, we plan to add several benchmarks to the appendix, e.g., CIFAR and ImageNet evaluations.\n\nConcerning the Xavier/Glorot initialization, both the author's first and last name have been used in\nliterature. We have adopted the more common (at least in our experience) naming."}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCcCNtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1300/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1300/Authors|ICLR.cc/2020/Conference/Paper1300/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158119, "tmdate": 1576860531544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment"}}}, {"id": "HkxX6B8nsS", "original": null, "number": 3, "cdate": 1573836218981, "ddate": null, "tcdate": 1573836218981, "tmdate": 1573836218981, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "rJgxbE8wcS", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment", "content": {"title": "Review #1 response", "comment": "We thank the reviewer for the extensive review and helpful comments. We hope to address all of them in \nthis response and the next version of the paper.\n\nConcerning the vanishing gradient problem, in the next iteration of the paper we will update the appendix\nwith some empirical results showing the activation magnitude w.r.t. sparsity and depth. Our experiments \nhave shown that sparsity-aware initialization is crucial for training deep networks. As mentioned in the paper, \nif given a layer density $d$, by using original Xavier initialization, we expect that each layer's \nactivation variance is $d$ times lower than that of the previous layer, causing an exponential drop and \npreventing training with only several sparse layers. In the updated version we will show a comparison of \nlayer activation variances with varying depths and sparsities.\n\nConcerning the lack of empirical data on real world datasets, we will update the next version of the paper\nwith experiments on real world datasets. \n\nFor the last point, we agree with the reviewer, and will compare our approach to those of training-time \npruning methods. While at the time of writing the paper we felt that these approaches have a different \nmotivation (i.e., model size reduction and not training speedups), we understand how these results may be \ninformative to the reader.\n\nConcerning the comment on matrix-vector product complexity, we agree that lack of reuse is the problem.\nWe aim to support batch size of 1 in order to allow data-parallel training on very large clusters, where \nthe cumulative batch size can be in hundreds of thousands of samples. By reducing the batch size on each\nindividual machine, we can keep the total batch size at a manageable level. Furthermore, if we are able \nto remove the memory bottleneck, batching is no longer necessary as a streaming architecture that keeps\nthe model on-chip should have the same performance as a large-batch architecture. We will clear up these \npoints in the next iteration of the paper.\n\nAgain, we wish to thank the reviewer for all the effort, the provided references, and the help in \nbettering this paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCcCNtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1300/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1300/Authors|ICLR.cc/2020/Conference/Paper1300/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158119, "tmdate": 1576860531544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment"}}}, {"id": "SJlvcB8hir", "original": null, "number": 2, "cdate": 1573836174617, "ddate": null, "tcdate": 1573836174617, "tmdate": 1573836174617, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "BJxHFA_PcS", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment", "content": {"title": "Review #4 response", "comment": "We thank the reviewer for the detailed and helpful comments.\n\nThe reviewer mentions that we should understand the trade-offs between using static and dynamic sparsity.\nWe agree and will update our paper with a performance and accuracy comparisons of applying dynamic sparsity \nduring training, vs. pre-selecting a topology for static sparsity. \n\nConcerning the comment on whether we want equal controllability, the reviewer gives an example of CNNs \nwhere we would want to exploit local structure. Note that our method does not sparsify the network in the\nspatial domain, but only attempts to sparsify inter-channel connections. Similarly to MobileNet, we still\nuse a large number of depthwise convolutions. Only the interpolation between the convolved channels is \naffected by a priori pruning. We will update the explanation to remove any ambiguities. \n\nDue to space constraints we did not add any evaluations on CIFAR10/ImageNet in the paper. We will update \nthe appendix with some recent results to showcase the applicability of our method. \n\nFinally, we will integrate the reviewer's comments about the related work.\n\nAgain, we thank the reviewer for their time and effort in helping us better this paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCcCNtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1300/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1300/Authors|ICLR.cc/2020/Conference/Paper1300/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158119, "tmdate": 1576860531544, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Authors", "ICLR.cc/2020/Conference/Paper1300/Reviewers", "ICLR.cc/2020/Conference/Paper1300/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Comment"}}}, {"id": "rkev_8yLtr", "original": null, "number": 1, "cdate": 1571317358585, "ddate": null, "tcdate": 1571317358585, "tmdate": 1572972486747, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper tackles the problem of finding a sparse network architecture before training, so as to facilitate training on resource-constrained platforms. To this end, the authors propose to replace dense layers with series of sparsely-connected linear layers. They then study how to initialize such sparse layers to avoid gradient vanishing. Furthermore, they propose an approach to finding the best topology by measuring how well the sparse layers can approximate random weights of their dense counterparts.\n\nMethodology/clarity:\nWhile I found the beginning of the paper, up to Section 4 (included), easy to follow, I must admit that I have been struggling with the remainder of the paper. In particular:\n- In Section 5, the authors propose to use the L2 difference between random weights of a dense layer and the reconstruction of these weights with sparse layers. It seems that the author take this as a proxy to network accuracy (as indicated by a statement just below Fig. 3). It is not clear to me, however, that a small L2 norm will indeed correspond to a similar accuracy in practice, and this is never demonstrated in the paper.\n- This L2 loss is then replaced by an L0 one, which again I fail to find any justification for. In fact, I don't see why the authors did not directly use the L0 norm in the first place.\n- If I understood correctly, the goal of Sections 5-6 is to find the best topology for the sparse layers. However, it seems the number and width of such layers still need to be pre-defined. Correct?\n- Below Eq. 6, the authors mention that the networks are still trained using SGD with L1 loss. I do not understand this statement. On what is the L1 loss computed?\n\nExperiments:\nUltimately, the authors argue that their approach allowed them to find a parallel butterfly architecture that outperforms the other ones. However:\n- There is no evidence that the resulting architecture is indeed better than the others when it comes to solving an actual problem (e.g., in terms of classification accuracy);\n- There still seem to be a fair bit of manual design in this parallel butterfly architecture, and it is therefore not clear to me that it is truly the best possible one.\n\nRelated work:\nA number of architecture designs have been proposed to obtain a compact network prior to training. MobileNets, used here as baseline CNN, is one of them, but so are MobileNetv2, ShuffleNet and ShuffleNetv2, SqueezeNet, and ERFNet. I believe that it would be worth discussing these architectures and showing the benefits of the proposed approach over them.\n\nSummary:\nI have been struggling to follow the second half of this paper, and I am not convinced by the hypothesis that the L2 (or L0) norm between dense and sparse parameters is a good proxy for network accuracy, which is not demonstrated. I therefore feel that this paper is not ready for publication.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575226975225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Reviewers"], "noninvitees": [], "tcdate": 1570237739381, "tmdate": 1575226975236, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Review"}}}, {"id": "rJgxbE8wcS", "original": null, "number": 3, "cdate": 1572459511973, "ddate": null, "tcdate": 1572459511973, "tmdate": 1572972486668, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new weight initialization method for sparse neural networks and develop a weight topology that satisfies desirable properties. Their derivation is data-free, and thus the analysis should generalize to arbitrary datasets. They demonstrate that their new topology outperforms existing approaches on a matrix reconstruction task.\n\nOverall I think this work is an interesting direction for designing static sparse neural network weight topologies, but it\u2019s lacking in empirical evidence of their claims and could do better to tie themselves to existing literature in training sparse neural networks. \n\nIf the authors could strengthen their results by a) experimenting with their newfound topology and initialization on standard sparsification benchmarks like CIFAR, ImageNet, and WMT EnDe b) comparing their approach to other static-sparse [1, 2, 3] and dynamic-sparse [4, 5] training algorithms this could be a good paper, but without more experimentation it\u2019s unclear what can be taken away from this work. If the authors added results in this direction I would be willing to increase my score.\n\nComments on Claims of the Paper:\n\n1. \u201cCascades\u201d are a known trick in both dense & sparse neural networks [2, 6].\n2. The authors describe their motivation for developing a new sparse initialization method in the first paragraph of section 4. It would be nice to see some of this experimental data, such that we could understand the magnitude of the vanishing gradient problem in these tests and see that the newly derived initialization alleviates it. The reason for my skepticism is that Liu et al [7] used a similar scheme where they re-scale the standard deviation of the gaussian based on the fraction of nonzero weights, but later found that it made no difference in their results for unstructured pruning (which I learned through discussion with the authors).\n4. The data-free derivation approach makes sense and I understand that this makes the approach theoretically applicable to arbitrary datasets, but the authors do not apply it to other datasets to show that it generalizes in practice.\n5. The authors show that their topology outperforms on the matrix reconstruction task, but they don\u2019t compare with other sparsification approaches used in deep learning like sparse evolutionary training [4], or SNIP [1] (note that these techniques also maintain a sparse network during training, as opposed to pruning approaches like magnitude pruning [8] that are dense during training but sparse for inference).\n\nComments on the Results of the Paper:\n\nThe authors appear to contextualize their work in deep neural networks, but all of their experimental results are on matrix reconstruction or linear models on MNIST. This is sufficient for analysis and motivation, but taking the developed approaches and applying them to a deep neural network and showing improvements would go a long way towards improving this paper.\n\nAssorted Notes:\n\nIn the first paragraph of the introduction:\n\n\u201cIn other words, doubling the size of layer inputs and outputs quadruples the size of the layer. This causes majority of the networks to be memory-bound, making DNN training impractical without batching, a method where training is performed on multiple inputs at a time and updates are aggregated per batch.\u201d\n\nIt is correct that a matrix-vector product (i.e., neural network training with batch size 1) is typically memory bound on accelerators like GPUs, but it\u2019s not clear why the quadratic growth in computational cost with input/output size has anything to do with this. The cause of memory bound-ness is lack of reuse of operands, which can be alleviated by increasing the batch size s.t. the computation becomes a matrix-matrix product. Batch size 1 training is also not desirable. Recent work has shown that large batch sizes do not degrade model quality with proper hyperparameter tuning [9], and larger batch sizes are desirable from a hardware perspective to achieve higher throughput.\n\nReferences:\n1. https://arxiv.org/abs/1810.02340\n2. https://d4mucfpksywv.cloudfront.net/blocksparse/blocksparsepaper.pdf\n3. https://arxiv.org/abs/1903.05895\n4. https://www.nature.com/articles/s41467-018-04316-3\n5. https://openreview.net/forum?id=ryg7vA4tPB\n6. https://arxiv.org/abs/1811.10495v3\n7. https://arxiv.org/pdf/1810.05270v2.pdf\n8. https://arxiv.org/abs/1710.01878\n9. https://arxiv.org/abs/1811.03600"}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575226975225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Reviewers"], "noninvitees": [], "tcdate": 1570237739381, "tmdate": 1575226975236, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Review"}}}, {"id": "BJxHFA_PcS", "original": null, "number": 4, "cdate": 1572470396565, "ddate": null, "tcdate": 1572470396565, "tmdate": 1572972486618, "tddate": null, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "invitation": "ICLR.cc/2020/Conference/Paper1300/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes to replace dense layers with multiple sparse linear layers.  The idea is that the product ABC (for A, B, C all sparse matrices)  can accurately approximate a dense matrix D, but A(B(Cx))) requires much less work than Dx.  The paper then continues with the assumption that topology of the sparse matrices should be fixed before training, and that given this assumption we would like to find the \"best\" fixed topology to pick.  The paper introduces a new task to determine the \"best\" topology - that of reconstructing a random dense matrix.  On somewhat of a tangent the paper also introduces a minor modification to the Xavier initialization scheme that works better for deep stacks of sparse layers.\n\nMy current decision is one of Weak Reject.  I think the paper tackles an interesting line of research with some interesting ideas, but I'm concerned that the implications of the major assumptions that are made are never examined.\n\nIt is briefly implied that if the topology were fixed, then we could build hardware for such a topology (motivating the approach).  But we could also build hardware to accelerate general dynamic sparsity.  Given that it seems important to try and understand the tradeoffs involved in using fixed sparse topologies like the ones proposed and dynamic sparsity techniques such as:\n\n1) Deep Rewiring (https://arxiv.org/abs/1711.05136)\n2) Sparse Evolutionary Training (https://www.nature.com/articles/s41467-018-04316-3)\n3) Dynamic Sparse Reparameterization (https://arxiv.org/abs/1902.05967) \n4) Sparse Networks from Scratch (https://arxiv.org/abs/1907.04840)\n5) Rigging the Lottery (https://openreview.net/forum?id=ryg7vA4tPB)\n\nThe tradeoffs both in terms of the hardware that could be built for these different regimes _and_ the effect of static topologies vs. dynamic ones on accuracy (for a given parameter / flop / energy / etc. budget).  These dynamic techniques could also be used to decompose a dense layer into a product of multiple learned sparse matrices.\n\nThe effect of assuming that we want each connection to have equal controllability seems non-obvious. For example, if we imagine that we're replacing a convolutional (or at least locally connected) layer, then we _want_ to be able to take advantage of the structure/particularities of the input and doing so will lead to a more efficient model than one which is forced to ignore them (as the proposed topology necessarily does.)   How much less efficient will the proposed architecture be in this case?\n\nMany of these questions could be answered by trying the dynamic sparse techniques and the proposed static topologies on real problems (MB on ImageNet for example), maybe some kind of language modeling task, etc.  I find the use of one artificial task (of matrix reconstruction) which serves mainly to confirm the assumptions that are made rather than test them on real data and real problems a big weakness of the paper.\n\nSome general notes:\n\nThe enforcing sparsity before training section should mention SNIP: Single-shot Network Pruning based on Connection Sensitivity https://arxiv.org/abs/1810.02340\n\nThe enforcing sparsity during training section, should mention both the dynamic techniques that are mentioned above, but also techniques that are dense -> sparse but which significantly outperform the L1 techniques mentioned.\n\nFor example:\n\n1) Iterative Pruning as used in Learning both Weights and Connections for Efficient Neural Networks (https://arxiv.org/abs/1506.02626) and popularized by The Lottery Ticket Hypothesis (https://arxiv.org/abs/1803.03635)\n2) To prune or not to Prune (https://arxiv.org/abs/1710.01878)\n3) Variational Dropout (https://arxiv.org/abs/1701.05369)\n4) Dynamic Network Surgery (https://arxiv.org/abs/1608.04493)\n\nThe mobilenet reference seems a bit out of place as mobilenets are never otherwise used in the rest of the paper.  It seems like something very similar has already been done in (https://dawn.cs.stanford.edu/2019/06/13/butterfly/).\n\nvraiance -> variance"}, "signatures": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1300/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks", "authors": ["Mihailo Isakov", "Michel A. Kinsy"], "authorids": ["mihailo@bu.edu", "mkinsy@bu.edu"], "keywords": ["Sparsity", "model compression", "training", "topology"], "TL;DR": "We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.", "abstract": "Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. ", "pdf": "/pdf/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "paperhash": "isakov|neurofabric_identifying_ideal_topologies_for_training_a_priori_sparse_networks", "original_pdf": "/attachment/71c66a02a05f8d6fb73063ca72939b8242649090.pdf", "_bibtex": "@misc{\nisakov2020neurofabric,\ntitle={NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse Networks},\nauthor={Mihailo Isakov and Michel A. Kinsy},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCcCNtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgCcCNtwH", "replyto": "HJgCcCNtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1300/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575226975225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1300/Reviewers"], "noninvitees": [], "tcdate": 1570237739381, "tmdate": 1575226975236, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1300/-/Official_Review"}}}], "count": 11}