{"notes": [{"id": "rJxF73R9tX", "original": "Bkx26o69Km", "number": 1381, "cdate": 1538087969392, "ddate": null, "tcdate": 1538087969392, "tmdate": 1545355427558, "tddate": null, "forum": "rJxF73R9tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJxomwfNlE", "original": null, "number": 1, "cdate": 1544984354808, "ddate": null, "tcdate": 1544984354808, "tmdate": 1545354488445, "tddate": null, "forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Meta_Review", "content": {"metareview": "The reviewers felt that the method was natural and the writing was mostly clear (although could be improved by providing better signposting and fixing typos). However, there was also general agreement that comparison to other methods was weak; one reviewer also points out that the way that the reported numbers compare the methods on different sets of data, which might be an inaccurate measure of performance (this is more minor than the overall issue of lack of comparisons). While the authors provided more comparison experiments during the author response, it is in general the responsibility of authors to have a close-to-final work at the time of submission.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "natural idea but insufficient comparison to other methods"}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1381/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352861067, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1381/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1381/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352861067}}}, {"id": "ryxJqsOcAQ", "original": null, "number": 7, "cdate": 1543306119023, "ddate": null, "tcdate": 1543306119023, "tmdate": 1543306119023, "tddate": null, "forum": "rJxF73R9tX", "replyto": "B1l87E4-CQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "content": {"title": "On baselines and comparisons", "comment": "Thank you for your suggestions.\n\nPlease see updated Section 3.1 and 3.3 for risk coverage curves involving softmax thresholds and the selective guaranteed risk method described in [1]. We use the authors' implementation in [2]. \n\nThe updated results in Section 4 on CIFAR_10 and CIFAR-100 report the performance of the DAC on residual and wide residual networks. See Section 4, and Table 1.\n\nRegards,\nDAC Authors.\n\n[1] - Geifman, Yonatan, and Ran El-Yaniv. \"Selective classification for deep neural networks.\" Advances in neural information processing systems. 2017.\n\n[2] https://github.com/geifmany/selective_deep_learning\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607830, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxF73R9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1381/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1381/Authors|ICLR.cc/2019/Conference/Paper1381/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607830}}}, {"id": "H1ga8q_cCX", "original": null, "number": 6, "cdate": 1543305812856, "ddate": null, "tcdate": 1543305812856, "tmdate": 1543305812856, "tddate": null, "forum": "rJxF73R9tX", "replyto": "rJlVL2PuhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for the detailed comments and numerous pointers to existing work; these were very helpful. We have taken these into account in the updates to the paper.\n\n In particular, based on your suggestion, we have added [1] and [2] as baselines in the updated results in Section 4 (Table 1). We note (as we did in the summary above), that this is the most significant update to the paper. Using these and other comparisons, we present results in Section 4 that illustrate the strong performance benefits of the DAC in the label cleaning scenario.  As you suggested., we have also added discussion (Section 4) on the advantages of the DAC compared to numerous other existing works in this field.\n\nIn regards to your point 1., structured noise is an occurrence in real-world data in many scenarios. See for example the discussions in [3],[4] and [5] (which we have added to the paper). Also, in our own work with cancer data, we have seen  correlations between the features of the data and the reliability of the labels. The noise in these cases are seldom i.i.d.\n\nRegards,\nDAC Authors.\n\n\n[1] Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NIPS, 2018.\n\n[2] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n\n[3]Nico G\u00f6rnitz, Anne Porbadnigk, Alexander Binder, Claudia Sannelli, Mikio Braun, Klaus-Robert\nM\u00fcller, and Marius Kloft. Learning and evaluation in presence of non-iid label noise. In Artificial\nIntelligence and Statistics, pp. 293\u2013302, 2014.\n\n[4]Anne K Porbadnigk, Nico G\u00f6rnitz, Claudia Sannelli, Alexander Binder, Mikio Braun, Marius Kloft,\nand Klaus-Robert M\u00fcller. When brain and behavior disagree: Tackling systematic label noise in\neeg data with machine learning. In Brain-Computer Interface (BCI), 2014 International Winter\nWorkshop on, pp. 1\u20134. IEEE, 2014.\n\n[5]Carla E Brodley and Mark A Friedl. Identifying mislabeled training data. Journal of artificial\nintelligence research, 11:131\u2013167, 1999."}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607830, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxF73R9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1381/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1381/Authors|ICLR.cc/2019/Conference/Paper1381/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607830}}}, {"id": "rkgyC8D9Am", "original": null, "number": 3, "cdate": 1543300806856, "ddate": null, "tcdate": 1543300806856, "tmdate": 1543304053503, "tddate": null, "forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "content": {"title": "Summary of Updates to Paper", "comment": "The authors thank the reviewers and commenters for their feedback and actionable suggestions for the paper. Since the main concern raised was lack of comparisons to existing work, we mainly address that in the update to the paper. The most significant update in this regard is Section 4  \u2014learning in the presence of unstructured noise \u2014 as most existing works tackle this kind of noise. We compare to multiple baselines  and demonstrate the strong performance of the DAC in this setting (Section 4, Table 1) . \n\nThe DAC was originally conceived as a representation learner for structured noise (even though it has proved useful in other scenarios as well, as detailed in the paper). The beginning of section 3 has been updated with  discussions on the motivation and citations to relevant work that discuss this type of noise . Even though here are very few works addressing the issue of structured  noise in deep learning, as suggested by reviews and comments, we have added comparisons of the DAC to other  abstention mechanisms in this setting (Sections 3.1 and 3.2) . We also show how the noise learning property of the DAC can be used in conjunction with such mechanisms to improve predictive performance.\n\nIn summary, updated results indicate that the DAC is a very effective booster of performance in the presence of multiple types of noise.  The added performance gain as well as the simplicity of implementation makes it a strong contender for being part of a deep learning pipeline that involves learning in the presence of noise.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607830, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxF73R9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1381/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1381/Authors|ICLR.cc/2019/Conference/Paper1381/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607830}}}, {"id": "rkxlX7u9Am", "original": null, "number": 5, "cdate": 1543303959824, "ddate": null, "tcdate": 1543303959824, "tmdate": 1543303959824, "tddate": null, "forum": "rJxF73R9tX", "replyto": "Hklv2HpdhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your comments and suggestions on improving the paper.\n\nWe have added new comparisons to abstention  mechanisms based on softmax thresholding and selective guaranteed risk (see response above) in Sections 3.1 and 3.2. Results show the performance boost resulting from using the DAC in scenarios with structured noise. These include numerous accuracy-coverage curves for clearer comparisons.\n\nNumerous comparisons have also been added to the results in Section 4. Please see Table 1 and accompanying discussion.\n\nRegarding the need for smudging in the openset detection task (Section 6), please see caption for figure 5 (this was inadvertently missing in the first submission) and the discussion in Section 6 that illustrates the procedure. The training process of the DAC results in the smudge (or any fixed feature) being strongly associated with the abstention class, except in the presence of features of known classes.  In the latter case, the activation of the fixed feature is suppressed and class features are dominant. When class features are not present, the fixed feature is dominant. The filter visualizations in Figure 5 illustrate this phenomenon.\n\nDuring inference, the image to be classified is augmented with the fixed feature, and unless known class features suppress the activation of the fixed feature, the classification is always routed to the abstention class. One might think of the fixed feature as a \"feature threshold\" that needs to be overcome by an object from a known class to be recognized as one.\n\nNote, that it is merely a matter of convenience that we chose the same fixed feature (smudge) in the open set detection task as in Section 3.1. The feature can be any pattern that is not expected to occur in the images of interest.\n\nFinally, as you suggested, a layout description has been added at the end of Section 1 to better guide the reader.\n\nRegards,\nDAC Authors."}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607830, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxF73R9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1381/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1381/Authors|ICLR.cc/2019/Conference/Paper1381/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607830}}}, {"id": "SklpLawcAm", "original": null, "number": 4, "cdate": 1543302485079, "ddate": null, "tcdate": 1543302485079, "tmdate": 1543302485079, "tddate": null, "forum": "rJxF73R9tX", "replyto": "BklSo4p9nm", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your comments and suggestions on improving the paper.\n\n1. The loss function introduced in the DAC allows for a new way of learning in the presence of noise. By allowing an abstention option while training (which to the best of our knowledge, has not been explored elsewhere), the DAC is able to very effectively learn signals that are indicative of noise (this is the structured noise scenario) and improve classification performance.   We also provide an updated discussion on motivation at the beginning of section 3.\n\n2.  Please  see updated results and risk-coverage  plots in section 3.1 and 3.2 where we compare to other abstention mechanisms. In particular, we compare to softmax thresholds as well as the recently proposed selective guaranteed risk in [1]. While the DAC offers improved performance (in terms of accuracy and coverage) in these settings, it can also be used alongside these  methods for quantifying uncertainty.\n\n3. Typos, citations, formatting errors and missing captions have been fixed.\n\nIn addition new results, comparison to multiple baselines and discussion of existing works have been added to Section 4. \n\nRegards,\nDAC Authors.\n\n1] - Geifman, Yonatan, and Ran El-Yaniv. \"Selective classification for deep neural networks.\" Advances in neural information processing systems. 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607830, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxF73R9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1381/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1381/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1381/Authors|ICLR.cc/2019/Conference/Paper1381/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607830}}}, {"id": "B1l87E4-CQ", "original": null, "number": 1, "cdate": 1542698013643, "ddate": null, "tcdate": 1542698013643, "tmdate": 1542698013643, "tddate": null, "forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Public_Comment", "content": {"comment": "The idea presented in this paper is sound. However, I feel that the experimental part is a bit weak in the demonstration of the method performance itself, and it is more focused on presenting the properties and use-cases of the method (such as DAC as a data cleaner).\n\n- It would be interesting to see a direct comparison (in the sense of risk coverage curves) to [1] (a post training thresholding method). \n- Some other uncertainty estimation methods such as MC-dropout [2], KNN distance [3] and ensemble [4] can also be compared as a post-training thresholding uncertainty measure.\n-The VGG network for Cifar-10/100 is over-parameterized. It is interesting to see your results over a top performing architecture for these dataset (e.g., Wide residual networks or Dense-net) or even a modified version of VGG that have been adapted to these datasets.\n\n\n[1] - Geifman, Yonatan, and Ran El-Yaniv. \"Selective classification for deep neural networks.\" Advances in neural information processing systems. 2017.\n[2] - Gal, Yarin, and Zoubin Ghahramani. \"Dropout as a Bayesian approximation: Representing model uncertainty in deep learning.\" international conference on machine learning. 2016.\n[3] - Mandelbaum, Amit, and Daphna Weinshall. \"Distance-based Confidence Score for Neural Network Classifiers.\" arXiv preprint arXiv:1709.09844 (2017).\n[4] - Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. \"Simple and scalable predictive uncertainty estimation using deep ensembles.\" Advances in Neural Information Processing Systems. 2017.", "title": "Missing baselines and direct comparison to existing work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311610541, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rJxF73R9tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1381/Authors", "ICLR.cc/2019/Conference/Paper1381/Reviewers", "ICLR.cc/2019/Conference/Paper1381/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311610541}}}, {"id": "BklSo4p9nm", "original": null, "number": 3, "cdate": 1541227677069, "ddate": null, "tcdate": 1541227677069, "tmdate": 1541533181422, "tddate": null, "forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Official_Review", "content": {"title": "Good paper, writing and comparison need to be improved", "review": "The paper introduces a new loss function for training a deep neural network which can abstain.\nThe paper was easy to read, and they had thorough experiments and looked at their model performance in different angles (in existence of structured noise, in existence of unstructured noise and open world detection).  However, I think this paper has some issues which are listed below:\n\n\n1)  Although there are very few works regarding abstaining in DNN, I would like to see what the paper offers that is not addressed by the existing literature. Right now, in the experiment, there is no comparison to the previous work, and in the introduction, the difference is not clear. I think having an extra related work section regarding comparison would be useful.\n\n2) The experiment section was thorough, and the authors look at the performance of DAC at different angles; however, as far as I understand one of the significant contributions of the paper is to define abstain class during training instead of post-processing (e.g., abstaining on all examples where the network has low confidence). Therefore, I would like to see a better comparison to a network that has soft-max score cut-off rather than plain DNN. In figure 1-d the comparison is not clear since you did not report the coverage. I think it would be great if you can compare either with related work or tune a softmax-score on a validation set and then compare with your method. \n\n3) There are some typos, misuse of \\citet instead of \\citep spacing between parenthesis; especially in figures, texts overlap, the spacing is not correct, some figures don\u2019t have a caption, etc.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Official_Review", "cdate": 1542234242107, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1381/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335934486, "tmdate": 1552335934486, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hklv2HpdhX", "original": null, "number": 2, "cdate": 1541096879098, "ddate": null, "tcdate": 1541096879098, "tmdate": 1541533181222, "tddate": null, "forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Official_Review", "content": {"title": "Re: Abstention classifiers", "review": "This manuscript introduces deep abstaining classifiers (DAC) which modifies the multiclass cross-entropy loss with an abstention loss, which is then applied to perturbed image classification tasks.  The authors report improved classification performance at a number of tasks.\n\nQuality\n+ The formulation, while simple, appears justified, and the authors provide guidance on setting/auto-tuning the hyperparameter.\n+ Several different settings were used to demonstrate their modification.\n- There are no comparisons against other rejection/abstention classifiers or approaches.  Post-learning calibration and abstaining on scores that represent uncertainty are mentioned and it would strengthen the argument of the paper since this is probably the most straightforward altnerative approach, i.e., learn a NN, calibrate predictions, have it abstain where uncertain.\n- The comparison against the baseline NN should also include the performance of the baseline NN on the samples where DAC chose not to abstain, so that accuracies between NN and DAC are comparable. E.g. in Table 1, (74.81, coverage 1.000) and (80.09, coverage 0.895) have accuracies based on different test sets (partially overlapping).\n- The last set of experiments adds smudging to the out-of-set (open set) classification tasks.  It is somewhat unclear why smudging needs to be combined with this task.\n\nClarity\n- The paper could be better organized with additional signposting to guide the reader. \n\nOriginality\n+ Material is original to my knowledge.\n\nSignificance\n+ The method does appear to work reasonably and the authors provide detail in several use cases.\n- However, there are no direct comparison against other abstainers and the perturbations are somewhat artificial.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Official_Review", "cdate": 1542234242107, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1381/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335934486, "tmdate": 1552335934486, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlVL2PuhX", "original": null, "number": 1, "cdate": 1541073995976, "ddate": null, "tcdate": 1541073995976, "tmdate": 1541533181017, "tddate": null, "forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1381/Official_Review", "content": {"title": "Comparsion with \"Generalized cross entropy loss for training deep neural networks with noisy labels\".", "review": "This paper formulates a new deep method called deep abstaining classifer. Their main idea is to introduce a new modified loss function that utilizes an absention output allowing the DNN to learn when abstention is a better option. The core idea resemble KWIK framework [1], which has been theoretical justified.\n\nPros:\n\n1. The authors find a new direction for learning with noisy labels. Based on Eq. (1) (the modified loss), the propose \\alpha auto-tuning algorithm, which is relatively novel. \n\n2. The authors perform numerical experiments to demonstrate the efficacy of their framework. And their experimental result support their previous claims.\nFor example, they conduct experiments on CIFAR-10 and CIFAR-100. Besides, they conduct experiments on open-world detection dataset.\n\nCons:\n\nWe have three questions in the following.\n\n1. Clarity: in Section 3, the author claim real-world data is corrupted in some non-arbitrary manner. However, in practice, it is really hard to reason the corrpution procedure for agnostic noisy dataset like Clothing1M [2]. The authors are encouraged to explain this point more.\n\n2. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [3], estimating noise transition matrix [4,5], and explicit and implicit regularization [6]. I would appreciate if the authors can survey and compare more baselines in their paper.\n\n3. Experiment: \n3.1 Baselines: For noisy labels, the author should compare with [7] directly, which is highly related to your work. Namely, designing new loss function can overcome the issue of noisy labels. Without this comparison, the reported result has less impact. Moreover, the authors should add MentorNet [2] as a baseline https://github.com/google/mentornet\n\n3.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data. Besides, the authors are encouraged to conduct 1 NLP dataset.\n\nReferences:\n\n[1] L. Li, M. Littman, and T. Walsh. Knows what it knows: a framework for self-aware learning. In ICML, 2008.\n\n[2] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.\n\n[3] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n\n[4] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.\n\n[5] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.\n\n[6] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.\n\n[7] Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NIPS, 2018.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1381/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers", "abstract": "We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training. This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples. We show that such deep abstaining classifiers can: (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features; (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes. We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.", "keywords": ["deep learning", "robust learning", "abstention", "representation learning", "abstaining classifier", "open-set detection"], "authorids": ["sunil@lanl.gov", "tanmoy@lanl.gov", "bilmes@uw.edu", "gchennupati@lanl.gov", "jamal@lanl.gov"], "authors": ["Sunil Thulasidasan", "Tanmoy Bhattacharya", "Jeffrey Bilmes", "Gopinath Chennupati", "Jamal Mohd-Yusof"], "TL;DR": "A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.", "pdf": "/pdf/d748f17dd4c23d2a45b79db6b5b55b715f9d3a83.pdf", "paperhash": "thulasidasan|knows_when_it_doesnt_know_deep_abstaining_classifiers", "_bibtex": "@misc{\nthulasidasan2019knows,\ntitle={Knows When it Doesn\u2019t Know: Deep Abstaining Classifiers},\nauthor={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeffrey Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxF73R9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1381/Official_Review", "cdate": 1542234242107, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxF73R9tX", "replyto": "rJxF73R9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1381/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335934486, "tmdate": 1552335934486, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1381/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}