{"notes": [{"id": "zFM0Uo_GnYE", "original": "zsvrHceiPP", "number": 3592, "cdate": 1601308399302, "ddate": null, "tcdate": 1601308399302, "tmdate": 1614985750192, "tddate": null, "forum": "zFM0Uo_GnYE", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On the Importance of Looking at the Manifold", "authorids": ["~Nil_Adell_Mill1", "~Jannis_Born1", "npark@us.ibm.com", "~James_Hedrick1", "mrm@zurich.ibm.com", "~Matteo_Manica1"], "authors": ["Nil Adell Mill", "Jannis Born", "Nathaniel Park", "James Hedrick", "Mar\u00eda Rodr\u00edguez Mart\u00ednez", "Matteo Manica"], "keywords": ["Topological Learning", "GNN", "VAE"], "abstract": "Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mill|on_the_importance_of_looking_at_the_manifold", "one-sentence_summary": "A study on the importance of the topology in representation learning using implicit and explicit graph structure information.", "pdf": "/pdf/853de54c6bb4f30bbec31d0002795f927a1ab5f5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pV7iGD9KMj", "_bibtex": "@misc{\nmill2021on,\ntitle={On the Importance of Looking at the Manifold},\nauthor={Nil Adell Mill and Jannis Born and Nathaniel Park and James Hedrick and Mar{\\'\\i}a Rodr{\\'\\i}guez Mart{\\'\\i}nez and Matteo Manica},\nyear={2021},\nurl={https://openreview.net/forum?id=zFM0Uo_GnYE}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KdweH3AfXdd", "original": null, "number": 1, "cdate": 1610040385597, "ddate": null, "tcdate": 1610040385597, "tmdate": 1610473979079, "tddate": null, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "invitation": "ICLR.cc/2021/Conference/Paper3592/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "While the motivation of the paper is interesting the reviewers expressed concerns about the experimental setup, comparison to related work, and paper framing. For experiments, it was unclear why authors compared such disparate methods instead of more fine-grained adjustments (e.g., such as corrupting graphs as suggested by R3). For comparison, other methods such as Deep Walk and VGAE (as suggested by R1) seemed missing. I think the biggest issue however was with framing: as the reviewers pointed out, it was not clear enough how looking at downstream performance relates to looking at the manifold. In fact the paper title is much too general and is also well-known: manifold learning has been around for 15+ years. I would urge the authors to take the recommendations of reviewers and either design new experiments that explicitly target the manifold or reframe the paper to design new evaluation metrics for latent (possibly structured) generative models."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Importance of Looking at the Manifold", "authorids": ["~Nil_Adell_Mill1", "~Jannis_Born1", "npark@us.ibm.com", "~James_Hedrick1", "mrm@zurich.ibm.com", "~Matteo_Manica1"], "authors": ["Nil Adell Mill", "Jannis Born", "Nathaniel Park", "James Hedrick", "Mar\u00eda Rodr\u00edguez Mart\u00ednez", "Matteo Manica"], "keywords": ["Topological Learning", "GNN", "VAE"], "abstract": "Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mill|on_the_importance_of_looking_at_the_manifold", "one-sentence_summary": "A study on the importance of the topology in representation learning using implicit and explicit graph structure information.", "pdf": "/pdf/853de54c6bb4f30bbec31d0002795f927a1ab5f5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pV7iGD9KMj", "_bibtex": "@misc{\nmill2021on,\ntitle={On the Importance of Looking at the Manifold},\nauthor={Nil Adell Mill and Jannis Born and Nathaniel Park and James Hedrick and Mar{\\'\\i}a Rodr{\\'\\i}guez Mart{\\'\\i}nez and Matteo Manica},\nyear={2021},\nurl={https://openreview.net/forum?id=zFM0Uo_GnYE}\n}"}, "tags": [], "invitation": {"reply": {"forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040385583, "tmdate": 1610473979062, "id": "ICLR.cc/2021/Conference/Paper3592/-/Decision"}}}, {"id": "gj1W5-VatE", "original": null, "number": 4, "cdate": 1604277339501, "ddate": null, "tcdate": 1604277339501, "tmdate": 1606780273549, "tddate": null, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "invitation": "ICLR.cc/2021/Conference/Paper3592/-/Official_Review", "content": {"title": "The idea is interesting but the method and experiments are not convincing", "review": "**Summary**:\nThis paper investigates different ways of incorporating topological information about the data in the machine learning models. The paper introduces a novel loss that aims to enforce the relational information between data points into the embedding space learned by a Vae on the node features. The experiments demonstrate that for data with a certain topology type, the introduced loss can provide performance when used together with existing methods. The paper opens up possibilities of further investigation into incorporating topological information (if available) into the learning procedure.\n\n**Pros**:\n1. The paper is very well-written and easy to follow. The illustrations also present the idea clearly.\n2. The problem of understanding the importance of topological information is interesting, and could lead to future works.\n\n**Cons**:\n1. I am not sure if there is enough novelty for acceptance to ICLR, especially when the proposed method does not provide obvious benefit over existing methods, both in theory and practice. Specifically, the GR-VAE is a simple extension of VAE which does not yield good performance, unless it\u2019s combined with DGI. Even in that case, it is not obvious to me the benefit is significant except in one case (namely for text representations), but when combined with DGI, it becomes unclear whether the performance boost is actually coming from the proposed loss or some other unintended regularization effect since DGI already uses message passing to incorporate the (local) topological structure. Am I missing something here?\n2. Since the paper is positioned to be an experimental study, it is perhaps acceptable to have limited novelty or improvement. However, the findings in the papers are somewhat expected. For example, we already know that GNN [1] based methods are superior to other methods on citation benchmarks since they account for both features and topology. In this light, I feel there is not too much new insight in the paper to warrant a publication at ICLR.\n3. For an empirical study, considering only four models may not be enough (e.g. different GNNs / graph models encode different topological information and that should be taken into account for a full spectrum). The same holds for feature-based methods. Some examples of this are Deep walk [3], GNN architecture or different VAE architecture and loss (especially there are so many variations of VAE). Most importantly, a comparison to [2] is missing.\n\n**Comment**:\n1. \u201cRegularized\u201d -> \u201cregularizer\u201d in conclusion line 6 paragraph 2?\n2. For a double-blind reviewing process, the funding information should perhaps be removed, although the one in this paper does not expose the authors' identity.\n\n**Conclusion**:\nWhile the work has interesting motivations and is well-written, it has not done a convincing job at demonstrating the effectiveness of their proposed method or shown a thorough experimental analysis. As such, I am inclined to reject the paper in its current form.\n\n**Reference**:\n\n[1] Semi-Supervised Classification with Graph Convolutional Networks (https://arxiv.org/abs/1609.02907)\n\n[2] Variational Graph Auto-Encoder (https://arxiv.org/abs/1611.07308)\n\n[3] Deepwalk: Online learning of social representations (https://arxiv.org/abs/1403.6652)\n\n\n**================================== Update after rebuttal ==================================**\n\nIt seems that the authors have only provided a general comment for all reviewers, which is understandable since most criticisms from all reviewers are on the same weaknesses of the paper.\nWhile I appreciate the author's effort in adding more experiments, I do not think the added experiments and reply properly addressed the concerns shared by other reviewers and myself. For example, it is still unclear what the advantage of the proposed method is or what insights we could gain from this study.\nI think this paper is not ready for publication. As today is the last day of discussion period, I will maintain my original assessment.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3592/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3592/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Importance of Looking at the Manifold", "authorids": ["~Nil_Adell_Mill1", "~Jannis_Born1", "npark@us.ibm.com", "~James_Hedrick1", "mrm@zurich.ibm.com", "~Matteo_Manica1"], "authors": ["Nil Adell Mill", "Jannis Born", "Nathaniel Park", "James Hedrick", "Mar\u00eda Rodr\u00edguez Mart\u00ednez", "Matteo Manica"], "keywords": ["Topological Learning", "GNN", "VAE"], "abstract": "Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mill|on_the_importance_of_looking_at_the_manifold", "one-sentence_summary": "A study on the importance of the topology in representation learning using implicit and explicit graph structure information.", "pdf": "/pdf/853de54c6bb4f30bbec31d0002795f927a1ab5f5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pV7iGD9KMj", "_bibtex": "@misc{\nmill2021on,\ntitle={On the Importance of Looking at the Manifold},\nauthor={Nil Adell Mill and Jannis Born and Nathaniel Park and James Hedrick and Mar{\\'\\i}a Rodr{\\'\\i}guez Mart{\\'\\i}nez and Matteo Manica},\nyear={2021},\nurl={https://openreview.net/forum?id=zFM0Uo_GnYE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3592/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073048, "tmdate": 1606915768304, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3592/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3592/-/Official_Review"}}}, {"id": "ZM9lGrL9s9A", "original": null, "number": 3, "cdate": 1606263953326, "ddate": null, "tcdate": 1606263953326, "tmdate": 1606263953326, "tddate": null, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "invitation": "ICLR.cc/2021/Conference/Paper3592/-/Official_Comment", "content": {"title": "Answer to the reviewers", "comment": "Answer to all reviewers.\n\nFirst of all we thank all the reviewers for the time and effort dedicated to reading and reviewing our paper. Seeing that several cons were shared across reviewers we decided to address them together.\n\nThe intent of the paper was to study the influence of the topology across the scale shown in the paper. The addition of the GR-VAE is motivated by the intent of filling a particular shade of that spectrum.\n\nWe appreciate the recommendation of including more more models into the study. We added text baselines for GAE and GraphSAGE in the text study, and we are aware that the study could be further expanded with more models. Furthermore, we expanded the text classification results for the DGI using subsampled graphs (i.e. same sampling method by which GR-VAE is trained) and incorporating the representations of the variational autoencoders as features.\n\nWe want to make the same point about the datasets, it has been commented that downstream prediction tasks may not be the optimal comparison method. It was a direct way for us to asses the goodness of the representations in respect to a specific task, however we are aware that can in itself be limiting. We welcome the different suggestions done in the comments (e.g. controlling adjacency or corrupting the input graphs) and we will be looking at them.\n\nFinally, just reiterate our thanks for the reviewers' comments and time invested in looking into our work. All the feedback we received is highly welcome and a helpful light on how to improve and iterate this work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3592/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3592/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Importance of Looking at the Manifold", "authorids": ["~Nil_Adell_Mill1", "~Jannis_Born1", "npark@us.ibm.com", "~James_Hedrick1", "mrm@zurich.ibm.com", "~Matteo_Manica1"], "authors": ["Nil Adell Mill", "Jannis Born", "Nathaniel Park", "James Hedrick", "Mar\u00eda Rodr\u00edguez Mart\u00ednez", "Matteo Manica"], "keywords": ["Topological Learning", "GNN", "VAE"], "abstract": "Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mill|on_the_importance_of_looking_at_the_manifold", "one-sentence_summary": "A study on the importance of the topology in representation learning using implicit and explicit graph structure information.", "pdf": "/pdf/853de54c6bb4f30bbec31d0002795f927a1ab5f5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pV7iGD9KMj", "_bibtex": "@misc{\nmill2021on,\ntitle={On the Importance of Looking at the Manifold},\nauthor={Nil Adell Mill and Jannis Born and Nathaniel Park and James Hedrick and Mar{\\'\\i}a Rodr{\\'\\i}guez Mart{\\'\\i}nez and Matteo Manica},\nyear={2021},\nurl={https://openreview.net/forum?id=zFM0Uo_GnYE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "zFM0Uo_GnYE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3592/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3592/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3592/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3592/Authors|ICLR.cc/2021/Conference/Paper3592/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3592/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835886, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3592/-/Official_Comment"}}}, {"id": "cQ9A4lkx6AM", "original": null, "number": 3, "cdate": 1603898270909, "ddate": null, "tcdate": 1603898270909, "tmdate": 1605023971923, "tddate": null, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "invitation": "ICLR.cc/2021/Conference/Paper3592/-/Official_Review", "content": {"title": "Review for On the Importance of Looking at the Manifold: Reject", "review": "Summary:\n\nThe authors present a regularisation term for Variational Auotencoders that forces the distance of mapped points in the embedding space to be similar to the distance of those points in the metagraph of the data derived from relational information about these points. The intention of the regularisation term is to enforce a consistent graph between the original representation and the embedded representation in a manner that is agnostic to the structural choices of the model to be estimated. \n\nStrengths:\n\n1) The paper provides a good organisation of existing methods for utilising topological information, and, thus, positions its contributions well in relation to existing work.\n\n2) The empirical results are presented honestly, even when they do not support the proposed method.\n\nWeaknesses:\n\nOrdered form less to more specific:\n\n1) The intention of the paper is unclear.\n\tIs this intended as a review paper of recent research on graph neural networks or to present a new regularisation term? The title and abstract seem to imply that this is intended as a review, but the paper only considers three existing methods and a single modification to VAEs. Unfortunately, the paper is not convincing in either regard, and the idea of analysing topological information to improve classifications and representations is already well discussed in the literature and a very active area of ongoing research.\n\n2)  The efficacy of the proposed regularisation is not convincingly supported either theoretically or empirically.\n\tThe authors state, \u2018Notably, GR-VAE is devised to infer topological information solely from a soft constraint, without any architectural requirements such as graph convolutions\u2019 (line 1, p. 4), but this is not discussed further. The intention of graph convolutions is to explicitly encode assumptions about the relationships present in the data, in this situation why would I prefer a soft constraint to a well motivated, explicit one? If structural constraints were unduly restricting the expressiveness of the models, I would expect to see this borne out in the empirical results, but this is not the case across the chemical reaction and citation network experiments. \n\n3) The paper struggles with clarity at points.\n\tSpecifically, equation (1), which describes the regularisation term is unclear as written: does the plus sign in the exponent denote absolute value or something else? This notation is non-standard and I would not be able to faithfully recreate the results as written.  \n\tAdditionally, the method for constructing the meta-graph G should be discussed in more detail. From what is this graph derived? Is it an existing observed graph that describes the observed relationships between the data, ex. the citation network or pixels in an image, or does it describe adjacency of the observations as defined by the observed labels or other meta-information. My concern is that using a soft-constraint which effectively focusses the model on the labels in an \u2018easy\u2019 task such as MNIST classification or the synthetic data task hides the fact that the constraint is too soft to produce a useful regularisation of the model, as evidenced by the failure of GR-VAE relative to DGI or even vanilla VAE in the citation network and chemical reactions tasks. \n\nReasons for score: \n\nI vote for rejecting the paper, as while I really do appreciate that the results of the paper are presented honestly, I think there are concerns with the current draft. \n\nQuestions for the rebuttal period:\n\nPlease refer to the questions in the weaknesses section. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3592/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3592/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Importance of Looking at the Manifold", "authorids": ["~Nil_Adell_Mill1", "~Jannis_Born1", "npark@us.ibm.com", "~James_Hedrick1", "mrm@zurich.ibm.com", "~Matteo_Manica1"], "authors": ["Nil Adell Mill", "Jannis Born", "Nathaniel Park", "James Hedrick", "Mar\u00eda Rodr\u00edguez Mart\u00ednez", "Matteo Manica"], "keywords": ["Topological Learning", "GNN", "VAE"], "abstract": "Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mill|on_the_importance_of_looking_at_the_manifold", "one-sentence_summary": "A study on the importance of the topology in representation learning using implicit and explicit graph structure information.", "pdf": "/pdf/853de54c6bb4f30bbec31d0002795f927a1ab5f5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pV7iGD9KMj", "_bibtex": "@misc{\nmill2021on,\ntitle={On the Importance of Looking at the Manifold},\nauthor={Nil Adell Mill and Jannis Born and Nathaniel Park and James Hedrick and Mar{\\'\\i}a Rodr{\\'\\i}guez Mart{\\'\\i}nez and Matteo Manica},\nyear={2021},\nurl={https://openreview.net/forum?id=zFM0Uo_GnYE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3592/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073048, "tmdate": 1606915768304, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3592/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3592/-/Official_Review"}}}, {"id": "EmdvrOVHYId", "original": null, "number": 2, "cdate": 1603860336634, "ddate": null, "tcdate": 1603860336634, "tmdate": 1605023971858, "tddate": null, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "invitation": "ICLR.cc/2021/Conference/Paper3592/-/Official_Review", "content": {"title": "A well-written paper, idea is simple, but experiment might be insufficient", "review": "==== Summary ==== \n\nThis paper proposes a variant of Variational Autoencoders (VAEs) which takes extra topological information (e.g. adjacency matrix) into account in the loss function during training. The principle objective of the proposed GR-VAEs is to use the learned latent space features as the input for improvement on downstream tasks especially for classification. \n\n==== Pros ====\n\n+ This paper is well-written and the idea is clear and easy to follow.\n+ The proposed GR-VAE, specifically the extra loss function on preserving the geodesic distance seems to be effective for learning desired latent space features from experiments.\n\n==== Cons ====\n\nMy main concern is on the experiment on showing the improvement for downstream tasks from the embedding learned by GR-VAE.\n\n-  I think the goal of the experiment is to demonstrate that, the embedding learned by GR-VAE is superior comparing to other kinds of features such as from the vanilla VAE, so I expect the embedding by GR-VAE is applied to different models (e.g. GraphSAGE, GCN, DeepWalk) instead of just DGI (at least I only notice that DGI is adapted), and on each model the result from using GR-VAE can outperform others using raw data features or other kinds of finetuned features. I think showing the improvement on different models can greatly enhance the soundness of this paper.\n\n- In addition, for the experiment on chemical reaction I think there should be another baseline showing the result of DGI by using the raw data features if possible, this can further demonstrate the importance of GR-VAE. Also, for the experiment on text representation I also expect some result like using finetuned GR-VAE as the input for DGI in the chemical reaction experiment, currently the text representation experiment only shows the strength of the DGI itself, which does not make sense to me.\n\n==== Reason for scoring ====\n\nOverall, I think the proposed GR-VAE is sound if its strength can be demonstrated by more experiment mentioned above, and I am willing to upgrade my rating and vote to accept if such concern can be addressed during the rebuttal period.\n\n==== Minor Comments ====\n\n- The plots in Figure 3 is too blurry to distinguish between the cross marker and round marker.\n- I notice for the synthetic dataset the direction of edges for each node is used as part of the input features, so what is the definition for the edge direction? Also, if we directly combine the raw data feature with embedding by some manifold learning technique, and input it into the vanilla VAE, can we get similar result (the graph topology is preserved) as GR-VAE has? \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3592/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3592/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Importance of Looking at the Manifold", "authorids": ["~Nil_Adell_Mill1", "~Jannis_Born1", "npark@us.ibm.com", "~James_Hedrick1", "mrm@zurich.ibm.com", "~Matteo_Manica1"], "authors": ["Nil Adell Mill", "Jannis Born", "Nathaniel Park", "James Hedrick", "Mar\u00eda Rodr\u00edguez Mart\u00ednez", "Matteo Manica"], "keywords": ["Topological Learning", "GNN", "VAE"], "abstract": "Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mill|on_the_importance_of_looking_at_the_manifold", "one-sentence_summary": "A study on the importance of the topology in representation learning using implicit and explicit graph structure information.", "pdf": "/pdf/853de54c6bb4f30bbec31d0002795f927a1ab5f5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pV7iGD9KMj", "_bibtex": "@misc{\nmill2021on,\ntitle={On the Importance of Looking at the Manifold},\nauthor={Nil Adell Mill and Jannis Born and Nathaniel Park and James Hedrick and Mar{\\'\\i}a Rodr{\\'\\i}guez Mart{\\'\\i}nez and Matteo Manica},\nyear={2021},\nurl={https://openreview.net/forum?id=zFM0Uo_GnYE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3592/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073048, "tmdate": 1606915768304, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3592/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3592/-/Official_Review"}}}, {"id": "caC1m8LsR8P", "original": null, "number": 1, "cdate": 1603733585112, "ddate": null, "tcdate": 1603733585112, "tmdate": 1605023971793, "tddate": null, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "invitation": "ICLR.cc/2021/Conference/Paper3592/-/Official_Review", "content": {"title": "The paper studies the importance of utilising manifold/topology information in prediction tasks. The method is not novel enough and the comparison seems problematic.", "review": "The paper focuses on studying the importance of utilising manifold/topology information for machine learning tasks. To this end, the authors benchmark four different approaches, including VAE, GR-VAE (using graph distances to regularise  embedding distances (as shown in Eq. 1)).  The paper performs experiments on four tasks, including synthetic data, MNIST, text representation, and chemical reactions. As conclusion, the paper demonstrates that in some cases, adding relational information is beneficial, while in other cases, the effect is subtle. Thus, the paper aims to provide a metric for understanding when and how manifold/topology information is needed. \n\nPros:\n\n1. Instead explicitly learning a graph, this paper proposes an implicit method with graph regularisation.\n\n2. The related work is well-explained. The paper did a well summarization of previous methods.\n\n3. The paper performs extensive experiments to study the importance of manifold for prediction tasks.\n\nCons:\n\n1. The latent graph method is not novel enough, since the method can be categorised as graph regularisation, which is a widely used method in recommendation and information retrieval. Could the author explain why this regularisation is picked from a spectrum of graph regularisation algorithms?\n\n2. The comparison of the paper is problematic. First, the methods (DGI, node2vec, GR-VAE, VAE) compared are quite different methods. Can the authors confirm that the comparison is fair and meaningful (e.g. eliminating other confounding factors like controlling the number of parameters)? Second, I am not sure whether this comparison is optimal. In particular, to study the importance of relational information, other method can be used. For example, we can control adjacency matrix received by graph neural networks. We can totally ignore the edge information (like VAE in the paper) or use a predefined graph (e.g. a fully connected graph like in Transformer). In between, we can corrupt input graphs (e.g. randomly adding or deleting some edges) before feeding it to graph neural networks. This approach seems more reasonable to me for studying the importance of manifold. It is difficult to control these in this paper because the methods used in this paper are totally different  (e.g. DGI and GR-VAE differs in both loss function and input format). So, the conclusion of the paper is skeptical. It mainly justifies which method can perform better in downstream tasks instead of justifying the importance of manifold. \n\n3. The introduction is lengthy and should be more focused on the contribution of this paper. Similarly, the other sections need a major revision to highlight the contribution, as the main contribution of the paper lies in the implicit graph regularisation and a comparison of a series of methods with/without relational information.\n\n4. Some baseline methods are not considered, for example the methods learning latent graphs: Semi-supervised classification with graph convolutional networks and Glomo: Unsupervised learning of transferable relational graphs.\n\n5. The acknowledgement of the paper reveals location information, which may be a violation of anonymity. \n\nBased on these cons, I think a more rigorous comparison is needed. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3592/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3592/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Importance of Looking at the Manifold", "authorids": ["~Nil_Adell_Mill1", "~Jannis_Born1", "npark@us.ibm.com", "~James_Hedrick1", "mrm@zurich.ibm.com", "~Matteo_Manica1"], "authors": ["Nil Adell Mill", "Jannis Born", "Nathaniel Park", "James Hedrick", "Mar\u00eda Rodr\u00edguez Mart\u00ednez", "Matteo Manica"], "keywords": ["Topological Learning", "GNN", "VAE"], "abstract": "Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mill|on_the_importance_of_looking_at_the_manifold", "one-sentence_summary": "A study on the importance of the topology in representation learning using implicit and explicit graph structure information.", "pdf": "/pdf/853de54c6bb4f30bbec31d0002795f927a1ab5f5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pV7iGD9KMj", "_bibtex": "@misc{\nmill2021on,\ntitle={On the Importance of Looking at the Manifold},\nauthor={Nil Adell Mill and Jannis Born and Nathaniel Park and James Hedrick and Mar{\\'\\i}a Rodr{\\'\\i}guez Mart{\\'\\i}nez and Matteo Manica},\nyear={2021},\nurl={https://openreview.net/forum?id=zFM0Uo_GnYE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "zFM0Uo_GnYE", "replyto": "zFM0Uo_GnYE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3592/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073048, "tmdate": 1606915768304, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3592/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3592/-/Official_Review"}}}], "count": 7}