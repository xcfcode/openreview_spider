{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396343854, "tcdate": 1486396343854, "number": 1, "id": "rJe6ofIug", "invitation": "ICLR.cc/2017/conference/-/paper79/acceptance", "forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The program committee appreciates the authors' response to the clarifying questions. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396344386, "id": "ICLR.cc/2017/conference/-/paper79/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396344386}}}, {"tddate": null, "tmdate": 1482235401839, "tcdate": 1482235401839, "number": 3, "id": "rJfGR9LEg", "invitation": "ICLR.cc/2017/conference/-/paper79/official/review", "forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "signatures": ["ICLR.cc/2017/conference/paper79/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper79/AnonReviewer3"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is interesting, and it tries to solve an important problem. However, it seems to me the paper is not ready to publish yet.\n\nComments:\n\n- The first section of the paper is clear and well-motivated. \n\n- The authors should report test running time. The proposed approach changes the weight matrix. As a result, it needs to reevaluate the values of hidden states and perform the greedy search for each iteration of optimizing Eq (7). This is actually pretty expensive in comparison to running the beam search or other inference methods. Therefore, I'm not convinced that the proposed approach is a right direction for solving this problem (In table, 1, the authors mention that they run 100 steps of SGD).   \n\n- If I understand correctly, Eq (7) is a noncontinuous function w.r.t W_\\lambda and the simple SGD algorithm will not be able to find its minimum.\n\n- For dependency parsing, there are standard splits of PTB. I would suggest the authors follow the same splits of train, dev, and test in order to compare with existing results. \n\n\nMinor comments: several sentences are misleading and should be rewritten carefully. \n\n- Beginning of Section 3: \"A major advantage of neural network is that once trained, inference is extremely efficient.\" This sentence is not generally right,  and I guess the authors mean if using greedy search as inference method, the inference is efficient. \n\n- The description in the end of section 2 is awkward. To me,  feed-forward and RNN  are general families that cover many specific types of neural networks, and the training procedures are not necessarily to aim to optimize Eq. (2). Therefore, the description here might not be true. In fact, I don't think there is a need to bring up feed-forward networks here; instead, the authors should provide more details the connection between RNN and Eq (2) here.\n\n- The second paragraph of section 3 is related to [1], where it shows the search space of the inference can be represented as an imperative program. \n\t\n\n\n[1] Credit assignment compiler for joint prediction, NIPS 2016\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512704784, "id": "ICLR.cc/2017/conference/-/paper79/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper79/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper79/AnonReviewer2", "ICLR.cc/2017/conference/paper79/AnonReviewer1", "ICLR.cc/2017/conference/paper79/AnonReviewer3"], "reply": {"forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512704784}}}, {"tddate": null, "tmdate": 1481935872333, "tcdate": 1481935872333, "number": 2, "id": "rk_Zn-G4x", "invitation": "ICLR.cc/2017/conference/-/paper79/official/review", "forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "signatures": ["ICLR.cc/2017/conference/paper79/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper79/AnonReviewer1"], "content": {"title": "Not very convincing", "rating": "3: Clear rejection", "review": "This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems, while keeping inference unconstrained. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured prediction energy networks (SPENs), recently proposed by Belanger et al. \n\nOverall, I didn't find the approach very convincing and the paper has a few problems regarding the empirical evaluation. There's also some imprecisions throughout. The proposed approach (secs 6 and 7) looks more like a \"little hack\" to try to make it vaguely similar to Lagrangian relaxation methods than something that is theoretically well motivated.\n\nBefore eq. 6: \"an exponential number of dual variables\" -- why exponential? it's not one dual variable per output.\n\nFrom the clarification questions:\n- The accuracy reported in Table 1 needs to be explained. \n- for the parsing experiments it would be good to report the usual F1 metric of parseval, and to compare with state of the art systems.\n- should use the standard training/dev/test splits of the Penn Treebank.\nThe reported conversion rate in Table 1 does not tell us how many violations are left by the unconstrained decoder to start with. It would be good to know what happens in highly structured problems where these violations are frequent, since these are the problems where the proposed approach could be more beneficial.\n\n\nMinor comments/typos:\n- sec.1: \"there are\" -> there is?\n- sec 1: \"We find that out method is able to completely satisfy constraints on 81% of the outputs.\" -> at this point, without specifying the problem, the model, and the constraints, this means very little. How many constrains does the unconstrained method satisfies?\n- sec 2 (last paragraph): \"For RNNs, each output depends on hidden states that are functions of previous output values\" -- this is not very accurate, as it doesn't hold for general RNNs, but only for those (e.g. RNN decoders in language modeling) where the outputs are fed back to the input in the next time frame. \n- sec 3: \"A major advantage of neural networks is that once trained, inference is extremely efficient.\" -- advantage over what? also, this is not necessarily true, depends on the network and on its size.\n- sec 3: \"our goal is take advantage\" -> to take advantage\n- last paragraph of sec 6: \"the larger model affords us\" -> offers?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512704784, "id": "ICLR.cc/2017/conference/-/paper79/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper79/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper79/AnonReviewer2", "ICLR.cc/2017/conference/paper79/AnonReviewer1", "ICLR.cc/2017/conference/paper79/AnonReviewer3"], "reply": {"forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512704784}}}, {"tddate": null, "tmdate": 1481828340535, "tcdate": 1481828340530, "number": 1, "id": "H13e_DgNg", "invitation": "ICLR.cc/2017/conference/-/paper79/official/review", "forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "signatures": ["ICLR.cc/2017/conference/paper79/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper79/AnonReviewer2"], "content": {"title": "Reject", "rating": "3: Clear rejection", "review": "This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems.\n\nMany things don't quite make sense to me:\n 1. Most seq2seq models (such as those used for parsing) have substantially better performance when coupled with beam search than greedy search, and exact search is infeasible. This is because these models are trained to condition on discrete values of past outputs in each timestamp, and hence the problem of finding the highest-scoring total sequence of outputs is not solvable efficiently. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-of-the-art, specially when constraint-aware beam search is used. This comparison is specially interesting because both constrained beam search and this dual-decomposition-like approach require multiple computations of the model's score.\n 2. It's unclear (to me at least) how to differentiate the constraint term g() in the objective function in the general case (though the particular example used here is understandable)\n 3. The paper claims that \"Lagrangian relaxation methods for NLP have multipliers for each output variable that can be combined with linear models [...] . Since our non-linear functions and global constraints do not afford us the same ability\" but it is possible to add linear terms to the outputs of neural networks, possibly avoiding rerunning all the expensive inference terms.\n\nMoreover, the justification for the particular method is hand-wavy at best, with inconvenient terms from equations ignored or changed at will. At this point it might be better to omit the attempted theoretical explanation and just present this method as a heuristic which is likely to achieve the desired result.\n\nThis, plus the concerns around lack of clear comparisons with baselines on benchmark problems lead me to recommend rejection. Further explanation of how this compares with beam search, how this relates to the state-of-the-art, and a better explanation for how to come up with differentiable constraint sets, are probably required for acceptance.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512704784, "id": "ICLR.cc/2017/conference/-/paper79/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper79/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper79/AnonReviewer2", "ICLR.cc/2017/conference/paper79/AnonReviewer1", "ICLR.cc/2017/conference/paper79/AnonReviewer3"], "reply": {"forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512704784}}}, {"tddate": null, "tmdate": 1481056567811, "tcdate": 1481056567806, "number": 2, "id": "HkgS-iNXl", "invitation": "ICLR.cc/2017/conference/-/paper79/public/comment", "forum": "S1Jhfftgx", "replyto": "HyYmyP1Qe", "signatures": ["~Michael_L_Wick1"], "readers": ["everyone"], "writers": ["~Michael_L_Wick1"], "content": {"title": "Re: About the parsing algorithms used", "comment": "Thanks for your questions. To clarify, in case there is a misunderstanding, we are not operating a classic shift-reduce parser, rather we are employing shift-reduce commands as a way of linearizing the parser tree for a sequence-to-sequence network (similar to Vinyals 2015a). As you suggest, in the spirit of a shift-reduce parser, we could have enforced the constraints as we decode at each time-step, but this can fail as seen by the examples in Table 3 and 5. Specifically, although greedily enforcing constraints would have resulted in a feasible output, the output would still contain errors because mistakes were made early in the sequence.\n\nOur method readily generalizes beyond CFLs as long as you can express a computable constraint function g that has the desired properties (0 when there is no violation and positive otherwise).  So yes, the approach can be applied to non-projective trees and generalizes beyond CFLs (note for example, CFLs are not closed under intersection so combining two g's is another alternative).  We had thought about casting the approach in a more general way, but decided that CFLs are nice because (1) they are well understood (2) there are existing algorithms for finding grammatical errors in CFLs (3) we find that as we apply the networks to programming language (PL) problems, CFLS are much more convenient (than FOL) for expressing the constraints of formal languages such as Python/Java etc.  (4) for some CFLs it is possible to identify multiple grammatical errors, affording us with the opportunity to employ gradients targeted directly at the specific errors (perhaps in future work). Finally, for the gradient, we treat g as a constant and compute the gradient of \\Psi w.r.t. W_\\lambda.  Since \\Psi is given by the neural net we safely assume it is differentiable.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287737573, "id": "ICLR.cc/2017/conference/-/paper79/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1Jhfftgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper79/reviewers", "ICLR.cc/2017/conference/paper79/areachairs"], "cdate": 1485287737573}}}, {"tddate": null, "tmdate": 1481056402874, "tcdate": 1481056402868, "number": 1, "id": "r1o9loVXx", "invitation": "ICLR.cc/2017/conference/-/paper79/public/comment", "forum": "S1Jhfftgx", "replyto": "rkheGYJQl", "signatures": ["~Michael_L_Wick1"], "readers": ["everyone"], "writers": ["~Michael_L_Wick1"], "content": {"title": "Re: A few questions", "comment": "Thanks for your questions.  We agree that these metrics would be good to report and will update the paper with the portion of initially violated sentences (21% for parsing), the evalb accuracy (80% on all sentences, 60.9% on violated sentences corrected via post-processing 69% on violated sentences corrected via the proposed method) and information about how we compute accuracy. To clarify, basic token-wise accuracy for a given system output is proportional to the number of time-steps at which the system output agrees with the true output. The accuracy is normalized by the longer of the two sequences (thus effectively penalizing any differences in length). Aligned accuracy is [1 - normalized edit distance], which inherently handles different lengths.\n\nIn retrospect, employing the standard split would have had the additional benefit of facilitating comparison with other parsers.  This was an oversight on our part, but does not change the conclusions of the paper: for the purpose of studying the behavior of the constraint-enforcing algorithm, any reasonable split of the data suffices.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287737573, "id": "ICLR.cc/2017/conference/-/paper79/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1Jhfftgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper79/reviewers", "ICLR.cc/2017/conference/paper79/areachairs"], "cdate": 1485287737573}}}, {"tddate": null, "tmdate": 1480720884249, "tcdate": 1480720884245, "number": 2, "id": "rkheGYJQl", "invitation": "ICLR.cc/2017/conference/-/paper79/pre-review/question", "forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "signatures": ["ICLR.cc/2017/conference/paper79/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper79/AnonReviewer1"], "content": {"title": "A few questions", "question": "What is the accuracy reported in Table 1? Since the output is of variable length, it's not clear how this accuracy is computed. \n\nAlso, for the parsing experiments it would be good to report the usual F1 metric of parseval, and to compare with state of the art systems. Why didn't you use the standard training/dev/test splits of the Penn Treebank?\n\nThe reported conversion rate in Table 1 does not tell us how many violations are left by the unconstrained decoder to start with. It would be good to know what happens in highly structured problems where these violations are frequent, since these are the problems where the proposed approach could be more beneficial."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959474811, "id": "ICLR.cc/2017/conference/-/paper79/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper79/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper79/AnonReviewer2", "ICLR.cc/2017/conference/paper79/AnonReviewer1"], "reply": {"forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959474811}}}, {"tddate": null, "tmdate": 1480711969192, "tcdate": 1480711969188, "number": 1, "id": "HyYmyP1Qe", "invitation": "ICLR.cc/2017/conference/-/paper79/pre-review/question", "forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "signatures": ["ICLR.cc/2017/conference/paper79/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper79/AnonReviewer2"], "content": {"title": "About the parsing algorithms used", "question": "My understanding is that most shift/reduce parsers have a set of valid actions at each point in time, and are hence already constrained not to produce invalid sequences. \n\nAlso the choice of context-free languages for constraints is surprising, since I assume it'd rule out non-projective parsing. Can the paper clarify whether this is the case?\n\nThe specific gradient of the constrained loss and how it is computed can be clarified as well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959474811, "id": "ICLR.cc/2017/conference/-/paper79/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper79/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper79/AnonReviewer2", "ICLR.cc/2017/conference/paper79/AnonReviewer1"], "reply": {"forum": "S1Jhfftgx", "replyto": "S1Jhfftgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper79/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959474811}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478622664225, "tcdate": 1478202023091, "number": 79, "id": "S1Jhfftgx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1Jhfftgx", "signatures": ["~Michael_L_Wick1"], "readers": ["everyone"], "content": {"title": "Enforcing constraints on outputs with unconstrained inference", "abstract": "  Increasingly, practitioners apply neural networks to complex\n  problems in natural language processing (NLP), such as syntactic\n  parsing, that have rich output structures. Many such applications\n  require deterministic constraints on the output values; for example,\n  requiring that the sequential outputs encode a valid tree. While\n  hidden units might capture such properties, the network is not\n  always able to learn them from the training data alone, and\n  practitioners must then resort to post-processing. In this paper, we\n  present an inference method for neural networks that enforces\n  deterministic constraints on outputs without performing\n  post-processing or expensive discrete search over the feasible\n  space. Instead, for each input, we nudge the continuous weights\n  until the network's unconstrained inference procedure generates an\n  output that satisfies the constraints. We find that our method\n  reduces the number of violating outputs by up to 81\\%, while\n  improving accuracy.", "pdf": "/pdf/a97734ee064ecfd6fa3c08eb26560950428a7d37.pdf", "TL;DR": "An inference method for enforcing hard constraints on the outputs of neural networks without combinatorial search, with applications in NLP and structured prediction.", "paperhash": "lee|enforcing_constraints_on_outputs_with_unconstrained_inference", "keywords": ["Natural language processing", "Structured prediction", "Deep learning"], "conflicts": ["cs.umass.edu", "oracle.com", "cmu.edu", "harvard.edu"], "authors": ["Jay Yoon Lee", "Michael L. Wick", "Jean-Baptiste Tristan"], "authorids": ["lee.jayyoon@gmail.com", "michael.wick@oracle.com", "jean.baptiste.tristan@oracle.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}