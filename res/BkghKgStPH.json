{"notes": [{"id": "BkghKgStPH", "original": "B1goBRxKDr", "number": 2449, "cdate": 1569439875883, "ddate": null, "tcdate": 1569439875883, "tmdate": 1577168243510, "tddate": null, "forum": "BkghKgStPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["as2436@stanford.edu", "jlmcc@stanford.edu"], "title": "Continual Learning using the SHDL Framework with Skewed Replay Distributions", "authors": ["Amarjot Singh", "Jay McClelland"], "abstract": "Human and animals continuously acquire, adapt as well as transfer knowledge throughout their lifespan. The ability to learn continuously is crucial for the effective functioning of agents interacting with the real world and processing continuous streams of information. Continuous learning has been a long-standing challenge for neural networks as the repeated acquisition of information from non-uniform data distributions generally lead to catastrophic forgetting or interference.  This work proposes a modular architecture capable of continuous acquisition of tasks while averting catastrophic forgetting.  Specifically, our contributions are: (i) Efficient Architecture: a modular architecture emulating the visual cortex that can learn meaningful representations with limited labelled examples, (ii) Knowledge Retention: retention of learned knowledge via limited replay of past experiences, (iii) Forward Transfer: efficient and relatively faster learning on new tasks, and (iv) Naturally Skewed Distributions: The learning  in the above-mentioned claims is performed on non-uniform data distributions which better represent the natural statistics of our ongoing experience. Several experiments that substantiate the above-mentioned claims are demonstrated on the CIFAR-100 dataset.", "keywords": ["Continual Learning", "Catastrophic Forgetting", "SHDL", "CIFAR-100"], "pdf": "/pdf/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "paperhash": "singh|continual_learning_using_the_shdl_framework_with_skewed_replay_distributions", "original_pdf": "/attachment/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "_bibtex": "@misc{\nsingh2020continual,\ntitle={Continual Learning using the {\\{}SHDL{\\}} Framework with Skewed Replay Distributions},\nauthor={Amarjot Singh and Jay McClelland},\nyear={2020},\nurl={https://openreview.net/forum?id=BkghKgStPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "sTAzO-WkbD", "original": null, "number": 1, "cdate": 1576798749421, "ddate": null, "tcdate": 1576798749421, "tmdate": 1576800886494, "tddate": null, "forum": "BkghKgStPH", "replyto": "BkghKgStPH", "invitation": "ICLR.cc/2020/Conference/Paper2449/-/Decision", "content": {"decision": "Reject", "comment": "The paper adapts a previously proposed modular deep network architecture (SHDL) for supervised learning in a continual learning setting.  One problem in this setting is catastrophic forgetting.  The proposed solution replays a small fraction of the data from old tasks to avoid forgetting, on top of a modular architecture that facilitates fast transfer when new tasks are added.  The method is developed for image inputs and evaluated experimentally on CIFAR-100.\n\nThe reviews were in agreement that this paper is not ready for publication.  All the reviews had concerns about the lack of explanation of the proposed solution and the experimental methods.  The reviewers were concerned about the choice of metrics not being comparable or justified: Reviewer4 wanted an apples-to-apples comparison, Reviewer1 suggested the paper follow the evaluation paradigm used in earlier papers, and Reviewer2 described the absence of an explained baseline value.  Two reviewers (Reviewer4 and Reviewer2) described the lack of details on the parameters, architecture, and training regime used for the experiments.  The paper did not not justify which aspects of the modular system contributed to the observed performance (Reviewer4 and Reviewer1).   Several additional concerns were also raised. \n\nThe authors did not respond to any of the concerns raised by the reviewers. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["as2436@stanford.edu", "jlmcc@stanford.edu"], "title": "Continual Learning using the SHDL Framework with Skewed Replay Distributions", "authors": ["Amarjot Singh", "Jay McClelland"], "abstract": "Human and animals continuously acquire, adapt as well as transfer knowledge throughout their lifespan. The ability to learn continuously is crucial for the effective functioning of agents interacting with the real world and processing continuous streams of information. Continuous learning has been a long-standing challenge for neural networks as the repeated acquisition of information from non-uniform data distributions generally lead to catastrophic forgetting or interference.  This work proposes a modular architecture capable of continuous acquisition of tasks while averting catastrophic forgetting.  Specifically, our contributions are: (i) Efficient Architecture: a modular architecture emulating the visual cortex that can learn meaningful representations with limited labelled examples, (ii) Knowledge Retention: retention of learned knowledge via limited replay of past experiences, (iii) Forward Transfer: efficient and relatively faster learning on new tasks, and (iv) Naturally Skewed Distributions: The learning  in the above-mentioned claims is performed on non-uniform data distributions which better represent the natural statistics of our ongoing experience. Several experiments that substantiate the above-mentioned claims are demonstrated on the CIFAR-100 dataset.", "keywords": ["Continual Learning", "Catastrophic Forgetting", "SHDL", "CIFAR-100"], "pdf": "/pdf/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "paperhash": "singh|continual_learning_using_the_shdl_framework_with_skewed_replay_distributions", "original_pdf": "/attachment/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "_bibtex": "@misc{\nsingh2020continual,\ntitle={Continual Learning using the {\\{}SHDL{\\}} Framework with Skewed Replay Distributions},\nauthor={Amarjot Singh and Jay McClelland},\nyear={2020},\nurl={https://openreview.net/forum?id=BkghKgStPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkghKgStPH", "replyto": "BkghKgStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724793, "tmdate": 1576800276497, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2449/-/Decision"}}}, {"id": "B1xG4i5TFH", "original": null, "number": 1, "cdate": 1571822378214, "ddate": null, "tcdate": 1571822378214, "tmdate": 1572972336756, "tddate": null, "forum": "BkghKgStPH", "replyto": "BkghKgStPH", "invitation": "ICLR.cc/2020/Conference/Paper2449/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper suggest to use the previously proposed ScatterNet Hybrid Deep Learning  (SHDL) network in a continual learning setting. This is motivated by the fact that the SHDL needs less supervised data, so keeping a small replay buffer can be enough to maintain performance while avoiding catastrophic forgetting.\n\nMy main doubt is the benchmark and evaluation of the proposed method. The metrics reported are all relative to a baseline value (which I could not find reported), and make it difficult to understand how the model is performing in absolute term. This is particularly a problem when comparing with existing state of the art method (Fig. 3, Table 4), since this does not exclude that they may have an overall much better accuracy in absolute terms.\n\nAlso concerning the comparison with the previous literature, I could find no details about the architecture and the training algorithm used. Notice that this may in particular affect some the reported metrics, since they depend on the shape of the training curve (reporting the training curves for all methods may also be useful). Also, since SHDL uses a small replay buffer, are EWC and the other method modified to use the replay buffer and make the comparison fair?\n\nWhile several standard tests for continual learning exists (for example the split CIFAR10/100 in Zenke et al., 2017), those are not used, and rather a simpler test is used which only attempt to learn continually two datasets.  It would be helpful to also report a direct comparison on those tests.\n\nRegarding the line: \"The autoencoder is jointly trained from scratch for classes of both phases to learn mid-level features\", does this mean that the auto-encoder is trained using data of the two distributions at the same time rather than one after the other? If it is the former case, while it is unsupervised training, it would be a deviation from the standard continual learning framework and should clearly be stated.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2449/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2449/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["as2436@stanford.edu", "jlmcc@stanford.edu"], "title": "Continual Learning using the SHDL Framework with Skewed Replay Distributions", "authors": ["Amarjot Singh", "Jay McClelland"], "abstract": "Human and animals continuously acquire, adapt as well as transfer knowledge throughout their lifespan. The ability to learn continuously is crucial for the effective functioning of agents interacting with the real world and processing continuous streams of information. Continuous learning has been a long-standing challenge for neural networks as the repeated acquisition of information from non-uniform data distributions generally lead to catastrophic forgetting or interference.  This work proposes a modular architecture capable of continuous acquisition of tasks while averting catastrophic forgetting.  Specifically, our contributions are: (i) Efficient Architecture: a modular architecture emulating the visual cortex that can learn meaningful representations with limited labelled examples, (ii) Knowledge Retention: retention of learned knowledge via limited replay of past experiences, (iii) Forward Transfer: efficient and relatively faster learning on new tasks, and (iv) Naturally Skewed Distributions: The learning  in the above-mentioned claims is performed on non-uniform data distributions which better represent the natural statistics of our ongoing experience. Several experiments that substantiate the above-mentioned claims are demonstrated on the CIFAR-100 dataset.", "keywords": ["Continual Learning", "Catastrophic Forgetting", "SHDL", "CIFAR-100"], "pdf": "/pdf/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "paperhash": "singh|continual_learning_using_the_shdl_framework_with_skewed_replay_distributions", "original_pdf": "/attachment/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "_bibtex": "@misc{\nsingh2020continual,\ntitle={Continual Learning using the {\\{}SHDL{\\}} Framework with Skewed Replay Distributions},\nauthor={Amarjot Singh and Jay McClelland},\nyear={2020},\nurl={https://openreview.net/forum?id=BkghKgStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkghKgStPH", "replyto": "BkghKgStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2449/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2449/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575308893890, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2449/Reviewers"], "noninvitees": [], "tcdate": 1570237722654, "tmdate": 1575308893904, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2449/-/Official_Review"}}}, {"id": "Hkgn9zNX9r", "original": null, "number": 2, "cdate": 1572188819533, "ddate": null, "tcdate": 1572188819533, "tmdate": 1572972336709, "tddate": null, "forum": "BkghKgStPH", "replyto": "BkghKgStPH", "invitation": "ICLR.cc/2020/Conference/Paper2449/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The author proposed a modular SHDL with skewed replay distributions to do continual learning and demonstrated the effectiveness of their model on the CIFAR-100 dataset. They made contributions in three aspects: (1) using a computationally efficient architecture SHDL which can learn rapidly with fewer labeled examples. (2) In addition to retain previous learned knowledge, the model is able to acquire new knowledge rapidly which leads to fast learning. (3) By adopting the naturally skewed distributions, the model has the advantage of efficient memory storing. \n\nOverall, the paper should be rejected because \n(1)the author spent too much space to introduce the off-the-shelf SHDL model which should be put in the Appendix or referred directly. In other words, the author should explain more details about the \u201creplay\u201d mechanism in their model and show the advantage of choosing SHDL rather than other deep neural nets under the continual learning paradigm. \n(2)The comparison with other methods are too simple. When comparing with other methods, the author should introduce the parameter setting and the detailed training strategy. Otherwise, the evidence made in the experimental section is not convincing. Besides, the author should follow the evaluation paradigm used in other published papers to make a fairer comparison.\n(3)The author should carry out more discussion about which part of their model contributes the most to the continual learning. After reading the paper thoroughly, I am still unclear about it. \n\nThe paper has some imprecise part, here are a few:\n(1)The caption in Table 1 is too simple. More details should be add to explain the table. \n(2)What is the DTSCNN in Table 1?\n(3)What is the green connections in Figure 1?\n(4)In the second contribution \u201cRapid learning with forward transfer\u201d, is the ability to \u201cretrain\u201d or \u201cretain\u201d the previous learned knowledge?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2449/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2449/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["as2436@stanford.edu", "jlmcc@stanford.edu"], "title": "Continual Learning using the SHDL Framework with Skewed Replay Distributions", "authors": ["Amarjot Singh", "Jay McClelland"], "abstract": "Human and animals continuously acquire, adapt as well as transfer knowledge throughout their lifespan. The ability to learn continuously is crucial for the effective functioning of agents interacting with the real world and processing continuous streams of information. Continuous learning has been a long-standing challenge for neural networks as the repeated acquisition of information from non-uniform data distributions generally lead to catastrophic forgetting or interference.  This work proposes a modular architecture capable of continuous acquisition of tasks while averting catastrophic forgetting.  Specifically, our contributions are: (i) Efficient Architecture: a modular architecture emulating the visual cortex that can learn meaningful representations with limited labelled examples, (ii) Knowledge Retention: retention of learned knowledge via limited replay of past experiences, (iii) Forward Transfer: efficient and relatively faster learning on new tasks, and (iv) Naturally Skewed Distributions: The learning  in the above-mentioned claims is performed on non-uniform data distributions which better represent the natural statistics of our ongoing experience. Several experiments that substantiate the above-mentioned claims are demonstrated on the CIFAR-100 dataset.", "keywords": ["Continual Learning", "Catastrophic Forgetting", "SHDL", "CIFAR-100"], "pdf": "/pdf/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "paperhash": "singh|continual_learning_using_the_shdl_framework_with_skewed_replay_distributions", "original_pdf": "/attachment/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "_bibtex": "@misc{\nsingh2020continual,\ntitle={Continual Learning using the {\\{}SHDL{\\}} Framework with Skewed Replay Distributions},\nauthor={Amarjot Singh and Jay McClelland},\nyear={2020},\nurl={https://openreview.net/forum?id=BkghKgStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkghKgStPH", "replyto": "BkghKgStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2449/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2449/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575308893890, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2449/Reviewers"], "noninvitees": [], "tcdate": 1570237722654, "tmdate": 1575308893904, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2449/-/Official_Review"}}}, {"id": "HJgg5GG59B", "original": null, "number": 3, "cdate": 1572639367676, "ddate": null, "tcdate": 1572639367676, "tmdate": 1572972336661, "tddate": null, "forum": "BkghKgStPH", "replyto": "BkghKgStPH", "invitation": "ICLR.cc/2020/Conference/Paper2449/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "I think there might be some interesting ideas in the work, but I think the authors somehow did not manage to position themselves well within the *recent* works on the topic or even with respect to what continual learning (CL) is understood to be in these recent works. \n\nE.g. CL is a generic learning problem, and most algorithms are generic (with a few caveats in terms of what information is available) in the sense that they can be applied regardless of task (be it RL, be it sequence modelling etc.). This work seems limited to image classification. The SHDL wavelets pre-processing, if I understood it, is specific for images and probably even there under some assumption (e.g natural images). \n\nThe autoencoder (middle bit) is trained on all tasks the CL needs to face, if I understood the work correctly (phase 0 + phase 1). This potentially makes the CL problem much simpler because you are limiting yourself to the top layer only when dealing with CL, not the rest. Not to mention that I don't understand the motivation of the autoencoder. I think ample results show that unsupervised learning fails in many instances to provide the right features and underperforms compared to learning discriminative features by just backproping from the cross entropy (discriminative loss) all the way down. The only instance I know of for doing this is in low data regime where there is no alternative. \n\nI think the modularity used needs to be better introduced. Why the autoencoder, why the first layer of wavelets? Is it for the benefit for CL? I can understand the wavelets, since they are not learnt. But the autoencoder? The autoencoder being trained on all data feels like a cheat. \n\nI think the citation of the perceptron a bit strange. Do you really use the original perceptrion from 58? Why? We have much better tools now !?\n\nI think the different metrics introduced are interesting and useful. Though you should somehow find common ground to existing works as well to ensure a point of comparison. In the results section I almost got lost. What is the final performance on Cifar. How does this compare to a model that is not trained in a CL regime? What loss do you get from the proposed parametrizaton?\n\nIn the comparison with EWC and iCarl, there the whole model was dealing with the CL problem, right? (all intermediary layers). I'm actually surprised iCarl is not doing better (I expect it can do better than EWC). Maybe provide a few more information of hyperparam used for this comparison.\n\nOverall I think the paper is not ready for being published. Not without addressing these points:\n * role of modularity (if not CL -- then why? ; is the modular structure original or part of the previous works cited, e.g. where the wavelets are introduced and so forth)\n * better integration with recent literature; provide answers and settings that allow apple to apple comparison so one can easily understand where this approach falls; if the method is not meant for this \"traditional settings and metrics\" please still provide them, and then motivate why this regime is not interesting and explain better the regime the method is meant for\n * as it stands the work is light on the low level details; Hyper-params and other details are not carefully provided (maybe consider adding an appendix with all of these). I have doubts that the work is reproducible without these details. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2449/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2449/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["as2436@stanford.edu", "jlmcc@stanford.edu"], "title": "Continual Learning using the SHDL Framework with Skewed Replay Distributions", "authors": ["Amarjot Singh", "Jay McClelland"], "abstract": "Human and animals continuously acquire, adapt as well as transfer knowledge throughout their lifespan. The ability to learn continuously is crucial for the effective functioning of agents interacting with the real world and processing continuous streams of information. Continuous learning has been a long-standing challenge for neural networks as the repeated acquisition of information from non-uniform data distributions generally lead to catastrophic forgetting or interference.  This work proposes a modular architecture capable of continuous acquisition of tasks while averting catastrophic forgetting.  Specifically, our contributions are: (i) Efficient Architecture: a modular architecture emulating the visual cortex that can learn meaningful representations with limited labelled examples, (ii) Knowledge Retention: retention of learned knowledge via limited replay of past experiences, (iii) Forward Transfer: efficient and relatively faster learning on new tasks, and (iv) Naturally Skewed Distributions: The learning  in the above-mentioned claims is performed on non-uniform data distributions which better represent the natural statistics of our ongoing experience. Several experiments that substantiate the above-mentioned claims are demonstrated on the CIFAR-100 dataset.", "keywords": ["Continual Learning", "Catastrophic Forgetting", "SHDL", "CIFAR-100"], "pdf": "/pdf/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "paperhash": "singh|continual_learning_using_the_shdl_framework_with_skewed_replay_distributions", "original_pdf": "/attachment/6de5485f5b08b4e8d677acf18e527ca0c6a84b9c.pdf", "_bibtex": "@misc{\nsingh2020continual,\ntitle={Continual Learning using the {\\{}SHDL{\\}} Framework with Skewed Replay Distributions},\nauthor={Amarjot Singh and Jay McClelland},\nyear={2020},\nurl={https://openreview.net/forum?id=BkghKgStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkghKgStPH", "replyto": "BkghKgStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2449/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2449/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575308893890, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2449/Reviewers"], "noninvitees": [], "tcdate": 1570237722654, "tmdate": 1575308893904, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2449/-/Official_Review"}}}], "count": 5}