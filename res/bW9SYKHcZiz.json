{"notes": [{"id": "bW9SYKHcZiz", "original": "OVNf8JtrFU", "number": 776, "cdate": 1601308090407, "ddate": null, "tcdate": 1601308090407, "tmdate": 1614985657434, "tddate": null, "forum": "bW9SYKHcZiz", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Cross-Probe BERT for Efficient and Effective Cross-Modal Search", "authorids": ["~TAN_YU2", "~Hongliang_Fei2", "~Ping_Li3"], "authors": ["TAN YU", "Hongliang Fei", "Ping Li"], "keywords": [], "abstract": "Inspired by the great success of BERT in NLP tasks, many text-vision BERT models emerged recently. Benefited from cross-modal attention,  text-vision BERT models have achieved excellent performance in many language-vision tasks including text-image retrieval.   Nevertheless,  cross-modal attentions used in text-vision BERT models require too expensive computation cost when solving text-vision retrieval, which is impractical for large-scale search. In this work, we develop a novel architecture, Cross-Probe BERT.  It relies on devised text and vision probes, and cross-modal attentions are conducted on text and vision probes.  It takes lightweight computation cost, and meanwhile effectively exploits cross-modal attention.  Systematic experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|crossprobe_bert_for_efficient_and_effective_crossmodal_search", "supplementary_material": "", "pdf": "/pdf/bfe1d62b8bfc84e8b127b5d895a48c66d4de5d0d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kWqE0UOPo", "_bibtex": "@misc{\nyu2021crossprobe,\ntitle={Cross-Probe {\\{}BERT{\\}} for Efficient and Effective Cross-Modal Search},\nauthor={TAN YU and Hongliang Fei and Ping Li},\nyear={2021},\nurl={https://openreview.net/forum?id=bW9SYKHcZiz}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Zuwgjlv34w", "original": null, "number": 1, "cdate": 1610040504961, "ddate": null, "tcdate": 1610040504961, "tmdate": 1610474112125, "tddate": null, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "invitation": "ICLR.cc/2021/Conference/Paper776/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "After the rebuttal phase, all reviewers give borderline scores (leaning slightly positive, one of these noted in the comment rather than final review). While the reviewers recognize the potential merit of the contribution (efficiency while preserving effectiveness), support for acceptance is not sufficient. The major concerns include novelty (shared by multiple reviewers) and the limited experimental settings."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Probe BERT for Efficient and Effective Cross-Modal Search", "authorids": ["~TAN_YU2", "~Hongliang_Fei2", "~Ping_Li3"], "authors": ["TAN YU", "Hongliang Fei", "Ping Li"], "keywords": [], "abstract": "Inspired by the great success of BERT in NLP tasks, many text-vision BERT models emerged recently. Benefited from cross-modal attention,  text-vision BERT models have achieved excellent performance in many language-vision tasks including text-image retrieval.   Nevertheless,  cross-modal attentions used in text-vision BERT models require too expensive computation cost when solving text-vision retrieval, which is impractical for large-scale search. In this work, we develop a novel architecture, Cross-Probe BERT.  It relies on devised text and vision probes, and cross-modal attentions are conducted on text and vision probes.  It takes lightweight computation cost, and meanwhile effectively exploits cross-modal attention.  Systematic experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|crossprobe_bert_for_efficient_and_effective_crossmodal_search", "supplementary_material": "", "pdf": "/pdf/bfe1d62b8bfc84e8b127b5d895a48c66d4de5d0d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kWqE0UOPo", "_bibtex": "@misc{\nyu2021crossprobe,\ntitle={Cross-Probe {\\{}BERT{\\}} for Efficient and Effective Cross-Modal Search},\nauthor={TAN YU and Hongliang Fei and Ping Li},\nyear={2021},\nurl={https://openreview.net/forum?id=bW9SYKHcZiz}\n}"}, "tags": [], "invitation": {"reply": {"forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040504945, "tmdate": 1610474112109, "id": "ICLR.cc/2021/Conference/Paper776/-/Decision"}}}, {"id": "EZRaBHsqB6h", "original": null, "number": 1, "cdate": 1603765384782, "ddate": null, "tcdate": 1603765384782, "tmdate": 1605024608349, "tddate": null, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "invitation": "ICLR.cc/2021/Conference/Paper776/-/Official_Review", "content": {"title": "Review comments for \u201cCross-Probe BERT for Efficient and Effective Cross-Modal Search\u201d", "review": "The efficiency of current SOTA methods on image-text retrieval, especially those visual-language cross-modality pre-training models, has always been a critical problem compared with traditional joint-embedding models. This paper aims to solve this problem by integrating the efficiency of two-tower models in lower layers and the effectiveness of cross-attention learning models in higher layers. Specifically, this paper proposes to generate several vision and text probes with local features to decrease the cost of computation. Experiments on MSCOCO 1K test set and Flickr30K are conducted to show the effectiveness of the proposed method. \n\nThe motivation is insightful, and the method seems reasonable, while the experiment is weak. Following is my detailed concerns.\n1.\tThe paper shows the overall cost of models while lacks the detailed analysis of each part in the model. For example, is the computational cost of each BERT layer on query and image the same? What is the unit cost for feed-forward and self-attention operation? These also have an influence on the total cost of the models.\n2.\tThe operation cost on CPU is not shown in the paper.\n3.\tSince the key of this paper is to discuss the efficiency of retrieval model, the influence of different parameters (e.g., number of probes, l) in terms of efficiency is very important. However, only comparison of performance is shown in the paper. \n4.\tResult of performance and cost time on 5K test set would better demonstrate the efficiency of the proposed model, while this is lacked in this paper.\n5.\tThis approach tries to improve efficiency of image-text retrieval while has some sacrifice on effectiveness. While most SOTA methods on this problem is based on text-vision (TV) BERT. How to effectively adapt this approach to existing models and the generalization of this method is not discussed in the paper.\n\nOverall, I think this paper could be improved, especially in experiment part. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper776/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper776/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Probe BERT for Efficient and Effective Cross-Modal Search", "authorids": ["~TAN_YU2", "~Hongliang_Fei2", "~Ping_Li3"], "authors": ["TAN YU", "Hongliang Fei", "Ping Li"], "keywords": [], "abstract": "Inspired by the great success of BERT in NLP tasks, many text-vision BERT models emerged recently. Benefited from cross-modal attention,  text-vision BERT models have achieved excellent performance in many language-vision tasks including text-image retrieval.   Nevertheless,  cross-modal attentions used in text-vision BERT models require too expensive computation cost when solving text-vision retrieval, which is impractical for large-scale search. In this work, we develop a novel architecture, Cross-Probe BERT.  It relies on devised text and vision probes, and cross-modal attentions are conducted on text and vision probes.  It takes lightweight computation cost, and meanwhile effectively exploits cross-modal attention.  Systematic experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|crossprobe_bert_for_efficient_and_effective_crossmodal_search", "supplementary_material": "", "pdf": "/pdf/bfe1d62b8bfc84e8b127b5d895a48c66d4de5d0d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kWqE0UOPo", "_bibtex": "@misc{\nyu2021crossprobe,\ntitle={Cross-Probe {\\{}BERT{\\}} for Efficient and Effective Cross-Modal Search},\nauthor={TAN YU and Hongliang Fei and Ping Li},\nyear={2021},\nurl={https://openreview.net/forum?id=bW9SYKHcZiz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper776/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135294, "tmdate": 1606915801478, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper776/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper776/-/Official_Review"}}}, {"id": "B5Psz-alhcY", "original": null, "number": 2, "cdate": 1603781220431, "ddate": null, "tcdate": 1603781220431, "tmdate": 1605024608278, "tddate": null, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "invitation": "ICLR.cc/2021/Conference/Paper776/-/Official_Review", "content": {"title": "This paper presents a method dubbed cross-probe BERT for cross-modal retrieval\uff0cwhich relies on devised text and vision probes, and the cross-modal attention is conducted on text and vision probes. Thus the cross-modal attention can be exploited with a lightweight computation cost in this method. Experiments demonstrate its effectiveness and efficiency.", "review": "This paper develops a architecture termed cross-probe BERT to deal with the costly computation from cross-modal attentions used in search task. Motivated by the success of the \u201csplit-merge\u201d style encoder in query document retrieval, it extends text-vision BERT to adopt it for speeding up the computation. In particular, it devises several vision probes and text probes along with the image\u2019s local features and the query\u2019s word features. Besides, the cross-modal attention is conducted on text and vision probes. Therefore, it can exploit cross-modal attention with a lightweight computation cost. Systematic experiments demonstrate the state-of-the-art effectiveness and efficiency of the proposed method.\n\nPros: \n- Relative strong results compared to baselines about the effectiveness and efficiency of cross-modal retrieval.\n- Paper is well written and easy to follow.\n\nCons: \n- Limited contribution of the proposed method. This paper extends text-vision BERT to adopt it for speeding up the computation. Besides, ideas are similar to the combination of split-merge BERT + Poly-Encoder based on the architecture in Figs 2 and 3.\n- The hyper-parameters \\alpha and \\beta in the equation of complexity of transformer layer are not clearly stated. What\u2019s the effect and role about them? In addition, the corresponding analysis is not provided.\n- Insufficient experimental analysis, such as the influence of the number of probes.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper776/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper776/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Probe BERT for Efficient and Effective Cross-Modal Search", "authorids": ["~TAN_YU2", "~Hongliang_Fei2", "~Ping_Li3"], "authors": ["TAN YU", "Hongliang Fei", "Ping Li"], "keywords": [], "abstract": "Inspired by the great success of BERT in NLP tasks, many text-vision BERT models emerged recently. Benefited from cross-modal attention,  text-vision BERT models have achieved excellent performance in many language-vision tasks including text-image retrieval.   Nevertheless,  cross-modal attentions used in text-vision BERT models require too expensive computation cost when solving text-vision retrieval, which is impractical for large-scale search. In this work, we develop a novel architecture, Cross-Probe BERT.  It relies on devised text and vision probes, and cross-modal attentions are conducted on text and vision probes.  It takes lightweight computation cost, and meanwhile effectively exploits cross-modal attention.  Systematic experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|crossprobe_bert_for_efficient_and_effective_crossmodal_search", "supplementary_material": "", "pdf": "/pdf/bfe1d62b8bfc84e8b127b5d895a48c66d4de5d0d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kWqE0UOPo", "_bibtex": "@misc{\nyu2021crossprobe,\ntitle={Cross-Probe {\\{}BERT{\\}} for Efficient and Effective Cross-Modal Search},\nauthor={TAN YU and Hongliang Fei and Ping Li},\nyear={2021},\nurl={https://openreview.net/forum?id=bW9SYKHcZiz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper776/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135294, "tmdate": 1606915801478, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper776/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper776/-/Official_Review"}}}, {"id": "4vTL0-WhKxI", "original": null, "number": 3, "cdate": 1603841465706, "ddate": null, "tcdate": 1603841465706, "tmdate": 1605024608218, "tddate": null, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "invitation": "ICLR.cc/2021/Conference/Paper776/-/Official_Review", "content": {"title": "Review", "review": "Summary:\n1. This paper proposed a new architecture to accelerate the Cross-Modal BERT inference speed. The motivation is current Cross-Modal BERT is too slow during the online inference, especially for the large data. The bottleneck is due to the cross-modal attention. This paper proposed a new architecture that accelerate the inference speed, while keeps the retrieval performance.\n\nStrength:\n1. The motivation is clean. The slow inference speed problem of current image-text BERT is an important problem.\n2. The idea is clean, although there might be some mistakes (listed in the weakness section). The idea basically contains two parts. First, caching as much information as possible during inference. Therefore, the proposed approach applies less layers for Cross-Modal attentions. Those cross-modal attention layers are applied at the end of the architecture. Second, accelerate the cross-modal attention by using less tokens for this module.\n3. The performance is good when comparing with the SoTA w/ and w/o pre-training.\n\nWeakness:\n1. The method seems not novel.  The method seems like a merge of Split-merge model and the Poly-Encoder model.\n2. The equation for $\\alpha_{CP}$ might not be correct. As $\\alpha_{CP}$ is the speed-up ratio of the proposed cross-probe BERT over the text-vision BERT, it should be $\\alpha_{CP} = \\frac{\\hat C_{TV}}{\\hat C_{CP}}$. Furthermore, the numerator seems incorrect. If the numerator is for $\\hat C_{TV}$, it should not include $C_T$. \n3. The Table 3 is a little bit surprising. The performance of $m = n = 15$ is even higher than the performance of cross-modal BERT (TV in Table 1). As the cross-modal BERT has much more cross-attention layer than the proposed approach, I somehow feel the TV model is not trained properly. The lower performance of TV might indicate the TV model is overfitting.\n4. For Table 4, the CP model achieves higher (or similar) performance than Unicoder-VL on COCO, but worse performance on Flickr30k. I wonder what is the reason?\n5. If I remembered correctly, ViLBERT is pretrained using only Conceptual-Caption data (SBU is not used), while the CP is pretrained using both SBU and Conceptual-Caption. Therefore claiming higher performance than ViLBERT might be misleading. \n6. The Table 5 is incomplete. More recent works should be included and compared in the Table, for example UNITER: Learning UNiversal Image-TExt Representations. \n7. The image-text BERT model is very sensitive to the hard negative mining during training. UNITER and UNICODER-VL uses online hard negative mining, which would leads to better performance, while ViLBERT uses offline hard negatives. I wonder what is the implementation detail for Table 1 and Table 5? Specifically, are the model in Table 1 trained using the same hard negative mining strategy?\n8. The paper shows the inference speed at 1K testing case. As COCO has 5K testing split, I wonder what is the speed and image retrieval accuracy for 5K testing case? What is the speed-up ratio against TT, PE, and SM over 5K split? Showing speed-up ratio in large amount of data is important for judging the effectiveness of the proposed approach.\n9. Why PE is much faster than the proposed approach (CP) in Table 2?\n10. As in the method part, the author shows the theoretical speed up ratio. I wonder what is actual number of theoretical speed-up for CP over TT, PE, SM, and TV. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper776/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper776/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Probe BERT for Efficient and Effective Cross-Modal Search", "authorids": ["~TAN_YU2", "~Hongliang_Fei2", "~Ping_Li3"], "authors": ["TAN YU", "Hongliang Fei", "Ping Li"], "keywords": [], "abstract": "Inspired by the great success of BERT in NLP tasks, many text-vision BERT models emerged recently. Benefited from cross-modal attention,  text-vision BERT models have achieved excellent performance in many language-vision tasks including text-image retrieval.   Nevertheless,  cross-modal attentions used in text-vision BERT models require too expensive computation cost when solving text-vision retrieval, which is impractical for large-scale search. In this work, we develop a novel architecture, Cross-Probe BERT.  It relies on devised text and vision probes, and cross-modal attentions are conducted on text and vision probes.  It takes lightweight computation cost, and meanwhile effectively exploits cross-modal attention.  Systematic experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|crossprobe_bert_for_efficient_and_effective_crossmodal_search", "supplementary_material": "", "pdf": "/pdf/bfe1d62b8bfc84e8b127b5d895a48c66d4de5d0d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kWqE0UOPo", "_bibtex": "@misc{\nyu2021crossprobe,\ntitle={Cross-Probe {\\{}BERT{\\}} for Efficient and Effective Cross-Modal Search},\nauthor={TAN YU and Hongliang Fei and Ping Li},\nyear={2021},\nurl={https://openreview.net/forum?id=bW9SYKHcZiz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper776/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135294, "tmdate": 1606915801478, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper776/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper776/-/Official_Review"}}}, {"id": "Y1hq2SxHZvA", "original": null, "number": 4, "cdate": 1604138367800, "ddate": null, "tcdate": 1604138367800, "tmdate": 1605024608154, "tddate": null, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "invitation": "ICLR.cc/2021/Conference/Paper776/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "This paper proposes an improved method, Cross-Probes BERT,  based on the split-merge architecture to further accelerate cross-modal retrieval. Specifically, it devises several vision probes and text probes along with the image inputs and query inputs in the low-level two-tower architecture. After that, the attended vision and text probes are concatenated and fed into cross-modal self-attention layers to interact in the high-level one-tower architecture. The experiments on two benchmarks, MS-COCO and Flickr30K show quite promising performance, especially on computation efficiency.\n\nOverall, the definition of the task and the overall architecture of this paper are both clear and straightforward, making this paper easy to understand. The idea of introducing some probes to collect information and reduce the computation cost in the split-merge architecture makes sense to me. Two kinds of probes, namely vision probes and text probes, are proposed to collect the global features of image inputs and query inputs, which is reasonable and interesting. Besides, the paper provides comprehensive experiments on performance and computation cost to validate the effectiveness and efficiency of their proposed method.\n\n\nHere are some of my questions and concerns for the paper:\n1. Compared with previous works (such as Ploy-Encoder, BERT_{SM}, DCBERT), the proposed method seems a bit incremental.\n2. The architecture proposed in this paper seems to be applicable to text retrieval tasks. Why not test the model in some text-text retrieval tasks explored in Ploy-Encoder and DCBERT.\n3. According to Section 3.5, the \\overline{w} is the sum-pooling of \\hat{w}_1, \u2026, \\hat{w}_N while it is the output of l transformer layers in Figure 3. Is there something wrong in Figure 3? Please make it clear.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper776/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper776/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Probe BERT for Efficient and Effective Cross-Modal Search", "authorids": ["~TAN_YU2", "~Hongliang_Fei2", "~Ping_Li3"], "authors": ["TAN YU", "Hongliang Fei", "Ping Li"], "keywords": [], "abstract": "Inspired by the great success of BERT in NLP tasks, many text-vision BERT models emerged recently. Benefited from cross-modal attention,  text-vision BERT models have achieved excellent performance in many language-vision tasks including text-image retrieval.   Nevertheless,  cross-modal attentions used in text-vision BERT models require too expensive computation cost when solving text-vision retrieval, which is impractical for large-scale search. In this work, we develop a novel architecture, Cross-Probe BERT.  It relies on devised text and vision probes, and cross-modal attentions are conducted on text and vision probes.  It takes lightweight computation cost, and meanwhile effectively exploits cross-modal attention.  Systematic experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|crossprobe_bert_for_efficient_and_effective_crossmodal_search", "supplementary_material": "", "pdf": "/pdf/bfe1d62b8bfc84e8b127b5d895a48c66d4de5d0d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=kWqE0UOPo", "_bibtex": "@misc{\nyu2021crossprobe,\ntitle={Cross-Probe {\\{}BERT{\\}} for Efficient and Effective Cross-Modal Search},\nauthor={TAN YU and Hongliang Fei and Ping Li},\nyear={2021},\nurl={https://openreview.net/forum?id=bW9SYKHcZiz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bW9SYKHcZiz", "replyto": "bW9SYKHcZiz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper776/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135294, "tmdate": 1606915801478, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper776/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper776/-/Official_Review"}}}], "count": 6}