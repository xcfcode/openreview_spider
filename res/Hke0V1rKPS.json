{"notes": [{"id": "Hke0V1rKPS", "original": "r1eRIkadvB", "number": 1673, "cdate": 1569439541704, "ddate": null, "tcdate": 1569439541704, "tmdate": 1583912028502, "tddate": null, "forum": "Hke0V1rKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "qFEsPkhf64", "original": null, "number": 9, "cdate": 1578491451938, "ddate": null, "tcdate": 1578491451938, "tmdate": 1578491451938, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "6PeMCKQdB", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment", "content": {"title": "Model Used Same as Madry et al.'s Setting", "comment": "Thank you for your interest in our paper. We use the Wide-Resnet from Madry et al., which would be the 28-10 you mentioned. We initially called it 32-10 to be consistent with how a previous paper \u201cAdversarial Training for Free!\u201d, by Shafahi et al., named it. We will remove the name and emphasize our model\u2019s similarity to Madry et al. for more clarity."}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0V1rKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1673/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1673/Authors|ICLR.cc/2020/Conference/Paper1673/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152552, "tmdate": 1576860543315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment"}}}, {"id": "6PeMCKQdB", "original": null, "number": 2, "cdate": 1578474050750, "ddate": null, "tcdate": 1578474050750, "tmdate": 1578474050750, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Public_Comment", "content": {"title": "Confused About Layers of Wide-Resnet.", "comment": "It's a nice paper. I just want to ask about a small problem. In experimental part 4.3, which Wide-Resnet do you use? 28-10 (Madry's setting) or 34-10? I never see the 32-10 before because WRN requires (N-4) % 6 == 0. Thanks."}, "signatures": ["~Haonan_Qiu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Haonan_Qiu1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0V1rKPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191416, "tmdate": 1576860576644, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Public_Comment"}}}, {"id": "Q45nRnJSGu", "original": null, "number": 1, "cdate": 1576798729494, "ddate": null, "tcdate": 1576798729494, "tmdate": 1576800907014, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper extends previous observations (Tsipars, Etmann etc) in relations between Jacobian and robustness and directly train a model that improves robustness using Jacobians that look like images. The questions regarding computation time (suggested by two reviewers, including one of the most negative reviewers) are appropriately addressed by the authors (added experiments). Reviewers agree that the idea is novel, and some conjectured why the paper\u2019s idea is a very sensible one. We think this paper would be an interest for ICLR readers. Please address any remaining comments from the reviewers before the final copy.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712085, "tmdate": 1576800261401, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Decision"}}}, {"id": "r1xkPfF6tr", "original": null, "number": 3, "cdate": 1571816022589, "ddate": null, "tcdate": 1571816022589, "tmdate": 1574343947136, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "Summary:\nIt was previously observed that models that were more robust to adversarial perturbation had more interpretable jacobian. The authors attempt to train for interpretable jacobian in order to improve the robustness of the model.\nThis is done by employing a GAN-like procedure where a discriminator attempts to distinguish between the transformed jacobian matrix (fake images, equivalent to generator) and real images.\n\nExperiments indicates that this improves robustness compared to unprotected models and approximately similarly to models trained with adversarial training.\n\nComments:\n* The motivation given for this line of research is the cost of adversarial training (2nd paragraph of Section 3)\nNo experimental comparison is given with regards to the time it takes to train a model with adversarial training, versus the time it takes to train a model with JARN. It is also important to note that this introduces additional complexity (needs to choose an architecture for the discriminator, tune proper learning rates, etc...), which is not mentionned.\n\n* Why not test simpler jacobian regularization method as proposed by other papers (see below). Proposition 3 of Simon-Gabriel et al. shows that results similar to adversarial training can be obtained, and they don't need several iterations like adversarial training, nor do they need to train an additional discriminator like your method.\n\nOpinion:\nThe paper provides an interesting proof of concept for a method, showing that it is feasible. It however doesn't make the the case for why it is a good idea. Discussion and comparison to very significant related work is missing and experimental measurement of any advantages of the proposed method vs. adversarial training is lacking. I think that these aspects should be improved before the paper is ready for publication.\n\nTypos:\nLine 11 in Algorithm 1 -> The label is wrong, i assume it's \"Update the discriminator f_disc to maximize L_adv\"\n\nRelated works that needs discussing:\n- Drucker, Lecun 91, \"Double backpropagation increasing generalization performance\" for other regularizer on the jacobian, discusses generalization rather than robustness.\n- Simon-Gabriel et al., \"First-order Adversarial Vulnerability of Neural Networks and Input Dimension\"\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575593017477, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Reviewers"], "noninvitees": [], "tcdate": 1570237733948, "tmdate": 1575593017491, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Review"}}}, {"id": "S1l4Bz3soH", "original": null, "number": 1, "cdate": 1573794364462, "ddate": null, "tcdate": 1573794364462, "tmdate": 1573824530782, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "ryxg23o2tS", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank the reviewer for the thoughtful comments and the interest in our work. Our detailed response to the questions follows.\n\n> The idea of Jacobian saliency from prior work:\nIn contrast to prior work, to the best of our knowledge, our paper is the first with the aim to train the classifier\u2019s Jacobian to resemble input images more closely as a way to improve robustness. We summarize those previous studies here to better contrast with our paper:\n\nThe focus of the work from (Tsipras et al., 2018) is on the trade-off between a model\u2019s standard accuracy and its robustness while showing that robust models learn different feature representations than their non-robust counterparts. The authors observed from their experiments that the adversarial examples generated from robust models look perceptually different from the original images. They attributed this to the higher saliency of the Jacobian generated at each gradient step of the adversarial attack on robust models. While (Tsipras et al., 2018) is one of the first to point out this observation, the authors did not propose a new defense based on it. \n\n(Etmann et al., 2019) further investigated this observation by proving that linearized robustness (distance from samples to decision boundary) increases as the alignment (unit vector cosine similarity) between the Jacobian and input image grows in linear models. The authors train robust models using double backpropagation (Drucker & Le Cun, 1992) and show empirically the relationship between robustness and alignment weakens for non-linear models but did not propose a new defense in the paper.\n\n(Drucker & Le Cun, 1991; Ross & Doshi-Velez, 2018; Jakubovitz & Giryes, 2018; Hoffman et al., 2019; Simon-Gabriel et al., 2019) use a regularization term to reduce the Jacobian's Frobenius norm. While their approaches constrain the effect of individual pixels\u2019 perturbation on the model prediction, our approach focuses on the intuition to improve robustness by training for salient Jacobians that look like images. Our newly added experiments (Section 4.3 and Table 3) show that JARN outperforms this earlier approach in adversarial robustness.\n\n\n> Discussion on why Jacobian saliency confers robustness:\nWe thank the reviewer for the insightful discussion and would like to share our interpretation. The input Jacobian indeed characterizes how the final logits are affected by small changes to the pixels. Indeed, regularizing the Jacobian to resemble the input would likely result in adversarial perturbations that perceptually change the labeled object. A possible explanation behind the improved robustness through increasing Jacobian saliency is that the space of input-output Jacobian shrinks under this regularization, i.e. Jacobians have to resemble non-noisy images. Intuitively, this means that there would be fewer paths for an adversarial example to reach an optimum in the loss landscape, improving the model\u2019s robustness. As suggested by the reviewer, we added more discussion on this in Section 4.3.2.\n\n\n> Computational efficiency of JARN compared to adversarial training:\nFollowing the suggestion of the reviewer, we have added experiments to compare JARN\u2019s computational efficiency with adversarial training in Section 4.3.4 and Table 4. Even when combined with 1-step adversarial training, JARN takes less than half the time compared to 7-step PGD adversarial training while outperforming it. In our experiments on CIFAR-10, the JARN discriminator only updates once every 20 classifier update steps and is much smaller in parameter size compared to the main classifier, explaining JARN\u2019s efficiency. Moreover, we find that using JARN framework only on the last few epoch (25%) to train the classifier confers similar adversarial robustness compared to training with JARN for the whole duration. \n\nReferences:\n- Tsipras et al., \u201cRobustness may be at odds with accuracy.\u201d arXiv preprint arXiv:1805.12152, 2018.\n\n- Etmann et al., \u201cOn the connection between adversarial robustness and saliency map interpretability.\u201d arXiv preprint arXiv:1905.04172, 2019.\n\n-  Drucker and Le Cun \"Double backpropagation increasing generalization performance\" IEEE Transactions on Neural Networks, 3(6):991\u2013997, 1992.\n\n- Ross & Doshi-Velez, \u201cImproving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients.\u201d In Thirty-second AAAI conference on artificial intelligence, 2018.\n\n- Jakubovitz & Giryes, \u201cImproving dnn robustness to adversarial attacks using jacobian regularization\u201d In Proceedings of the European Conference on Computer Vision (ECCV), pp.\n514\u2013529, 2018.\n\n- Hoffman et al., \u201cRobust learning with jacobian regularization.\u201d arXiv preprint arXiv:1908.02729, 2019.\n\n- Simon-Gabriel et al., \"First-order Adversarial Vulnerability of Neural Networks and Input Dimension\" In International Conference on Machine Learning, pp. 5809\u20135817, 2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0V1rKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1673/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1673/Authors|ICLR.cc/2020/Conference/Paper1673/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152552, "tmdate": 1576860543315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment"}}}, {"id": "H1eCvDhiiH", "original": null, "number": 6, "cdate": 1573795685984, "ddate": null, "tcdate": 1573795685984, "tmdate": 1573801306154, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment", "content": {"title": "Summary of Revision", "comment": "We would like to thank all the reviewers for their insightful and constructive comments to improve the paper. We have uploaded a revision of the paper with the following updates:\n\n> Added more discussion on why Jacobian saliency confers robustness Section 4.3.2, as suggested by Reviewer #1.\n\n> Included results on JARN\u2019s computational efficiency compared to adversarial training in Section 4.3.4 and Table 4, as advised by Reviewer #1, 2 & 3.\n\n> Added experiments to show JARN\u2019s stability across key hyperparameter changes in Section 4.3.5 and Appendix Figure 5, as recommended by Reviewer # 2 & 3.\n\n> Added experiments to more thoroughly evaluate JARN and show its effectiveness against black-box and transfer attacks in Section 4.3.6 and Table 5, as suggested by Reviewer # 2.\n\n> Included comparison with double backpropagation Jacobian norm regularization baseline and showed (Section 4.3 and Table 3) JARN\u2019s approach for Jacobian saliency outperforms double backpropagation\u2019s approach through minimizing Jacobian\u2019s Frobenius norm values, as suggested by Reviewer # 3.\n\n> Added further discussion on differences between JARN and closely related prior art in Section 2: \u201cNon-Adversarial Training Regularization\u201d, as recommended by Reviewer # 3.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0V1rKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1673/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1673/Authors|ICLR.cc/2020/Conference/Paper1673/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152552, "tmdate": 1576860543315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment"}}}, {"id": "rkllMwhsjr", "original": null, "number": 5, "cdate": 1573795591610, "ddate": null, "tcdate": 1573795591610, "tmdate": 1573795591610, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "Byg5Bb6GiH", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment", "content": {"title": "Response to Comment", "comment": "We thank the commenter for mentioning this paper. The main contribution of [1] lies in an algorithm to efficiently approximate the input-class probability output Jacobians to minimize their Frobenius norms to improve prediction stability and adversarial robustness [1]. Similar to works like [2-4], [1]\u2019s objective is to reduce the effect that perturbations at individual pixels have on the classifier\u2019s prediction through the Jacobians\u2019 norm. Different from our paper, the work in [1-4] did not aim to optimize Jacobians to explicitly resemble their corresponding images. In contrast, we propose JARN with the aim to make the Jacobian resemble input images more closely through an adversarial loss term, to explore whether this leads to improved robustness. We will add this paper to related work to make a better distinction (Section 2: Non-Adversarial Training Regularization). Furthermore, we have also included additional experiments to compare with double backpropagation [2-4] and found that JARN outperforms it in PGD attacks on CIFAR-10 (Section 4.3 and Table 3).\n\n[1] Judy Hoffman, Daniel A Roberts, and Sho Yaida. \u201cRobust learning with jacobian regularization.\u201d arXiv:1908.02729, 2019.\n\n[2] Drucker and Le Cun \"Double backpropagation increasing generalization performance\" IEEE Transactions on Neural Networks, 3(6):991\u2013997, 1992.\n\n[3] Simon-Gabriel et al., \"First-order Adversarial Vulnerability of Neural Networks and Input Dimension\" ICML, pp. 5809\u20135817, 2019\n\n[4] Ross et al. \u201cImproving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients.\u201d AAAI, 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0V1rKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1673/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1673/Authors|ICLR.cc/2020/Conference/Paper1673/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152552, "tmdate": 1576860543315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment"}}}, {"id": "SkeInMnsoH", "original": null, "number": 2, "cdate": 1573794478512, "ddate": null, "tcdate": 1573794478512, "tmdate": 1573795469590, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "rklhsmOwFH", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank the reviewer for the constructive comments and would like to respond to them as follows,\n\n> Computational cost of JARN:\nFollowing the advice of the reviewer, we have added experiments to compare JARN\u2019s computational efficiency with PGD adversarial training in Section 4.3.4 and Table 4. Even when combined with 1-step adversarial training, JARN takes less than half the time compared to 7-step PGD adversarial training while outperforming it. JARN\u2019s GAN component adds a relatively small computational burden since the discriminator only updates once every 20 classifier update steps in the CIFAR-10 experiments and is much smaller in parameter size compared to the main classifier. Moreover, we find that using JARN framework only on the last few epoch (25%) to train the classifier confers similar adversarial robustness compared to training with JARN for the whole duration, further increasing its efficiency.\n\n\n> Reproducibility and sensitivity to hyperparameters:\nTo address the reviewer\u2019s comment, we have added more experiments to test JARN performance across a range of key hyperparameters ($\\lambda_{adv}$, batch size and discriminator update intervals) that are different from Section 4.3 and find that its performance is relatively stable across key hyperparameter changes (Section 4.3.5 and Appendix Figure 5). \n\nIn a typical GAN framework, each training step involves a real image sample and an image generated from noise that is decoupled from the real sample. In contrast, a Jacobian is conditioned on its original input image and both are used in the same training step of JARN. This training step resembles that of VAE-GAN [1] where pairs of real images and its reconstructed versions are used for training together, resulting in generally more stable gradients and convergence than GAN. We believe that this similarity favors JARN's stability over a wider range of hyperparameters. To further ensure reproducibility, we will release the source code after the paper\u2019s acceptance.\n\n\n> More evaluation of the defense:\nFollowing the reviewer\u2019s suggestion to evaluate the JARN classifier\u2019s robustness more comprehensively [2], we have added experiments of black-box transfer attacks on JARN (Section 4.3.6 and Table 5). Defenses relying on gradient masking will display lower robustness towards transfer attacks than white-box attacks [2,3]. When evaluated on such black-box attacks using adversarial examples generated from a PGD-AT7 trained model and their differently initialized version, both JARN and JARN-AT1 display higher accuracy than when under white-box attacks (Table 5), a sign that JARN's robustness does not rely on gradient masking. We would like to also point out Section 4.3.1 where evaluation is carried out on attacks over a range of different parameters to test the generalization of JARN\u2019s robustness.\n\n\n> Other ways to encourage Jacobian saliency:\nSince our main research question is to study if regularizing for image-resembling Jacobians can lead to robustness, we wish to study this by directly training for generated Jacobian to look like images. Due to GAN\u2019s remarkable success in synthetic image generation, we incorporate GAN into the JARN framework for this objective. While we agree that other ways that can encourage Jacobian saliency are interesting future work, we would like to point out that this paper is the first to show this approach can lead to improved robustness.\n\n\n[1] Larsen et al., 2015 \u201cAutoencoding beyond pixels using a learned similarity metric.\u201d arXiv preprint arXiv:1512.09300, 2015\n\n[2] Carlini, Nicholas, et al. \"On evaluating adversarial robustness.\" arXiv preprint arXiv:1902.06705, 2019.\n\n[3] Papernot et al., 2015 \u201cTransferability in machine learning: from phenomena to black-box attacks using adversarial samples.\u201d arXiv preprint arXiv:1605.07277, 2016\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0V1rKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1673/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1673/Authors|ICLR.cc/2020/Conference/Paper1673/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152552, "tmdate": 1576860543315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment"}}}, {"id": "SklSSInojB", "original": null, "number": 4, "cdate": 1573795389261, "ddate": null, "tcdate": 1573795389261, "tmdate": 1573795389261, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "r1xkPfF6tr", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank the reviewer for the helpful comments and would like to respond to them as follows,\n\n> Cost of JARN training:\nTo address the comment on computational cost, we have added experiments to compare JARN\u2019s computational efficiency with PGD adversarial training in Section 4.3.4 and Table 4. Even when combined with 1-step adversarial training, JARN takes less than half the time compared to 7-step PGD adversarial training while outperforming it. JARN\u2019s GAN component adds a relatively small computational burden since the discriminator only updates once every 20 classifier update steps in the CIFAR-10 experiments and is much smaller in parameter size compared to the main classifier. Moreover, we find that using JARN framework only on the last few epoch (25%) to train the classifier confers similar adversarial robustness compared to training with JARN for the whole duration, further supporting its efficiency.\n\nRegarding the concern of hyperparameter tuning, we have added more experiments to test JARN performance across a range of key hyperparameters ($\\lambda_{adv}$, batch size and discriminator update intervals) that are different from Section 4.3 and find that its performance is relatively stable across hyperparameter changes (Appendix Figure 5). In a typical GAN framework, each training step involves a real image sample and an image generated from noise that is decoupled from the real sample. In contrast, a Jacobian is conditioned on its original input image and both are used in the same training step of JARN. This training step resembles that of VAE-GAN [1] where pairs of real images and its reconstructed versions are used for training together, resulting in generally more stable gradients and convergence than GAN. We believe that this similarity favors JARN's stability over a wider range of hyperparameters.\n\n\n> Why not test other Jacobian regularization methods?\nFollowing the suggestion of the reviewer, we implemented double backpropagation (DBP) [2-4] as a baseline with additional results to compare in our paper (Section 4.3 and Table 3). We found that DBP provides robustness against FGSM attacks compared to standard training but is outperformed by JARN across all the attacks. While Proposition 3 in [5] shows that DBP is equivalent to training with $l_2$ adversarial examples, we believe they are single-step adversarial examples rather than the stronger multi-step adversarial examples generated by iterative methods like PGD. This explains DBP\u2019s performance under the more recent and stronger PGD attacks. The comparison demonstrates that JARN\u2019s approach to robustness through the saliency of Jacobian is fundamentally different and more effective than DBP\u2019s approach through minimizing Jacobian's Frobenius norm. We further discuss the difference from the prior art in the following paragraphs for completeness.\n\nSince the contribution of our paper is to study if regularizing for image-resembling Jacobians can be a new way to improve robustness, we wish to study this by directly training for generated Jacobians to look like images. [2-4] employ a regularization term to minimize the Jacobian's Frobenius norm together with the standard training objective. While their approaches improve robustness by reducing the effect that perturbations in individual pixel have on the classifier\u2019s prediction through the Jacobians\u2019 norm, it does not have the aim to optimize Jacobians to explicitly resemble their corresponding images. We describe those studies in more detail here and in the revision (Section 2: Non-Adversarial Training Regularization): \n\n[2] first proposed this to improve model generalization on natural test samples and called it \u2018double backpropagation\u2019 while [3,4] are concurrent studies that evaluate this method against adversarial examples. [5] proved that double backpropagation is equivalent to adversarial training with $l_2$ examples. [6] trained robust models using double backpropagation to study the link between robustness and alignment in non-linear models but did not propose a new defense in their paper.\n\nAll in all, our work aims to open up Jacobian\u2019s saliency as a new avenue to boost adversarial robustness.\n\n[1] Larsen et al., 2015 \u201cAutoencoding beyond pixels using a learned similarity metric.\u201d arXiv:1512.09300, 2015\n\n[2] Drucker and Le Cun \"Double backpropagation increasing generalization performance\" IEEE Transactions on Neural Networks, 3(6):991\u2013997, 1992.\n\n[3] Ross et al. \u201cImproving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients.\u201d AAAI, 2018.\n\n[4] Jakubovitz et al. \u201cImproving dnn robustness to adversarial attacks using jacobian regularization\u201d ECCV, pp. 514\u2013529, 2018.\n\n[5] Simon-Gabriel et al., \"First-order Adversarial Vulnerability of Neural Networks and Input Dimension\" ICML, pp. 5809\u20135817, 2019\n\n[6] Etmann et al. \u201cOn the connection between adversarial robustness and saliency map interpretability.\u201d arXiv:1905.04172, 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0V1rKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1673/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1673/Authors|ICLR.cc/2020/Conference/Paper1673/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152552, "tmdate": 1576860543315, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Comment"}}}, {"id": "Byg5Bb6GiH", "original": null, "number": 1, "cdate": 1573208386392, "ddate": null, "tcdate": 1573208386392, "tmdate": 1573208426390, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Public_Comment", "content": {"title": "related reference", "comment": "Hi, I just came across one arXiv article that also addresses the same question that Jacobian Regularization leads to robustness to adversarial perturbations.\n\nIt would be good to compare the differences.\n\nhttps://arxiv.org/pdf/1908.02729.pdf\n"}, "signatures": ["~Zhengyu_Zhao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhengyu_Zhao1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0V1rKPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191416, "tmdate": 1576860576644, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1673/Authors", "ICLR.cc/2020/Conference/Paper1673/Reviewers", "ICLR.cc/2020/Conference/Paper1673/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Public_Comment"}}}, {"id": "rklhsmOwFH", "original": null, "number": 1, "cdate": 1571419044013, "ddate": null, "tcdate": 1571419044013, "tmdate": 1572972437987, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "I think the main contribution of this paper is that it introduces a new way of robust training by encouraging Jacobian saliency. Previous research like Etmann et al 2019 and Tsipras et al 2018 showed that robustness leads to saliency. But surprisingly, this paper shows the other way, saliency map can also lead to robustness, which indicates a stronger connection between these two. In general, I like the intuition behind this paper, since it introduces a new perspective of robust training.\n\nThe training method proposed in this paper is still kind of preliminary, though. I suspect that training a GAN together with the classifier will cost even more time than min-max adversarial training or some certified robust training methods. It would be great if the authors can provide the training time comparison between JARN and some state-of-the-art robust training methods. Another concern is reproducibility since the training process of GAN is sensitive to hyperparameter selection. It would be better if the author can have some discussion on the training process to show that the reported performance of the defense is easy to reproduce instead of cherry-picking.  Also, there are too many works on robustness defense that have been proven ineffective (consider the works by Carlini). Since this is a completely new way of robust training and there is no certified guarantee, I suggest the authors refer [1] to evaluate the effectiveness of the defense more thoroughly to convince the readers that it really works. Especially, evaluation under adaptive attack is necessary.\n\nI think this is a very interesting work. But since this method is completely new, more detailed information is needed to convince me that it really works. If it does work, I believe there must exist better ways to encourage Jacobian saliency than using a GAN.\n\n[1] Carlini, Nicholas, et al. \"On evaluating adversarial robustness.\" arXiv preprint arXiv:1902.06705 (2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575593017477, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Reviewers"], "noninvitees": [], "tcdate": 1570237733948, "tmdate": 1575593017491, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Review"}}}, {"id": "ryxg23o2tS", "original": null, "number": 2, "cdate": 1571761319617, "ddate": null, "tcdate": 1571761319617, "tmdate": 1572972437952, "tddate": null, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "invitation": "ICLR.cc/2020/Conference/Paper1673/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel regularization strategy for improving the robustness of networks to adversarial noise. A term is added to the standard supervised cross-entropy loss that encourages the Jacobian of the network to itself be interpreted as a valid image. This \"regularization\" term is constructed by running the input-output Jacobian of the classification network through an \"adapter network\" and then in turn interpreting its output as a \"generator\" in a GAN setup. A separate discriminator network is training to distinguish real input images from these adapter-processed input-output Jacobians. The overall regularization is the standard minimax GAN loss applied to this generator/discriminator setup. The impetus for this stems from a previous observation that salient or interpretable input-output Jacobians naturally arise for networks that have undergone adversarial training to increase robustness.\n\nAlthough this whole setup seems to be a little \"Rube-Goldberg\"-esque, I think there's some real sensible reasons for this sort of regularization to make intuitive sense. The input-output Jacobian characterizes how much the output (i.e. the logits) are affected by small changes to the input. The Jacobian, reinterpreted as living in the input image space (as the authors do), is a map of which input pixels have the strongest effect on the output of the network. If the Jacobian image looks like the underlying input image -- in particular, highlighting the labeled object -- this indicates that changing those pixels will result in the largest change on the network output. (This should be clear when looking at Figure 4 of the paper.) On the other hand, adversarial noise by definition leaves the underlying object alone (so that a human isn't aware of the perturbation) and modifies other pixels. Models that fall for such adversarial noise will not have salient Jacobians.\n\nThis is an amusing original idea, and I think this paper probably should be accepted to ICLR -- though I don't hold that position very strongly. However, I think the most interesting point is idea of Jacobian saliency, which is from prior work (Tsipras et al., 2018) that I haven't read. Therefore, I'm not sure how significant this paper is on it's own. Regardless, I would have liked to see more discussion in the paper of why Jacobian saliency should confer robustness (as I tried to do in the paragraph above), with perhaps some additional experiments designed around understanding whether this intuition (or something similar) is actually correct. There's some discussion of the theory behind the method in section 3.1, but it's not very intuitive to the situation at hand (non-linear neural networks), and I don't find it particularly informative.\n\nFinally, some effort is spent arguing that this method is more computational efficient than adversarial training -- I wonder if that's still true when the all the complexity of GAN training is taken into account or how to consider that point when part of the conclusion is that their method is best when it is also combined with some amount of adversarial training."}, "signatures": ["ICLR.cc/2020/Conference/Paper1673/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1673/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["guoweial001@e.ntu.edu.sg", "ytay017@e.ntu.edu.sg", "asysong@ntu.edu.sg", "jie.fu@polymtl.ca"], "title": "Jacobian Adversarially Regularized Networks for Robustness", "authors": ["Alvin Chan", "Yi Tay", "Yew Soon Ong", "Jie Fu"], "pdf": "/pdf/b71a044605b9fb104c10c4aeff0af76763e44138.pdf", "TL;DR": "We show that training classifiers to produce salient input Jacobian matrices with a GAN-like regularization can boost adversarial robustness.", "abstract": "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training.", "keywords": ["adversarial examples", "robust machine learning", "deep learning"], "paperhash": "chan|jacobian_adversarially_regularized_networks_for_robustness", "_bibtex": "@inproceedings{\nChan2020Jacobian,\ntitle={Jacobian Adversarially Regularized Networks for Robustness},\nauthor={Alvin Chan and Yi Tay and Yew Soon Ong and Jie Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0V1rKPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d4be400372717ad63adc592e4323630726cefade.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hke0V1rKPS", "replyto": "Hke0V1rKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1673/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575593017477, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1673/Reviewers"], "noninvitees": [], "tcdate": 1570237733948, "tmdate": 1575593017491, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1673/-/Official_Review"}}}], "count": 13}