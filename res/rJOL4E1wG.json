{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124474397, "tcdate": 1518449743574, "number": 138, "cdate": 1518449743574, "id": "rJOL4E1wG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rJOL4E1wG", "signatures": ["~Alex_Hern\u00e1ndez-Garc\u00eda1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Do deep nets really need weight decay and dropout?", "abstract": "The impressive success of modern deep neural networks on computer vision tasks has been achieved through models of very large capacity compared to the number of available training examples. This overparameterization is often said to be controlled with the help of different regularization techniques, mainly weight decay and dropout. However, since these techniques reduce the effective capacity of the model, typically even deeper and wider architectures are required to compensate for the reduced capacity. Therefore, there seems to be a waste of capacity in this practice. In this paper we build upon recent research that suggests that explicit regularization may not be as important as widely believed and carry out an ablation study that concludes that weight decay and dropout may not be necessary for object recognition if enough data augmentation is introduced.", "paperhash": "hern\u00e1ndezgarc\u00eda|do_deep_nets_really_need_weight_decay_and_dropout", "keywords": ["convolutional neural networks", "regularization", "data augmentation"], "_bibtex": "@misc{\n  hern\u00e1ndez-garc\u00eda2018do,\n  title={Do deep nets really need weight decay and dropout?},\n  author={Alex Hern\u00e1ndez-Garc\u00eda and Peter K\u00f6nig},\n  year={2018},\n  url={https://openreview.net/forum?id=rJOL4E1wG}\n}", "authorids": ["alexhg15@gmail.com", "pkoenig@uos.de"], "authors": ["Alex Hern\u00e1ndez-Garc\u00eda", "Peter K\u00f6nig"], "TL;DR": "Weight decay and dropout regularization might benot only unnecessary, but their generalization gains can be achieved by data augmentation alone.", "pdf": "/pdf/04798f4cbb44883be83ebde213f39811de5e6264.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582919045, "tcdate": 1520378369555, "number": 1, "cdate": 1520378369555, "id": "BJYWGinOf", "invitation": "ICLR.cc/2018/Workshop/-/Paper138/Official_Review", "forum": "rJOL4E1wG", "replyto": "rJOL4E1wG", "signatures": ["ICLR.cc/2018/Workshop/Paper138/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper138/AnonReviewer3"], "content": {"title": "No need for regularization in ConvNets?", "rating": "5: Marginally below acceptance threshold", "review": "The paper argues through experiments that weight decay and dropout are not necessary compared with extensive data augmentation.\n\nIt is an interesting point to make, and confirms with some of the experience in the literature. But to me this conclusion is quite obvious even without this paper, and the usefulness is narrower than it was presented. In many practical problems that are not about images, it is hard to have such extensive data augmentation.\n\nIn the last paragraph in section 2.1, it says \"We deliberately avoid designing a particularly successful scheme.\" Can there be more clarification on this?\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do deep nets really need weight decay and dropout?", "abstract": "The impressive success of modern deep neural networks on computer vision tasks has been achieved through models of very large capacity compared to the number of available training examples. This overparameterization is often said to be controlled with the help of different regularization techniques, mainly weight decay and dropout. However, since these techniques reduce the effective capacity of the model, typically even deeper and wider architectures are required to compensate for the reduced capacity. Therefore, there seems to be a waste of capacity in this practice. In this paper we build upon recent research that suggests that explicit regularization may not be as important as widely believed and carry out an ablation study that concludes that weight decay and dropout may not be necessary for object recognition if enough data augmentation is introduced.", "paperhash": "hern\u00e1ndezgarc\u00eda|do_deep_nets_really_need_weight_decay_and_dropout", "keywords": ["convolutional neural networks", "regularization", "data augmentation"], "_bibtex": "@misc{\n  hern\u00e1ndez-garc\u00eda2018do,\n  title={Do deep nets really need weight decay and dropout?},\n  author={Alex Hern\u00e1ndez-Garc\u00eda and Peter K\u00f6nig},\n  year={2018},\n  url={https://openreview.net/forum?id=rJOL4E1wG}\n}", "authorids": ["alexhg15@gmail.com", "pkoenig@uos.de"], "authors": ["Alex Hern\u00e1ndez-Garc\u00eda", "Peter K\u00f6nig"], "TL;DR": "Weight decay and dropout regularization might benot only unnecessary, but their generalization gains can be achieved by data augmentation alone.", "pdf": "/pdf/04798f4cbb44883be83ebde213f39811de5e6264.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582918859, "id": "ICLR.cc/2018/Workshop/-/Paper138/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper138/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper138/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper138/AnonReviewer2"], "reply": {"forum": "rJOL4E1wG", "replyto": "rJOL4E1wG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper138/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper138/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582918859}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582797428, "tcdate": 1520624990580, "number": 2, "cdate": 1520624990580, "id": "SJwvHwetM", "invitation": "ICLR.cc/2018/Workshop/-/Paper138/Official_Review", "forum": "rJOL4E1wG", "replyto": "rJOL4E1wG", "signatures": ["ICLR.cc/2018/Workshop/Paper138/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper138/AnonReviewer2"], "content": {"title": "Dropout is not widely believed to be important in presence of BN", "rating": "4: Ok but not good enough - rejection", "review": "This paper performs an experimental study comparing the benefits of data augmentation to those of weight decay and dropout for image classification using convolutional networks.\n\nThe paper is not about a novel idea, but rather about evaluating the utility of certain types of regularization used for neural networks. \n\nI do not think the study has high significance yet, since the results do not lead to novel or important generalizable insights. \n\nCons:\n\nThe two types of \"explicit\" regularization considered are weight decay and dropout. But the tested networks all use batch normalization (BN), which is rather powerful regularizer. It has been known since the introduction of BN that it reduces or altogether removes the need for other regularization methods such as dropout, and many networks with BN do not use dropout. Clearly, the generalization gains for networks without weight decay and dropout are coming not just through data augmentation alone as claimed, but also from BN. Based on these experiments alone, it also doesn't seem that one can claim that weight decay will not help in general.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do deep nets really need weight decay and dropout?", "abstract": "The impressive success of modern deep neural networks on computer vision tasks has been achieved through models of very large capacity compared to the number of available training examples. This overparameterization is often said to be controlled with the help of different regularization techniques, mainly weight decay and dropout. However, since these techniques reduce the effective capacity of the model, typically even deeper and wider architectures are required to compensate for the reduced capacity. Therefore, there seems to be a waste of capacity in this practice. In this paper we build upon recent research that suggests that explicit regularization may not be as important as widely believed and carry out an ablation study that concludes that weight decay and dropout may not be necessary for object recognition if enough data augmentation is introduced.", "paperhash": "hern\u00e1ndezgarc\u00eda|do_deep_nets_really_need_weight_decay_and_dropout", "keywords": ["convolutional neural networks", "regularization", "data augmentation"], "_bibtex": "@misc{\n  hern\u00e1ndez-garc\u00eda2018do,\n  title={Do deep nets really need weight decay and dropout?},\n  author={Alex Hern\u00e1ndez-Garc\u00eda and Peter K\u00f6nig},\n  year={2018},\n  url={https://openreview.net/forum?id=rJOL4E1wG}\n}", "authorids": ["alexhg15@gmail.com", "pkoenig@uos.de"], "authors": ["Alex Hern\u00e1ndez-Garc\u00eda", "Peter K\u00f6nig"], "TL;DR": "Weight decay and dropout regularization might benot only unnecessary, but their generalization gains can be achieved by data augmentation alone.", "pdf": "/pdf/04798f4cbb44883be83ebde213f39811de5e6264.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582918859, "id": "ICLR.cc/2018/Workshop/-/Paper138/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper138/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper138/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper138/AnonReviewer2"], "reply": {"forum": "rJOL4E1wG", "replyto": "rJOL4E1wG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper138/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper138/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582918859}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573593380, "tcdate": 1521573593380, "number": 215, "cdate": 1521573593041, "id": "rkWykkycG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rJOL4E1wG", "replyto": "rJOL4E1wG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do deep nets really need weight decay and dropout?", "abstract": "The impressive success of modern deep neural networks on computer vision tasks has been achieved through models of very large capacity compared to the number of available training examples. This overparameterization is often said to be controlled with the help of different regularization techniques, mainly weight decay and dropout. However, since these techniques reduce the effective capacity of the model, typically even deeper and wider architectures are required to compensate for the reduced capacity. Therefore, there seems to be a waste of capacity in this practice. In this paper we build upon recent research that suggests that explicit regularization may not be as important as widely believed and carry out an ablation study that concludes that weight decay and dropout may not be necessary for object recognition if enough data augmentation is introduced.", "paperhash": "hern\u00e1ndezgarc\u00eda|do_deep_nets_really_need_weight_decay_and_dropout", "keywords": ["convolutional neural networks", "regularization", "data augmentation"], "_bibtex": "@misc{\n  hern\u00e1ndez-garc\u00eda2018do,\n  title={Do deep nets really need weight decay and dropout?},\n  author={Alex Hern\u00e1ndez-Garc\u00eda and Peter K\u00f6nig},\n  year={2018},\n  url={https://openreview.net/forum?id=rJOL4E1wG}\n}", "authorids": ["alexhg15@gmail.com", "pkoenig@uos.de"], "authors": ["Alex Hern\u00e1ndez-Garc\u00eda", "Peter K\u00f6nig"], "TL;DR": "Weight decay and dropout regularization might benot only unnecessary, but their generalization gains can be achieved by data augmentation alone.", "pdf": "/pdf/04798f4cbb44883be83ebde213f39811de5e6264.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1520414564434, "tcdate": 1520414564434, "number": 1, "cdate": 1520414564434, "id": "H13DkVpOf", "invitation": "ICLR.cc/2018/Workshop/-/Paper138/Official_Comment", "forum": "rJOL4E1wG", "replyto": "rJOL4E1wG", "signatures": ["ICLR.cc/2018/Workshop/Paper138/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper138/Authors"], "content": {"title": "Added additional results from training WRN on ImageNet", "comment": "We have updated the pre-print manuscript on arXiv ( https://arxiv.org/abs/1802.07042 ) after getting the results of WRN trained on ImageNet, which further support the main hypothesis of the paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do deep nets really need weight decay and dropout?", "abstract": "The impressive success of modern deep neural networks on computer vision tasks has been achieved through models of very large capacity compared to the number of available training examples. This overparameterization is often said to be controlled with the help of different regularization techniques, mainly weight decay and dropout. However, since these techniques reduce the effective capacity of the model, typically even deeper and wider architectures are required to compensate for the reduced capacity. Therefore, there seems to be a waste of capacity in this practice. In this paper we build upon recent research that suggests that explicit regularization may not be as important as widely believed and carry out an ablation study that concludes that weight decay and dropout may not be necessary for object recognition if enough data augmentation is introduced.", "paperhash": "hern\u00e1ndezgarc\u00eda|do_deep_nets_really_need_weight_decay_and_dropout", "keywords": ["convolutional neural networks", "regularization", "data augmentation"], "_bibtex": "@misc{\n  hern\u00e1ndez-garc\u00eda2018do,\n  title={Do deep nets really need weight decay and dropout?},\n  author={Alex Hern\u00e1ndez-Garc\u00eda and Peter K\u00f6nig},\n  year={2018},\n  url={https://openreview.net/forum?id=rJOL4E1wG}\n}", "authorids": ["alexhg15@gmail.com", "pkoenig@uos.de"], "authors": ["Alex Hern\u00e1ndez-Garc\u00eda", "Peter K\u00f6nig"], "TL;DR": "Weight decay and dropout regularization might benot only unnecessary, but their generalization gains can be achieved by data augmentation alone.", "pdf": "/pdf/04798f4cbb44883be83ebde213f39811de5e6264.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222448651, "id": "ICLR.cc/2018/Workshop/-/Paper138/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJOL4E1wG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper138/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper138/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper138/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper138/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper138/Reviewers", "ICLR.cc/2018/Workshop/Paper138/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222448651}}, "tauthor": "alexhg15@gmail.com"}], "count": 5}