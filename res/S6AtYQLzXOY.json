{"notes": [{"id": "S6AtYQLzXOY", "original": "PU-m0icaxyh", "number": 1822, "cdate": 1601308200928, "ddate": null, "tcdate": 1601308200928, "tmdate": 1614985739585, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zpu8wy-iGen", "original": null, "number": 1, "cdate": 1610040398687, "ddate": null, "tcdate": 1610040398687, "tmdate": 1610473994266, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents an approach to use spatio-temporal self-similarity (STSS) as a feature for a convolutional neural network for video understanding. The proposed approach extracts STSS as a descriptor capturing similarities between local spatio-temporal regions, and adds conventional layers such as soft-argmax, fully connected layers, and conv. layers on top of it.\n\nOn one hand, all of the reviewers agree that the novelty of the paper is limited. On the other hand, most of the reviewers (except R1) appreciated thoroughness of the experiments and ablations. In the end, the reviewers gave 3 marginally above the acceptance threshold ratings and 1 marginally below the threshold rating.\n\nThe AC views this paper as a borderline paper. None of the reviewers are excited about the paper, and it is a typical \"Nice experiments with limited novelty\" (by R1) paper. The concept of the STSS itself was already proposed in prior studies as mentioned in the paper and by the reviewers, and this paper 'engineers' a new way to take advantage of STSS without further theoratical or conceptual justifications on why it should work. The newly added Kinetics and HMDB results in the rebuttal are nice, but the impact of STSS seems to be minimal in these results.\n\nOverall, the AC find the paper slighly lacking to be considered for ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040398674, "tmdate": 1610473994246, "id": "ICLR.cc/2021/Conference/Paper1822/-/Decision"}}}, {"id": "h2_Jy4AaHk", "original": null, "number": 14, "cdate": 1606255977047, "ddate": null, "tcdate": 1606255977047, "tmdate": 1606255977047, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "IEswbf6A5BL", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Thank you for the rather extensive evaluation and response", "comment": "Thanks for providing the clarifications. I really appreciate your very detailed responses! Having read the other reviews and the author's responses, I feel that the paper makes a good contribution to learning good representations for visual understanding.  The additional experiments on HMDB51 and Kinetics show the generalization ability of the approach to beyond motion-centric video data. Overall, I think this is a good paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "pHi6sj8LoqT", "original": null, "number": 13, "cdate": 1606234819701, "ddate": null, "tcdate": 1606234819701, "tmdate": 1606234819701, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "OMtgvH4-ZTy", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Experimental results on a depth enriched dataset (NTU RGB+D 120)", "comment": "> [R3] Besides these video based action benchmarks, it should be interesting to see results on a depth enriched data set (i.e., RGB-D), as missing depth information is one of the limitations from video data. Ideally, we should see similar good performance if the proposed methodology is effective for representation learning.\n\nAs the reviewer suggested, we have also validated our method on NTU RGB+D 120 dataset [1] for action recognition in RGB-D videos as follows. We use TSN-R18 and TSM-R18 two-stream networks, which consist of RGB and depth stream networks [2], as our backbone and add a single SELFY block, of which (L,U,V) is (5,9,9), into each network stream after res_3 stage. We follow the cross-subject evaluation criteria and report the top-1 accuracy. Table E summarizes the results. The SELFY block improves the accuracy of depth-stream networks as well as rgb-stream networks for both TSN-R18 and TSM-R18. In terms of the two-stream accuracy, the SELFY block also shows consistent improvement on both TSN-R18 and TSM-R18.  \n\nTable E: **Performance comparison with TSN-R18 on NTU RGB+D 120**. The top-1 accuracy is reported, following the cross-subject evaluation criteria.\n\n|model|rgb|depth|rgb+depth|\n|:---|:---:|:---:|:---:|\n|TSN-R18|64.2|70.3|76.2|\n|SELFYNet-TSN-R18|83.8|83.4|89.2|\n|TSM-R18|85.6|83.9|89.5|\n|SELFYNet-TSM-R18|86.3|84.6|90.1|\n\nIn the following, we provide experimental details. We adopt the segment-based frame sampling method and sample a clip of 8 frames as input.  For training, we train the networks from the scratch using 8 rgb (or depth) images. We use a cosine learning rate schedule with the first 10 epochs for warm-up, with setting the total epochs to 50 and the initial learning rate to 0.1. For testing, we sample a single center-cropped video clip. Following cross-subject evaluation criteria, we evaluate top-1 prediction scores of the rgb- and depth-stream, and then, we average the two scores to obtain the final accuracy of the two-stream networks. Other details are the same as those in Appendix A.\n\n[1] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.Y. Duan, and A.C. Kot. \u201cNTU RGB+D 120: A large scale benchmark for 3D human activity understanding.\u201d TPAMI. 2019. \\\n[2] Y. Wang, Y. Xiao, F. Xiong, W. Jiang, Z. Cao, J. T. Zhou, and J. Yuan. \u201c3DV: 3D dynamic voxel for action recognition in depth video.\u201d CVPR. 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "IEswbf6A5BL", "original": null, "number": 11, "cdate": 1606234424173, "ddate": null, "tcdate": 1606234424173, "tmdate": 1606234725679, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "QYK7znHctcg", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Experiments on HMDB51 to study the robustness to natural occlusions", "comment": "> [R2] While the robustness experiments test out the occlusion setting by cutting out a rectangular patch of a single center-frame, that does not, IMHO, shows robustness to occlusion since the features from surrounding frames (on both sides) will help mitigate this artificial occlusion. In practice, the occlusion can have a larger effect since it would not zero out the appearance but will add some noise into the feature extraction and hence provide a stronger challenge. Experiments on HMDB51, which has strong camera motion and hence introduces natural occlusions, would be a stronger experiment to show the generalization of the model beyond datasets with a strong motion-centric bias.\n\nAs the reviewer suggested, we have experimented on HMDB51 to study the robustness to natural occlusions; we compare the results of TSM-R18 and SELFYNet variants like Fig. 5a, 5b of our manuscript. Table E summarizes the results on HMDB51. Note that in this empirical study we train our models from scratch as in 3D-R18 [10] and thus the overall performance in the table is low compared to the state of the arts. The results show that SELFYNets with the long temporal range ({-2,...,2} and {-3,...,3}) outperforms other models in top-1 and top-5 accuracy, being consistent with the results in Fig. 5a, 5b of our paper. This demonstrates that the proposed STSS representation with a sufficient volume of neighborhood helps video-processing models to be more robust to natural occlusions in HMDB51.\n\nTable E. **Results of SELFYNets with different ranges of $l$ on HMDB51.**\n\n|model|spatial size $(H,W)$| range of $l$ | top-1 | top-5 |\n|:---|:---:|:---:|---:|---:|\n|3D-R18[10]|$(112,112)$|-|17.1|-|\n|TSM-R18|$(112,112)$|-|21.8|53.5|\n|TSM-R18|$(224,224)$|-|27.3|59.8|\n|SELFYNet|$(224,224)$|{0}|27.4|60.2|\n|SELFYNet|$(224,224)$|{1}|28.5|62.3|\n|SELFYNet|$(224,224)$|{-1,0,1}|30.9|64.7|\n|SELFYNet|$(224,224)$|{-2,...,2}|33.4|66.9|\n|SELFYNet|$(224,224)$|{-3,...,3}|34.3|67.1|\n\nIn the following, we provide experimental details. We adopt the dense frame sampling method [1] and sample a clip of 16 frames as input.  For training, we use a cosine learning rate schedule with the first 10 epochs for warm-up, with setting the total epochs to 50 and the initial learning rate to 0.1. For testing, we sample 10 uniform clips per video and average the softmax scores for the final prediction. Other details are the same as those in Appendix A.\n\n[1] J. Carreira and A. Zisserman, \u201cQuo vadis, action recognition? a new model and the kinetics dataset.\u201d CVPR. 2017\\\n[2] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. \u201cHMDB: A Large Video Datasbase for Human Motion Recognition.\u201d ICCV. 2011\\\n[3] J. Lin, C. Gan, and S. Han. \u201cTSM: Temporal Shift Module for Efficient Video Understanding.\u201d ICCV. 2019.\\\n[4] Y. Li, B. Ji, X. Shi, J. Zhang, B. Kang, and L. Wang. \u201c TEA: Temporal Excitation and Aggregation for Action Recognition.\u201d CVPR. 2020.\\\n[5] H. Kwon, M. Kim, S. Kwak, and M. Cho. \u201cMotionSqueeze: neural motion feature learning for video understanding.\u201d ECCV. 2020.\\\n[6] C. Feichtenhofer, H. Fan, J. Malik, and K. He. \u201cSlowFast Networks for Video Recognition.\u201d ICCV. 2019.\\\n[7] H. Wang, D. Tran, L. Torrensani, and M. Feiszli. \u201cVideo modeling with correlation networks.\u201d CVPR, 2020.\\\n[8] C. Yang, Y. Xu, J. Shi, B. Dai, B. Zhou. \u201cTemporal Pyramid Network for Action Recognition.\u201d CVPR. 2020.\\\n[9] X. Wang, R. Girshick, A. Gupta, and K. He. \u201cNon-local neural networks.\u201d CVPR. 2018.\\\n[10] K. Hara, H. Kataoka, and Y. Satoh. \u201cCan Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?.\u201d CVPR. 2018."}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "arkRT8h9zY-", "original": null, "number": 12, "cdate": 1606234475491, "ddate": null, "tcdate": 1606234475491, "tmdate": 1606234475491, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "QYK7znHctcg", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Experimental results on appearance-oriented datasets (Kinetics and HMDB51)", "comment": "> [R2] My concern is that all of the chosen datasets are very motion-centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB-51. In fact, the results on SS-V1 and SS-V2 show some gains (~2-3%) compared to the very closely related MotionSqueeze Network (Kwon et al, ECCV 2020) which computes correlation (rather than similarity) between adjacent frames (t and t+1). MSNet in datasets (HMDB-51 and Kinetics) where appearance plays a greater role and I am not sure how this proposed approach would help in those conditions.\n\nAs you suggested, we have validated our method on the appearance-oriented datasets, Kinetics [1] and HMDB51 [2]. The results are compared to recent state-of-the-art methods below. \n\nTable C below summarizes the results on Kinetics. The first section of the table shows the results of the models with ResNet50 using 16 frames [3,4,5], which is the same size of backbone and input as ours. The second and third section of the table shows the results of the models with the same backbone (ResNet50) using 32 frames [6,7,8], and a larger backbone (ResNet101) using a bigger input [6,7,8], respectively. The proposed SELFYNet, the last section of the table, obtains a clear improvement of 2.4 \\%p in top-1 accuracy compared to the TSM baseline [3], achieving the best performance among the models with ResNet50 using 16 frames.\n\nTable C. **Results of the state-of-the-art methods on Kinetics.**\n\n|model|backbone|# frames|top-1|top-5|\n|:---|:---:|:---:|:---:|:---:|\n|TSM [3]|ResNet 50|16|74.7|-|\n|TEA [4]|ResNet 50|16|76.1|92.5|\n|MSNet-TSM [5]|ResNet 50|16|76.4|-|\n|------------------------------|---------------|----------|------|------|\n|SlowFast 8$\\times$8 [6] |ResNet 50|32|77.0|-|\n|CorrNet [7]|ResNet 50|32|77.2|92.6|\n|TPN [8]|ResNet 50|32|77.7|93.3|\n|------------------------------|---------------|----------|------|------|\n|SlowFast 16$\\times$8 [6]|ResNet 101|32|77.9|93.2|\n|SlowFast 16$\\times$8 [6]|ResNet 101|64|78.9|93.5|\n|TPN [8]|ResNet 101|32|78.9|93.9|\n|CorrNet [7]|ResNet 101|32|79.2|-|\n|------------------------------|---------------|----------|------|------|\n|SELFYNet-TSM (ours)|ResNet 50|16|77.1|93.0|\n\nTable D below summarizes the results on HMDB51. We compare SELFYNet with the TSM baseline and MSNet [5]. SELFYNet obtains a significant improvement of 4.7 \\%p in top-1 accuracy compared to the baseline, and outperforms MSNet. We observe that our method shows a bigger improvement on HMDB51 than Kinetics since HMDB51 clips have stronger motions as the reviewer mentioned.\n\nTable D. **Results of the proposed method on HMDB51.**\n\n|model|backbone|# frames|top-1|top-5|\n|:---|:---:|:---:|:---:|:---:|\n|TSM (our impl.)|ResNet 50|16|73.3|94.2|\n|MSNet-TSM [5]|ResNet 50|16|77.4|-|\n|SELFYNet-TSM (ours)|ResNet 50|16|78.0|95.6|\n\nThe reviewer\u2019s concern on our approach appears to come from that the recent motion-learning method of MSNet [5] does not show a significant gain on Kinetics. In our opinion, the limited gain is more related to the use of limited resources in training, e.g., in terms of the size of the backbone or the number of input frames, than to the use of correlations. As can be seen in Table C, SlowFast network [6] achieves better performance when a larger backbone or a longer input is used. Also, CorrNet [7] , which learns to extract motion features similarly to MSNet, obtains the state-of-the-art accuracy with ResNet101 backbone using 32 frames. While the results of SELFYNet with a larger backbone using longer input is not available for now due to the lack of time and GPU resources, we believe that SELFYNet can achieve state-of-the-art performance in the same conditions with aforementioned methods. We will do our best to add such large-scale experiments to our final version. \n\nIn the following, we provide implementation details for our experiments. For both datasets, we adopt the dense frame sampling method [1] and sample a clip of 16 frames. For training on Kinetics, we use a cosine learning rate schedule with the first 10 epochs for warm-up. We set the initial learning rate to 0.01 and total epochs to 65. For training on HMDB51, we fine-tune the Kinetics pre-trained model like as recent approaches [1,3,4,5] and adopt a cosine learning rate schedule with 5 epochs for warm-up. We set the initial learning rate to 0.001 and total epochs to 35. For testing, we sample 10 uniform clips per video and average the softmax scores for the final prediction. We follow the strategy of non-local networks [9] to pre-process the frames and take 3 crops as input. Other experimental details are the same as those in Appendix A of our paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "FVvwpWhzczJ", "original": null, "number": 10, "cdate": 1606234143620, "ddate": null, "tcdate": 1606234143620, "tmdate": 1606234143620, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Paper revised", "comment": "We thank all reviewers for giving thoughtful comments and questions. The revised paper has been uploaded online. The major updates are summarized below:\n\n- Add reasoning the model design choice in Sec. 3.2.\n- Add a study of the effect of feature integration stage in Sec.4.3.\n- Update new top-1 and top-5 accuracies of SELFYNet TSM-R18 (5,9,9): (48.2, 77.2) -> (48.4, 77.4) in Sec.4.3 and Appendix B.\n- Add FLOPs in Sec.4.3 and Appendix B.\n- Add a study of the effect of the original $\\mathbf{V}$ in Appendix B.\n- Update a study of the comparison with non-local methods in Appendix B.\n- Add a study of the comparison with correlation-based methods in Appendix B.\n- Add the results of $\\textrm{pool}_1$ and $\\textrm{res}_5$ in Appendix B.\n- Add figures comparing our method with non-local methods and correlation-based methods in Appendix B (the figures are provided for better comprehension).\n- Add a discussion section on the relationship to the local self-attention in Appendix C."}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "VFFZQ6l58q8", "original": null, "number": 7, "cdate": 1605802079851, "ddate": null, "tcdate": 1605802079851, "tmdate": 1606207656920, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "xkIEMtxjN0", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (2/2)", "comment": "> [R4] In the feature integration block, firstly 3\u00d73 spatial convolution kernel is applied, then the temporal offset (L) and the channels (CF\u2217) dimension is flattened, and finally 1\u00d71\u00d71 spatio-temporal convolution is applied. Is this design chosen experimentally or is there any intuition behind?\n\nOur intuition is that each of $L$ channel-groups in the flattened $LC_{F^\u2217}$-dimensional STSS representation can be viewed as a spatial self-similarity feature within each relative temporal offset so that the $1 \\times 1 \\times 1$ convolutional layer, i.e. fully connected layer, is able to reduce the flattened feature to the $C$-dimensional feature vector considering the information of different temporal offsets. We also empirically found that a pooling operation such as max pooling or average pooling, which reduces $\\mathbf{F} \\in \\mathbb{R}^{T \\times X \\times Y \\times L \\times C_{F^*}}$ along $L$ dimension, significantly degrades the performance. We conjecture that such pooling operations, which get rid of the information of temporal offsets, damage long-range temporal modeling.\n\n> [R4] It may be interesting to discuss the relationship with self-attention based networks such as [1-3].\n\nThe local self-attention [6,7,8] and our method have a common denominator of using the self-similarity tensor but use it in a very different way and purpose. The local self-attention mechanism aims to aggregate the local context features using the self-similarity tensor and it thus uses the self-similarity values as attention weights for feature aggregation. However, our method aims to learn a generalized motion representation from the local STSS, so the final STSS representation is directly fed into the neural network instead of multiplying it to local context features.\nFor a clear comparison, we conduct an ablation experiment as follows. We add a single *spatio-temporal* local self-attention layer, extended from [7], after $res_3$ followed by feature integration layers in Sec.3.2.2. All experimental details are the same as those in Appendix A, except that we reduce the channel dimension $C$  of appearance feature $\\mathbf{V}$ to 32. Table C summarizes the results on SS-V1. The spatio-temporal local self-attention layer is accurate as 43.8 \\%p at top-1 accuracy, and both of SELFY blocks using the embedded Gaussian and the cosine similarity outperform the local self-attention by achieving top-1 accuracy as 47.6 \\%p and 47.8 \\%p, respectively. These results are in alignment with the prior work [9], which reveals that the self-attention mechanism hardly captures motion features in video. We will add this to the discussion section in our final manuscript.\n\nTable C: **Performance comparison with the local self-attention.** R.P.E. is an abbreviation for relative positional embeddings.\n\n|model|similarity function|feature extraction method|top-1|top-5|\n|:---|:---:|:---:|:---:|:---:|\n|TSM-R18|-|-|43.0|72.3|\n|SELFYNet|embedded Gaussian|multiplication with $\\mathbf{V}$+R.P.E|43.8|72.9|\n|SELFYNet|embedded Gaussian|conv|47.6|76.8|\n|SELFYNet (ours)|cosine|conv|**47.8**|**77.1**|\n\n[6] H. Hu, Z. Zhang, Z. Xie, and S. Lin. \u201cLocal relation networks for image recognition.\u201d ICCV. 2019.\\\n[7] P. Ramachandran, N. Parmar, A. Vaswani, I. Vello, A. Levskaya, and J. Shlens. \u201cStand-alone self-attention in vision models.\u201d NIPS. 2019.\\\n[8] H. Zhao, J. Jia, and V. Koltun. \u201cExploring self-attention for image recognition.\u201d CVPR. 2020.\\\n[9] X. Liu, J.Y. Lee, and H. Jin. \u201cLearning video representations from correspondence proposals.\u201d CVPR. 2019.\n\n> [R4] About Table 7 in Appendix, how is the result if SELFY block is used in res1  and res5 ? I guess these options do not work well, but I think it is beneficial to list all the results.\n\nWe conducted the additional experiments and the results on SS-V1 are in Table D below, where we add the SELFY block to the different places. Here, we set the spatial resolution of $\\mathbf{V}$ to $14 \\times 14$. The SELFY block after $pool_1$ improves 2.7 \\%p at top-1 accuracy, while the SELFY block after $res_5$ does not. We conjecture that the spatial resolution ($7 \\times 7$) is too small to extract meaningful motion features for action recognition. We will update Table 7 in our manuscript.\n\nTable D: **Performance comparison with different positions of SELFY block.**\n\n|model|position|top-1|top-5|\n|:---|---|---|---|\n|TSM-R18|-|43.0|72.3|\n|SELFYNet |after $pool_1$|45.7|74.6|\n| SELFYNet|after $res_2$|47.2|76.6|\n|SELFYNet |after $res_3$|**48.4**|**77.6**|\n|SELFYNet |after $res_4$|46.6|76.0|\n|SELFYNet |after $res_5$|42.8|72.6|"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "glJSIWBWO4c", "original": null, "number": 9, "cdate": 1605925787128, "ddate": null, "tcdate": 1605925787128, "tmdate": 1605925787128, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "VFFZQ6l58q8", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Thanks for the clarification", "comment": "Thank you for the clarification and the effort for the additional experiments.\nTaking the response together with other reviewers' comments and the authors' response to them, I am still positive in accepting the paper, and thus would like to keep my rating.\nIn my opinion, the clarified novelty still sounds moderate, but combined with the detailed experiment and the analysis, I think the paper is valuable to the community.\nI recommend the authors to include the clarification of the novelty, intuitions behind the design choices, and the discussion on the relation to the local and global self-attention networks in the final version."}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "BkFFHWHQ8YV", "original": null, "number": 3, "cdate": 1605801441219, "ddate": null, "tcdate": 1605801441219, "tmdate": 1605856527312, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "UbNjlFSBh3", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (1/2)", "comment": "Thanks for your constructive comments, we will revise our paper by reflecting all of them as much as possible.\n\n> [R1] The first concern is about the limited novelty of the method. Despite revisiting the self-similarity, from section 3.1, the learning of generalized and long-term information is a property of the STSS rather than a contribution of this work. If necessary, authors should give more details on the classifical STSS to show the improvement. \n\n\nWhile the concept of STSS is already proposed in the prior work [1,2], there are important differences in the way of using STSS. First of all, note that no representation learning is involved in the prior work. E.g., in [1], they extract a pixel-wise hand-crafted STSS feature from the correlation volume $(L,U,V)$ by transforming the correlation volume into a binned log-polar representation and selecting the maximal correlation value in each bin. In contrast, the main contribution of our work is to use STSS as a basis for end-to-end learning a powerful long-term relational representation rather than using STSS feature itself as in the prior work. This is motivated by our novel view on the conventional motion (i.e., optical flow) as one-hot cross-similarity, which is a form of sparse and temporally-restricted STSS as briefly mentioned in our introduction. To this end, we explore different representation learning strategies (Sec.3.2.1 & 3.2.2) and propose our model that consists of convolutions along the $(L,U,V)$ dimension (Sec.3.2.1), which learn relational patterns among similarity values, and spatial convolutions (Sec 3.2.2), which integrate pixel-wise STSS features by extending the receptive fields. \n\nFurthermore, unlike the previous methods, we propose to combine it with the ordinary visual feature (via addition in Eq.(9)) as a complementary one. This is also important. How complementary are they to each other? We find a clear synergic effect of combining STSS with visual features. Denoting the relational feature $\\mathrm{ReLU}(\\mathbf{F}^\\star \\times_5 \\mathbf{W}_\\theta)$ by $\\mathbf{R}$ and the ordinary visual feature by $\\mathbf{V}$, Table A below compare the cases of $\\mathbf{V}$, $\\mathbf{R}$, and $\\mathbf{R}+\\mathbf{V}$ on SS-V1. As we use the combination of $\\mathbf{V}$ and $\\mathbf{R}$, we obtain the additional gain of 2.9 \\%p. \n\nTable A: **The effect of STSS representation ($\\mathbf{R}$) with the ordinary visual feature ($\\mathbf{V}$)**\n\n|model|feature|top-1|top-5|\n|:---|:---:|:---:|:---:|\n|TSM-R18|$\\mathbf{V}$|43.0|72.3|\n|SELFYNet|$\\mathbf{R}$|45.5|75.9|\n|SELFYNet (ours)|$\\mathbf{R}$ + $\\mathbf{V}$|**48.4**|**77.6**|\n\n[1] E. Shechtman  and M. Irani. \u201cMatching local self-similarities across images and videos.\u201d CVPR. 2007.\\\n[2] I.N Junejo, E. Dexter, I. Laptev, and P. Perez. \u201cCross-view action recognition from\ntemporal self-similarities.\u201d ECCV. 2008."}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "qdBbasvs6RU", "original": null, "number": 8, "cdate": 1605802440871, "ddate": null, "tcdate": 1605802440871, "tmdate": 1605805124551, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "xkIEMtxjN0", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (1/2) ", "comment": "Thanks for appreciating our work and providing constructive comments. Below are our responses and we will reflect them in our final manuscript.\n\n> [R4] The novelty and the originality of the paper may be relatively low because the concept of the STSS itself was already proposed in prior studies as mentioned in the paper.\n\nWhile the concept of STSS is already proposed in the prior work [1,2], there are important differences in the way of using STSS. The main contribution of our work is to use STSS as a basis for end-to-end learning a powerful long-term relational representation rather than using STSS feature itself as in the prior work. This is motivated by our novel view on the conventional motion (i.e., optical flow) as one-hot cross-similarity, which is a form of sparse and temporally-restricted STSS as briefly mentioned in our introduction. To this end, we explore different representation learning strategies (Sec.3.2.1 & 3.2.2) and propose our model that consists of convolutions along the (L,U,V) dimension (Sec.3.2.1), which learn relational patterns among similarity values, and spatial convolutions (Sec 3.2.2), which integrate pixel-wise STSS features by extending the receptive fields. \n\nFurthermore, unlike the previous methods, we propose to combine it with the ordinary visual feature (via addition in Eq.(9)) as a complementary one. This is also important. How complementary are they to each other? We find a clear synergic effect of combining STSS with visual features. Denoting the relational feature $\\mathrm{ReLU}(\\mathbf{F}^\\star \\times_5 \\mathbf{W}_\\theta)$ by $\\mathbf{R}$ and the ordinary visual feature by $\\mathbf{V}$, Table A below compares the cases of $\\mathbf{V}$, $\\mathbf{R}$, and $\\mathbf{R}+\\mathbf{V}$ on SS-V1. As we use the combination of $\\mathbf{V}$ and $\\mathbf{R}$, we obtain the additional gain of 2.9 \\%p. \n\nTable A: **The effect of STSS representation ($\\mathbf{R}$) with the ordinary visual feature ($\\mathbf{V}$)**\n\n|model|feature|top-1|top-5|\n|:---|:---:|:---:|:---:|\n|TSM-R18|$\\mathbf{V}$|43.0|72.3|\n|SELFYNet|$\\mathbf{R}$|45.5|75.9|\n|SELFYNet (ours)|$\\mathbf{R}$+$\\mathbf{V}$|**48.4**|**77.6**|\n\n[1] E. Shechtman  and M. Irani. \u201cMatching local self-similarities across images and videos.\u201d CVPR, 2007.\\\n[2] I.N Junejo, E. Dexter, I. Laptev, and P. Perez. \u201cCross-view action recognition from\ntemporal self-similarities.\u201d ECCV, 2008.\n\n\n> [R4] The reasoning or intuition behind some of the design choices are not always clear\u2026.  In other words, even if the output is F\u2208RT\u00d7X\u00d7Y\u00d7CF, the motion information can be encoded in the output tensor. Possibly the authors empirically found the present design choice is better. If so, how does the performance change if L is not preserved?\n\nThe design choice of preserving the $L$ dimension to the last part of SELFY is inspired by the recent action recognition approach [3,4,5]; they maintain the temporal resolution throughout the architecture to the global pooling layer before classification. E.g., The work of [4] finds that holding fine temporal resolution is helpful for capturing detailed motion, and the architecture is thus designed to use no temporal downsampling layers. To validate our design choice, we have conducted experiments regarding the $L$ dimension: (1) preserving $L$ (ours), (2) reducing $L$ once via a convolution layer, and (3) reducing $L$ gradually via convolution layers. Table B below shows the results on SS-V1. Our method (1) shows the best performance at top-1 and top-5 accuracy. The case of (2) performs better than the case of (3), which implies that preserving L is more beneficial for capturing detailed information from STSS.\n\nTable B: **The effect of reducing the L in feature extraction.** $n * (k,k,k)$ denotes n convolution layers along the $(L,U,V)$ volume with a kernel size of $(k,k,k)$. We reduce the $L$ using convolutions without padding.\n\n|model|feature extraction method ($(L,U,V)=(5,9,9)$)|top-1|top-5|\n|:---|:---:|:---:|:---:|\n|SELFYNet (ours)|(1) $4*(1,3,3)$|**48.4**|**77.6**|\n|SELFYNet|(2) $4*(1,3,3) + 1*(5,1,1)$|47.5|76.8|\n|SELFYNet|(3) $2*(1,3,3)+1*(3,1,1)+2*(1,3,3)+1*(3,1,1)$|47.3|76.4|\n\n[3] J. Lin, C. Gan, and S. Han. \u201cTSM: Temporal Shift Module for Efficient Video Understanding.\u201d ICCV. 2019.\\\n[4] C. Fiechtenhofer, H. Fan, J. Malik, and K. He. \u201cSlowFast Networks for Video Recognition.\u201d ICCV. 2019.\\\n[5] C. Fiechtenhofer. \u201cX3D: Expanding Architectures for Efficient Video Recognition.\u201d CVPR. 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "QYK7znHctcg", "original": null, "number": 6, "cdate": 1605801977484, "ddate": null, "tcdate": 1605801977484, "tmdate": 1605802654698, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "UCrbsWsJABL", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks for appreciating our work and providing constructive comments. Below are our responses and we will reflect them in our final manuscript.\n\n> [R2] My concern is that all of the chosen datasets are very motion-centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB-51.\n\nWe are conducting the experiments on the appearance-oriented datasets. We will leave the comment as soon as the experiments are finished.\n\n> [R2] I assume the function sim(.) from Equation 1 is cosine similarity since it was not explicitly mentioned anywhere. Is there any specific reason the correlation function was not used? What is the effect of using correlation in place of cosine similarity?\n\nAs pointed out, we use the cosine similarity as our default similarity function. We have explored the effect of different similarity functions on learning STSS, and found that the cosine similarity performs the best. Table A compares the performance on SS-V1 with different similarity functions: cosine similarity (ours), dot product [1], and embedded Gaussian [2]. The experimental details are the same as those in Appendix A, except that we reduce the channel dimension $C$  of appearance feature $\\mathbf{V}$ to 32 for the less GPU memory consumption. The cosine similarity gives better accuracy, but not much, by 0.1 \\%p and 0.2 \\%p than the dot product and the embedded Gaussian at top-1 accuracy. We will clarify this point in our final manuscript.\n\nTable A: **Results of different similarity functions.** \n\n|model|\\# channels|similarity function|top-1|top-5|\n|:---|:---:|:---:|:---:|:---:|\n|TSM-R18|-|-|43.0|72.3|\n|SELFYNet (ours)|32|cosine|**47.8**|**77.1**|\n|SELFYNet|32|dot product|47.7|76.6|\n|SELFYNet|32|embedded Gaussian|47.6|76.8|\n\n[1] P. Fischer, et al. \u201cFlowNet: learning optical flow with convolutional networks.\u201d ICCV. 2015.\\\n[2] X. Wang, R. Girshick, A. Gupta, and K. He. \u201cNon-local neural networks.\u201d CVPR. 2018.\n\n> [R2] It would be interesting to see the effect of this since it would essentially allow us to see how modeling the longer temporal context would affect MS Net (the most closely related network) and help highlight the contribution of the proposed model beyond the obvious empirical gains.\n\nBelow are the comparisons between SELFYNet and MSNet [3] to demonstrate the effectiveness of STSS as well as our feature extraction methods. For an apple-to-apple comparison, we apply the kernel soft-argmax and the max-pooling operations (*KS+CM* in [3]) to the feature extraction method by following their official github code. Please note that their official code also uses the cosine similarity as the same as ours. As we restrict the temporal offset $l$ of the SELFY variant to $\\{ +1 \\}$, it is equivalent to MS module of which *feature transformation layers* are the standard 2D conv layers. All experimental details are the same as those in Appendix A. \nTable B summarizes the results on SS-V1. KS+CM method achieves 46.1 \\%p at top-1 accuracy. As we enlarge the temporal window $L$ to 5, we obtain an additional gain as 1.3 \\%p. The learnable convolution layers improve top-1 accuracy by 1.0 \\%p in both cases. We will add this experiment in our final manuscript.\n\nTable B: **Performance comparison with MSNet.**\n\n| model | feature extraction method | $(L,U,V)$ | top-1 | top-5 |\n|:---|:---:|:---:|---:|---:|\n|TSM-R18|-|-|43.0|72.3|\n|SELFYNet|KS+CM|$(1,9,9)$|46.1|75.3|\n|SELFYNet|KS+CM|$(5,9,9)$|47.4|76.8|\n|SELFYNet|conv|$(1,9,9)$|47.1|76.3|\n|SELFYNet|conv|$(5,9,9)$|**48.4**|**77.6**|\n\n[3] H. Kwon, M. Kim, S. Kwak, and M. Cho. \u201cMotionSqueeze: neural motion feature learning for video understanding.\u201d ECCV. 2020.\n\n> [R2] the occlusion can have a larger effect since it would not zero out the appearance but will add some noise into the feature extraction and hence provide a stronger challenge. Experiments on HMDB51, which has strong camera motion and hence introduces natural occlusions, would be a stronger experiment to show the generalization of the model beyond datasets with a strong motion-centric bias.\n\nWe are conducting the experiments on HMDB51. We will leave the comment as soon as the experiments are finished.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "28PjGvcUkUd", "original": null, "number": 2, "cdate": 1605801329035, "ddate": null, "tcdate": 1605801329035, "tmdate": 1605802367789, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "UbNjlFSBh3", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (2/2)", "comment": "> [R1] The authors briefly mention the differences between the new proposed and non-local approaches (Wang et al., 2018; Liu et al., 2019) and correlation-based methods (Wang et al., 2020; Kwon et al., 2020), which appears inadequate and is better to integrate them into a comparison in the form of Figure 1.\n\nWhile computing the self-similarity values as ours, the non-local approaches [3,4] use them as attention weights for feature aggregation by multiplying them to the visual features [3] or aligning top-$K$ corresponding features [4]; they both do not use STSS itself as a relational representation. In contrast, our method does it indeed and learns a more powerful relational feature from STSS. As for the performance comparison, Table 7a of our appendix shows that our method clearly outperforms non-local approaches in top-1, top-5 accuracy and FLOPs.\n\nFurthermore, while the correlation-based methods [5,6] extract motion features between two adjacent frames only and are thus limited to short-term motion, our method effectively captures bi-directional and long-term motion information via learning with the sufficient volume of STSS. Our method can also exploit richer information from the self-similarity values than other methods. MS module [5] only focuses on the maximal similarity value of the $(U,V)$ dimension to extract flow information, and Correlation block [6] uses an $1 \\times 1$  convolution layer for extracting motion features from the similarity values. In contrast to the two methods, we introduce a generalized motion learning framework using the self-similarity tensor at Sec 3.2 of our paper and demonstrate the effectiveness of our method through ablation studies. \n\nFollowing the reviewers\u2019 suggestion, we will add a figure that describes these differences between the previous ones and ours in our final manuscript. \n\n[3] X. Wang, R. Girshick, A. Gupta, and K. He. \u201cNon-local neural networks.\u201d CVPR. 2018.\\\n[4] X. Liu, J.Y. Lee, and H. Jin. \u201cLearning video representations from correspondence proposals.\u201d CVPR. 2019.\\\n[5] H. Wang, D. Tran, L. Torrensani, and M. Feiszli. \u201cVideo modeling with correlation networks.\u201d CVPR. 2020.\\\n[6] H. Kwon, M. Kim, S. Kwak, and M. Cho. \u201cMotionSqueeze: neural motion feature learning for video understanding.\u201d ECCV. 2020.\n\n> [R1] How are parameters (5, 9, 9) and (9, 9 ,9) chosen? The best result occurs as the temporal offsets being chosen from {-3, ..., 3}. Will there be better accuracy when the range is larger?\n\nWe set the matching region $(L,U,V)$ to $(5,9,9)$ and $(9,9,9)$ since they perform the best accuracy-computation trade-offs. When we use 8 frames as the input, the accuracy saturates as the $(L,U,V)$ volume becomes larger than $(5,9,9)$, resulting in worse trade-offs. When increasing the input frames from 8 to 16, we simply extend $L$ to 9 maintaining the time span of STSS. Table B summarizes the SS-V1 results of FLOPs and accuracies varying $(L,U,V)$ given 8 RGB frames.\n\nTable B: **Performance comparison with different matching regions of SELFY block.**\n\n|model|$(L,U,V)$|FLOPs (G)|top-1|top-5|\n|:---|:---:|:---:|:---:|:---:|\n|TSM-R18 |-|14.6|43.0|72.3|\n|SELFYNet|$(1,9,9)$|15.3|47.1|76.3|\n|SELFYNet|$(3,9,9)$|16.3|47.8|76.7|\n|SELFYNet|$(5,9,9)$|17.3|48.4|77.6|\n|SELFYNet|$(7,9,9)$|18.3|**48.6**|77.7|\n|SELFYNet|$(11,9,9)$|20.2|48.4|77.6|\n|SELFYNet|$(5,5,5)$|17.1|47.8|77.1|\n|SELFYNet|$(5,13,13)$|18.4|48.4|77.8|\n|SELFYNet|$(5,17,17)$|19.8|**48.6**|**78.3**|\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "eWPaOIcKdzL", "original": null, "number": 5, "cdate": 1605801775349, "ddate": null, "tcdate": 1605801775349, "tmdate": 1605801857171, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "WmuKjlP7SKL", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (1/2)", "comment": "Thanks for appreciating our work and providing constructive comments. Below are our responses and we will reflect them in our final manuscript.\n\n> [R3] From Figure. 2 or Section 3.2.2, the final STSS representation Z is the combination of original input V and the STSS feature F(S(V)), so kind of unclear how much contribution is made from the original V in terms of the final recognition performance. Maybe this part is addressed in the experimental section somewhere. \n\nFollowing your comments, we evaluate SELFYNet purely based on STSS features to see how much the ordinary visual feature $\\mathbf{V}$ contributes for the final prediction. That is, we pass the STSS features, $\\mathrm{ReLU}(\\mathbf{F}^\\star \\times_5 \\mathbf{W}_\\theta) $, into the downstream layers without the visual features $\\mathbf{V}$ (Eq.9 in our main paper). For the simplicity of description, we denote the relational feature $\\mathrm{ReLU}(\\mathbf{F}^\\star \\times_5 \\mathbf{W}_\\theta)$ by $\\mathbf{R}$ . \nTable A compares the results of using different cases of the output tensor $\\mathbf{Z}$ ($\\mathbf{Z}= \\mathbf{V}$, $\\mathbf{Z}=\\mathbf{R}$, and $\\mathbf{Z}=\\mathbf{R}+\\mathbf{V}$) on SS-V1. Interestingly, SELFYNet using only $\\mathbf{R}$ achieves 45.5 \\%p at top-1 accuracy, which is higher as 2.5 \\%p than the baseline. As we add $\\mathbf{V}$ to $\\mathbf{R}$, we obtain an additional gain as 2.9 \\%p. It indicates that the STSS features and the visual features are complementary to each other. We will add the experiment in our final manuscript.\n\nTable A: **The effect of fusing STSS with ordinary visual features.**\n\n|model|feature|top-1|top-5|\n|:---|:---:|:---:|:---:|\n|TSM-R18|$\\mathbf{V}$|43.0|72.3|\n|SELFYNet|$\\mathbf{R}$|45.5|75.9|\n|SELFYNet (ours)|$\\mathbf{R}+\\mathbf{V}$|**48.4**|**77.6**|\n\n> [R3] The proposed neural block, SELFY, includes 3 parts as self-similarity transformation, feature extraction, and feature integration, it seems none of them is original but the combination of these put into the end-of-end NN structure is new as claimed by authors. There are some additional experiment results presented in section 4.3 (for different types of similarity, and for different feature extraction), still, it seems unclear how important or sensitive for each step within the whole framework based on TSN ResNets and TSM ResNets, given the complexity here. \n\nEach of the three stages in our SELFY block plays a crucial role in learning fine-grained STSS features. The first stage, i.e., STSS transformation, determines the volume of the STSS $(L,U,V)$. To validate the importance and sensitivity of the first stage, we conducted experiments that compare the performance with different $(L,U,V)$. The results on SS-V1 are shown in Table B below. In most cases, the larger STSS volume results in better accuracy. \n\nDuring the second and the third stages, i.e., feature extraction and integration, the SELFY block learns to capture fine-grained structural patterns within the STSS tensor, resulting in the richer STSS representation. To validate the importance of the two stages, we conducted experiments that compare the performance of different combinations of feature extraction and integration methods. The results on SS-V1 are shown in Table C below. Compared to the baseline, we improve top-1 accuracy by 1.0 \\%p, 2.9 \\%p, and 3.7 \\%p by changing the feature extraction method from soft-argmax to MLP to convolution, respectively. We also obtain additional gain as 0.5 \\%p and 1.2 \\%p at the top-1 accuracy by replacing the feature-integrating FC layer with MLP and with stacked convolutional layers, respectively. These results demonstrate that our design choice of using convolutions along $(U,V)$ and $(H,W)$ is the most effective in learning the geometry-aware STSS representation.\n\nTable B: **Performance comparison with different volumes of SELFY block.**\n\n|model|$(L,U,V)$|FLOPs (G)|top-1|top-5|\n|:---|:---:|---|---|---|\n|TSM-R18 |-|14.6|43.0|72.3|\n|SELFYNet|$(1,9,9)$|15.3|47.1|76.3|\n|SELFYNet|$(3,9,9)$|16.3|47.8|76.7|\n|SELFYNet|$(5,9,9)$|17.3|48.4|77.6|\n|SELFYNet|$(7,9,9)$|18.3|**48.6**|77.7|\n|SELFYNet|$(11,9,9)$|20.2|48.4|77.6|\n|SELFYNet|$(5,5,5)$|17.1|47.8|77.1|\n|SELFYNet|$(5,13,13)$|18.4|48.4|77.8|\n|SELFYNet|$(5,17,17)$|19.8|**48.6**|**78.3**|\n\nTable C: **Ablation results on different feature extraction and feature integration methods.** Smax denotes the soft-argmax operation. MLP layer consists of four fully connected (FC) layers. The $1 \\times 1 \\times 1$ convolutional layer in the feature integration stage is omitted from this table.\n\n|model|feature extraction|feature integration|FLOPs (G)|top-1|top-5|\n|:---|:---:|:---:|:---:|:---:|:---:|\n|TSM-R18|-|-|14.6|43.0|72.3|\n|SELFYNet|Smax|FC|14.8|44.0|73.2|\n|SELFYNet|MLP|FC|15.4|45.9|75.1|\n|SELFYNet|Conv|FC|16.1|46.7|75.8|\n|SELFYNet|Conv|MLP|16.3|47.2|75.9|\n|SELFYNet (ours)|Conv|Conv|17.3|**48.4**|**77.6**|"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "OMtgvH4-ZTy", "original": null, "number": 4, "cdate": 1605801695152, "ddate": null, "tcdate": 1605801695152, "tmdate": 1605801695152, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "WmuKjlP7SKL", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (2/2)", "comment": "> [R3] Self-similarity can be applied for both image and videos (i.e., Section 3 or Ref [5], etc), I know the focus of this submission is video action recognition, still, seems interesting to know the proposed framework \"3 steps combined + CNN end-of-end\" apply to image object recognition. If that also achieved good results compared with state-of-the-art for object recognition, it will be a strong support for the proposed methodology, and if not, which is also good to know & could be multiple reasons behind. \n\nWe agree with the point and find that learning self-similarity is also effective in image classification. We validated it on CIFAR-10 and -100, which are the popular image classification benchmarks. For the experiments, we organize two basic blocks: a light-weighted version of SELFY block (SELFY$_l$) and a spatial convolutional block (SCB) with four 3x3 spatial convolutional layers as the counterpart. Both blocks have the same receptive field as ($9 \\times 9$), and the similar FLOPs. We insert each of the basic blocks and their combinations (please refer to Fig.4c,4d, and 4f of our paper) into the middle of ResNet20.\nTable D summarizes the results on CIFAR-10 and -100. ResNet20, which is our baseline, performs as 8.44 \\%p and 30.34 \\%p at top-1 error on CIFAR-10 and -100, respectively. \nResNet20 with SELFY$_l$ reduces the top-1 errors to 1.1. \\%p and 1.98 \\%p on the both benchmarks. The results are in alignment with those in Table 3 of our paper; SELFYNet with $l=\\{0\\}$ that learns spatial self-similarity improves the accuracy on both TSN and TSM backbones. Interestingly, ResNet20 with SEFLY$_l$ is more accurate than ResNet with SCB. Furthermore, in terms of the block combinations (4th - 6th rows), SELFY$_l$ + SCB and SELFY$_l$ + SELFY$_l$ show the lowest top-1 error rate as 64.7 \\%p and 27.50 \\%p on CIFAR-10 and -100, respectively. These results indicate that the spatial self-similarity features and the ordinary visual features are complementary to each other also on image classification, which is consistent with the results in Table 5 of our paper.\n\nTable D: **The effect of spatial self-similarity for image classification.**\n\n|model|FLOPS (G)|# params (K)|CIFAR-10 err-1|CIFAR-100 err-1|\n|:---|:---:|:---:|:---:|:---:|\n|baseline (ResNet20)|35.79|220|8.44|30.34|\n|$+$ SCB|50.41|277|7.81|28.77|\n|$+$ SELFY$_l$ |49.99|254|7.34|28.36|\n|$+$ SCB $+$ SCB |65.02|333|7.17|27.83|\n|$+$ SELFY$_l$ $+$ SELFY$_l$  |64.16|287|6.79|**27.50**|\n|$+$ SELFY$_l$ $+$ SCB |64.59|310|**6.47**|27.59|\n\n> [R3] Besides these video based action benchmarks, it should be interesting to see results on a depth enriched data set (i.e., RGB-D), as missing depth information is one of the limitations from video data. Ideally, we should see similar good performance if the proposed methodology is effective for representation learning.\n\nWe also agree with the point, and thus, we\u2019ve been conducting the experiments on the depth enriched dataset, e.g. NTU RGB+D, to validate our methods. We will leave the comment as soon as the experiments are finished.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "S6AtYQLzXOY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1822/Authors|ICLR.cc/2021/Conference/Paper1822/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Comment"}}}, {"id": "UbNjlFSBh3", "original": null, "number": 1, "cdate": 1602740232057, "ddate": null, "tcdate": 1602740232057, "tmdate": 1605024350694, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Review", "content": {"title": "Nice experiments with limited novelty", "review": "The paper proposes a novel nerual block based on the classifical spatio-temporal self-similarity (STSS), named SELFY, which can be easily inserted into nerual architectures and learned end-to-end without additional supervision. SELFY could capture long-term interaction and fast motion in the video with a sufficient volume  of the neighborhood in time and space. \nThe nice point of the method is that it is heavily investigtated through experiments. Evaluated on three standard action recognition benchmark datasets,  the proposed SELFY is demonstrated the superiority over previous methods for motion modeling as well as the complementarity to spatio-temporal features from direct convolution. Moreover, the paper is clear and seems correct ,technically.\n\ncomments:\n\n-The first concern is about  the limited novelty of the method. Despite revisting the self-similarity, from section 3.1, the learning of generalized and long-term information is a property of the STSS rather than a contribution of this work. If necessary, authors should give more details on the classifical STSS to show the improvement. \n-The authors briefly mention the differences between the new proposed and non-local approaches (Wang et al., 2018; Liu et al., 2019) and correlation-based methods (Wang et al., 2020; Kwon et al., 2020), which appears inadequate and is better to integrate them into a comparison in the form of Figure 1.\n-How are parameters (5, 9, 9) and (9, 9 ,9) chosen? The best result occurs as the  temporal offsets being chosen from {-3, ..., 3}. Will there be better accuracy when the range is larger? ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110053, "tmdate": 1606915772234, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1822/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Review"}}}, {"id": "WmuKjlP7SKL", "original": null, "number": 2, "cdate": 1603403331731, "ddate": null, "tcdate": 1603403331731, "tmdate": 1605024350627, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Review", "content": {"title": "This is an interesting work utilized STSS to help learn representation end-to-end and achieved good empirical results on several action recognition benchmarks.", "review": "### Summary:\u00a0\n\nThis submission proposed a motion representation method based on spatio-temporal self-similarity (STSS), which\u00a0represents each local region as similarities to its neighbors in both spatial and temporal dimension. There are previous works (e.g., Ref[1] , [2], [5] listed here) which utilize\u00a0STSS for feature extractions, authors claim that this work is the first one to learn STSS representation based on modern CNN\u00a0architecture. The proposed method is implemented as a neural block, i.e., SELFY, which can be applied into neural architectures and learned end-to-end without additional supervision. On 3 standard human action recognition data sets, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves quite good empirical results. \n\n### Strengths & Originality:\u00a0\n\nThe idea of utilizing\u00a0patio-temporal self-similarity (STSS) for feature representation in the modern CNN framework for human action recognition is interesting. I also like the concept that \"fix our attention to the similarity map to the very next frame within STSS and attempt to extract a single displacement vector to the most likely position at the frame, the problem reduces to optical flow ......\u00a0In contrast, we leverage the whole volume of STSS ......\", as to view the proposed method as a generalized\u00a0& rich optical flow. \n\nFurthermore, STSS should be helpful for view invariant action\u00a0recognition, i.e., one of the fundamental challenges from video data recognition.\u00a0\n\nEmpirical results on\u00a0Something-Something V1 & V2, Diving-48, FineGym show that the proposed\u00a0achieves the state-of-the-art results, though seems marginally.\u00a0 For example, the proposed\u00a0Ensemble model (SELFYNet-TSM-R50EN ) achieved 56.6% and 67.7% attop-1 accuracy for V1 & V2, vs. the\u00a0MSNet-TSM-R50EN got quite similar performance as 55.1% and 67.1% (Kwon et al., 2020, Ref. [3]).\u00a0\n\nOn FineGym,\u00a0SELFYNet-TSM-R50 achieves 49.5% and 87.7%, these do look better & more clearly, compared with\u00a046.5%, 81.2% reported from (Shao et al, 2020, Ref. [4]).\n\n### Weakness:\u00a0\n\nFrom Figure. 2 or Section 3.2.2, the\u00a0final STSS representation Z is the combination of original input V and the STSS feature\u00a0F(S(V)), so kind of unclear how much contribution is made from the original V in terms of the final recognition performance. Maybe this part is addressed in the experimental\u00a0section somewhere.\u00a0\n\nThe proposed neural block, SELFY, includes 3 parts as self-similarity transformation, feature extraction, and feature integration, it seems none of them is original but the combination of these put into the end-of-end NN structure is new as claimed by authors. There are some additional experiment results presented in section 4.3 (for different types of similarity, and for different feature extraction), still, it seems unclear how important or sensitive for each step within the whole framework based on TSN ResNets and TSM ResNets, given the complexity here.\u00a0\n\nSelf-similarity can be applied for both image and videos (i.e., Section 3 or Ref [5], etc), I know the focus of this submission is video action\u00a0recognition, still, seems interesting to know the proposed\u00a0framework \"3 steps combined + CNN end-of-end\" apply to image object recognition. If that also achieved good results compared with state-of-the-art for object recognition, it will be a strong support for the proposed methodology, and if not, which is also good to know & could be multiple reasons behind.\u00a0\n\nBesides these video based action benchmarks, it should be interesting to see results on a depth enriched data set (i.e., RGB-D), as missing depth information is one of the limitations from video data.\u00a0Ideally, we should see similar good performance if the proposed methodology\u00a0is effective for representation learning.\n\n### Reference:\n\n1.\u00a0Videos as Space-Time Region Graphs,\u00a0Xiaolong Wang, Abhinav Gupta, Proc. European Conference on Computer Vision (ECCV) 2018\n\n2.\u00a0TSM: Temporal Shift Module for Efficient Video Understanding, Ji Lin, Chuang Gan, and Song Han,\u00a0 Proc. IEEE International Conference on Computer Vision (ICCV), 2019\n\n3.\u00a0MotionSqueeze: Neural Motion Feature Learning for Video Understanding,\u00a0Heeseung Kwon,\u00a0Manjin Kim,\u00a0Suha Kwak,\u00a0Minsu Cho,\u00a0Proc. European Conference on Computer Vision (ECCV) ,\u00a02020\n\n4.\u00a0Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for finegrained action understanding. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020\u00a0\n\n5.\u00a0Imran N. Junejo, Emilie Dexter, Ivan Laptev and Patrick Perez, Cross-View Action Recognition from TemporalSelf-Similarities,\u00a0Proc. European Conference on Computer Vision (ECCV) 2008", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110053, "tmdate": 1606915772234, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1822/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Review"}}}, {"id": "UCrbsWsJABL", "original": null, "number": 3, "cdate": 1603767335466, "ddate": null, "tcdate": 1603767335466, "tmdate": 1605024350562, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Review", "content": {"title": "Interesting model to integrate bi-directional self-similarity into spatial-temporal representations with empirical results showing gains in performance over SOTA on motion-centric data but some concerns remain over applicability to data that requires more appearance-centric representation learning.", "review": "The paper introduces SELFY, a neural module that learns spatio-temporal self-similarity across longer timescales in both directions to obtain visual features that provide consistent empirical gains on three action recognition datasets. Ablation studies show that modeling longer, bi-directional motion similarity can help handle motion blurs and (artificially induced) occlusion.\n\nStrengths:\n+ The paper is well written and shows strong empirical gains over prior SOTA.\n+ The ablation experiments on the effect of temporal length and the different feature extraction methods are nicely done and show the effect of different design choices.\n+ The experiments for testing the robustness of the learned representations is nice to see and helps highlight the strength of the proposed approach.\n\nConcerns:\n- From what I can see, the contribution is in the modeling of (motion) similarity in both directions, across a longer time scale. My concern is that all of the chosen datasets are very motion-centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB-51. In fact, the results on SS-V1 and SS-V2 show some gains (~2-3%) compared to the very closely related MotionSqueeze Network (Kwon et al, ECCV 2020) which computes correlation (rather than similarity) between adjacent frames (t and t+1). MSNet did not see any significant advances in datasets (HMDB-51 and Kinetics) where appearance plays a greater role and I am not sure how this proposed approach would help in those conditions.\n- I assume the function sim(.) from Equation 1 is cosine similarity since it was not explicitly mentioned anywhere. Is there any specific reason the correlation function was not used? What is the effect of using correlation in place of cosine similarity? It would interesting to see the effect of this since it would essentially allow us to see how modeling the longer temporal context would affect MS Net (the most closely related network) and help highlight the contribution of the proposed model beyond the obvious empirical gains.\n- While the robustness experiments test out the occlusion setting by cutting out a rectangular patch of a single center-frame, that does not, IMHO, shows robustness to occlusion since the features from surrounding frames (on both sides) will help mitigate this artificial occlusion. In practice, the occlusion can have a larger effect since it would not zero out the appearance but will add some noise into the feature extraction and hence provide a stronger challenge. Experiments on HMDB51, which has strong camera motion and hence introduces natural occlusions, would be a stronger experiment to show the generalization of the model beyond datasets with a strong motion-centric bias.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110053, "tmdate": 1606915772234, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1822/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Review"}}}, {"id": "xkIEMtxjN0", "original": null, "number": 4, "cdate": 1603856701345, "ddate": null, "tcdate": 1603856701345, "tmdate": 1605024350490, "tddate": null, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "invitation": "ICLR.cc/2021/Conference/Paper1822/-/Official_Review", "content": {"title": "Good paper with good results", "review": "#### General\nThis paper proposes spatio-temporal self-similarity (STSS), which captures structural patterns in space and time, for action recognition from videos.  \nOverall, I would like to recommend ICLR to accept the paper.  \nPros and Cons I found in the paper are summarized as follows.\n\n\n#### Pros.\n1. SELFY, the proposed neural block that implements STSS, is a good extention and adjustment of prior self-similarity works to modern CNNs, resulted in achieving state-of-the-art performance on different datasets.\n1. The ablation study reveals that SELFY with a long temporal range successfully incorporate the temporal information without explicitly exploiting optical flows. \n1. The paper is generally well and clearly written.\n1. The literature review is thorough, and the work is well contextualized in the literatures.\n\n#### Cons\n1. The novelty and the originality of the paper may be relatively low because the concept of the STSS itself was already proposed in prior studies as mentioned in the paper.\n1. The reasoning or intuition behind some of the design choices are not always clear. For example,  \n    - The output of the feature extraction block is $F \\in \\mathbb{R}^{T \\times X \\times Y \\times L \\times C_F}$, and the authors say \n        > The dimension of L is preserved to extract motion information across different temporal offsets in a consistent manner.\n\n        But the motion information across different temporal offsets can be extracted in the feature extraction module if MLP or convolution is employed in the feature extraction block. In other words, even if the output is $F \\in \\mathbb{R}^{T \\times X \\times Y \\times C_F}$, the motion information can be encoded in the output tensor. Possibly the authors empirically found the present design choice is better. If so, how does the performance change if $L$ is not preserved?\n    - In the feature integration block, firstly $3 \\times 3$ spatial convolution kernel is applied, then the temporal offset ($L$) and the channels ($C^*_F$) dimension is flattened, and finally $1 \\times 1 \\times 1$ spatio-temporal convolution is applied. Is this design chosen experimentally or is there any intuition behind?   \n\n#### Minor comments\n1. It may be interesting to discuss the relationship with self-attention based networks such as [1-3]\n1. About Table 7 in Appendix, how is the result if SELFY block is used in $res_1$ and $res_5$? I guess these option do not work well, but I think it is beneficial to list all the results.\n\n[1] Hu+, Local Relation Networks for Image Recognition, ICCV 2019  \n[2] Ramachandran+, Stand-Alone Self-Attention in Vision Models, NeurIPS 2019  \n[3] Zhao+, Exploring Self-attention for Image Recognition, CVPR 2020", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1822/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1822/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "authorids": ["~Heeseung_Kwon2", "~Manjin_Kim1", "~Suha_Kwak3", "~Minsu_Cho1"], "authors": ["Heeseung Kwon", "Manjin Kim", "Suha Kwak", "Minsu Cho"], "keywords": ["Action recognition", "Video understanding", "Motion analysis"], "abstract": "Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation method based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. \nThe proposed method is implemented as a neural block, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.\nOur experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|learning_selfsimilarity_in_space_and_time_as_a_generalized_motion_for_action_recognition", "one-sentence_summary": "This paper proposes to learn a generalized, far-sighted motion representations from spatio-temporal self-similarity for video understanding.", "pdf": "/pdf/6b6e8178e089d48fbdd2ba2572984d60262d9502.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=p7gQNfrSX", "_bibtex": "@misc{\nkwon2021learning,\ntitle={Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition},\nauthor={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},\nyear={2021},\nurl={https://openreview.net/forum?id=S6AtYQLzXOY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "S6AtYQLzXOY", "replyto": "S6AtYQLzXOY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1822/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110053, "tmdate": 1606915772234, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1822/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1822/-/Official_Review"}}}], "count": 19}