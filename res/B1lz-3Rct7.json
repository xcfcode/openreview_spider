{"notes": [{"id": "B1lz-3Rct7", "original": "S1lVbDa9Y7", "number": 1152, "cdate": 1538087930358, "ddate": null, "tcdate": 1538087930358, "tmdate": 1550586146611, "tddate": null, "forum": "B1lz-3Rct7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Syxwvjn-eV", "original": null, "number": 1, "cdate": 1544829790933, "ddate": null, "tcdate": 1544829790933, "tmdate": 1545354530640, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Meta_Review", "content": {"metareview": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1152/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352945244, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352945244}}}, {"id": "SyxswZfEkV", "original": null, "number": 9, "cdate": 1543934306806, "ddate": null, "tcdate": 1543934306806, "tmdate": 1543934306806, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "Skldcfu0hm", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "content": {"title": "I stick to my rating", "comment": "The authors have taken my comment into account in the new revision of the paper and adequately addressed issues pointed out by other reviewers. So, I keep my rating unchanged."}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1152/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610500, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1152/Authors|ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610500}}}, {"id": "B1eBMzYupm", "original": null, "number": 5, "cdate": 1542128141142, "ddate": null, "tcdate": 1542128141142, "tmdate": 1543515580689, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "rJx5uduA2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "content": {"title": "Comments", "comment": "Q1: Agreed\n\nQ2: You are right about weight decay on gamma only affecting the complexity of the model due to the last layer which can be merged with the softmax layer weights (as also pointed out by van Laarhoven). May be mention this below Eq. 5 (while citing van Laarhoven) to remind the reader of this fact.\n\nQ3:\nOn page 6 (left of Figure 4), I recommend changing the sentence \n\"In all cases, we observe that whether weight decay was applied to the top (fully connected) layer did not appear to matter;\"\nto something like \n\"In all cases, we observe that whether weight decay was applied to the top (fully connected) layer did not have a significant impact;\"\n\nQ4: OK\n\nQ5: Thank you for clarifying. I can see the technical mistake made in the 1st submission involving expectation over the input-output Jacobian for ReLU networks. However the current Theorem 1 on deep linear network makes the claim weak and the authors have used earlier work on deep linear networks as a justification.\n\nQ6,7,8,9: OK\n\nComments:\n\nThere were a few technical mistakes in the original submission that were overlooked by the reviewers and the authors have themselves identified and corrected them. However, these corrections have made the results for the second order methods weaker (section 4.2) since they apply to deep linear networks, which is a bit disappointing. But I still think this paper deserves to be read because 1. even though based on intuitions from deep linear networks, experiments are shown for deep non-linear networks confirming the insights drawn from them; 2. other sections have complementary analysis of weight decay for additional cases.\n\n(I have increased my original score by 1)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1152/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610500, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1152/Authors|ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610500}}}, {"id": "B1g5xlXqnm", "original": null, "number": 3, "cdate": 1541185521816, "ddate": null, "tcdate": 1541185521816, "tmdate": 1543280190646, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Review", "content": {"title": "Writing needs improvement; many handwavy explanations", "review": "I have read the author's response, and I would like to stick to my rating. From the authors' response on the convergence issue, the result from [1] does not directly apply since the activation function that the authors use in this paper is relu (not linear). Having said that, authors didn't find any issues empirically.\n\nQ7: Yes, I agree that the result depends on the gradient structure of the relu activations. But my point was that, it is still a calculation that one has to carry out, and the insight we gain from the calculation seem computational: that one can regularize jacobian norm easily. True, but is that necessary? Or in other words, can we use techniques (not-so) recent  implicit regularization literature to analyze KFAC? I still think that the work is good, these are just my questions.\n====\n\nThe paper investigates how weight decay (according to the authors, this is done by scaling weights at each iteration) can be used as a regularizer while using standard first order methods and KFAC. As far as I can see, the experimental conclusion seem pretty consistent with other papers that the authors themselves cite (for eg: Neelakantan et al. (2015);  Martens & Grosse, 2015. \n\nIn page 2, the authors mention the three different mechanisms by which weight decay has a regularizing effect. First, what is the definition of \"effective learning rate\"? If the authors mean that regularization just changes the learning rate in some case, that is true. In fact, it is only true while using l2-norm. I looked through the paper, and I couldn't find one. Similarly, I find point #1. to be confusing: why does reducing the scale of the weights increase the effective learning rate? (This confusion carries over to/remains in section 4.1.). The sentence starting (in point #1.) with \"As evidence,\", what is the evidence for? Is it for the previous statement that weight decay helps as a regularizer? Looking at Figure 1., Table 1., I can see that weight decay is actually helpful even with BN+D. In fact, the improvement provided by weight decay is uniform across the board. \n\nThe conclusion of mechanism 1 is that for layers with BN, weight decay is implicitly using higher learning rate and not by limiting the capacity as pointed out by van Laarhoven (2017). The two paragraphs below (12) are contradictory or I'm missing something: first paragraph says that \"This is contrary to our intuition that weight decay results in a simple function.\" but immediately below, \"We show empirically that weight decay only improves generalization by controlling the norm, and therefore the effective learning rate.\" Can the authors please explain what the \"effective learning rate\" argument is?\n\nProposition 1 and theorem 1 are extensions from Martens & Gross, 2015, I didn't fully check the calculations. I glanced through them, and they mostly use algebraic manipulations. The main empirical takeaway as the authors mention is that: weight decay in both KFAC-F and KFAC-G serves as a complexity regularizer which sounds trivial (assuming Martens & Grosse, 2015) since in both of these cases, BN is not used and the fact that weight decay is regularization using the local norm. \n\nIf I understand correctly, KFAC is an approximate second order method with the approximation chosen to be such that it is invariant under affine transformations. Are there any convergence guarantees at all for either of these approaches? Newton's method, even for strongly convex loss functions, requires self-concordance to ensure convergence, so I'm a bit skeptic when using approximate (stochastic) Jacobian norm. \n\nSome of the plots have loss values, some have accuracy etc., which is also confusing while reading. I strongly suggest that Figure 1 be shown differently, especially the x-axis! Essentially weight decay improves the accuracy about 2-4% but it is hard to interpret that from the figure.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Review", "cdate": 1542234293657, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335884145, "tmdate": 1552335884145, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rygRc8IopQ", "original": null, "number": 6, "cdate": 1542313621610, "ddate": null, "tcdate": 1542313621610, "tmdate": 1542314204049, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "HklYSxUoTX", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "content": {"title": "Re: Related work", "comment": "Thank you for pointing out this work.\n\nSection 2.2 of this paper is indeed related to our mechanism 1. However, the argument of effective learning rate was first identified by van Laarhoven 2017 and we did properly discuss the relationship with van Laarhoven 2017 (also see the response to AnonReview1). In the upcoming version, we will cite the paper you mentioned."}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610500, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1152/Authors|ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610500}}}, {"id": "SJgG4O8op7", "original": null, "number": 7, "cdate": 1542314025910, "ddate": null, "tcdate": 1542314025910, "tmdate": 1542314025910, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1eBMzYupm", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "content": {"title": "Re: Comments", "comment": "Thank you for your new comments. We will update the paper according to your suggestions (Q2 and Q3)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610500, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1152/Authors|ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610500}}}, {"id": "HklYSxUoTX", "original": null, "number": 1, "cdate": 1542312001118, "ddate": null, "tcdate": 1542312001118, "tmdate": 1542312001118, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Public_Comment", "content": {"comment": "The first mechanism, increasing the effective learning rate, is also identified in this work https://arxiv.org/abs/1709.04546 (Sec. 2.2 and 3.2). The authors may want to discuss how they are related.", "title": "Related work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311666542, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1lz-3Rct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311666542}}}, {"id": "rygtlWdRnm", "original": null, "number": 2, "cdate": 1541468401394, "ddate": null, "tcdate": 1541468401394, "tmdate": 1542207552907, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1g5xlXqnm", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for the insightful comments. According to your suggestions, we revised the statements of the paper (including 4.1) to make them clearer.\n\nQ1: what is the definition of \"effective learning rate\"\nFor \"effective learning rate\", you can understand it as the \"learning rate\" for normalized networks (see equation 9).\n\nQ2: regularization just changes the learning rate (Mechanism 1)\nNote: it's true for weight decay in general (not L2-norm). We also tested weight decay in the case of Adam (see Figure 2) where weight decay and L2 regularization are not identical.\n\nQ3: why reducing the scale of the weights increase the effective learning rate\nAs explained in equation 9, the effective learning is inversely proportional to the weight norm.\n\nQ4: The sentence starting (in point #1.) with \"As evidence,\", what is the evidence for?\nSee Figure 2 and Figure 4. Most of the generalization effect of weight decay is due to applying it to layers with BN.\n\nQ5: The improvement provided by weight decay is uniform across the board. \nWeight decay does improve the performance consistently, but the mechanisms behind are different (depending on the optimization algorithm and network architecture). Figure 1 and Table 1 are mostly to emphasize the difference between L2 regularization and weight decay so as to motivate three mechanisms.\n\nQ6: Argument of Mechanism 1 (or effective learning rate)\nIn mechanism 1, we basically argue that the scaling of weights for BN layers doesn't influence the underlying function (see equation 8), so it doesn't meaningfully constrain the function to be simple (you can always scale down the weights but the function represented by the network is still the same, also see the first paragraph in 4.1). However, the scaling of the weights does influence the updates (see equation 9) by controlling the effective learning rate. The regularization effect of weight decay is achieved by scaling the weights, and therefore the effective learning rate.\n\nQ7: Proposition 1 and Theorem 1 are extensions from Martens & Gross, 2015\nWe have removed Proposition 1 in the latest version. Theorem 1 (Lemma 2 in the latest version) is not just an extension from the K-FAC paper (martens & Grosse, 2015). Actually, it has little to do with the K-FAC paper.  We don't think it's trivial for the following reasons:\n\n- Theorem 1 (Lemma 2 in the latest version) is new and it heavily relies on the Lemma 1 (gradient structure) which has nothing to do with the original K-FAC (Martens & Grosse, 2015) paper. \n- Theorem 1 (Lemma 2 in the latest version) is an important part to connect Gauss-Newton norm to approximate Jacobian norm. The result of approximate Jacobian norm is non-trivial and we didn't see any similar theoretical result before. In practice, it's quite expensive to directly regularize Jacobian norm due to the extra computation overhead. In this work, we provide a simple yet cheap way to approximately regularize Jacobian norm and we believe it's useful and novel.\n\nQ8: K-FAC (convergence?)\nK-FAC is currently the most popular approximate natural gradient method in training deep neural networks. It works very well (due to the use of curvature information) in practice and we didn't see any convergence issue. Recently, Bernacchia, 2018 [1] provided convergence guarantee for natural gradient in the case of deep linear networks (where the loss is non-convex). Beyond that, they also gave some theoretical justifications for the performance of K-FAC.\n\n[Reference]\n[1] Exact natural gradient in deep linear networks and application to the nonlinear case"}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610500, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1152/Authors|ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610500}}}, {"id": "Skldcfu0hm", "original": null, "number": 3, "cdate": 1541468815538, "ddate": null, "tcdate": 1541468815538, "tmdate": 1542135979691, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1eS7cTdhm", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for the positive feedback. \n\nWe have revised the conclusion section to discuss the observed results and potential new directions for future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610500, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1152/Authors|ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610500}}}, {"id": "rJx5uduA2m", "original": null, "number": 4, "cdate": 1541470322329, "ddate": null, "tcdate": 1541470322329, "tmdate": 1542130454143, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "rJx3XFFv2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for the useful feedback. We have updated the paper (especially 4.2) taking into account several of your comments.\n\nQ1: Mechanism 1 is more of a discussion on existing work rather than novel contribution\nWe agree that the argument of \"effective learning rate\" itself is not novel and has been observed by van Laarhoven 2017. \nHowever, we don't think the mechanism 1 is just a discussion of existing work. Particularly, van Laarhoven 2017 didn\u2019t show any experiments that weight decay improves generalization performance. In Figure 2 of van Laarhoven 2017, they only showed that small learning rate is preferred when weight decay is applied. The important point we made is that weight decay actually improves the generalization performance even with well-tuned learning rate parameter and the gain of applying weight decay cannot be achieved by tunning the learning rate directly (we shouldn't ignore the interaction between the learning rate and weight decay).\n\nFurthermore, van Laarhoven 2017 was just talking about L2 regularization which is not equivalent to weight decay in adaptive gradient methods. We don't think the author realized the subtle difference between L2 regularization and weight decay. In the combination of L2 regularization and adaptive gradient methods, the argument of effective learning rate might not hold exactly since L2 regularization can affect both the scale and direction of the weights. In our paper, we extend the argument of \"effective learning rate\" to first-order optimization algorithms (including SGD and Adam) by identifying the subtle difference between L2 regularization and weight decay.\n\nQ2: The effect of weight decay on the gamma parameter of batch-norm.\nAs discussed in van Laarhoven 2017, only the gamma of the last BN layer affects the complexity of the network. The role of it is quite similar to the scale of the last fully connected layer since you can always merge the gamma parameter into the last fc layer. In practice, the gain of regularizing the gamma parameter of the last BN layer is quite small which is consistent with our observation that regularizing the last fc layer gives marginal improvement. That's why we fixed the gamma parameter throughout the paper.\n\nQ3: In Figure 2 and 4, there is a noticeable difference between training without weight decay, and training with weight decay only on the last layer. \nIn Figure 2, the gap is pretty small (<1%). \nIn Figure 4, regularizing the last layer does help a little bit (~1%) while the improvement of regularizing conv layers is much larger (~3%). \nAccording to your suggestion, we revised our statements in 4.1 to make the arguments softer.\n\nQ4: In the line right above remark 1, what does \u201cassumption\u201d refer to?\nIt does refer to spherical Gaussian input distribution. We have improved the writing for this part, it should be much clearer now.\n\nQ5: Regarding the equivalence of L2 norm of theta under Gauss-Newton metric and the Frobenius norm of input-output Jacobian, why does f_theta need to be a linear function without any non-linearity?\nThat\u2019s because we want the input-output Jacobian to be independent of the input x (which is not true for non-linear networks). Under this assumption, we can take J_x out of the expectation (see revised Theorem 1).\n\nNote: if the (all) input x has entries \u00b11 (so that xx^T is an identity matrix), then the assumption of f_theta being linear is not necessary. In that case, it is easy to show that the Gauss-Newton norm is proportional to the expectation of squared Jacobian norm over input distribution.\n\nQ6: In remark 1, what does it mean by \u201cFurthermore, if G is approximated by KFAC\u201d?\nThis original claim is a little misleading, we have rewritten this part. Basically, when G is approximated by K-FAC (it's intractable to use exact G in practice), the K-FAC Gauss-Newton norm is still proportional to the squared Jacobian norm, but the constant becomes (L+1), not (L+1)**2.\n\nQ7: In the 1st line of the last paragraph of page 6, what are the general conditions under which the connection between Gauss-Newton norm and Jacobian norm does not hold true?\nIf the network is not linear, then the connection will not hold exactly. We need the assumption of the network being linear so that the input-output Jacobian J_x is independent of the input x.\n\nQ8: In Figure 5, how are the different points in the plots achieved? By varying hyper-parameters?\nSorry, we didn't explain Figure 5 clearly in the submitted version. Different points are achieved by varying optimizers and architectures (we mentioned that on page 7 of the updated version). Specifically, we trained feed-forward networks with a variety optimizers on both MNIST and CIFAR-10. For MNIST, we used simple fully-connected networks with different depth and width. For CIFAR-10, we adopted the VGG family (From VGG11 to VGG19). \n\nQ9: Missing citations\nThank you for pointing out missing citations. We added multiple citations in the latest version.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610500, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1152/Authors|ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610500}}}, {"id": "rJx3XFFv2Q", "original": null, "number": 1, "cdate": 1541015844017, "ddate": null, "tcdate": 1541015844017, "tmdate": 1542128161765, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Review", "content": {"title": "Nice insights about second order methods", "review": "This paper discusses the effect of weight decay on the training of deep network models with and without batch normalization and when using first/second order optimization methods. \n\nFirst, it is discussed how weight decay affects the learning dynamics in networks with batch normalization when trained with SGD. The dominant generalization benefit due to weight decay comes from increasing the effective learning rate of parameters on which batch normalization is applied. The authors therefore hypothesize that a larger learning rate has a regularization effect.\n\nSecond, the role of weight decay is discussed when training with second order methods without batch normalization. Under the approximation of not differentiating the curvature matrix used in second order method, it is shown that using weight decay is equivalent to adding to the loss an L2 regularization in the metric space of the curvature matrix considered. It is then shown that if the curvature matrix is the Gauss-Newton matrix, this L2 regularization (and hence the weight decay) is equivalent to the Frobenius norm of the input-output Jacobian when the input has a spherical Gaussian distribution. Similar arguments are made about KFAC with Gauss-Newton norm. The generalization benefit due to weight decay in this case is claimed based on the recent paper by Novak et al 2018 which empirically shows a strong correlation between input-output Jacobian norm and generalization error.\n\n\nFinally, the role of weight decay is discussed for second order methods when using batch normalization. In this case it is discussed for Gauss-Newton KFAC that the benefit mostly comes from the application of weight decay on the softmax layer and the effect of weight decay on other weights cancel out due to batch normalization. A comparison between Gauss-Newton KFAC and Fischer KFAC is also made. Thus the generalization benefit is presumably attributed to the second order properties of KFAC and a smaller norm of softmax layer weights.\n\nComments:\nThe paper is technically correct and proofs look good.\n\nI have mixed comments about this paper. I find the analysis in section 4.2 and 4.3 which discuss about the role of weight decay for second order methods (with and without batch-norm) to be novel and insightful (described above). \n\nBut on the other hand, I feel section 4.1 is more of a discussion on existing work rather than novel contribution. Most of what is said, both analytically and experimentally, is a repetition of van Laarhoven 2017, except for a few details. It would have been interesting to carefully study the effect of weight decay on the gamma parameter of batch-norm which controls the complexity of the network along with the softmax layer weights as it was left for future work in van Laarhoven 2017. But instead the authors brush it under the carpet by saying they did not find the gamma and beta parameters to have significant impact on performance, and fixed them during training.  I also find the claim of section 4.1 to be a bit mis-leading because it is claimed that weight decay applied with SGD and batch normalization only has benefits due to batch-norm dynamics, and not due to complexity control even though in Fig 2 and 4, there is a noticeable difference between training without weight decay, and training with weight decay only on last layer. Furthermore, when hypothesizing the regularization effect of large learning rate in section 4.1, a large body of literature that has studied this effect has not been cited. Examples are [1], [2], [3]. \n\nI have other concerns which mainly stem from lack of clarity in writing:\n\n1. In the line right above remark 1, it is not clear what \u201cassumption\u201d refer to. I am guessing the distribution of the input being spherical Gaussian?\n2. In remark 1, regarding the claim about the equivalence of L2 norm of theta under Gauss-Newton metric and the Frobenius norm of input-output Jacobian, why does f_theta need to be a linear function without any non-linearity? I think the linearity part is only needed for the KFAC result.\n3. In remark 1, what does it mean by \u201cFurthermore, if G is approximated by KFAC\u201d? For linear f_theta, given lemma 1 and theorem 1, the claimed equivalence always holds true, no?\n4. In the 1st line of last paragraph of page 6, what are the general conditions under which the connection between Gauss-Newton norm and Jacobian norm does not hold true?\n5. In figure 5, how are the different points in the plots achieved? By varying hyper-parameters?\n\nA minor suggestion: in theorem 1 (and lemma 1), instead of assuming network has no bias, it can be said that the L2 regularization term does not have bias terms. This is more reasonable because bias terms have no effect on complexity and so it is reasonable to not apply weight decay on bias.\n\nOverall I think the paper is good *if* section 4.1 is sorted out and writing (especially in section 4.2) is improved. For these reasons, I am currently giving a score of 6, but I will increase it if my concerns are addressed.\n\n[1] a bayesian perspective on generalization and stochastic gradient descent\n[2] Train longer, generalize better: closing the generalization gap in large batch training of neural networks\n[3] Three Factors Influencing Minima in SGD", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Review", "cdate": 1542234293657, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335884145, "tmdate": 1552335884145, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eS7cTdhm", "original": null, "number": 2, "cdate": 1541098013331, "ddate": null, "tcdate": 1541098013331, "tmdate": 1541533378279, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Review", "content": {"title": "Solid work on understanding of weight decay regularization", "review": "This paper identifies and investigates three mechanisms of weight decay regularization. The authors consider weight decay for DNN architectures with/without BN and different types of optimization algorithms (SGD, Adam, and two versions of KFAC). The paper unravels insights on weight decay regularization effects, which cannot be explained only by traditional L2 regularization approach. This understanding is of high importance for the further development of regulations techniques for deep learning.\n\nStrengths:\n+ The authors draw connections between identified mechanisms and effects observed in prior work.\n+ The authors provide both clear theoretical analysis and adequate experimental evidence supporting identified regularization mechanisms.\n+ The paper is organized and written clearly.\n\nI cannot point out any flaws in the paper. The only recommendation I would give is to discuss in more detail possible implications of the observed results for new methods of regularization in deep learning and potential directions for future work. It would emphasize the significance of the obtained results.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Review", "cdate": 1542234293657, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335884145, "tmdate": 1552335884145, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlhD5wRnQ", "original": null, "number": 1, "cdate": 1541466724121, "ddate": null, "tcdate": 1541466724121, "tmdate": 1541472181934, "tddate": null, "forum": "B1lz-3Rct7", "replyto": "B1lz-3Rct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "content": {"title": "Paper revision", "comment": "We have updated the paper and improved the writing a lot. In particular, we rewrote the section 4.1 and 4.2 as requested by AnonReview1 and AnonReview2."}, "signatures": ["ICLR.cc/2019/Conference/Paper1152/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks.", "keywords": ["Generalization", "Regularization", "Optimization"], "authorids": ["gdzhang.cs@gmail.com", "cqwang@cs.toronto.edu", "bowenxu@cs.toronto.com", "rgrosse@cs.toronto.edu"], "authors": ["Guodong Zhang", "Chaoqi Wang", "Bowen Xu", "Roger Grosse"], "TL;DR": "We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.", "pdf": "/pdf/539eb13a780efd46200116a4729cd31dbea0770d.pdf", "paperhash": "zhang|three_mechanisms_of_weight_decay_regularization", "_bibtex": "@inproceedings{\nzhang2018three,\ntitle={Three Mechanisms of Weight Decay Regularization},\nauthor={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lz-3Rct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1152/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610500, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lz-3Rct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1152/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1152/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1152/Authors|ICLR.cc/2019/Conference/Paper1152/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1152/Reviewers", "ICLR.cc/2019/Conference/Paper1152/Authors", "ICLR.cc/2019/Conference/Paper1152/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610500}}}], "count": 14}