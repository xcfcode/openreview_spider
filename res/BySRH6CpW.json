{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730188955, "tcdate": 1508984269523, "number": 104, "cdate": 1518730188945, "id": "BySRH6CpW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BySRH6CpW", "original": "r1SRB6RaZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Learning Discrete Weights Using the Local Reparameterization Trick", "abstract": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.", "pdf": "/pdf/579bdc6fa8604a507c5394defc2a73d4adb6bcda.pdf", "TL;DR": "Training binary/ternary networks using local reparameterization with the CLT approximation", "paperhash": "shayer|learning_discrete_weights_using_the_local_reparameterization_trick", "_bibtex": "@inproceedings{\nshayer2018learning,\ntitle={Learning Discrete Weights Using the Local Reparameterization Trick},\nauthor={Oran Shayer and Dan Levi and Ethan Fetaya},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BySRH6CpW},\n}", "keywords": ["deep learning", "discrete weight network"], "authors": ["Oran Shayer", "Dan Levi", "Ethan Fetaya"], "authorids": ["oran.sh@gmail.com", "dan.levi@gm.com", "ethanf@cs.toronto.edu"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260097072, "tcdate": 1517249386129, "number": 168, "cdate": 1517249386108, "id": "SyfO71aSz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "BySRH6CpW", "replyto": "BySRH6CpW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "Well written paper on a novel application of the local reprarametrisation trick to learn networks with discrete weights. The approach achieves state-of-the-art results.\n\nNote: I apreciate that the authors added a comparison to the Gumbel-softmax continuous relaxation approach during the review period, following the suggestion of a reviewer. This additional comparison strengthens the paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Discrete Weights Using the Local Reparameterization Trick", "abstract": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.", "pdf": "/pdf/579bdc6fa8604a507c5394defc2a73d4adb6bcda.pdf", "TL;DR": "Training binary/ternary networks using local reparameterization with the CLT approximation", "paperhash": "shayer|learning_discrete_weights_using_the_local_reparameterization_trick", "_bibtex": "@inproceedings{\nshayer2018learning,\ntitle={Learning Discrete Weights Using the Local Reparameterization Trick},\nauthor={Oran Shayer and Dan Levi and Ethan Fetaya},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BySRH6CpW},\n}", "keywords": ["deep learning", "discrete weight network"], "authors": ["Oran Shayer", "Dan Levi", "Ethan Fetaya"], "authorids": ["oran.sh@gmail.com", "dan.levi@gm.com", "ethanf@cs.toronto.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1515773910877, "tcdate": 1515773910877, "number": 4, "cdate": 1515773910877, "id": "r1kyevIEM", "invitation": "ICLR.cc/2018/Conference/-/Paper104/Official_Comment", "forum": "BySRH6CpW", "replyto": "r1OyY_vZG", "signatures": ["ICLR.cc/2018/Conference/Paper104/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper104/AnonReviewer1"], "content": {"title": "Rebuttal response", "comment": "Thank you very much for the response as it clears up some misunderstandings. I have updated the review and the score of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Discrete Weights Using the Local Reparameterization Trick", "abstract": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.", "pdf": "/pdf/579bdc6fa8604a507c5394defc2a73d4adb6bcda.pdf", "TL;DR": "Training binary/ternary networks using local reparameterization with the CLT approximation", "paperhash": "shayer|learning_discrete_weights_using_the_local_reparameterization_trick", "_bibtex": "@inproceedings{\nshayer2018learning,\ntitle={Learning Discrete Weights Using the Local Reparameterization Trick},\nauthor={Oran Shayer and Dan Levi and Ethan Fetaya},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BySRH6CpW},\n}", "keywords": ["deep learning", "discrete weight network"], "authors": ["Oran Shayer", "Dan Levi", "Ethan Fetaya"], "authorids": ["oran.sh@gmail.com", "dan.levi@gm.com", "ethanf@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825739211, "id": "ICLR.cc/2018/Conference/-/Paper104/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BySRH6CpW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper104/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper104/Authors|ICLR.cc/2018/Conference/Paper104/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper104/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper104/Authors|ICLR.cc/2018/Conference/Paper104/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper104/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper104/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper104/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper104/Reviewers", "ICLR.cc/2018/Conference/Paper104/Authors", "ICLR.cc/2018/Conference/Paper104/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825739211}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515763426691, "tcdate": 1511779725554, "number": 2, "cdate": 1511779725554, "id": "BJHcawFxM", "invitation": "ICLR.cc/2018/Conference/-/Paper104/Official_Review", "forum": "BySRH6CpW", "replyto": "BySRH6CpW", "signatures": ["ICLR.cc/2018/Conference/Paper104/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Simple idea that seem to work but the novelty is limited and some regularization choices might not do what is expected.", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization. The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution. As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick. The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting. The method is evaluated on multiple experiments.\n\nThis paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice. My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1]. While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward. I would thus suggest that the authors update the paper accordingly. \n\nOther than that, I have some other comments:\n- The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve? \n- For the binary setting you mentioned that you had to reduce the entropy thus added a \u201cbeta density regulariser\u201d. Did you add R(p) or log R(p) to the objective function? Also, with alpha, beta = 2 the beta density is unimodal with a peak at p=0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid. To force the probability near the endpoints you have to use alpha, beta < 1 which results into a \u201cbowl\u201d shaped Beta distribution. I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization.  \n- I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful. Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT). \n\n[1] Wang & Manning, Fast Dropout Training.\n\nEdit: After the authors rebuttal I have increased the rating of the paper: \n- I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1].\n- The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p. The lambda term would then serve as an indicator to how much entropy is necessary.\n- There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved).\n- The authors showed benefits compared to a continuous relaxation baseline.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning Discrete Weights Using the Local Reparameterization Trick", "abstract": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.", "pdf": "/pdf/579bdc6fa8604a507c5394defc2a73d4adb6bcda.pdf", "TL;DR": "Training binary/ternary networks using local reparameterization with the CLT approximation", "paperhash": "shayer|learning_discrete_weights_using_the_local_reparameterization_trick", "_bibtex": "@inproceedings{\nshayer2018learning,\ntitle={Learning Discrete Weights Using the Local Reparameterization Trick},\nauthor={Oran Shayer and Dan Levi and Ethan Fetaya},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BySRH6CpW},\n}", "keywords": ["deep learning", "discrete weight network"], "authors": ["Oran Shayer", "Dan Levi", "Ethan Fetaya"], "authorids": ["oran.sh@gmail.com", "dan.levi@gm.com", "ethanf@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642379114, "id": "ICLR.cc/2018/Conference/-/Paper104/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper104/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper104/AnonReviewer2", "ICLR.cc/2018/Conference/Paper104/AnonReviewer1", "ICLR.cc/2018/Conference/Paper104/AnonReviewer3"], "reply": {"forum": "BySRH6CpW", "replyto": "BySRH6CpW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper104/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642379114}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642379229, "tcdate": 1511536544171, "number": 1, "cdate": 1511536544171, "id": "SkOjP3Hlf", "invitation": "ICLR.cc/2018/Conference/-/Paper104/Official_Review", "forum": "BySRH6CpW", "replyto": "BySRH6CpW", "signatures": ["ICLR.cc/2018/Conference/Paper104/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "The idea of making use of the local parameterisation trick to learn discrete networks is straight forward but novel and leads to SOTA results.", "rating": "7: Good paper, accept", "review": "Summary of the paper:\nThe paper suggests to use stochastic parameters in combination with the local reparametrisation trick (previously introduced by Kingma et al. (2015)) to train neural networks with binary or ternary wights. Results on MNIST, CIFAR-10 and ImageNet are very competitive. \n\nPros:\n- The proposed method leads to state of the art results .\n- The paper is easy to follow and clearly describes the implementation details needed to reach the results. \n\nCons:\n- The local reprarametrisation trick it self is not new and applying it to a multinomial distribution (with one repetition) instead of a Gaussian is straight forward, but its application for learning discrete networks is to my best knowledge novel and interesting. \n\nIt could be nice to include the results of Zuh et al (2017) in the results table and to indicate the variance for different samples of weights resulting from your methods in brackets. \n\n\nMinor comments:\n- Some citations have a strange format: e.g. \u201cin Hubara et al. (2016); Restegari et al. (2016)\u201c would be better readable as   \u201cby Hubara et al. (2016) and Restegari et al. (2016)\u201c\n-  To improve notation, it could be directly written that W is the set of all w^l_{i,j} and \\mathcal{W} is the joint distribution resulting from independently sampling from  \\mathcal{W}^l_{i,j}. \n- page 6: \u201con the last full precision network\u201d: should probably be \u201con the last full precision layer\u201d\n                    \u201c distributions has\u201d ->  \u201c distributions have\u201d \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Discrete Weights Using the Local Reparameterization Trick", "abstract": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.", "pdf": "/pdf/579bdc6fa8604a507c5394defc2a73d4adb6bcda.pdf", "TL;DR": "Training binary/ternary networks using local reparameterization with the CLT approximation", "paperhash": "shayer|learning_discrete_weights_using_the_local_reparameterization_trick", "_bibtex": "@inproceedings{\nshayer2018learning,\ntitle={Learning Discrete Weights Using the Local Reparameterization Trick},\nauthor={Oran Shayer and Dan Levi and Ethan Fetaya},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BySRH6CpW},\n}", "keywords": ["deep learning", "discrete weight network"], "authors": ["Oran Shayer", "Dan Levi", "Ethan Fetaya"], "authorids": ["oran.sh@gmail.com", "dan.levi@gm.com", "ethanf@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642379114, "id": "ICLR.cc/2018/Conference/-/Paper104/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper104/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper104/AnonReviewer2", "ICLR.cc/2018/Conference/Paper104/AnonReviewer1", "ICLR.cc/2018/Conference/Paper104/AnonReviewer3"], "reply": {"forum": "BySRH6CpW", "replyto": "BySRH6CpW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper104/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642379114}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642379131, "tcdate": 1511834168859, "number": 3, "cdate": 1511834168859, "id": "ryZHzH9gz", "invitation": "ICLR.cc/2018/Conference/-/Paper104/Official_Review", "forum": "BySRH6CpW", "replyto": "BySRH6CpW", "signatures": ["ICLR.cc/2018/Conference/Paper104/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The Kingma's reparametrization trick for binary and ternary nets", "rating": "6: Marginally above acceptance threshold", "review": "This paper introduces the LR-Net, which uses the reparametrization trick inspired by a similar component in VAE. Although the idea of reparametrization itself is not new, applying that for the purpose of training a binary or ternary network, and sample the pre-activations instead of weights is novel.  From the experiments, we can see that the proposed method is effective. \n\nIt seems that there could be more things to show in the experiments part. For example, since it is using a multinomial distribution for weights, it makes sense to see the entropy w.r.t. training epochs. Also, since the reparametrization is based on the Lyapunov Central Limit Theorem, which assumes statistical independence, a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram. \n\nAlso, in the literature of low precision networks, people are concerning both training time and test time computation demand. Since you are sampling the pre-activations instead of weights, I guess this approach is also able to reduce training time complexity by an order. Thus a calculation of train/test time computation could highlight the advantage of this approach more boldly. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Discrete Weights Using the Local Reparameterization Trick", "abstract": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.", "pdf": "/pdf/579bdc6fa8604a507c5394defc2a73d4adb6bcda.pdf", "TL;DR": "Training binary/ternary networks using local reparameterization with the CLT approximation", "paperhash": "shayer|learning_discrete_weights_using_the_local_reparameterization_trick", "_bibtex": "@inproceedings{\nshayer2018learning,\ntitle={Learning Discrete Weights Using the Local Reparameterization Trick},\nauthor={Oran Shayer and Dan Levi and Ethan Fetaya},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BySRH6CpW},\n}", "keywords": ["deep learning", "discrete weight network"], "authors": ["Oran Shayer", "Dan Levi", "Ethan Fetaya"], "authorids": ["oran.sh@gmail.com", "dan.levi@gm.com", "ethanf@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642379114, "id": "ICLR.cc/2018/Conference/-/Paper104/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper104/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper104/AnonReviewer2", "ICLR.cc/2018/Conference/Paper104/AnonReviewer1", "ICLR.cc/2018/Conference/Paper104/AnonReviewer3"], "reply": {"forum": "BySRH6CpW", "replyto": "BySRH6CpW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper104/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642379114}}}], "count": 6}