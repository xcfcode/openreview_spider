{"notes": [{"id": "6htjOqus6C3", "original": "wVPeLqzIfl7", "number": 1198, "cdate": 1601308134316, "ddate": null, "tcdate": 1601308134316, "tmdate": 1614985630097, "tddate": null, "forum": "6htjOqus6C3", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "DynamicVAE: Decoupling Reconstruction Error and Disentangled Representation Learning", "authorids": ["~Huajie_Shao1", "lhh2017@zju.edu.cn", "qmyang@zju.edu.cn", "~Shuochao_Yao1", "~Han_Zhao1", "~Tarek_Abdelzaher1"], "authors": ["Huajie Shao", "Haohong Lin", "Qinmin Yang", "Shuochao Yao", "Han Zhao", "Tarek Abdelzaher"], "keywords": ["disentangled representation learning", "dynamic learning", "Variational Autoencoder", "PID contoller"], "abstract": "This paper challenges the common assumption that the weight $\\beta$, in $\\beta$-VAE, should be larger than $1$ in order to effectively disentangle latent factors. We demonstrate that $\\beta$-VAE, with $\\beta < 1$, can not only attain good disentanglement but also significantly improve reconstruction accuracy via dynamic control. The paper \\textit{removes the inherent trade-off} between reconstruction accuracy and disentanglement for $\\beta$-VAE. Existing methods, such as $\\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for the sake of better disentanglement. To mitigate this problem, a ControlVAE has recently been developed that dynamically tunes the KL-divergence weight in an attempt to \\textit{control the trade-off} to more a favorable point. However, ControlVAE fails to eliminate the conflict between the need for a large $\\beta$ (for disentanglement) and the need for a small $\\beta$ (for smaller reconstruction error). Instead, we propose DynamicVAE that maintains a different $\\beta$ at different stages of training, thereby \\textit{decoupling disentanglement and reconstruction accuracy}. In order to evolve the weight, $\\beta$, along a trajectory that enables such decoupling, DynamicVAE leverages a modified incremental PI (proportional-integral) controller, a variant of proportional-integral-derivative controller (PID) algorithm, and employs a moving average as well as a hybrid annealing method to evolve the value of KL-divergence smoothly in a tightly controlled fashion. We theoretically prove the stability of the proposed approach. Evaluation results on three benchmark datasets demonstrate that DynamicVAE significantly improves the reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The results verify that our method can separate disentangled representation learning and reconstruction, removing the inherent tension between the two. ", "one-sentence_summary": "The goal of this paper is to decouple disentangling and reconstruction for disentangled representation learning via dynamic control.", "pdf": "/pdf/76824fd024e07e4fd1e2ba536e5b9e7f5fb29dc4.pdf", "supplementary_material": "/attachment/ac01d67aa70ec745ae3c655492ff37bf8aa029db.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|dynamicvae_decoupling_reconstruction_error_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=K2C8frX-O4", "_bibtex": "@misc{\nshao2021dynamicvae,\ntitle={Dynamic{\\{}VAE{\\}}: Decoupling Reconstruction Error and Disentangled Representation Learning},\nauthor={Huajie Shao and Haohong Lin and Qinmin Yang and Shuochao Yao and Han Zhao and Tarek Abdelzaher},\nyear={2021},\nurl={https://openreview.net/forum?id=6htjOqus6C3}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "iRXjHFh1W0", "original": null, "number": 1, "cdate": 1610040530357, "ddate": null, "tcdate": 1610040530357, "tmdate": 1610474139831, "tddate": null, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "invitation": "ICLR.cc/2021/Conference/Paper1198/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper is in general well written and easy to follow, and the considered approach of controlling beta is sensible. However, all reviewers identify shortcomings in the empirical analysis of the proposed method (missing comparison with stronger baselines, convergence issues of the considered baselines, considered datasets, etc.). Furthermore, compared to the ControlVAE the contribution of the paper seems limited; and the empirical evaluation is insufficient to claim superior results in general. The authors did not address most of the concerns raised by the reviewers in their rebuttal. The authors can improve their paper substantially by performing the experimental results proposed by the reviewers and clarifying differences to the ControlVAE\u2014but in its current form the paper does not meet the standard of ICLR.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynamicVAE: Decoupling Reconstruction Error and Disentangled Representation Learning", "authorids": ["~Huajie_Shao1", "lhh2017@zju.edu.cn", "qmyang@zju.edu.cn", "~Shuochao_Yao1", "~Han_Zhao1", "~Tarek_Abdelzaher1"], "authors": ["Huajie Shao", "Haohong Lin", "Qinmin Yang", "Shuochao Yao", "Han Zhao", "Tarek Abdelzaher"], "keywords": ["disentangled representation learning", "dynamic learning", "Variational Autoencoder", "PID contoller"], "abstract": "This paper challenges the common assumption that the weight $\\beta$, in $\\beta$-VAE, should be larger than $1$ in order to effectively disentangle latent factors. We demonstrate that $\\beta$-VAE, with $\\beta < 1$, can not only attain good disentanglement but also significantly improve reconstruction accuracy via dynamic control. The paper \\textit{removes the inherent trade-off} between reconstruction accuracy and disentanglement for $\\beta$-VAE. Existing methods, such as $\\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for the sake of better disentanglement. To mitigate this problem, a ControlVAE has recently been developed that dynamically tunes the KL-divergence weight in an attempt to \\textit{control the trade-off} to more a favorable point. However, ControlVAE fails to eliminate the conflict between the need for a large $\\beta$ (for disentanglement) and the need for a small $\\beta$ (for smaller reconstruction error). Instead, we propose DynamicVAE that maintains a different $\\beta$ at different stages of training, thereby \\textit{decoupling disentanglement and reconstruction accuracy}. In order to evolve the weight, $\\beta$, along a trajectory that enables such decoupling, DynamicVAE leverages a modified incremental PI (proportional-integral) controller, a variant of proportional-integral-derivative controller (PID) algorithm, and employs a moving average as well as a hybrid annealing method to evolve the value of KL-divergence smoothly in a tightly controlled fashion. We theoretically prove the stability of the proposed approach. Evaluation results on three benchmark datasets demonstrate that DynamicVAE significantly improves the reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The results verify that our method can separate disentangled representation learning and reconstruction, removing the inherent tension between the two. ", "one-sentence_summary": "The goal of this paper is to decouple disentangling and reconstruction for disentangled representation learning via dynamic control.", "pdf": "/pdf/76824fd024e07e4fd1e2ba536e5b9e7f5fb29dc4.pdf", "supplementary_material": "/attachment/ac01d67aa70ec745ae3c655492ff37bf8aa029db.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|dynamicvae_decoupling_reconstruction_error_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=K2C8frX-O4", "_bibtex": "@misc{\nshao2021dynamicvae,\ntitle={Dynamic{\\{}VAE{\\}}: Decoupling Reconstruction Error and Disentangled Representation Learning},\nauthor={Huajie Shao and Haohong Lin and Qinmin Yang and Shuochao Yao and Han Zhao and Tarek Abdelzaher},\nyear={2021},\nurl={https://openreview.net/forum?id=6htjOqus6C3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040530344, "tmdate": 1610474139816, "id": "ICLR.cc/2021/Conference/Paper1198/-/Decision"}}}, {"id": "ly54xXa1OJ1", "original": null, "number": 3, "cdate": 1603907974493, "ddate": null, "tcdate": 1603907974493, "tmdate": 1607350916881, "tddate": null, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "invitation": "ICLR.cc/2021/Conference/Paper1198/-/Official_Review", "content": {"title": "I am not convinced that the proposed method has sufficient novelty. More experimental results might be necessary.", "review": "In $\\beta$-VAE, one challenge is to choose the hyper-parameter $\\beta$ that controls the trade-off between the reconstruction quality and the disentanglement. This paper proposes a method called DynamicVAE. Rather than using a fixed hyperparameter $\\beta$, the method leverages a modified incremental Proportional-integral (PI) controller, which dynamically tunes $\\beta$ at different stages of training. The method is tested on benchmark datasets.\n\nThe paper is not difficult to follow. The idea of dynamically tuning the hyper-parameter looks interesting. However, there exists a previous method called control-VAE, which also dynamically tunes $\\beta$ with a PI controller. Although it is reported in the paper that the proposed method outperforms controlVAE, it is not clear to me how it differs from it. Therefore, I am not sure whether the proposed method has sufficient novelty.\n\nAs shown in Table 1, DynamicVAE outperforms FactorVAE, but the difference is tiny. In particular, Dynamic VAE is better in disentangling shape and scale, while FactorVAE is better in disentangling pos. x, pos. y and orientation. The results make me doubt whether DynamicVAE consistently outperforms FactorVAE in various datasets. It would make the paper more convincing if the authors can report quantitative results on more datasets.\n\nThe authors claim that the proposed method decouples reconstruction and disentanglement. However, no quantitative measurements for reconstruction are reported. Therefore, it is not clear whether the proposed method outperforms the baselines in terms of reconstruction.\n\nIn summary, I do not suggest accepting this paper. I am not convinced that the proposed method has sufficient novelty. More experimental results might be necessary.\n\n==============================================================================================\n\nThanks for the authors' response. I am still inclined to reject this paper. Compared to the existing ControlVAE, the contributions of this paper looks incremental. More experimental results might be necessary to make the paper more convincing.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1198/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1198/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynamicVAE: Decoupling Reconstruction Error and Disentangled Representation Learning", "authorids": ["~Huajie_Shao1", "lhh2017@zju.edu.cn", "qmyang@zju.edu.cn", "~Shuochao_Yao1", "~Han_Zhao1", "~Tarek_Abdelzaher1"], "authors": ["Huajie Shao", "Haohong Lin", "Qinmin Yang", "Shuochao Yao", "Han Zhao", "Tarek Abdelzaher"], "keywords": ["disentangled representation learning", "dynamic learning", "Variational Autoencoder", "PID contoller"], "abstract": "This paper challenges the common assumption that the weight $\\beta$, in $\\beta$-VAE, should be larger than $1$ in order to effectively disentangle latent factors. We demonstrate that $\\beta$-VAE, with $\\beta < 1$, can not only attain good disentanglement but also significantly improve reconstruction accuracy via dynamic control. The paper \\textit{removes the inherent trade-off} between reconstruction accuracy and disentanglement for $\\beta$-VAE. Existing methods, such as $\\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for the sake of better disentanglement. To mitigate this problem, a ControlVAE has recently been developed that dynamically tunes the KL-divergence weight in an attempt to \\textit{control the trade-off} to more a favorable point. However, ControlVAE fails to eliminate the conflict between the need for a large $\\beta$ (for disentanglement) and the need for a small $\\beta$ (for smaller reconstruction error). Instead, we propose DynamicVAE that maintains a different $\\beta$ at different stages of training, thereby \\textit{decoupling disentanglement and reconstruction accuracy}. In order to evolve the weight, $\\beta$, along a trajectory that enables such decoupling, DynamicVAE leverages a modified incremental PI (proportional-integral) controller, a variant of proportional-integral-derivative controller (PID) algorithm, and employs a moving average as well as a hybrid annealing method to evolve the value of KL-divergence smoothly in a tightly controlled fashion. We theoretically prove the stability of the proposed approach. Evaluation results on three benchmark datasets demonstrate that DynamicVAE significantly improves the reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The results verify that our method can separate disentangled representation learning and reconstruction, removing the inherent tension between the two. ", "one-sentence_summary": "The goal of this paper is to decouple disentangling and reconstruction for disentangled representation learning via dynamic control.", "pdf": "/pdf/76824fd024e07e4fd1e2ba536e5b9e7f5fb29dc4.pdf", "supplementary_material": "/attachment/ac01d67aa70ec745ae3c655492ff37bf8aa029db.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|dynamicvae_decoupling_reconstruction_error_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=K2C8frX-O4", "_bibtex": "@misc{\nshao2021dynamicvae,\ntitle={Dynamic{\\{}VAE{\\}}: Decoupling Reconstruction Error and Disentangled Representation Learning},\nauthor={Huajie Shao and Haohong Lin and Qinmin Yang and Shuochao Yao and Han Zhao and Tarek Abdelzaher},\nyear={2021},\nurl={https://openreview.net/forum?id=6htjOqus6C3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1198/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124368, "tmdate": 1606915808552, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1198/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1198/-/Official_Review"}}}, {"id": "For3EgHTeU", "original": null, "number": 2, "cdate": 1603893250734, "ddate": null, "tcdate": 1603893250734, "tmdate": 1605024505427, "tddate": null, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "invitation": "ICLR.cc/2021/Conference/Paper1198/-/Official_Review", "content": {"title": "Limited empirical evaluation.", "review": "#### Summary \nThe paper proposes DynamicVAE, a variant of ControlVAE that is claimed to decouple the optimization reconstruction error and disentanglement, hence mitigating the inherent trade-off between the two.\n#### Pros:\n- The paper sheds light on the tradeoff problem in disentangled representation learning from an optimization perspective.  \n- They empirically show that their proposed method can in fact decouple the two optimization objectives. \n\n#### Cons:\n- It will be very helpful if the authors can clarify the exact difference between DynamicVAE and ControlVAE. My understanding is that the main difference lies in their usage of Hybrid Annealing, which is certainly important but somewhat incremental given the overall results.\n- My main concern with the paper is that it lacks extensive empirical evaluation.  \n  1. Currently, the baselines used are very week. Please consider using baseline methods such as [1] or others that also tackle the trade-off problem. \n  2. Please consider using more challenging datasets like SmallNorb or Cars (see [2] for options). This is important since dSprites can be fully specified using only the disentangled factors. For these difficult datasets please consider doing a quantitative evaluation. Currently, I only see qualitative evaluation for Chairs (which indeed is a reasonable dataset).\n  3. What values of beta and gamma did you try for the baseline methods? The RMIG numbers reported for beta vae are very low and different from what RMIG paper reports.\n\n#### Comments:\n- It is known that the pixel-wise reconstruction score does not give a clear picture of the sample quality [3]. Please consider using FID (see [4]).\n- I am interested in knowing how the high KL of 20 affects the NLL of the model? Also, how is the sample quality of generated samples?\n\nMy current rating is primarily based on the fact that the paper requires a lot more evaluation with competitive baselines on more relevant datasets. Also, clarifying the difference from ControlVAE will really help.\n\n[1] Overcoming the Disentanglement vs Reconstruction Trade-off via Jacobian Supervision\n[2] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\n[3] Generating Diverse High-Fidelity Images with VQ-VAE-2\n[4] Improving the Reconstruction of Disentangled Representation Learners via Multi-Stage Modelling", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1198/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1198/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynamicVAE: Decoupling Reconstruction Error and Disentangled Representation Learning", "authorids": ["~Huajie_Shao1", "lhh2017@zju.edu.cn", "qmyang@zju.edu.cn", "~Shuochao_Yao1", "~Han_Zhao1", "~Tarek_Abdelzaher1"], "authors": ["Huajie Shao", "Haohong Lin", "Qinmin Yang", "Shuochao Yao", "Han Zhao", "Tarek Abdelzaher"], "keywords": ["disentangled representation learning", "dynamic learning", "Variational Autoencoder", "PID contoller"], "abstract": "This paper challenges the common assumption that the weight $\\beta$, in $\\beta$-VAE, should be larger than $1$ in order to effectively disentangle latent factors. We demonstrate that $\\beta$-VAE, with $\\beta < 1$, can not only attain good disentanglement but also significantly improve reconstruction accuracy via dynamic control. The paper \\textit{removes the inherent trade-off} between reconstruction accuracy and disentanglement for $\\beta$-VAE. Existing methods, such as $\\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for the sake of better disentanglement. To mitigate this problem, a ControlVAE has recently been developed that dynamically tunes the KL-divergence weight in an attempt to \\textit{control the trade-off} to more a favorable point. However, ControlVAE fails to eliminate the conflict between the need for a large $\\beta$ (for disentanglement) and the need for a small $\\beta$ (for smaller reconstruction error). Instead, we propose DynamicVAE that maintains a different $\\beta$ at different stages of training, thereby \\textit{decoupling disentanglement and reconstruction accuracy}. In order to evolve the weight, $\\beta$, along a trajectory that enables such decoupling, DynamicVAE leverages a modified incremental PI (proportional-integral) controller, a variant of proportional-integral-derivative controller (PID) algorithm, and employs a moving average as well as a hybrid annealing method to evolve the value of KL-divergence smoothly in a tightly controlled fashion. We theoretically prove the stability of the proposed approach. Evaluation results on three benchmark datasets demonstrate that DynamicVAE significantly improves the reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The results verify that our method can separate disentangled representation learning and reconstruction, removing the inherent tension between the two. ", "one-sentence_summary": "The goal of this paper is to decouple disentangling and reconstruction for disentangled representation learning via dynamic control.", "pdf": "/pdf/76824fd024e07e4fd1e2ba536e5b9e7f5fb29dc4.pdf", "supplementary_material": "/attachment/ac01d67aa70ec745ae3c655492ff37bf8aa029db.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|dynamicvae_decoupling_reconstruction_error_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=K2C8frX-O4", "_bibtex": "@misc{\nshao2021dynamicvae,\ntitle={Dynamic{\\{}VAE{\\}}: Decoupling Reconstruction Error and Disentangled Representation Learning},\nauthor={Huajie Shao and Haohong Lin and Qinmin Yang and Shuochao Yao and Han Zhao and Tarek Abdelzaher},\nyear={2021},\nurl={https://openreview.net/forum?id=6htjOqus6C3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1198/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124368, "tmdate": 1606915808552, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1198/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1198/-/Official_Review"}}}, {"id": "621aQGPY-pm", "original": null, "number": 1, "cdate": 1603386765007, "ddate": null, "tcdate": 1603386765007, "tmdate": 1605024505292, "tddate": null, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "invitation": "ICLR.cc/2021/Conference/Paper1198/-/Official_Review", "content": {"title": "Contribution too small for an ICLR paper", "review": "This paper introduces a strategy for controlling the beta value of a beta-VAE during training using approaches from control theory that allow it to target a designated level of KL divergence between the encoder distribution and the prior.  This is done in a way that aims to achieve good reconstructions while maintaining disentangling performance.  It can be viewed as a refinement of the ControlVAE approach of Shao et al 2020, varying only in the specifics of the strategy used.\n\nThough the approach is sensible and empirical results of the work are reasonable, I believe the contribution of the work is too small and too specific for publication at ICLR.  None of the high-level ideas are new, having already been introduced in Shao et al 2020, with the contribution effectively equating to making relatively low-level adjustments to the precise approach of that work.  This makes the content very niche and I believe it will be of limited interest to the ICLR community.   Beyond this, I also have serious concerns about the fact that this is an improvement on a somewhat outdated and discredited approach to disentanglement (the beta-VAE), along with the fact that the paper suffers from very serious overclaiming (such as the repeated incorrect claim of \"removing the inherent trade-off between reconstruction accuracy and disentanglement).  I also have some misgivings with the experimental evaluation as quantitative assessment of disentanglement is only provided for one dataset and reconstruction comparisons for only two of the three datasets; not enough to reach any solid conclusions about the approaches' performance, particularly once we consider how inconsistent disentangling methods and metrics are known to be across random seeds (e.g. the gains over ControlVAE do not look statistically significant).\n\n*Strengths*\n- The paper is reasonably well written and easy to follow.\n- The general approach is sensible and the control-theory approaches used to control beta are well-principled.\n- The experimental results are reasonable, albeit not particularly impressive.\n\n\n*Weaknesses*\n- The contributions are very small and specific, constituting low-level changes relative to Shao et al 2020.\n- The approach builds on the beta-VAE which is not only far from the state-of-the-art, but actively discredited in various ways (e.g. Mathieu et al 2019 and Locatello et al 2019).  Though overwise quite well-referenced, the paper critically omits any discussion of these limitations and I feel like the community has somewhat moved on from the beta-VAE by now; I do not think this is a line of work which will last the test of time or which has any particularly useful transferable ideas.  Given that unsupervised disentanglement methods have rather limited applications at the moment, I thus do not feel the work adds much to the community.\n- The paper severely overclaims: it results suggest that it is potentially able to improve on the Pareto front between reconstruction and disentanglement, but it is blatantly untrue that it completely removes the trade-off in the way that is repeatedly claimed.  Firstly, one still has to have a target KL and how this is fixed naturally induces a trade-off (with higher targets being more focused on disentanglement).  Secondly, there are various reasons why there will always be a degree of trade-off between these objectives regardless of the approach used (e.g. the best reconstructions are achieved if the autoencoder is completely noiseless, but removing the noise also removes any pressure for the latent space to structure in any way as one can just construct a lookup table), such that the claims clearly cannot hold.\n- As explained earlier, the numerical experimental results are not really extensive enough to confirm the approach offers meaningful improvements across a range of tasks.  For example, disentanglement is only quantitively evaluated on one dataset (Dsprites) and even more reruns are needed because the differences to the ControlVAE do not look like they are statistically significant.\n- I think there are much stronger baselines that could have been considered such as a) disentangling approaches published since the FactorVAE, b) using hierarchical VAEs which has shown to already give a much better trade-off (see eg arXiv:1906.00230), c) more heuristical ways of varying beta, and d) a FactorVAE where the scaling on the total correlation term is varied.  Relatedly, the FactorVAE is still rapidly improving its reconstruction loss with more iterations in Fig 2a, making the comparisons a little unfair as it seems like it might quickly catch up if the training was run for longer.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1198/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1198/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynamicVAE: Decoupling Reconstruction Error and Disentangled Representation Learning", "authorids": ["~Huajie_Shao1", "lhh2017@zju.edu.cn", "qmyang@zju.edu.cn", "~Shuochao_Yao1", "~Han_Zhao1", "~Tarek_Abdelzaher1"], "authors": ["Huajie Shao", "Haohong Lin", "Qinmin Yang", "Shuochao Yao", "Han Zhao", "Tarek Abdelzaher"], "keywords": ["disentangled representation learning", "dynamic learning", "Variational Autoencoder", "PID contoller"], "abstract": "This paper challenges the common assumption that the weight $\\beta$, in $\\beta$-VAE, should be larger than $1$ in order to effectively disentangle latent factors. We demonstrate that $\\beta$-VAE, with $\\beta < 1$, can not only attain good disentanglement but also significantly improve reconstruction accuracy via dynamic control. The paper \\textit{removes the inherent trade-off} between reconstruction accuracy and disentanglement for $\\beta$-VAE. Existing methods, such as $\\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for the sake of better disentanglement. To mitigate this problem, a ControlVAE has recently been developed that dynamically tunes the KL-divergence weight in an attempt to \\textit{control the trade-off} to more a favorable point. However, ControlVAE fails to eliminate the conflict between the need for a large $\\beta$ (for disentanglement) and the need for a small $\\beta$ (for smaller reconstruction error). Instead, we propose DynamicVAE that maintains a different $\\beta$ at different stages of training, thereby \\textit{decoupling disentanglement and reconstruction accuracy}. In order to evolve the weight, $\\beta$, along a trajectory that enables such decoupling, DynamicVAE leverages a modified incremental PI (proportional-integral) controller, a variant of proportional-integral-derivative controller (PID) algorithm, and employs a moving average as well as a hybrid annealing method to evolve the value of KL-divergence smoothly in a tightly controlled fashion. We theoretically prove the stability of the proposed approach. Evaluation results on three benchmark datasets demonstrate that DynamicVAE significantly improves the reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The results verify that our method can separate disentangled representation learning and reconstruction, removing the inherent tension between the two. ", "one-sentence_summary": "The goal of this paper is to decouple disentangling and reconstruction for disentangled representation learning via dynamic control.", "pdf": "/pdf/76824fd024e07e4fd1e2ba536e5b9e7f5fb29dc4.pdf", "supplementary_material": "/attachment/ac01d67aa70ec745ae3c655492ff37bf8aa029db.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|dynamicvae_decoupling_reconstruction_error_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=K2C8frX-O4", "_bibtex": "@misc{\nshao2021dynamicvae,\ntitle={Dynamic{\\{}VAE{\\}}: Decoupling Reconstruction Error and Disentangled Representation Learning},\nauthor={Huajie Shao and Haohong Lin and Qinmin Yang and Shuochao Yao and Han Zhao and Tarek Abdelzaher},\nyear={2021},\nurl={https://openreview.net/forum?id=6htjOqus6C3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1198/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124368, "tmdate": 1606915808552, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1198/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1198/-/Official_Review"}}}, {"id": "xEMTvZTbbyN", "original": null, "number": 4, "cdate": 1604873371249, "ddate": null, "tcdate": 1604873371249, "tmdate": 1605024505227, "tddate": null, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "invitation": "ICLR.cc/2021/Conference/Paper1198/-/Official_Review", "content": {"title": " Dynamically controlling the beta parameter is an interesting idea but this paper has several shortcomings and is not ready for publication.", "review": "Positive points:\n\n- Dynamically controlling the beta parameter of beta VAE is an interesting idea\n\nNegative points:\n\n- 1. The authors do not put their work into the context of the closely related information bottleneck principle\n- 2. The authors do not provide further details on several mechansims of their proposed method:\n  - 2. a) How is the constraint threshold C on the KL divergence term chosen? Does it have to be tuned for every dataset? Can it be tuned at all in the unsupervised setting, when no information about the ground truth factors is available?\n  - 2. a) Why does a constraint on the KL divergence result in a decoupling of reconstruction quality and disentanglement?\n  - 2. b) Is the beta schedule of starting with a large beta value which is then reduced a result of the control algorithm or is it pre-defined by the Hybrid annealing method the authors describe in section 3?\n- 3. Experiments:\n  - 3. a) FactorVAE has not converged yet, and it is the closest competitor.\n  - 3. b) The RMIG scores of FactorVAE, ControlVAE and DynamicVAE reported in Table 1 are not significantly different and therefore the score of DynamicVAE should not be in bold font.\n- 4. Missing references\n\n\n1. Missing references and context to related work:\n\nThe method seems closely related to the information bottleneck method (Tishby et al. 2000, Kirsch et al. 2020): the constraint on the KL divergence term to remain below a fixed value, defines an upper bound on the bandwith of the latent channel. The authors should explore this connection further, or at least should discuss the relation between the proposed method and the information bottleneck method.\n\n(Stuehmer et al. 2020) et al. propose a structured prior that also reduces the trade-off between reconstruction accuracy and disentanglement. It would be interesting to compare the performance of the proposed method to their methood ISA-VAE.\n\n\n2. Method\n\n2. a) How is the constraint threshold C on the KL divergence term chosen? Does it have to be tuned for every dataset? Can it be tuned at all in the unsupervised setting, when now information about the ground truth factors is available?\n\n2. b) Is the beta schedule of starting with a large beta value which is then reduced a result of the control algorithm or is it pre-defined by the Hybrid annealing method the authors describe in section 3?\n\n\n3. Experiments\n\nFig. 2 a) depicts the reconstruction error of the proposed method and different baselines for up to 1250K iterations. Figure 6 depicts the reconstruction loss of the the proposed method for up to 1600K iterations. \n\n3. a) FactorVAE has not converged yet (Fig. 2 a), and it is the closest competitor. Further iterations are recquired to assess if there is a difference in the reconstruction error between the proposed method and FactorVAE.\n\n3. b) The RMIG scores of FactorVAE, ControlVAE and DynamicVAE reported in Table 1 are not significantly different and therefore the score of DynamicVAE should not be in bold font. Are the reported error values standard deviation? This needs to be explained in the caption of the table.\n\n3. c) How does the proposed PI-controller based approach compare to a simpler approach of beta-VAE with a fixed beta schedule: After a pre-defined number of iterations beta goes down to a lower value. Does the dynamic control have any benefits?\n\n3. d) Related to 3 c): Is the schedule for the beta parameter that is generated by the PI-controller significantly different for different datasets?\n\n\nConclusion: Dynamically controlling the beta parameter is an interesting idea but this paper has several shortcomings and is not ready for publication.\n\n\nReferences:\n\nTishby et al., The information bottleneck method, ACCCC 1999\nKirsch et al., Unpacking Information Bottlenecks: Unifying Information-Theoretic Objectives in Deep Learning, arXiv 2020\nStuehmer et al., Independent Subspace Analysis for Unsupervised Learning of Disentangled Representations, ICML 2020", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1198/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1198/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DynamicVAE: Decoupling Reconstruction Error and Disentangled Representation Learning", "authorids": ["~Huajie_Shao1", "lhh2017@zju.edu.cn", "qmyang@zju.edu.cn", "~Shuochao_Yao1", "~Han_Zhao1", "~Tarek_Abdelzaher1"], "authors": ["Huajie Shao", "Haohong Lin", "Qinmin Yang", "Shuochao Yao", "Han Zhao", "Tarek Abdelzaher"], "keywords": ["disentangled representation learning", "dynamic learning", "Variational Autoencoder", "PID contoller"], "abstract": "This paper challenges the common assumption that the weight $\\beta$, in $\\beta$-VAE, should be larger than $1$ in order to effectively disentangle latent factors. We demonstrate that $\\beta$-VAE, with $\\beta < 1$, can not only attain good disentanglement but also significantly improve reconstruction accuracy via dynamic control. The paper \\textit{removes the inherent trade-off} between reconstruction accuracy and disentanglement for $\\beta$-VAE. Existing methods, such as $\\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for the sake of better disentanglement. To mitigate this problem, a ControlVAE has recently been developed that dynamically tunes the KL-divergence weight in an attempt to \\textit{control the trade-off} to more a favorable point. However, ControlVAE fails to eliminate the conflict between the need for a large $\\beta$ (for disentanglement) and the need for a small $\\beta$ (for smaller reconstruction error). Instead, we propose DynamicVAE that maintains a different $\\beta$ at different stages of training, thereby \\textit{decoupling disentanglement and reconstruction accuracy}. In order to evolve the weight, $\\beta$, along a trajectory that enables such decoupling, DynamicVAE leverages a modified incremental PI (proportional-integral) controller, a variant of proportional-integral-derivative controller (PID) algorithm, and employs a moving average as well as a hybrid annealing method to evolve the value of KL-divergence smoothly in a tightly controlled fashion. We theoretically prove the stability of the proposed approach. Evaluation results on three benchmark datasets demonstrate that DynamicVAE significantly improves the reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The results verify that our method can separate disentangled representation learning and reconstruction, removing the inherent tension between the two. ", "one-sentence_summary": "The goal of this paper is to decouple disentangling and reconstruction for disentangled representation learning via dynamic control.", "pdf": "/pdf/76824fd024e07e4fd1e2ba536e5b9e7f5fb29dc4.pdf", "supplementary_material": "/attachment/ac01d67aa70ec745ae3c655492ff37bf8aa029db.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|dynamicvae_decoupling_reconstruction_error_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=K2C8frX-O4", "_bibtex": "@misc{\nshao2021dynamicvae,\ntitle={Dynamic{\\{}VAE{\\}}: Decoupling Reconstruction Error and Disentangled Representation Learning},\nauthor={Huajie Shao and Haohong Lin and Qinmin Yang and Shuochao Yao and Han Zhao and Tarek Abdelzaher},\nyear={2021},\nurl={https://openreview.net/forum?id=6htjOqus6C3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6htjOqus6C3", "replyto": "6htjOqus6C3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1198/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124368, "tmdate": 1606915808552, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1198/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1198/-/Official_Review"}}}], "count": 6}