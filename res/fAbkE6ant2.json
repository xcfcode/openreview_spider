{"notes": [{"id": "fAbkE6ant2", "original": "wEYJxnPg-T6", "number": 2786, "cdate": 1601308309029, "ddate": null, "tcdate": 1601308309029, "tmdate": 1611671316192, "tddate": null, "forum": "fAbkE6ant2", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XELPIZbvPqK", "original": null, "number": 1, "cdate": 1610040407371, "ddate": null, "tcdate": 1610040407371, "tmdate": 1610474004205, "tddate": null, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "All reviewers agree that the paper brings new knowledge in the field of locally supervised learning, and as such it should be accepted.  The authors should keep all reviewers' comments into account when preparing their camera ready version."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040407357, "tmdate": 1610474004188, "id": "ICLR.cc/2021/Conference/Paper2786/-/Decision"}}}, {"id": "8JoPOQ4wVhK", "original": null, "number": 2, "cdate": 1603807446542, "ddate": null, "tcdate": 1603807446542, "tmdate": 1606618094861, "tddate": null, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Review", "content": {"title": "Insightful, some shortcomings in empirical evaluation", "review": "The paper proposes a strategy for training feed-forward networks in a more memory-efficient manner by employing local as opposed to end-to-end supervision. End-to-end/global (E2E) supervision as the dominant paradigm in training deep networks considers a loss function at the very end of the network for backpropagation of the resulting gradients, whereas local supervision injects supervisory signals (such as the same E2E objective, e.g. classification) at intermediate layers in the network. The benefit of such intermediate supervision is the ability to train larger networks in smaller chunks piece by piece, where each individual training is more memory efficient due to reduced need to store activations (and weights and biases) in GPU memory. As a drawback, however, it had been shown earlier that such local training is less optimal than global training in terms of the achievable generalization performance. The authors propose a new training strategy that aims at combining the memory efficiency of local supervision and piecewise training with the error performance of global training. Considering a given intermediate layer, the paper motivates to maximize the mutual information between the activations in this layer and the input signal to retain relevant information, while minimizing the mutual information of the activations and a nuisance variable, where the nuisance is defined as having no mutual information with the target variable (e.g. the classification prediction). The authors argue that this local supervision allows to train the features at the intermediate layer such that they carry relevant information from the input to the target variable without resorting to direct supervision with the target variable. Direct computation of the nuisance variable is infeasible and the authors propose a bounded approximation. \nEmpirical results are discussed on five common vision datasets and two CNN architectures in relation to the existing state of the art.\n\n### Strengths\n**[S1]** The paper is largely written well and addresses a relevant problem. Contributions and claims are laid out well.\n\n**[S2]** The method has potentially multiple benefits: (a) more memory efficient training without loss of performance, (b) speedups for asynchronous training, (c) use of larger batch size or larger models.\n\n**[S3]** The method is motivated reasonably well by the argument of minimizing the loss of mutual information with the input variable while minimizing the mutual information of nuisance variables with respect to the target quantity.\n\n**[S4]** The inclusion of Greedy SL+ to account for the additional degrees of freedom of the proxy networks in InfoPro is a good attention to detail in the empirical evaluation.\n\n**[S5]** The paper looks at a reasonable set of tasks and studies the performance on relevant data, for instance Imagenet and Cityscapes semantic segmentation (but consider [W1] below)\n\n### Weaknesses:\n**[W1]** Baselines of empirical comparisons: With the exception of Fig 3, the empirical results are compared to only a comparatively simple baseline (Greedy SL/SL+), but not the same state of the art mentioned in section 4.1 (DGL, DIB, BoostResNet). This seems a rather odd omission, particularly as Fig 3/Section 4.1 elaborate on alternative methods (DGL, DIB, BoostResNet). It would support the paper's case to either include the results of those in table 2 or elaborate on their absence.\n\n**[W2]** Fig 3: The scale of the y-axis is chosen in a way that a casual reader may visually misinterpret the absolute difference in error of the shown methods. For instance, at first glance DGL at K=2 seems twice as bad as Infopro, while in actuality is only about 14% worse (error from 7.76% to ~8.8%)\n\n**[W3]** A methodical concern is the choice of $\\phi$ and $\\psi$ and the limited discussion around their choice (section 3.3, App. E): What is the sensitivity of the optimization with respect to their size and structure, what happens if I make them larger or smaller, how small can I make them? How would they look like for objectives other than classification?\n\n### Further comments:\n**[C1]** It would be insightful to consider what the implications for networks with recurrent structures would be.\n\n**[C2]** It took a while to infer in section 3.3. that $\\mathcal{R}$ is the reconstruction of the input data. A short sentence there may help future readers go through more smoothly.\n\nI feel that I have learned something from the paper. It discusses the contributions, motivations and method reasonably thorough. My concerns are largely around the empirical substantiation of the claims, see [W1].", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088668, "tmdate": 1606915774677, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2786/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Review"}}}, {"id": "JB6woEcmO7", "original": null, "number": 3, "cdate": 1605875924448, "ddate": null, "tcdate": 1605875924448, "tmdate": 1605942899606, "tddate": null, "forum": "fAbkE6ant2", "replyto": "8yIna2y6Ri6", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (part 1) ", "comment": "Thank you very much for providing the valuable suggestions. Our responses to the comments are listed below:\n\n\n\n**Q1**: Using the additional network might not be fair when strictly comparing the performance. But practically, when training the large network, the size of the auxiliary network seems ok   \n**A1**: Indeed, we involve additional networks. However, as you noted, the computational overhead is minimal. In addition, our new results, as shown in Figure 4, indicate that their size can be further reduced.\n\n\n\n**Q2**: It is not strongly required. Is it available to apply the method to the detection task?  \n**A2**: (1) The formulation of the proposed InfoPro loss is general and flexible. As a matter of fact, its final optimization objective merely involves I(h, x) and I(h, y). We believe that, once these two terms are reasonably estimated, it can be applied to most of deep learning tasks, including object detection. For example, when dealing with image-based object detection tasks, we can estimate I(h, x) with the same decoder as image classification and semantic segmentation. The task-relevant information I(h, y) can be estimated with existing detection heads, such as FPN+RPN or the head used in YOLO. One may need to add a decoder together with a detection head at the end of each local module, attach a reconstruction loss to the regular detection loss function, and sequentially trigger the back-propagation process of all local modules with every mini-batch of training data.   \n      (2) However, due to the limitation at time, computational resources and the space of the conference paper (and the CVPR ddl, frankly :D), we prefer to add the detection results to the extended journal version of this paper. We are now focusing on it. If we finish before the deadline of discussion, we will present the results in Appendix. Last but not least, we believe that the current results of image classification and semantic segmentation on five widely-used datasets might be sufficient to evaluate our method comprehensively.\n\n\n\n**Q3**: Adding the experiments for other widely used network s.a. VGG, would be meaningful.  \n**A3**: We have added the results with VGG in Appendix I (due to the spatial limitation of the paper).\n\n\n\n**Q4**: Can we use LR-ASPP in MobileNet v3 instead of ASPP? or Can we use a heavier Decoder for better performance?  \n**A4**: (1) At first, we hope to clarify a (possibly) potential misunderstanding. The ASPP head is used to estimate the task-relevant information I(h,y) for the semantic segmentation task on Cityscapes, instead of ImageNet. We introduce the architecture of the decoder and the auxiliary classifier used on ImageNet in Appendix E. Both of them follow a regular design, and involve little computational cost.  \n      (2) The LA-ASPP module can be applied to InfoPro since it is also designed for obtaining the segmentation results. However, for a fair comparison with the end-to-end trained DeepLab-V3, we simply report the results with the basic version of ASPP.  \n      (3) To investigate how the size of the decoder affects the performance of InfoPro, we scale its width with a certain factor (which equals 1 for the original design) and present the corresponding test errors. The results are shown in the left plot of Figure 4 in the revised paper. We find that the current decoder is generally sufficient for achieving the excellent performance. Using larger decoders merely improves the accuracy slightly.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fAbkE6ant2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2786/Authors|ICLR.cc/2021/Conference/Paper2786/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844488, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment"}}}, {"id": "lHev6lFwGW", "original": null, "number": 7, "cdate": 1605879021431, "ddate": null, "tcdate": 1605879021431, "tmdate": 1605879021431, "tddate": null, "forum": "fAbkE6ant2", "replyto": "oxUDvHcbTOb", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for your valuable comments on the experiments. We have added the results with regards to your concerns. Our responses are listed below:\n\n\n**Q1**: This is seldom comparisons with other methods in this paper. Only comparisons with the baseline are provided.  \n**A1**: Thank you for the suggestion. The results of the state-of-the-art baseline, DGL (published in ICML 2020), have been added in Table 2. Our method compares favorably with it in most of the settings. \n\n\n**Q2**: Is the computational overhead is measured by training time or theorical computation cost? The comparison of real training time is more informative.  \n**A2**: The computational overhead reported in the original paper is the theoretical result. In the revised paper, we have added the practical additional training time (wall time cost), and clearly distinguished the theoretical and practical cost (see: Tables 3, 4, 5). \n\n**Q3**: Comparisons with other methods for reducing the memory consumption, e.g., [a] should also be added since they share similar aim. [a] Chen et al. Training Deep Nets with Sublinear Memory Cost.  \n**A3**: In our revision, we compare our method with this baseline in Table 3. Our method achieves competitive performance with it, but significantly reduces the computational and time cost.\n\n**Q4**: Recent contrastive learning works show that hyper-parameter \u03c4 in contrastive loss plays a very important role, like MOCO. Though \u03c4 = 0.07 is fit for MOCO, the setting here is different from MOCO. In this paper, the contrastive loss makes use of label information. If the ablation study for training hyper-parameter \u03c4 in the contrastive loss function is provided, it will be good.  \n**A4**: As suggested, we have conducted the sensitivity test for the temperature $\\tau$ and reported the results in Figure 5. InfoPro (Contrast) is relatively robust to $\\tau$ when $0.005 < \\tau < 0.1$.\n\n\n**Q5**: In the experiments, the contrastive loss outperforms cross-entropy loss. And the authors attribute this phenomenon to using larger batch size for contrastive loss. However, I think the strong regularization on intra-class representation may be the key factor. Anyhow, ablation study of different batch sizes for contrastive loss should be added here.  \n**A5**: Thank you for the suggestion. We have added the results of the ablation study for different batch sizes in Table 7. We find that the contrastive loss generally benefits from a large batch size and a long training schedule. This observation is consistent with existing work [1]. In addition, we totally agree that the regularization on intra-class representation is an important factor of its strong performance. As a matter of fact, we believe that with larger mini-batches, where there will be more samples in each class, both the intra-class and the inter-class regularization effects will grow stronger, leading to better performance.\n[1] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fAbkE6ant2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2786/Authors|ICLR.cc/2021/Conference/Paper2786/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844488, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment"}}}, {"id": "AW8OBwCVgEM", "original": null, "number": 6, "cdate": 1605878302182, "ddate": null, "tcdate": 1605878302182, "tmdate": 1605878302182, "tddate": null, "forum": "fAbkE6ant2", "replyto": "8JoPOQ4wVhK", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for your valuable suggestions on the empirical evaluation. We have revised the paper by adding the results with regards to your concerns. Our responses are listed below:\n\n\n**Q1**: [W1] Baselines of empirical comparisons: With the exception of Fig 3, the empirical results are compared to only a comparatively simple baseline (Greedy SL/SL+), but not the same state of the art mentioned in section 4.1 (DGL, DIB, BoostResNet). This seems a rather odd omission, particularly as Fig 3/Section 4.1 elaborate on alternative methods (DGL, DIB, BoostResNet). It would support the paper's case to either include the results of those in table 2 or elaborate on their absence.  \n**A1**: We have added the results of the state-of-the-art baseline, DGL (published in ICML 2020), in Table 2. Our method compares favorably with it. In addition, DIB and BoostResNet are both layer-wise training methods, while our method performs layer-wise training only when it is applied to ResNet-32 (K=16). For a fair comparison, we present the results of DIB and BoostResNet in Figure 3 instead of Table 2.\n\n**Q2**: [W2] Fig 3: The scale of the y-axis is chosen in a way that a casual reader may visually misinterpret the absolute difference in error of the shown methods. For instance, at first glance DGL at K=2 seems twice as bad as Infopro, while in actuality is only about 14% worse (error from 7.76% to ~8.8%).  \n**A2**: Thank for the suggestion. We have revised Figure 3 by resizing the y-axis to make it start from 0. \n\n**Q3**: [W3] A methodical concern is the choice of $\\phi$ and $\\psi$ and the limited discussion around their choice (section 3.3, App. E): What is the sensitivity of the optimization with respect to their size and structure, what happens if I make them larger or smaller, how small can I make them? How would they look like for objectives other than classification?  \n**A3**: (1) Thank you for the valuable comment. We have added a detailed analysis on both the size and architecture of the auxiliary networks (see: Figure 4 and Table 6). In specific, since $\\phi$ and $\\psi$ have the same network architecture except only for the final linear layer, we study $\\phi$ for example. We first scale its width with a certain factor (which equals 1 for the original design), and show that the size of $\\phi$ can be shrunk by up to 2 times without severely degrading the accuracy. On the other hand, using a wider $\\phi$ makes little sense in terms of the performance. We also test other architectures for $\\phi$, including inserting/removing convolutional layers and linear lavers. It shows that involving at least 1 convolutional layer in $\\phi$ is important. For more details, please refer to the revised paper.  \n(2) The auxiliary network $\\phi$ or $\\psi$ is used to estimate the task-relevant information $I(h,y)$. Apart from classification tasks, the technique for estimating $I(h,y)$ we mentioned in section 3.3 can be easily extended to the regression tasks (e.g., depth estimation, bounding box regression for object detection). For example, consider a target value $y_i \\in [0, 1]$ corresponding to the sample $x_i $ and the hidden representation $h_i$. It can be view as a Bernoulli distribution where $P(y=1)=y_i$. As a result, the auxiliary network $q_{\\psi}(y|{h})$ can be trained with the binary cross-entropy loss. This might also be approximated by the mean-square loss as it has the same minima as the binary cross-entropy loss. We have added more discussions in Appendix J, please refer to it for more details.\n\n**Q4**: [C1] It would be insightful to consider what the implications for networks with recurrent structures would be.  \n**A4**: Thank you for pointing out the promising direction. However, it may not be straightforward to directly transfer InfoPro to recurrent neural networks (RNN). Consider inputting a sentence to RNN, if we split RNN in the temporal dimension, the sentence will also be split to short sub-sentences. Each sub-sentence may not contain enough information with regard to the target by itself. A possible direction might be implementing InfoPro in stacked recurrent networks, where we may consider training earlier and later recurrent units isolatedly with local supervision. We will focus on this in our future work.\n\n**Q5**: [C2] It took a while to infer in section 3.3. that $\\mathcal{R}$ is the reconstruction of the input data. A short sentence there may help future readers go through more smoothly.  \n**A5**: Thank you for the suggestion. We have revised the paper to make it clear.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fAbkE6ant2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2786/Authors|ICLR.cc/2021/Conference/Paper2786/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844488, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment"}}}, {"id": "_7UhiO9XU8n", "original": null, "number": 4, "cdate": 1605877189273, "ddate": null, "tcdate": 1605877189273, "tmdate": 1605878022305, "tddate": null, "forum": "fAbkE6ant2", "replyto": "awlOJvlPzF", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (part 2)", "comment": "**Q5**: \\# Softmax vs Contrastive \\# I am curious as to why contrastive works better. Could it be a coincidence of better hyperparameter tuning? Because the softmax version is a direct estimate on mutual information, whereas the contrastive one optimizes the lower bound. It would be nice if this was discussed in more detail (I do understand that there is already very little space though!)  \n**A5**: We have reported more analytical results in the revised paper, including 1) changing the temperature $\\tau$ for the contrastive loss (Figure 5), and 2) changing the batch size for training (Table 7). The results suggest that, the higher performance of InfoPro (Contrast) compared with InfoPro (Softmax) comes from adopting a proper temperature $\\tau$, a relatively large batch size, and a long training schedule. For one thing, InfoPro (Contrast) introduces an additional tunable hyper-parameter (the temperature $\\tau$), which affects the performance. For another, as we show in Appendix D, the lower bound estimated by the contrastive loss tends to be tight with a large batch size, leading to the improved test accuracy. In addition, we find that InforPro needs a relatively long schedule to converge.\n\n**Q6**: \\# Computational overhead \\# Is the computational overhead including the memory transfer cost? Are the results the actual physical measures observed via monitoring the resource usage? Or are they theoretical? I might have missed it, but this is not very clear to me.  \n**A6**: The computational overhead reported in the original paper is the theoretical result. However, in the revised paper, we have added the practical additional training time (wall time cost), and clearly distinguished the theoretical and practical cost (see: Tables 3, 4, 5).\n\n**Q7**: \\# Typos and grammar errors \\# There are quite a few grammatical mistakes throughout the paper. For example, at the beginning of Section 2, it is more natural to write \"we start by considering\" than \"we start with considering\". In the italic question in the second paragraph of Section 2, \"..., even the former...\" should be \"..., even though the former...\". While I generally did not find these errors to be critical, I would suggest a thorough proofread. I also found a typo in Appendix B. The last sentence should refer to equation (10) not (9).  \n**A7**: Thanks for your comments. We have carefully checked and revised the whole paper for several times according to your suggestion. All typos we found have been corrected. The writing is should be more acceptable now. \n\n**Q8**: \\# Early stopping, and the choice of the number of epochs \\# The training process in this paper does not utilize early stopping. While this is somewhat mitigated by the fact that multiple runs are performed, this is in fact another source of overfitting to the dataset and is strictly speaking tuning hyperparameters on the test set. This is a practice that should be avoided.  \n**A8**: Thank you for the comment. However, we want to clarify that we have never tuned the number of epochs on the test set. We tried our best to avoid (potential) overfitting to the test set. For training epochs, we simply follow existing work [1, 2], and do not change them if not specially mentioned.\n[1] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.\n[2] Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fAbkE6ant2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2786/Authors|ICLR.cc/2021/Conference/Paper2786/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844488, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment"}}}, {"id": "U-wzZme7xu3", "original": null, "number": 5, "cdate": 1605877265776, "ddate": null, "tcdate": 1605877265776, "tmdate": 1605877265776, "tddate": null, "forum": "fAbkE6ant2", "replyto": "awlOJvlPzF", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (part 1)", "comment": "Thank you for your detailed and encouraging comments. We have carefully revised our paper according to the suggestions. Our responses are listed below:\n\n**Q1**: \\# Section 3.1, regarding equation (1) \\# I am not 100% sure on this, but shouldn't the third last sentence of Section 3.1 read \"...under the goal of retaining as much information of the input as possible\"? I am not sure this is actually a constraint, and the I(h,x) term is not explicitly task-relevant information.  \n**A1**: Indeed, the major effect of minimizing $L_{\\text{InfoPro}}(h)$ should be maximally discarding the task-irrelevant information under the goal of retaining as much information of the input as possible. It is not a constraint. We have revised it for a clear description.\n\n**Q2**: \\# Section 3.3, estimating I(h,y) \\# It is not clear to me how I(h,y) is finally approximated. Shouldn't p(y|h) disappear after approximation? In the provided approximation it still exists. If p(y|h) is somehow directly used, isn't q not needed at all?  \n**A2**: Thank you for the valuable comment. In fact, we estimate the expectation over $h$ by sampling $(x_i, h_i, y_i)$ from the true distribution (i.e., $x_i$, $h _i$ and $y_i$ refer to the training sample, the hidden representation and the label respectively). As a result, $p(y|h)$ is only computed as $p(y_i|h_i)$ in the final form, instead of being directly used. We have revised our paper to make it clear. Please refer to Section 3.3 for details.\n\n**Q3**: \\# Section 3.3 final equation \\# In my opinion, even when it is not referred to in text, equations should have numbers so that future readers can refer to it  \n**A3**: Thank you for the comment. We have added a number to this equation.\n\n**Q4**: \\# Regarding Asy-InfoPro 1/2 \\# It is somewhat unclear whether the dynamic caching was used at the end. Are the experiments in Table 2 with dynamic caching? From my current understanding, it does not seem to be the case, which leads to my second issue.  \n\\# Regarding Asy-InfoPro 2/2 \\# This is assuming that the results are without dynamic caching as this seems most logical. The explanation in the \"Asynchronous and parallel training\" paragraph was not obvious to me during the first read. The second sentence could be rephrased and split so that it becomes clear that the distinction between the two modes is that transient feature maps are seen/unseen and that this has a regularizing effect. This then brings up the important question, whether the dynamic caching version suffers from the same fate---it should not. Having this experimental verification would greatly strengthen the observations in this paper.  \n**A4**: (1) Indeed, the results presented in Table 2 do not use the dynamic caching technique. We have revised Section 4 (both the \"Two training modes\" paragraph the \"Asynchronous and parallel training\" paragraph) to make it clearer.  \n(2) The dynamic caching version of InfoPro inherently does not suffer from lacking regularizing effects from the noisy outputs of earlier modules during training. Because all its training process is exactly the same as simultaneous training (i.e., \u201cInfoPro (Contrast/Softmax)\u201d in Table 2). The only difference is that we dynamically cache the outputs of earlier modules and use them to train later modules on another GPU. We have revised the \"Asynchronous and parallel training\" paragraph to highlight this point.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fAbkE6ant2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2786/Authors|ICLR.cc/2021/Conference/Paper2786/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844488, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment"}}}, {"id": "B_s1eqZO7EZ", "original": null, "number": 2, "cdate": 1605875650750, "ddate": null, "tcdate": 1605875650750, "tmdate": 1605876640977, "tddate": null, "forum": "fAbkE6ant2", "replyto": "8yIna2y6Ri6", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (part 2) ", "comment": "\n\n**Q5**: How come when we set K=4 and set a larger batch size? We do not need to waste the remaining memory.  \n**A5**: As a matter of fact, we show the effects of using larger batch sizes in semantic segmentation (Table 5). We also preliminarily test enlarging the batch size of InfoPro*, K=4 on CIFAR-10 from 1024 to 2048, but the improvement seems not significant (6.93 --> 6.86). However, we show in Table 5 that a more efficient approach to exploiting the memory saved by InfoPro* might be training models using data with higher-resolution. When increasing the crop size from 769x769 to 1024x1024, InfoPro* outperforms the end-to-end trained DeepLab-V3 by 0.65% in terms of mIOU. Note that this does not involve additional training or inference cost.\n\n\n**Q6**: Training the network larger than ResNet-152 from the proposed method would be impressive.  \n**A6**: Thank you for the encouraging suggestion! However, due to the limitation at time, computational resources and the space of the paper, it might be difficult to report this result in the revised paper. We believe that the presented results with the large ResNet-152 and ResNeXt-101, 32\u00d78d on ImageNet strongly verify the effectiveness of the proposed method. We will focus on the networks larger than ResNet-152 in our future work.\n\n\n**Q7**: Can we replace the imageNet pre-trained (by E2E) backbone with that trained by the proposed method? Experiments or discussion verifying the transferability of the trained network would be required.  \n**A7**: Thank you for the suggestion. To verify the transferability of the models trained by the proposed method, we initialize the backbone of a Faster-RCNN using ResNet-101 trained by E2E training and InfoPro*, and train the network for the object detection task on MS COCO. The results are presented in Appendix I (due to the spatial limitation of the paper).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fAbkE6ant2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2786/Authors|ICLR.cc/2021/Conference/Paper2786/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844488, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Comment"}}}, {"id": "awlOJvlPzF", "original": null, "number": 3, "cdate": 1603843210012, "ddate": null, "tcdate": 1603843210012, "tmdate": 1605024131899, "tddate": null, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Review", "content": {"title": "Review", "review": "# Summary\n\nThe paper analyzes the pitfalls of locally supervised learning from the point of view of information propagation and proposes a new auxiliary loss that can facilitate locally supervised learning. The proposed loss, \"infopro loss\", is then relaxed to a tractable upper bound, which is then used instead. To implement the loss, mutual information is approximated with a decoder, as well as a classifier. The authors further introduce now contrastive learning fits in the framework as a lower bound maximization process regarding mutual information. The experimental results on standard datasets demonstrate the efficacy of the proposed method.\n\nI have enjoyed reading the paper quite a lot, and therefore recommend accepting the paper. Still, I have some reservations that I would love the authors to clarify via the rebuttal.\n\n# Strength\n\nThe paper is well written. There are some issues (which I detail in the weaknesses section), but most are very clear and easy to follow. There are some parts that are repetitive, but it does allow readers to scheme through without missing the important point.\n\nThe part that I like most about the paper is how proposition 1 and appendix D is presented. They are theoretically well-motivated and gladly seems to work, despite the relaxation. I personally think appendix D deserves more attention in the main text, but this is my personal preference.\n\nThe results clearly show that the method improves over simple greedy locally supervised learning, as well as other attempts at this problem. \n\n# Weaknesses\n\n## Section 3.1, regarding equation (1)\n\nI am not 100% sure on this, but shouldn't the third last sentence of Section 3.1 read \"...under the goal of retaining as much information of the input as possible\"? I am not sure this is actually a constraint, and the I(h,x) term is not explicitly task-relevant information.\n\n## Section 3.3, estimating I(h,y)\n\nIt is not clear to me how I(h,y) is finally approximated. Shouldn't p(y|h) disappear after approximation? In the provided approximation it still exists. If p(y|h) is somehow directly used, isn't q not needed at all?\n\n## Section 3.3 final equation\n\nIn my opinion, even when it is not referred to in text, equations should have numbers so that future readers can refer to it.\n\n## Regarding Asy-InfoPro\n\nIt is somewhat unclear whether the dynamic caching was used at the end. Are the experiments in Table 2 with dynamic caching? From my current understanding, it does not seem to be the case, which leads to my second issue.\n\nThis is assuming that the results are without dynamic caching as this seems most logical. The explanation in the \"Asynchronous and parallel training\" paragraph was not obvious to me during the first read. The second sentence could be rephrased and split so that it becomes clear that the distinction between the two modes is that transient feature maps are seen/unseen and that this has a regularizing effect.  This then brings up the important question, whether the dynamic caching version suffers from the same fate---it should not. Having this experimental verification would greatly strengthen the observations in this paper.\n\n## Softmax vs Contrastive\n\nI am curious as to why contrastive works better. Could it be a coincidence of better hyperparameter tuning? Because the softmax version is a direct estimate on mutual information, whereas the contrastive one optimizes the lower bound.  It would be nice if this was discussed in more detail (I do understand that there is already very little space though!)\n\n## Computational overhead\n\nIs the computational overhead including the memory transfer cost? Are the results the actual physical measures observed via monitoring the resource usage? Or are they theoretical? I might have missed it, but this is not very clear to me.\n\n## Typos and grammar errors\n\nThere are quite a few grammatical mistakes throughout the paper. For example, at the beginning of Section 2, it is more natural to write \"we start by considering\" than \"we start with considering\". In the italic question in the second paragraph of Section 2, \"..., even the former...\" should be \"..., even though the former...\". While I generally did not find these errors to be critical, I would suggest a thorough proofread.\n\nI also found a typo in Appendix B. The last sentence should refer to equation (10) not (9).\n\n## Early stopping, and the choice of the number of epochs\n\nThe training process in this paper does not utilize early stopping. While this is somewhat mitigated by the fact that multiple runs are performed, this is in fact another source of overfitting to the dataset and is strictly speaking tuning hyperparameters on the test set. This is a practice that should be avoided.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088668, "tmdate": 1606915774677, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2786/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Review"}}}, {"id": "8yIna2y6Ri6", "original": null, "number": 4, "cdate": 1603895354676, "ddate": null, "tcdate": 1603895354676, "tmdate": 1605024131827, "tddate": null, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Review", "content": {"title": "Reviewer1 Comments", "review": "Briefing:\nThis paper proposes a new infoPro loss for locally supervised training that alleviates the problem from greedy supervised learning, which collapsing task-relevant Information at the beginning of the layers.\nThe infoPro loss requires an auxiliary network to infer the Information I(h,y) (For ImageNet, the paper used ASPP).\n\nStrong points:\n\nAnalysis of the phenomenon when applying the greedy supervised learning (GSL): \nThe two observations from the authors seem natural.\n\n(1): GSL underperforms E2E training (little trivial for the reviewer)\n\n(2): Each separated layer trained by GSL captures more discriminative features.\n\nHowever, the information-based explanation of why the task-relevant info at the early stage of the network is essential (information collapse hypothesis) seems new and worth considering for the reviewer. \n\nInfoPro-loss:\nTheoretical analysis of the loss seems credible to the reviewer.\n\nExperiments:\nMutual Information at each layer index looked interesting.\n\nInterestingly, the proposed method outperforms E2E training in the ImageNet classification task.\n\nHyper-parameter does not seem to be sensitive.\n\nWeek point:\n\nUsing the additional network might not be fair when strictly comparing the performance. But practically, when training the large network, the size of the auxiliary network seems ok.\n\nComments:\n\n(1) It is not strongly required. Is it available to apply the method to the detection task?\n\n(2) Adding the experiments for other widely used network s.a. VGG, would be meaningful.\n\n(3) Can we use LR-ASPP in MobileNet v3 instead of ASPP? or Can we use a heavier Decoder for better performance?\n\n(4) How come when we set K=4 and set a larger batch size? We do not need to waste the remaining memory.\n\n(5) Training the network larger than ResNet-152 from the proposed method would be impressive.\n\n(6) Can we replace the imageNet pre-trained (by E2E) backbone with that trained by the proposed method? Experiments or discussion verifying the transferability of the trained network would be required.\n\n \nRating: Nice paper, Accept \n\n(1) But the reviewer wants a discussion with the author about the comments mentioned above\n\n(2) Literature checks from the other reviewers may affect the rating.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088668, "tmdate": 1606915774677, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2786/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Review"}}}, {"id": "oxUDvHcbTOb", "original": null, "number": 1, "cdate": 1603617327459, "ddate": null, "tcdate": 1603617327459, "tmdate": 1605024131682, "tddate": null, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "invitation": "ICLR.cc/2021/Conference/Paper2786/-/Official_Review", "content": {"title": "Good motivation but lack of comparisons", "review": "This paper analyzed the reason why locally supervised training led to performance degradation. And based on the analysis, the author proposed the information propagation loss (can be understood as the combination of a classification loss and a reconstruction loss) aiming to prevent information collapse. Equipped with the proposed method, 40% memory footprint can be reduced demonstrated by their experiments, which is surprising. \n\nStrength \n(1) Deep neural networks need heavy memory cost, especially for object detection and segmentation task. Reducing memory cost in the training can enlarge the batch size in the training, thus speed up the training process. This is a very promising direction.   \n(2) The paper is well motivated by analyzing traditional locally supervised training. Moreover, the analyze of information in features is also informative and reasonable. \n(3) The experimental results are good. The proposed method can achieve comparable performance with 40% less memory footprint. \n(4) The paper is clearly and is easy to understand.\n(5) The proposed method is simple but efficient. And it can be used in different tasks, like classification and semantic segmentation.\n\nWeaknesses \n(1) This is seldom comparisons with other methods in this paper. Only comparisons with the baseline are provided.\n(2) Is the computational overhead is measured by training time or theorical computation cost? The comparison of real training time is more informative.\n(3) Comparisons with other methods for reducing the memory consumption, e.g., [a] should also be added since they share similar aim. [a] Chen et al. Training Deep Nets with Sublinear Memory Cost.\n(4) Recent contrastive learning works show that hyper-parameter \u03c4 in contrastive loss plays a very important role, like MOCO. Though \u03c4 = 0.07 is fit for MOCO, the setting here is different from MOCO. In this paper, the contrastive loss makes use of label information. If the ablation study for training hyper-parameter \u03c4 in the contrastive loss function is provided, it will be good. \n(5) In the experiments, the contrastive loss outperforms cross-entropy loss. And the authors attribute this phenomenon to using larger batch size for contrastive loss. However, I think the strong regularization on intra-class representation may be the key factor. Anyhow, ablation study of different batch sizes for contrastive loss should be added here.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2786/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2786/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end Training", "authorids": ["~Yulin_Wang1", "~Zanlin_Ni1", "~Shiji_Song1", "~Le_Yang2", "~Gao_Huang1"], "authors": ["Yulin Wang", "Zanlin Ni", "Shiji Song", "Le Yang", "Gao Huang"], "keywords": ["Locally supervised training", "Deep learning"], "abstract": "Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|revisiting_locally_supervised_learning_an_alternative_to_endtoend_training", "one-sentence_summary": "We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.", "pdf": "/pdf/ae46b2e0daac3e1e7af2c0b30ca3ed05b9675f66.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021revisiting,\ntitle={Revisiting Locally Supervised Learning: an Alternative to End-to-end Training},\nauthor={Yulin Wang and Zanlin Ni and Shiji Song and Le Yang and Gao Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fAbkE6ant2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fAbkE6ant2", "replyto": "fAbkE6ant2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2786/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088668, "tmdate": 1606915774677, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2786/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2786/-/Official_Review"}}}], "count": 12}