{"notes": [{"id": "B1x996EKPS", "original": "BkxKCW0vPB", "number": 717, "cdate": 1569439122011, "ddate": null, "tcdate": 1569439122011, "tmdate": 1577168232802, "tddate": null, "forum": "B1x996EKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ozBpXh05D", "original": null, "number": 1, "cdate": 1576798704114, "ddate": null, "tcdate": 1576798704114, "tmdate": 1576800931947, "tddate": null, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Decision", "content": {"decision": "Reject", "comment": "This paper is concerned with learning in the context of so-called Byzantine failures. This is relevant for for example distributed computation of gradients of mini-batches and parameter updates. The paper introduces the concept and Byzantine servers and gives theoretical and practical results for algorithm for this setting.\n\nThe reviewers had a hard time evaluating this paper and the AC was unable to find an expert reviewer. Still, the feedback from the reviewers painted a clear picture that the paper did not do enough to communicate the novel concepts used in the paper.\n\nRejection is recommended with a strong encouragement to use the feedback to improve the paper for the next conference.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718377, "tmdate": 1576800268849, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper717/-/Decision"}}}, {"id": "S1gnPiXQjH", "original": null, "number": 5, "cdate": 1573235556328, "ddate": null, "tcdate": 1573235556328, "tmdate": 1573825817945, "tddate": null, "forum": "B1x996EKPS", "replyto": "Hklqi8B45S", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment", "content": {"title": "We thank AnonReviewer1 for the comments. We answer below to the points raised by the reviewer", "comment": "\u201cIn general, I miss a more clear indication of how the individual contributions are different from other methods. it's not clear to me what the real novelty of the work is.\u201d\n>> We would like to clarify the main contributions of this work. First, utilizing filtering techniques to tolerate Byzantine servers is novel as previous proposals use robust aggregation to do so. This goes into designing a novel filtering mechanism, which we call models filter, in addition to the novel adaptation of the Lipschitz filter idea to tolerate Byzantine models. For this, we design the local, speculative step that should be done by workers in order to compute the Lipschitz coefficient correctly; this last step is novel in this work. Second, we propose a communication protocol, which we call scatter/gather, to control the communication between servers and workers. Such a protocol is also novel, and it (along with the proposed filtering components) contributes to reducing the communication overhead by not only reducing the number of required communication rounds per iteration but also with reducing the number of communicated messages per round. Third, we theoretically prove that our algorithm, along with the proposed communication protocol, guarantees Byzantine behavior tolerance and learning convergence. Finally, we empirically show the performance of our algorithm compared to two baselines in practical setups, where we discuss convergence overhead, Byzantine tolerance, and system\u2019s throughput.\n\n\u201cI am also missing more detailed ablation studies showing which of the new ideas contribute the most to efficient learning.\u201d\n>> We achieve efficient learning mainly by reducing the communication overhead by reducing both (1) the number of communication rounds per iteration and (2) the number of messages per communication round. This is enabled by the novel idea of using filtering instead of robust aggregation to tolerate Byzantine servers in addition to the design of our scatter/gather protocol. Both help drastically reduce the communication overhead and contribute to the learning efficiency. \nAn interesting experiment that we are working on now (and planning to add it to the paper before the deadline) is to show the effect of the value of T (the number of learning iterations per one scatter step) on the learning convergence. We believe such an experiment will also shed light on the inherent tradeoff between the learning quality and the system\u2019s throughput.\n\n\u201cthe experiments do not really show an improvement over existing methods in this domain. it's not clear to me that the paper improves on existing methods.\u201d\n>> To show the improvement of LiuBei over existing methods, we evaluate it against two baselines: TensorFlow and GuanYu [2]. Compared to TensorFlow, LiuBei guarantees tolerance to both Byzantine servers and workers. Compared to GuanYu, LiuBei offers a 70% throughput gain with the same convergence behavior. Compared to other Byzantine tolerant algorithms, LiuBei offers an additional tolerance to Byzantine servers, which were assumed trusted and correct in such algorithms.\n\n[1] Georgios Damaskinos, et al. \"Asynchronous Byzantine machine learning (the case of SGD).\" ICML'18.\n[2] El-Mhamdi, El-Mahdi, et al. \"SGD: Decentralized Byzantine Resilience.\" arXiv preprint arXiv:1905.03853 (2019).\n\nPost rebuttal:\nThank you very much for your time. We will be happy to address any other concerns you might raise after the discussion with the other reviewers."}, "signatures": ["ICLR.cc/2020/Conference/Paper717/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper717/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x996EKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper717/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper717/Authors|ICLR.cc/2020/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167292, "tmdate": 1576860554117, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment"}}}, {"id": "Hklqi8B45S", "original": null, "number": 2, "cdate": 1572259489937, "ddate": null, "tcdate": 1572259489937, "tmdate": 1573815524180, "tddate": null, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "The paper considers distributed stochastic gradient descent, where some (unknown) compute nodes may be unreliable. New heuristics for filtering out replies from unreliable servers are introduced alongside a new protocol that helps keeping nodes in sync.\n\nIn general, I miss a more clear indication of how the individual contributions are different from other methods. I am also missing more detailed ablation studies showing which of the new ideas contribute the most to efficient learning. As far as I can tell, the experiments do not really show an improvement over existing methods in this domain.\n\nThis is not my area of expertise, but I cannot recommend the paper for publication in its current form as\n(a) it's not clear to me that the paper improves on existing methods, and\n(b) it's not clear to me what the real novelty of the work is.\n\nPost-rebuttal:\nI acknowledge the response of the authors. They clarified some aspects for me, and the paper appears to have improved over the rebuttal period.\nI did not change my rating, but I want to emphasize that this is only because my knowledge of this field is so limited. My rating is largely based on \"gut feeling\" rather than actual knowledge, and I won't argue against acceptance.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper717/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574776701549, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper717/Reviewers"], "noninvitees": [], "tcdate": 1570237748112, "tmdate": 1574776701563, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Review"}}}, {"id": "H1e7kebjor", "original": null, "number": 10, "cdate": 1573748698686, "ddate": null, "tcdate": 1573748698686, "tmdate": 1573748698686, "tddate": null, "forum": "B1x996EKPS", "replyto": "SkgAzo19jr", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you very much for your time. We will be happy to address any other concern you might raise after the discussion with the other reviewers."}, "signatures": ["ICLR.cc/2020/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x996EKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper717/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper717/Authors|ICLR.cc/2020/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167292, "tmdate": 1576860554117, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment"}}}, {"id": "SygnRcQXsH", "original": null, "number": 4, "cdate": 1573235411995, "ddate": null, "tcdate": 1573235411995, "tmdate": 1573725304147, "tddate": null, "forum": "B1x996EKPS", "replyto": "BkxhT9ISqB", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment", "content": {"title": "We thank AnonReviewer3 for the comments. We answer below to the points raised by the reviewer", "comment": "\u201cIt's not immediately clear to me what practical setup this is useful in. The authors assume perfect network synchrony. Who would ever use this and why? What's the plan for getting data to the untrusted workers?\u201d\n>> As a practical setup, think of a hospital for example that runs an ML application to help doctors give medications to their patients [7]. To accommodate for the huge data such a hospital gathers from patients and for the complex models it trains to achieve high accuracy for such a sensitive task, the hospital distributes learning on multiple machines [1]. For increased security, these machines run different implementations of the code (for the model training) [8]. The synchrony assumption could be achieved in such a controlled environment, i.e., engineers can expect an upper bound on the communication and computation delays. Several kinds of failures could happen to this setup, ranging from software bugs to adversarial behavior resulting from hacks to these machines. Moreover, the gathered data could be sometimes misleading or incomplete, which may pose a critical threat to training the hospital ML model. Another avenue where tolerating data from untrusted workers is of growing interest is on-device ML [2].\nWe believe that the distributed ML literature, and specifically the Byzantine ML literature (e.g., [3-6] to name a few), also focused on environments with network synchrony. We take these efforts one step forward and address the inevitable case of server\u2019s failures. We provide a proven solution to this problem which guarantees not only tolerance to such failures but also the convergence of the training procedure.\n\n\u201c they have a 25% overhead on TensorFlow\u201d\n>> We believe that 24% of convergence overhead, compared to vanilla TensorFlow, is acceptable in the Byzantine ML literature. For instance, AggregaThor [9], a state-of-the-art system that tolerates only Byzantine workers, reports 19% to 43% convergence overhead. GuanYu [6], the only existing algorithm to tolerate Byzantine workers and servers, reports a 30% convergence overhead. Based on that, we believe the performance of LiuBei lies in the typical range of the overhead achieved by similar algorithms, also keeping in mind the strong guarantees LiuBei provides.\n\n\u201cthey have a comparison to another algorithm that operates under different assumptions.\u201d\n>> We confirm that GuanYu, the main baseline, was designed to be used in a different environment than what we are considering in this paper (we assume network synchrony while GuanYu assumed network asynchrony). Yet, we compare with GuanYu as it is the only proposal, to the best of our knowledge, that addresses Byzantine resilience to both servers and workers. We would also like to clarify that we run GuanYu in a synchronous environment in our experiments to maintain comparison fairness. We also compare with vanilla TensorFlow that assumes network synchrony, yet does not tolerate Byzantine workers nor servers.\n\n[1] Li, Mu, et al. \"Scaling distributed machine learning with the parameter server.\" OSDI'14.\n[2] Kone\u010dn\u00fd, Jakub, et al. \"Federated learning: Strategies for improving communication efficiency.\" arXiv preprint arXiv:1610.05492 (2016).\n[3] Chen, Lingjiao, et al. \"Draco: Byzantine-resilient distributed training via redundant gradients.\" arXiv preprint arXiv:1803.09877 (2018).\n[4] Xie, Cong, et al. \"Zeno: Byzantine-suspicious stochastic gradient descent.\" arXiv preprint arXiv:1805.10032 (2018).\n[5] Zhixiong Yang et al. \"BRIDGE: Byzantine-resilient Decentralized Gradient Descent.\" arXiv preprint arXiv:1908.08098 (2019).\n[6] El-Mhamdi, El-Mahdi, et al. \"SGD: Decentralized Byzantine Resilience.\" arXiv preprint arXiv:1905.03853 (2019).\n[7] Esteva, Andre, et al. \"Dermatologist-level classification of skin cancer with deep neural networks.\" Nature 542.7639 (2017): 115.\n[8] Castro, Miguel, and Barbara Liskov. \"Practical Byzantine fault tolerance.\" OSDI' 99.\n[9] Georgios Damaskinos, et al. \"AGGREGATHOR: Byzantine Machine Learning via Robust Gradient Aggregation.\" SysML'19."}, "signatures": ["ICLR.cc/2020/Conference/Paper717/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper717/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x996EKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper717/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper717/Authors|ICLR.cc/2020/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167292, "tmdate": 1576860554117, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment"}}}, {"id": "SkgAzo19jr", "original": null, "number": 9, "cdate": 1573677845609, "ddate": null, "tcdate": 1573677845609, "tmdate": 1573677845609, "tddate": null, "forum": "B1x996EKPS", "replyto": "SklS6sQXsH", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment", "content": {"title": "Thanks for the new results", "comment": "I have seen the revised version of the paper, the new results look very interesting. Overall, I think Weak Accept is a fair score for this paper, so I will leave it unchanged and argue for acceptance."}, "signatures": ["ICLR.cc/2020/Conference/Paper717/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x996EKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper717/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper717/Authors|ICLR.cc/2020/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167292, "tmdate": 1576860554117, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment"}}}, {"id": "SJg4t_YKsH", "original": null, "number": 8, "cdate": 1573652604005, "ddate": null, "tcdate": 1573652604005, "tmdate": 1573652604005, "tddate": null, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment", "content": {"title": "Uploaded a revised version based on the reviewers' recommendations", "comment": "Based on the reviewers' recommendations, we uploaded a new version of our paper that describes a few additional experiments. We added the following experiments to the paper:\n\n1) We show the performance of LiuBei, our algorithm, with Byzantine servers. We experimented with 4 Byzantine behavior and showed that LiuBei converges safely despite such a behavior.\n2) We show the effect of different values for T on the convergence. The main finding is: the smaller the value of T, the slower the algorithm is yet, the more robust it is.\n3) We show the percentage of false positives and false negatives produced by the filters of LiuBei. We show that our filters do not have false positives (i.e., the filters never pass a Byzantine model) while keeping the ratio of false negatives < 1% with different values for T."}, "signatures": ["ICLR.cc/2020/Conference/Paper717/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper717/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x996EKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper717/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper717/Authors|ICLR.cc/2020/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167292, "tmdate": 1576860554117, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment"}}}, {"id": "SklS6sQXsH", "original": null, "number": 6, "cdate": 1573235645428, "ddate": null, "tcdate": 1573235645428, "tmdate": 1573235871520, "tddate": null, "forum": "B1x996EKPS", "replyto": "r1lEfZxCKr", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment", "content": {"title": "We thank AnonReviewer2 for the comments. We answer below to the points raised by the reviewer", "comment": "\u201cOne of the main ideas introduced in the paper is that of filters to check the legitimacy of models from model servers. While these ideas are sensible from a technical point of view, I feel the experimental section is not properly demonstrating all the robustness claims made in the paper.\u201d\n>> To experiment robustness against Byzantine workers, we did experiments with a state-of-the-art attack coined as \u201cA little is enough attack\u201d [1]. If the reviewer is talking about experiments with Byzantine servers, then we will add a few Byzantine behaviors on the servers\u2019 side, and we will report the algorithm convergence in the updated version of the paper, which we plan to have before the end of the rebuttal period.\nIt is important to note however that in the case of Byzantine failures, which by definition can be arbitrary, the standard guarantee is to provide formal proof (which we did), experiments are only for illustrative purposes.\n\n\u201cFor example, in the beginning of training with high learning rates the models will change a lot, are these filters effective in this situation as well? How are these filters working in terms of false positive/negatives in the experiments?\u201d\n>> We designed our filters to be adaptive to the learning status. Concretely, the filters adapt their behavior based on the learning rate and the gradient norm of the current scatter step. Based on that, at the beginning of learning where the learning rate is high, such filters accept a wider range of models. Later on, this window decreases with the learning convergence. Our analysis guarantees the absence of false positives, i.e., no corrupted model will pass the filter; this is the safety guarantee that our algorithm provides. Nonetheless, showing empirically false positives/negatives of our filters is an interesting aspect that we are planning to add to the paper before the deadline.\n\n\u201cHow are models corrupted during training? What's the performances of the filters with different corruption techniques (e.g. adversarial attacks)?\u201d\n>> In theory, the models could be corrupted in any means, ranging from dropping some values in the model weights to carefully crafting corrupted models to diverge the learning procedure. We will report the behavior of our algorithm under different models\u2019 corruption (i.e., Byzantine servers) in the updated version of the paper.\n\n\u201cWhat's the impact of the choice of T in the experiments?\u201d\n>> In theory, the higher the T, the better the throughput, the less the learning quality. The tradeoff between the quality of models on servers (how far they are from each other) and the throughput is very interesting to study. We will add experiments to show this in the updated version of the paper.\n\n[1] Baruch, Moran, Gilad Baruch, and Yoav Goldberg. \"A Little Is Enough: Circumventing Defenses For Distributed Learning.\" arXiv preprint arXiv:1902.06156 (2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper717/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper717/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x996EKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper717/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper717/Authors|ICLR.cc/2020/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167292, "tmdate": 1576860554117, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper717/Authors", "ICLR.cc/2020/Conference/Paper717/Reviewers", "ICLR.cc/2020/Conference/Paper717/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Comment"}}}, {"id": "r1lEfZxCKr", "original": null, "number": 1, "cdate": 1571844364400, "ddate": null, "tcdate": 1571844364400, "tmdate": 1572972561181, "tddate": null, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces an algorithm to build distributed SDG-based training algorithm that are robust to Byzantine workers and servers.\n\nI am not very familiar with this area of research, but I feel the authors did a good job providing clear explanations and introducing all the relevant concepts needed to understand the proposed algorithm. Overall, I found the paper an interesting read.\n\nThe experimental section of the paper is lacking in some aspects:\n- One of the main ideas introduced in the paper is that of filters to check the legitimacy of models from model servers. While these ideas are sensible from a technical point of view, I feel the experimental section is not properly demonstrating all the robustness claims made in the paper. For example, in the beginning of training with high learning rates the models will change a lot, are these filters effective in this situation as well? How are these filters working in terms of false positive/negatives in the experiments?\n- How are models\u00a0corrupted during training? What's the performances of the filters with different corruption techniques (e.g. adversarial attacks)?\n- What's the impact of the choice of T in the experiments?"}, "signatures": ["ICLR.cc/2020/Conference/Paper717/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574776701549, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper717/Reviewers"], "noninvitees": [], "tcdate": 1570237748112, "tmdate": 1574776701563, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Review"}}}, {"id": "BkxhT9ISqB", "original": null, "number": 3, "cdate": 1572330179853, "ddate": null, "tcdate": 1572330179853, "tmdate": 1572972561085, "tddate": null, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "invitation": "ICLR.cc/2020/Conference/Paper717/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "My review got deleted because the title kept creating an unexplained error.\nHere's another shorter attempt\n\nI haven't really followed along the literature for this. But from the results, it's not immediately clear to me what practical setup this is useful in. The authors assume perfect network synchrony, they have a 25% overhead on TensorFlow and they have a comparison to another algorithm that operates under different assumptions.\n\nWho would ever use this and why? What's the plan for getting data to the untrusted workers?"}, "signatures": ["ICLR.cc/2020/Conference/Paper717/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper717/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["elmahdi.elmhamdi@epfl.ch", "rachid.guerraoui@epfl.ch", "arsany.guirguis@epfl.ch"], "title": "Fast Machine Learning with Byzantine Workers and Servers", "authors": ["El-Mahdi El-Mhamdi", "Rachid Guerraoui", "Arsany Guirguis"], "pdf": "/pdf/89a7a256d6081d76e6b56f054112df6ed1a14de2.pdf", "TL;DR": "We present an algorithm that tolerates not only Byzantine workers but also Byzantine servers in synchronous networks with a low overhead.", "abstract": "Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\u2019 gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another state\u2013of\u2013the\u2013art Byzantine\u2013resilient ML algorithm (that assumes network asynchrony) is 70%.", "code": "https://github.com/anonconfsubmit/submit-4", "keywords": ["Distributed machine learning", "Byzantine resilience", "Fault tolerance"], "paperhash": "elmhamdi|fast_machine_learning_with_byzantine_workers_and_servers", "original_pdf": "/attachment/9c04c987b6663236783677f4d0c2435e15cfe145.pdf", "_bibtex": "@misc{\nel-mhamdi2020fast,\ntitle={Fast Machine Learning with Byzantine Workers and Servers},\nauthor={El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x996EKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x996EKPS", "replyto": "B1x996EKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper717/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574776701549, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper717/Reviewers"], "noninvitees": [], "tcdate": 1570237748112, "tmdate": 1574776701563, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper717/-/Official_Review"}}}], "count": 11}