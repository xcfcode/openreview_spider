{"notes": [{"id": "r1lEjlHKPH", "original": "Syx1qJWFDB", "number": 2505, "cdate": 1569439900118, "ddate": null, "tcdate": 1569439900118, "tmdate": 1577168267555, "tddate": null, "forum": "r1lEjlHKPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "U4wu2YQF4P", "original": null, "number": 1, "cdate": 1576798750651, "ddate": null, "tcdate": 1576798750651, "tmdate": 1576800885104, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "r1lEjlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Decision", "content": {"decision": "Reject", "comment": "Catastrophic forgetting in neural networks is a real problem, and this paper suggests a mechanism for avoiding this using a k-nearest neighbor mechanism in the final layer. The reason is that the layers below the last layer should not change significantly when very different data is introduced. \n\nWhile the idea is interesting none of the reviewers is entirely convinced about the execution and empirical tests, which had partially inconclusive. The reviewers had a number of questions, which were only partially satisfactorily answered. While some of the reviewers had less familiarity with the specific research topic, the seemingly most knowledgeable reviewer does not think the paper is ready for publication.\n\nOn balance, I think the paper cannot be accepted in its current state. The idea is interesting, but needs more work.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1lEjlHKPH", "replyto": "r1lEjlHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704794, "tmdate": 1576800252445, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Decision"}}}, {"id": "HJlvCi32KS", "original": null, "number": 2, "cdate": 1571765198550, "ddate": null, "tcdate": 1571765198550, "tmdate": 1574421238717, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "r1lEjlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper applies metric learning to reduce catastrophic forgetting on neural networks. By improving the expressiveness of the final layer, the authors claim that lower layers do not change weights as much, leading to better results in continual learning. They provide large-scale experiments on different datasets.\n\nI like the idea that the authors propose and the intuition for why it works, and the paper is well-written. However, I have some concerns and questions. My main concern is that experiments are only performed in the two-task setting, which is highly restrictive.\n\nThe authors claim that they tackle the general 'continuous task-agnostic learning' setting. However, they only test on the two-task setting. There are various problems with considering only a two-task setting (see for example Farquhar and Gal, \"Towards Robust Evaluations of Continual Learning\"). It is too easy to optimise parameters and methods to work in the two-task setting that will not generalise to more than two tasks, which the authors seem to claim. I would need to see experiments on more than two tasks. Aside from this, the experiments seem detailed, with a reasonable baseline, large-scale experiments (on ImageNet), and with an ablation study. \n\nIt seems to me like the anchors need to be chosen before training. This means that this method requires memory / storage of past data examples. It is usually fine to do store a small subset of examples in continual learning, but should be made explicit, because it may not always be possible (eg if there are data privacy laws). \n\nI do not understand the reason why the output embeddings need to be normalised (Section 3.3)? I can see from Table 4 that it improves results, but do not see any intuition.\n\nI would also like to see the computational cost of this method, perhaps as a run-time compared to the baseline. There are many hyperparameters to tune on the validation set which may slow the method down. The sensitivity analysis did not consider changing 'd' or 'M', which seem like crucial hyperparameters to me.\n\n------------\nEDIT: I will keep my score after the the discussion with authors. Although the paper has improved in my opinion, I still recommend Weak Reject. I very much appreciated the 5-task CIFAR-10 results. However, there are simple baselines in this setting that I believe need to be explored and reported. Namely, baselines but with samples, eg EWC+samples, akin to the RWalk paper that AnonReviewer1 mentions (https://arxiv.org/pdf/1801.10112.pdf). This is because the proposed method also uses samples. Going from the RWalk paper, this improves results for the baselines considerably, but this may depend on number of samples etc. I understand there was not much time during the rebuttal period to include this. I hope that the authors will consider doing so in the future.\n\nThe discussion/explanation regarding 'task-agnostic' (train and test time) and also regarding how the anchors are chosen needs to be made clearer.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lEjlHKPH", "replyto": "r1lEjlHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575845620162, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Reviewers"], "noninvitees": [], "tcdate": 1570237721893, "tmdate": 1575845620176, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Review"}}}, {"id": "HygFUMZKYS", "original": null, "number": 1, "cdate": 1571521104970, "ddate": null, "tcdate": 1571521104970, "tmdate": 1574017050581, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "r1lEjlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper considers the use of a metric learning approach in a continual/lifelong classification settings. Experiments show in the case of two tasks forgetting can be minimized by using the approach. \n\nMethods\nThe proposed method appears to be a standard triplet loss. The authors add a second term to the triplet loss that is essentially making the loss a combination of the triplet and siamese loss. It\u2019s not really explained anywhere why they do this and whether its essential to the performance. \n\nIs there anything specific to continual learning done or is the paper essentially pointing out this existing method (metric learning + nearest neighbor) is surprisingly effective for forgetting. If this is the case the authors should present it in this way I think. \n\nAlthough triplet loss can often yield reasonably performance on classification problems it tends to not perform as well as cross entropy loss, this is observed in other works as well as this one.\n\nA major question of mine: it is not clear from the method nor experiments what samples are stored after task A for the kNN classifier. Is it all of the data samples from the previous task? \n\nExperiments\nThe experimental results consider a custom continual learning setup where there is two sets of categories. Overall the experiments seem lacking at the moment in rigorous comparisons. \n\nMNIST experimental comparisons are currently suspect. It is  very surprising that LwF does so poorly, do the authors have some explanation for this. LwF is typically a reasonable baseline for these 2 task settings (e.g. https://arxiv.org/pdf/1704.01920.pdf).  Similarly the well known EWC is shown to simply not work at all for the very task it was designed for on the MNIST dataset. LwF and EWC simply not working to any degree seem to me like  rather dramatic claims to make without any explanation. \nCryptically the fine-tuning baseline described in 4.2 is not shown here for MNIST? This seems a major oversight\n\nCIFAR10/Imagenet Experiments\nIt is not clear if the baseline finetuning is done on only the top weights or the entire network. Both of these baselines should be considered. Another good baseline to consider is finetuning with cosine distance and only the top weights as in https://arxiv.org/pdf/1804.09458.pdf and other recent works should also be considered\n\nWhy do the authors not include any of the baselines from MNIST experiments here, for example LwF.\n\nAblations study the need for normalization and dynamic margin, it seems these are helpful for accuracy and forward transfer (and not as critical for minimizing forgetting).\n\n\nThe author state their method is agnostic to the task boundaries, its a bit unclear what this means in this context. The procedure is not online and the labels of the samples are being used? If the authors are referring to the need to add additional outputs to the \u201cvanilla\u201d model this seems like it can be trivially addressed by simply saying outputs are added the first time a new class is seen thereby making it agnostic to the boundary in the same sense as this method. \n\nClarity \nCan be problematic at times. Although all the elements of the approach are outlined the motivations are overly wordy and repetitive making them actually hard to follow. \n\n-(minor) first/2nd paragraph of 3.1 seems a bit redundant making it hard to follow\n\nOverall I think the idea to consider metric learning and local adaptation for continual learning is interesting, however the current work is currently lacking in both experimental evidence (appropriate comparisons) and clear motivation/difference to existing work  for its particular instantiation of this idea. \n\n++++Post Rebuttal++++\n\nThank you for your detailed responses.\n\nThe clarification about \u201ctask-agnostic\u201d for the experiments does make them look more relevant than I had previously assessed. I do want to note that the language used for this is inconsistent with the ones used in other papers, which typically calls this a \u201cshared-head\u201d setting (https://arxiv.org/pdf/1801.10112.pdf, https://arxiv.org/pdf/1805.09733.pdf, https://arxiv.org/pdf/1903.08671.pdf ). It is also somewhat inconsistent with the authors own definition of \u201ctask agnostic learning\u201d given in the introduction of this paper which implies it is something related to task boundaries at training time, in fact this is something related to availability of the task id at test time. I suggest the authors to make this more clear. Furthermore, the authors should highlight all this in the experiment text, e.g. noting EWC does poorly but this is because we use a different protocol than this and this paper etc.\n\nRegarding the experiments under this light they do look more reasonable. Indeed it has been observed that EWC works poorly in the shared-head setting https://arxiv.org/pdf/1801.10112.pdf\nRegarding the new 5 task CIFAR-10 the results are interesting, however I will point the authors to the work above (Rwalk) which also reports results in this setting better than theirs (but not by too much). \n\nI do however still have issues regarding the memory usage of the method, specifically which data needs to be stored from previous tasks. It is still not completely clear and I find obfuscated since just one sentence not even fully answering the concern about this was added to the manuscript despite myself and another reviewer asking about it. My understanding based on the (somewhat conflicted responses) of the authors is they store a substantial amount of prior task data, but most of this is only used  at test time. For example for imagenet as much as 1000 images/class are stored for testing time. This begs the question why not use this data for training as well if it is allowed to be used by the model at testing time (and therefore preserved from the first task), why is the storage cost of this data not considered and how do the authors justify this still being a lifelong learning setup. As an alternative, why can't one use a much bigger fully parametric model that uses the same amount of storage as the authors model + stored images. It seems it is not fair to compare these to methods that cant utilize this large storage amount. \n\nFinally its not clear if this data is stored as raw images or somehow stored as embeddings. If it is stored as embeddings this would require some discussion on how the authors avoid representation drift when the next task is training. If the authors store raw images, it means at evaluation time the entire raw dataset needs to be re-encoded, therefore the model can\u2019t perform easily anytime inference. \n\nUnfortunately the discussion period ended but I would have liked more clarification on this, on the other hand these pieces of information should really have been in the manuscript in the first place. \n\nOverall, my impression of the paper is improved.  But I do think it could use some further writing revisions to emphasize/clarify key points: a) the method is not new (it says e.g. in abstract \u201cnew model\u201d which is misleading) but its application in CL is under-explored b) the experiments show poor performance on existing methods because most of those are not designed nor work well for the shared head \u201ctask agnostic\u201d setting, while metric learning handles it gracefully.  c) be explicit about what is the memory being stored when moving onto the next task (this should be somewhere visible and explicit) and how this is justified\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lEjlHKPH", "replyto": "r1lEjlHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575845620162, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Reviewers"], "noninvitees": [], "tcdate": 1570237721893, "tmdate": 1575845620176, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Review"}}}, {"id": "rylvJlj2sH", "original": null, "number": 7, "cdate": 1573855198891, "ddate": null, "tcdate": 1573855198891, "tmdate": 1573861021112, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "rkxA4junjS", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment", "content": {"title": "Thank you for the reply, here is the response:", "comment": "Q1: \u201cYou state there is only one sample used per class to serve as anchor, but this is also the sample used for the nearest neighbor search at test time? During test time how many samples are being compared (is it one per class?) \u201c\nA1: Yes, those anchors are included during testing. At test time, we use a subset of the training set (e.g. for ImageNet, we used 50 samples per class with k=15, which were not used during training).\n\nQ2: \u201crelationship to this (admittedly somewhat recent) paper https://arxiv.org/abs/1905.09447 \u201d\nA2: Though both methods adopt metric learning, they have some major differences:\n\t1. The referenced paper *trains* on stored training examples from the old tasks when training on a new task, whereas our method does not.\n\t2. Our kNN classifier generates the output by finding the most common label of the k nearest training samples, whereas the other method classifies using a softmax over distance in the embedding space. \n\t3. Our experiment setting (split MNIST/CIFAR/ImageNet) is more challenging and general compared to the setting in that paper (which they call the Split CIFAR10 \u201cincremental class\u201d setting), where the every subsequent task includes all classes that are in all previous tasks. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEjlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2505/Authors|ICLR.cc/2020/Conference/Paper2505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140357, "tmdate": 1576860540235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment"}}}, {"id": "rkxA4junjS", "original": null, "number": 6, "cdate": 1573845814227, "ddate": null, "tcdate": 1573845814227, "tmdate": 1573845814227, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "r1eKEaEnoH", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment", "content": {"title": "A few quick questions", "comment": "Thanks for your response I will definitely take it into account and update my review later. I just had a few quick questions:\n\n-You state there is only one sample used per class to serve as anchor, but this is also the sample used for the nearest neighbor search at test time? During test time how many samples are being compared (is it one per class?) it seems like the seleciton of these samples would be kind of important. \n\n-I was also wondering if you have some thoughts on the relationship to this (admittedly somewhat recent) paper https://arxiv.org/abs/1905.09447 "}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEjlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2505/Authors|ICLR.cc/2020/Conference/Paper2505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140357, "tmdate": 1576860540235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment"}}}, {"id": "r1eKEaEnoH", "original": null, "number": 1, "cdate": 1573829937063, "ddate": null, "tcdate": 1573829937063, "tmdate": 1573832184464, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "HygFUMZKYS", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment", "content": {"title": "Response to Review 1", "comment": "Thanks for your review. Below is our response:\n\nQ1: \u201cThe authors add a second term to the triplet loss that is essentially making the loss a combination of the triplet and siamese loss. It\u2019s not really explained anywhere why they do this\u201d\nA1: This is in fact explained in the paper. As mentioned at the bottom of page 4, \u201cIn practice, we found that training using L_{triplet} results in overlap between clusters for different classes, as shown in Figure 2. To discourage this, we add another term to the loss function to encourage tight clusters\u201d.\n\nQ2: \u201cIs the paper essentially pointing out this existing method (metric learning + nearest neighbor) is surprisingly effective for forgetting. If this is the case the authors should present it in this way I think.\u201d\nA2: As we stated at the end of the introduction section, \u201cWe will show\nthat this simple modification is surprisingly effective at reducing catastrophic forgetting\u201d, which is already in line with this suggestion. \n\nThe simplicity of our method is an important advantage compared to other methods that add specialized regularizers, because (1) a simpler method is more broadly applicable because it can be used in settings when the signals required by the regularizers (e.g.: task identity or boundary - more on this later) are unavailable, (2) and is more flexible because it can be combined with other approaches. \n\nThe insight that metric learning can be used effectively for continual learning is novel and did not appear in prior work to our knowledge - this represents a new perspective for continual learning research that catastrophic forgetting may be partly caused by limitations in the model itself rather than problems with the training objective. \n\nQ3: \u201cAlthough triplet loss can often yield reasonably performance on classification problems it tends to not perform as well as cross entropy loss, this is observed in other works as well as this one.\u201d\nA3: While this may be true if the goal is to maximize single-task performance, the point of this paper is to demonstrate that metric learning is quite effective if the goal is to minimize catastrophic forgetting. Future improvements to metric learning techniques could help narrow the gap between triplet loss and cross-entropy loss on single tasks, but are orthogonal to our method. \n\nQ4: \u201cA major question of mine: it is not clear from the method nor experiments what samples are stored after task A for the kNN classifier. Is it all of the data samples from the previous task?\u201d\nA4: We only store one example from each class (to serve as anchors). \n\nQ5: \u201cMNIST experimental comparisons are currently suspect. It is  very surprising that LwF does so poorly, do the authors have some explanation for this. LwF is typically a reasonable baseline for these 2 task settings (e.g. https://arxiv.org/pdf/1704.01920.pdf).\u201d\nA5: LwF in the paper the reviewer referenced is evaluated under a different setting than the setting considered in our paper. In the referenced paper, the method is assumed to know which dataset (a.k.a. task) each test example belongs to (i.e.: the task learning setting). As a result, it only needs to discriminate among the classes within that dataset. In our paper, the method is not assumed to know which dataset each test example belongs to (i.e.: the task-agnostic learning setting), and so is required to discriminate among the classes across all datasets. As shown in various prior papers (e.g.: https://arxiv.org/pdf/1810.12488.pdf and https://arxiv.org/pdf/1904.07734.pdf), LwF only achieves an average accuracy in the 20% range under the evaluation setting we consider, whereas it achieves an a near-perfect accuracy in the easier evaluation setting (i.e.: the task learning setting) considered in the original LwF paper.  \n\n(continued below...)"}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEjlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2505/Authors|ICLR.cc/2020/Conference/Paper2505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140357, "tmdate": 1576860540235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment"}}}, {"id": "B1lOLa42jr", "original": null, "number": 2, "cdate": 1573829968324, "ddate": null, "tcdate": 1573829968324, "tmdate": 1573831835290, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "r1eKEaEnoH", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment", "content": {"title": "Response to Review 1 (Continued)", "comment": "\nQ6: \u201cSimilarly the well known EWC is shown to simply not work at all for the very task it was designed for on the MNIST dataset. LwF and EWC simply not working to any degree seem to me like  rather dramatic claims to make without any explanation.\u201d\nA6: EWC was again evaluated under a different setting than the setting considered in our paper. In the EWC paper, all the different datasets (a.k.a. tasks) are assumed to share the same classes (i.e.: the domain learning setting), and so the method only needs to discriminate among these classes. On the other hand, in our paper, the classes in each dataset are disjoint (i.e.: the task-agnostic learning setting), and so the method needs to discriminate among all the different classes across datasets. Moreover, EWC was evaluated on *permuted* MNIST in the original paper, whereas it was evaluated on *split* MNIST in our paper. In permuted MNIST, each task is a different random permutation of the pixels of images in MNIST, and all ten classes are represented in each task. In split MNIST, each task is a different subset of MNIST classes. The latter is more challenging because the method does not have access to all ten classes at any given time. See appendix B of https://arxiv.org/pdf/1805.09733.pdf for an explanation of why a method that works well on permuted MNIST may fail on split MNIST. As shown in prior literature (e.g.: https://arxiv.org/pdf/1904.07734.pdf), EWC achieves 94% average accuracy in the domain learning setting (which is consistent with the results in the original EWC paper), but only achieves 64% average accuracy when the dataset is changed to split MNIST. In the harder setting of class learning (which is still easier than the task-agnostic learning setting that we consider, but shares the same evaluation protocol as our setting), the average accuracy of EWC drops to 20% (this suggests 100% accuracy on the most recent task and close to 0% accuracy on four earlier tasks, which is consistent with the results reported in our paper). \n\nQ7: \u201cIt is not clear if the baseline finetuning is done on only the top weights or the entire network. \u201d\nA7: The baseline finetuning was only performed on the top weights (all the convolutional layers are fixed). Finetuning on the entire network leads to even more forgetting. \n\nQ8: \u201cAnother good baseline to consider is finetuning with cosine distance and only the top weights\u201d\nA8: As mentioned in Sect. 3.3, normalizing the output embedding vectors and minimizing the Euclidean distance (both of which we do already) is equivalent to maximizing cosine similarity. This is because || u - v ||^2 = <u - v, u - v> = ||u||^2 - 2 <u, v> + ||v||^2 = 2 - 2 <u, v>, and so cosine similarity is a monotonic transformation of Euclidean distance. \n\nQ9: \u201cThe author state their method is agnostic to the task boundaries, its a bit unclear what this means in this context.\u201d\nA9: Regularization-based methods like EWC and SI need to estimate how important each parameter is to the previous tasks, which will be used to penalize changes to parameters when training on future tasks. If there weren\u2019t task boundaries, i.e.: different tasks were interlaced, then it is unclear when this estimation should be done. Similarly, methods like LwF need to generate pseudo-labels on the data from the new task from the model trained on previous tasks. If different tasks were interlaced, it is unclear when the pseudo-labels should be generated. This is why methods like EWC, SI and LwF are not applicable to the setting without task boundaries. \n\nIn our case, the proposed method doesn\u2019t need to do anything when one task ends and another one begins, and so the method can be naturally used in the setting without task boundaries. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEjlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2505/Authors|ICLR.cc/2020/Conference/Paper2505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140357, "tmdate": 1576860540235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment"}}}, {"id": "B1x-F64hsS", "original": null, "number": 4, "cdate": 1573830009275, "ddate": null, "tcdate": 1573830009275, "tmdate": 1573830845591, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "HkeBUpcTKr", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment", "content": {"title": "Response to Review 2", "comment": "Thanks for your review. Below is our response:\n\nQ1: Interpretation of results in Table 1\nA1: Our method achieves the second highest absolute performance and also the second smallest drop in performance on Set A after seeing Set B. The only method that achieved better performance on Set A after seeing Set B is LwF, which is because its performance on Set B is very poor. This indicates that our method achieves a good balance in learning well on new data and retaining performance on old data. It is also a lot less complex than methods that can achieve reasonable retention performance (DGR, DGR + distillation and RtF) - these methods require training a generative model (a VAE) to produce pseudo-training examples, whereas our method does not require training an external model. This also makes our method more broadly applicable, since it is more difficult to train high-performing generative models on some datasets, e.g.: CIFAR-10 (more on this below). \n\nQ2: Additional SGD + dropout baseline\nA3: We have added this baseline (with a dropout probability of 0.5, which is standard in literature and recommended by https://arxiv.org/abs/1312.6211) in Table 1. As shown, our method outperforms the baseline in terms of retention ability by a large margin. \n\nQ3: Other baselines for CIFAR-10\nA3: We have added results of other baselines on CIFAR-10 in Table 3. As shown, our method achieves the highest absolute performance on Set A after seeing Set B and also the smallest drop in performance on Set A after seeing Set B. Interestingly, on this more complex dataset, none of the baselines (except for LwF) are able to retain significant amounts of knowledge after training on Set B. As discussed above, methods that rely on generative models (DGR, DGR + distillation and RtF) no longer perform well because training high-performing generative models on CIFAR-10 is more difficult due to the increased complexity of the data. \n\nQ4: Extension to the RL setting with continuous action space\nR4: We plan to explore this in future work and consider replacing the k-nearest neighbour classifier with k-nearest neighbour regression. This requires changing the triplet loss to encourage samples within the neighbourhood that have similar outputs as the ground truth to be moved closer, and samples within the neighbourhood that have dissimilar outputs as the ground truth to be moved farther. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEjlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2505/Authors|ICLR.cc/2020/Conference/Paper2505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140357, "tmdate": 1576860540235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment"}}}, {"id": "BkglOaNhsH", "original": null, "number": 3, "cdate": 1573829991890, "ddate": null, "tcdate": 1573829991890, "tmdate": 1573830106207, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "HJlvCi32KS", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment", "content": {"title": "Response to Review 3", "comment": "Thanks for your review. Below is our response:\n\nQ1: Experiments on more than two tasks\nA1: We added new results on a five-task CIFAR-10 dataset (where each task is a consecutive pair of classes), along with results using the baselines, which are presented in Table 2. As shown, our method outperforms all baselines. \n\nQ2: Anchors and requirements for storage\nA2: We only used one anchor per class, so only a minimal number of past data examples need to be stored (we\u2019ve made this clearer in the manuscript). The anchor for a class can be chosen the first time an example from that class is encountered. \n\nQ3: Why normalization helps intuitively\nA3: By normalizing vectors, we essentially project them onto a unit sphere. The benefit of this is that the sphere is a closed surface, unlike the original Euclidean space the vectors lie in. In Euclidean space, all points can easily be pushed very far away from each other, or brought very close together - neither of these scenarios help with classifying points correctly. On the other hand, pushing points away from a point on a unit sphere must make them closer to a point on the opposite side of the sphere - this property of the sphere helps us avoid either of the two scenarios above. \n\nQ4: Computational cost of the method\nA4: Our method ~2 minutes on MNIST and ~12 minutes on CIFAR-10, which are comparable to the runtimes of the baselines. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lEjlHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2505/Authors|ICLR.cc/2020/Conference/Paper2505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140357, "tmdate": 1576860540235, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Authors", "ICLR.cc/2020/Conference/Paper2505/Reviewers", "ICLR.cc/2020/Conference/Paper2505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Comment"}}}, {"id": "HkeBUpcTKr", "original": null, "number": 3, "cdate": 1571822925158, "ddate": null, "tcdate": 1571822925158, "tmdate": 1572972329736, "tddate": null, "forum": "r1lEjlHKPH", "replyto": "r1lEjlHKPH", "invitation": "ICLR.cc/2020/Conference/Paper2505/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a possible way to mitigate catastrophic forgetting by using a k-nearest neighbor (kNN) classifier as the last layer of a neural network as opposed to a SoftMax classifier.  I think this an interesting and possibly novel use of a kNN layer (I haven't seen similar uses although I'm not that familiar with the specific research area).  At the same time it's not presenting a ground breaking new algorithm or anything like that.\n\nOverall the paper is fairly well written and not too hard to follow.  I would say overall results in Table 1 are positive although the authors' approach has the lowest performance after just training on set A if that initial accuracy is important, and also doesn't have quite as high of an accuracy on test B compared to most of the other baselines.  Additionally, if you add the accuracy on both set A and set B after training on set B the sum is slightly higher for Rtf.  If you look at the minimum accuracy between set A and set B after training on set B, however, the authors' method has the highest value which might be what someone is looking to maximize.  \n\nOne weakness of this is paper is that I think there are other baselines that should be compared against in Table 1 such as something as basic as SGD with dropout (some of the baselines that are compared against in Table 1 were compared against SGD with dropout in their citations).  There are a number of additional approaches outlined in https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf.  Also maybe even something with self attention such as Serra at al. https://arxiv.org/pdf/1801.01423.pdf.\n\nAnother potential issue I have with this paper is that it only reports results for the authors' method and the vanilla baseline for more complex CIFAR-10 and ImageNet data sets in Table 2.  Assuming there aren't restrictive assumptions for some of the methods that prevent them from being run on the other data sets (at least SI was previously evaluated on CIFAR-10), I would like to see how other baselines perform on these more complex datasets too.\n\nThe lack of some more baselines such as SGD with dropout, and not reporting the performance of the same baselines from Table 1 in Table 2, cause me to be very borderline on this paper.  I do appreciate the sensitivity analysis and ablation study provided.\n\nAs alluded to in future work I'm curious how the authors' approach might be applied to reinforcement learning, and if there could be a way to deal with continuous action spaces in RL."}, "signatures": ["ICLR.cc/2020/Conference/Paper2505/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2505/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "kailasv@berkeley.edu", "malik@eecs.berkeley.edu"], "title": "Better Knowledge Retention through Metric Learning", "authors": ["Ke Li*", "Shichong Peng*", "Kailas Vodrahalli*", "Jitendra Malik"], "pdf": "/pdf/3d6c872dd68d2aba9b46a4bd6feeaba31e30091a.pdf", "TL;DR": "We show metric learning can help reduce catastrophic forgetting", "abstract": "In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "keywords": ["metric learning", "continual learning", "catastrophic forgetting"], "paperhash": "li|better_knowledge_retention_through_metric_learning", "original_pdf": "/attachment/1b431c9878c78788216e38f3b80b0b90bac4e6f5.pdf", "_bibtex": "@misc{\nli*2020better,\ntitle={Better Knowledge Retention through Metric Learning},\nauthor={Ke Li* and Shichong Peng* and Kailas Vodrahalli* and Jitendra Malik},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lEjlHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lEjlHKPH", "replyto": "r1lEjlHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575845620162, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2505/Reviewers"], "noninvitees": [], "tcdate": 1570237721893, "tmdate": 1575845620176, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2505/-/Official_Review"}}}], "count": 11}