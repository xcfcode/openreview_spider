{"notes": [{"id": "Hye1kTVFDS", "original": "HyeuOKzSwB", "number": 285, "cdate": 1569438934535, "ddate": null, "tcdate": 1569438934535, "tmdate": 1583912027789, "tddate": null, "forum": "Hye1kTVFDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "T6aT6-i3bd", "original": null, "number": 17, "cdate": 1579619659645, "ddate": null, "tcdate": 1579619659645, "tmdate": 1579619659645, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "kjZ93onALg", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Thanks.", "comment": "Hello Rakesh, \n\nThanks for taking time and reading our work.\n\nyou are right regarding missing brackets. Regarding the sign, there's \\beta in the objective, \ndepending upon which it can either be positive or negative. "}, "signatures": ["ICLR.cc/2020/Conference/Paper285/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "kjZ93onALg", "original": null, "number": 1, "cdate": 1579613090104, "ddate": null, "tcdate": 1579613090104, "tmdate": 1579613090104, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "Hye1kTVFDS", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Public_Comment", "content": {"title": "Clarification on derivation", "comment": "Nice work!\n\nHowever, while going through the derivation of equation 3 (in appendix section \"EASY OPTIMIZATION OF KL OBJECTIVE\", not \"Tractable Optimization of the KL Objective\" as mentioned in the main paper [typo]), I found that a few parantheses were missing. \n\nBy deriving the expression by hand, I got a different expression (with the same terms though). I was wondering if the author(s) could provide some clarification.\n\nEquation (3) from paper (calling b = p(f(s,g))):\n\n- d_cap * E_z[ log(d_cap) ]  + (1-d_cap) * [ log(b) ]  - log( d_cap*b ) + (1-d_cap)\n\nHowever, what I got was (applying eq 9 to eq 10):\n\nd_cap * E_z[ log(d_cap) ]  - (1-d_cap) * [ log(b) - log( d_cap*b + (1-d_cap) ) ]\n                                                                                     -------------------a--------------\n                                                                    ----------------------b--------------------------\n----------------------------------------------c------------------------------------------------------\n\n\nIn particular note:\n(a) the last two terms come under the same logarithm.\n(b) the three terms combined are multiplied by (1-d_cap).\n(c) The sign is flipped.\n\n\nThanks in advance!"}, "signatures": ["~Rakesh_R_Menon2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rakesh_R_Menon2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504211263, "tmdate": 1576860578835, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Public_Comment"}}}, {"id": "IEvzkBmrU", "original": null, "number": 1, "cdate": 1576798692262, "ddate": null, "tcdate": 1576798692262, "tmdate": 1576800943073, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "Hye1kTVFDS", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Existing implementation of information bottleneck need access to privileged information which goes against the idea of compression. The authors propose variational bandwidth bottleneck which estimates the value of the privileged information and then stochastically decided whether to access this information or not. They provide a suitable approximation and show that their  method improves generalisation in RL while reducing access to expensive information.\n\nThese paper received only two reviews. However, both the reviews were favourable. During discussions with the AC the reviewers acknowledged that most of their concerns were addressed. R2 is still concerned that VBB does not result in improvement in terms of sample efficiency. I request the authors to adequately address this in the final version. Having said that, the paper does make other interesting contributions, hence I recommend that this paper should be accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hye1kTVFDS", "replyto": "Hye1kTVFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729926, "tmdate": 1576800282615, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper285/-/Decision"}}}, {"id": "HklauJG3jr", "original": null, "number": 12, "cdate": 1573818229029, "ddate": null, "tcdate": 1573818229029, "tmdate": 1573819108100, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "B1xJyr2osB", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Thanks again for responding to the comments!", "comment": ">We did not observe any change in sample efficiency b/w InfoBot as well as the proposed method.\nWe note that in the current work goal of the proposed method is not to obtain better sample efficiency, but to generalize better with respect to the privileged input. \n\nThanks for responding to this. The observation (assuming \"any change in sample efficiency b/w InfoBot\" indicates no significant difference) reported in the first sentence highlights that VBB isn't worse compared to InfoBot in terms of sample-efficiency. This, supported by improvements in success-rate seem to indicate a somewhat positive result -- VBB takes as much time to attain better performance relative to InfoBot but accesses privileged information sparsely. Regardless, while it's understandable that the goal of the proposed approach may not have been centered towards sample efficiency, I would like to point out that one seemingly reasonable expectation is that learning when to access privileged information in a minimal manner would lead to increased sample efficiency -- learning when and where to access privileged information would avoid wasteful computation and therefore, may impact sample-efficiency. \n\nIn light of the responses, I have updated my score to reflect the increased rating."}, "signatures": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "SJe_pvITtH", "original": null, "number": 1, "cdate": 1571805120163, "ddate": null, "tcdate": 1571805120163, "tmdate": 1573817444903, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "Hye1kTVFDS", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "Summary - The paper proposes an approach, called the Variational Bandwidth Bottleneck (VBB), capable of compressing only a part of the input and still learn representations that are informative of the output. The approach is motivated from the following perspective -- there might be situations where a ``part\u2019\u2019 of the input is privileged in the sense that it may be costly / wasteful to maintain access to all the time, thereby rendering the standard IB pipeline infeasible (as it requires unrestricted access to the entire input). By breaking down the input into standard (always available) and privileged (not always available) components, the paper proposes a module that decides whether to access the privileged input during compression based on the standard input. The goal is to be able to decide when to access privileged information and not how to break the overall input into standard and privileged components. The approach tackles a narrow subset of problems compared to the standard information bottleneck. The authors show the applicability of the proposed approach in reinforcement learning setups -- specifically, (1) when to access an expensive model-based planner for goal-driven navigation; (2) when to access goal-information in goal-driven navigation and (3) treating communication in a multi-agent cooperative setting as ``privileged\u2019\u2019 information. The experimental results demonstrate that VBB accesses privileged information in a feasible and minimal manner and results in better generalization performance.\n\nStrengths\n\n- Apart from the flaws (highlighted in weaknesses), the paper is well-written and generally easy to follow.\n\n- The problem statement is well-motivated. The authors did a good job of first identifying when the standard IB approach would be infeasible / costly -- even though representations are compressed and relevant, computing them still requires unrestricted access to input -- and then motivating the need for selective access to information while decision-making. The problem is well-grounded in the experimental settings the authors provide results for.\n\n- Apart from the concerns (highlighted in weaknesses), the experimental results generally support the claims of the paper. Results in Sec. 7.1 (loosely) demonstrate that VBB accesses privileged information states where degree of freedom in terms of possible trajectories is higher -- this result, although not explored in it\u2019s entirety, also correlates with notions of decision-states (as pointed out by authors) and bottleneck states in the existing literature. Results in Sec. 7.2 demonstrate improved generalization performance in terms of transfer. Additionally, results in Table.3 suggest that VBB accesses privileged information the least number of times. Similarly, results in Sec. 7.3 indicate VBB results in improvements in performance in the multi-agent setting while resulting in minimal communication among the agents. The baselines being compared with in the paper are also reasonable.\n\n- The problem statement and the proposed approach have some degree of novelty. Most works along the lines of restricting access to relevant information still assume unrestricted access to the `entire\u2019 information. \n\nWeaknesses\n\nThat being said, the paper does have some flaws / clarity issues that make several associated details confusing when judged in context of prior work. These concerns (in addition to the strengths highlighted above) mostly surrounding experiments form the basis of my rating and addressing these would definitely make the paper stronger.\n\n- One of the underlying motivations / intuitions behind restricting access to the privileged information is that fact that -- \u201c..avoid accessing the privileged input because we want to generalize with respect to it..\u201d. This statement in isolation is misleading and makes things unclear. From a complete generalization standpoint, preventing the encoder from overfitting to the input (\u2018privileged\u2019 or \u2018standard\u2019) can be addressed by just tracking the relevance (predictive performance) based on the representations on a held-out set. Unless I am missing something, generalization of the learned representations in standard IB becomes major problem only when we restrict (costly or otherwise) access to the privileged input. The statement seems to suggest that learning representations completely and generally agnostic to the privileged input is a good idea. However, this is only true when at test-time, the privileged input may be significantly different than what was seen during training time -- a different model-based planner / entirely different goal-specifications, etc. Could the authors comment more on this and reframe the intuition wherever applicable?\n\n- Last 3 lines of the 1st paragraph in section 3 (page 2) are incomplete -- \u201c...constraining the channel capacity .. is permitted to differ from the prior r(Z)....\u201d. The difference in the posterior p(Z|X) and the prior r(Z) quantify the channel capacity only under expectation over the distribution over inputs -- p(X). Therefore, in practice, it is also governed by the empirical distribution of the data (see page 4 of https://arxiv.org/pdf/1612.00410.pdf). The authors should change the lines to reflect the same.\n\n- One of the key contributions of the paper is to develop a mechanism where one isn\u2019t required to access the privileged input all the time -- depending on the standard input, one can decide whether to access the privileged information. This manifests in form of a mixture-distribution over a dirac-delta transformation of the deterministic encoder and a prior distribution. At inference-time, the channel capacity (d_cap, based on the standard input) can be used to decide whether to access the privileged information or not. There\u2019s lack of clarity in terms of what happens at training time -- is it the case that (1) d_cap is computed, f(s, g) is also computed and based on the KL-term in Eq (3), B(S) is incentivized to generate d_cap that results in less frequent access the privileged information (d_cap < 0.5 for the most part) or (2) d_cap is computed, a sample is drawn from the bernoulli and z is sampled from either r(Z) or \\del_(f(S, G) and KL is either 0 (if z ~ r(Z)) or a finite value (if z ~  \\del_(f(S, G))? (1) involves an exact computation and always requires access to G (privileged input) during training whereas (2) is approximate and does not always require access to G. It\u2019s unclear what the pipeline is from the paper. Can the authors clarify this? Are there specific reasons why (1) / (2) was chosen?\n\n- Experimental Results: Highlighting these below:\n     - In the experimental setting of Figure 2, do the authors notice any (1) qualitative differences in terms of where the agent accesses the output of the planner when InfoBot is used (adapted to this setting) and (2) quantitative differences (Table. 2) in terms of how often does InfoBot and the other proposed baselines access the planner output at junctions and hallways? Junctions in 2D mazes can potentially be identified as important decision states using simpler approaches. As a demonstrative experiment, the key point to be made using this experiment seems to be how often does VBB access the planner output compared to InfoBot and other baselines. \n\n    - In Sec. 7.2, it is unclear how VBB is being used to generalize to novel environments -- as in, is the pipeline same as InfoBot, where a frozen encoder is used to provide an exploration incentive? \n\n    - While the generalization results in Table. 2 are impressive in terms of success values, I think it lacks a few numbers (assuming the transfer pipeline to novel environments is same as InfoBot) -- given that the environments being tested on are different from InfoBot, how well do count-based exploration and goal-conditioned A2C baselines perform? This is to understand whether InfoBot is the right thing to compare with in these environments. \n\n    - Furthermore, for the goal-driven navigation set of experiments can the authors report comparisons in terms of sample-efficiency as well -- how do success-rates and average task-returns vary with time-steps of training? \n\n    - For results in Sec. 7.3, Table. 4, I would encourage the authors to compare with IC3Net (https://arxiv.org/pdf/1812.09755.pdf) which learns \u201cwhen to communicate\u201d in a multi-agent setting irrespective of whether it\u2019s a cooperative situation or not.\n\nReasons for rating\n\nApart from the points mentioned above, I don\u2019t have major weaknesses to point out. The paper is generally easy to follow and the proposed approach and problem statement is well-grounded and somewhat novel. However, the paper suffers from lack of experimental details and comparisons (and other weaknesses highlighted above) and therefore I am inclined towards my current rating. Addressing those would significantly benefit the paper and help me in  increasing my score.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye1kTVFDS", "replyto": "Hye1kTVFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576076940052, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper285/Reviewers"], "noninvitees": [], "tcdate": 1570237754351, "tmdate": 1576076940063, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Review"}}}, {"id": "B1xJyr2osB", "original": null, "number": 11, "cdate": 1573795031275, "ddate": null, "tcdate": 1573795031275, "tmdate": 1573795927710, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "SyxBif3ooH", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "comparisons w.r.t. sample-efficiency of VBB versus InfoBot + revise score ? ", "comment": "We appreciate the response by the reviewer. \n\n\"comparisons w.r.t. sample-efficiency of VBB versus InfoBot (and other approaches). While success and frequency of accessing privileged information are interesting metrics to look at, comparisons w.r.t. sample-efficiency would reflect if restricted access to privileged information via VBB results in faster training -- by very carefully accessing privileged information only at specific states.\"\n\nWe did not observe any change in sample efficiency b/w InfoBot as well as the proposed method.\nWe note that in the current work goal of the proposed method is not to obtain better sample efficiency, but to generalize better with respect to the privileged input.  Future work would investigate how to combine efficiently the proposed information regularizer with other information theoretic exploration methods which can also improve sample efficiency. \n\nAnother interesting thing which we observed was, that in our navigation experiments we measured percentage of time steps on which (Infobot as well as the proposed method) access the goal information when the agent is near the junction point (or branching points) in the maze. We observe that the\nproposed method learns to access the privileged input (in this case, the goal) only when necessary, as compared to Infobot baseline. This even holds when we evaluate on different task distribution as compared to what it was trained on. (For more details, ref. Tab 3 in the main paper). This further shows, that the encoder in \"Infobot\" can be improved by conditionally computing whether the privileged information should be accessed or not (as in the proposed method).\n\n\n\"Given the responses to my comments, I am generally supportive of the paper.\"\n\nWe appreciate that the reviewer supports the paper. Would reviewer like to revise their score ? \n\nThanks a lot for your time. :)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper285/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "SyxBif3ooH", "original": null, "number": 10, "cdate": 1573794461428, "ddate": null, "tcdate": 1573794461428, "tmdate": 1573794461428, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "SJe_pvITtH", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Thanks for responding to the comments!", "comment": "Thanks to the authors for providing detailed comments and justifications wherever applicable and apologies for the late reply. I'll discuss / respond to the comments of the authors below.\n\n> The VBB is indeed meant to provide robustness to distributional shift. While this can happen, as the reviewer says, in cases where there \"a different model-based planner / entirely different goal-specifications,\" this kind of distributional shift more commonly occurs when we study generalization to related but distinct problem instances. For example, in the settings considered in our experiments, we train on procedurally generated mazes with different sizes (like different number of rooms, and size of different rooms) during training and testing.\n\nThanks for responding to this. Just to clarify, my point here was that the degree to which a model / algorithm should be agnostic to the privileged input depends on the problem setting (as rightly pointed out by the authors). My intention behind the comment was to encourage the authors to soften the statement / claims around the robustness aspect of the motivation since beyond \"the cost of accessing privileged input\", robustness w.r.t. privileged input doesn't seem to be the problem in the experimental settings presented in the paper.\n\nThanks for clarifying the training pipeline and making it clear that access to privileged information is still always required during training (but not during testing). I think out of the two options that I discussed in the comment, (1) seems the more reasonable thing to try first. My comment was motivated by trying to draw parallels with sampling procedures in mixture models.\n\nThanks for making the transfer pipeline clear. \n\nThanks for presenting the comparisons to the count-based and goal-conditioned A2C baseline in terms of success values.\n\nThanks for including the comparison with IC3Net. Comparisons across a range over the number of agents is especially interesting. The fact that VBB scales better relative to IC3Net with number of agents does indeed demonstrate the utility of VBB.\n\nThe response provided by the authors cover most of my points, except for one -- comparisons w.r.t. sample-efficiency of VBB versus InfoBot (and other approaches). While success and frequency of accessing privileged information are interesting metrics to look at, comparisons w.r.t. sample-efficiency would reflect if restricted access to privileged information via VBB results in faster training -- by very carefully accessing privileged information only at specific states.\n\nGiven the responses to my comments, I am generally supportive of the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "SklhHucooB", "original": null, "number": 9, "cdate": 1573787715697, "ddate": null, "tcdate": 1573787715697, "tmdate": 1573787715697, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "SJe_pvITtH", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Feedback ? ", "comment": "Dear reviewer #2, \n\nWe\u2019d like to thank you again for your review and feedback! We have provided response to your questions. In particular, we clarified connections to InfoBot, did comparisons to Count Based exploration, some more experiments with I3C paper.\n\nWould you have any other questions regarding the rebuttal?  \n\nSince the review discussion period is going to end, we would appreciate any feedback that you might have. We would be very happy to provide more clarifications, if any. \n\nMany thanks again for your review, feedback and time. We appreciate it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper285/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "Hyln8Yk5sr", "original": null, "number": 6, "cdate": 1573677395672, "ddate": null, "tcdate": 1573677395672, "tmdate": 1573677395672, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "r1gG7rR-ir", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Rebuttal feedback", "comment": "Thank you for the response. Given my limited expertise on the topic, I am unlikely to increase my Rating. However, I am generally supportive of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "HylyE_2FiH", "original": null, "number": 5, "cdate": 1573664806590, "ddate": null, "tcdate": 1573664806590, "tmdate": 1573664899097, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "SJe_pvITtH", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Updated Impression ?", "comment": "R2, We believe, we have addressed your concerns and clarified some of your points. \n\nWe hope to have changed your assessment of our work for the better; should that not be the case, please do not hesitate to get in touch with us.\n\nThanks for your time. "}, "signatures": ["ICLR.cc/2020/Conference/Paper285/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "HkloHmR-sB", "original": null, "number": 3, "cdate": 1573147459046, "ddate": null, "tcdate": 1573147459046, "tmdate": 1573148004443, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "H1xDZmAWsH", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Comparison with IC3net (3/3)", "comment": "\u201cthe authors to compare with IC3Net\u201d\n\nWe thank the reviewer for the reference. As asked by reviewer, we ran the IC3Net baseline for our task by varying different number of agents. IC3Net performs better for 6 agents, but VBB scales better (and outperforms IC3Net baseline) when there are more number of agents (like 10 or 15). We also study how many times an agent access the privileged input (in brackets). For all the scenarios, (6 agents, 10 agents and 15 agents), proposed method (VBB) access privileged input less number of times as compared to IC3Net. We think the reason VBB scales better (in this scenario is because VBB is fully differentiable, but in IC3Net requires a non-differentiable decision. Future work would be how to improve IC3Net using ideas from VBB. (Less is better).\n\nMethod              6 agents                              10 agents                          15 agents\nVBB (ours)        4.72 (23%)                            5.22 (34%)                            5.76 (38%)                   \nIC3net               4.68 (34%)                            5.28 (42%)                           5.91 (45%) \n\nThanks again for your time in reviewing our submission. We really appreciate it. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper285/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "r1gG7rR-ir", "original": null, "number": 4, "cdate": 1573147930129, "ddate": null, "tcdate": 1573147930129, "tmdate": 1573147980041, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "H1lnUmn7cH", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Response to Reviewer", "comment": "We thank the reviewer for their feedback and their generally positive assessment of our work.\n\n\u201cI wonder how it connects to active learning or more specifically \"active feature acquisition\"\n\nWe thank the reviewer for pointing this out. We agree that their are some intriguing connections to \u201cactive feature acquisition\u201d. More generally, the  efficient use of limited computational resources is probably an important ingredient.  There\u2019s a whole field of rational meta-reasoning which decide which computations to perform but this is computationally intractable. VBB provides a tractable way to maintain a trade-off b/w \u201ccost\u201d of performing a computation, and \u201cvalue\u201d of computation, in the sense perform a computation if value of computation exceeds that of the cost of computation. Future work would be to rigorously study this in active feature acquisition scenario.\n\n\n\u201cTypos\u201d\n\nWe thank the reviewer for pointing out these corrections. We would update it in the next version of the paper. \n\n\u201cReferences\u201d\n\nWe thank the reviewer for those references. We\u2019ll look into them. \n\nAre there any other experiments or clarifications that we can provide such that would help to improve the understanding of our work and hence making it more likely for you to raise your score?\n\nThanks again for your time in reviewing our submission. We really appreciate it. "}, "signatures": ["ICLR.cc/2020/Conference/Paper285/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "BJxTAM0bsS", "original": null, "number": 1, "cdate": 1573147348744, "ddate": null, "tcdate": 1573147348744, "tmdate": 1573147504095, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "SJe_pvITtH", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "Response to Reviewer (1/3)", "comment": "\nWe thank the reviewer for their feedback and their generally positive assessment of our work. We believe this would further improve the readability of our work. Thanks!!\n\n\nWe have ran additional experiments requested by the reviewer. To summarize. \n(a) We ran the comparison to I3C as requested by reviewer.\n(b) We have also compared to count based exploration as requested by reviewer. \n\n\u201c..avoid accessing the privileged input because we want to generalize with respect to it..\u201d\n\nThe VBB is indeed meant to provide robustness to distributional shift. While this can happen, as the reviewer says, in cases where there \"a different model-based planner / entirely different goal-specifications,\" this kind of distributional shift more commonly occurs when we study generalization to related but distinct problem instances. For example, in the settings considered in our experiments, we train on procedurally generated mazes with different sizes (like different number of rooms, and size of different rooms) during training and testing.\n\n\u201cthe authors should change the lines to reflect the same.\u201d\n\nThanks for pointing this out. Its indeed an omission from our side. \n\n\u201cIt\u2019s unclear what the pipeline is from the paper. Can the authors clarify this? \u201d\n\nWe follow the first pipeline. \n(1) d_cap is computed.\n(2)  f(s, g) is also computed and based on the KL-term in Eq (3).\n(3) B(S) is incentivized to generate d_cap that results in less frequent access the privileged information.\n(4) During test time, we sample from the bernouilli treating d_prob as the probability. Main motivation was, it resonates well with our result in Proposition 1, and during training everything is still differentiable ( though we do need to access privileged information during training)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper285/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "H1xDZmAWsH", "original": null, "number": 2, "cdate": 1573147391443, "ddate": null, "tcdate": 1573147391443, "tmdate": 1573147413752, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "BJxTAM0bsS", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment", "content": {"title": "VBB and InfoBot (2/3)", "comment": "\u201cas in, is the pipeline same as InfoBot, where a frozen encoder is used to provide an exploration incentive? it is unclear how VBB is being used to generalize to novel environments\u201d\n\nAuthors in InfoBot evaluate generalization by 2 ways. (1) using policy transfer, and (2) Using the encoder for providing exploration bonus in a new environment.  In the proposed method, we evaluate generalization by directly transferring policy in a new environment (and not by freezing the encoder, and providing exploration bonus). Our motivation is we want to test whether agent can generalize better by dynamically deciding when to access the privileged input.  Hence, we  demonstrate that training an agent with a VBB leads to more effective policy transfer (as compared to just having a goal conditioned bottleneck). We train policies on smaller versions of the MiniGrid environments, but evaluate them on larger versions throughout training. This is similar to section 4.2 in InfoBot paper. (https://openreview.net/pdf?id=rJg8yhAqKm).\n\n\u201cComparison to Count based exploration\u201d\n\nWe compare the proposed method to Goal Conditioned A2C (UVFA) as in Table 2, as well as Count based exploration as requested by the reviewer. For this paper, we consider direct policy transfer where we train policy in one environment and test it in other environments. \n\nMethod                                   FindObjS7       FindObjS10\nVBB                                           73% \u00b1 2%          57% \u00b1 4%\nGoal conditioned A2C            40% \u00b1 2%          24% \u00b1 3% \nCount Based Exploration      45% \u00b1 3%          38% \u00b1 5%\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper285/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye1kTVFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper285/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper285/Authors|ICLR.cc/2020/Conference/Paper285/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173665, "tmdate": 1576860545472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper285/Authors", "ICLR.cc/2020/Conference/Paper285/Reviewers", "ICLR.cc/2020/Conference/Paper285/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Comment"}}}, {"id": "H1lnUmn7cH", "original": null, "number": 2, "cdate": 1572221779844, "ddate": null, "tcdate": 1572221779844, "tmdate": 1572972614970, "tddate": null, "forum": "Hye1kTVFDS", "replyto": "Hye1kTVFDS", "invitation": "ICLR.cc/2020/Conference/Paper285/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a type of conditional Information Bottleneck (IB) that addresses the following problem: given that some features may be expensive to obtain for use in prediction, when should they be obtained such that the overall benefit outweighs the cost? A variant of the IB is proposed to model this question. However, optimization is intractable. The paper replaces a certain non-differentiable operation by a deterministic neural network which outputs the probability of seeking the expensive features. The main application here is reinforcement learning, where an agent could compute some plan or communicate with other agents at a cost, and the goal is to solve the task more efficiently while making use of this additional information. It is shown that the proposed method, VBB, makes judicious use of the limited number of costly feature acquisitions it makes, resulting in improved task performance across 3 tasks.\n\nI am not an expert on the topic, but I find that this paper is well-written and tackles a basic question with effective methods that work well in practice. \n\nHowever, since the problem is so basic, I wonder how it connects to active learning or more specifically \"active feature acquisition\":\n\n-Saar-Tsechansky, Maytal, Prem Melville, and Foster Provost. \"Active feature-value acquisition.\" Management Science 55.4 (2009): 664-684.\n-Shim, Hajin, Sung Ju Hwang, and Eunho Yang. \"Joint active feature acquisition and classification with variable-size set encoding.\" Advances in Neural Information Processing Systems. 2018.\n-Ma, Chao, et al. \"Eddi: Efficient dynamic discovery of high-value information with partial VAE.\" arXiv preprint arXiv:1809.11142 (2018). \n\nMinor:\n- \"((Bahdanau et al., 2014; Mnih et al., 2014; Xu et al., 2015))\": double parentheses\n- \"minimizing unnecessary access? . We compare\": drop the . after ?\n- \"to dynamically adjusts\" --> \"to dynamically adjust\"\n- \"The agent always access the\" --> \"The agent always accesses the\"\n- \"Tables 3a, 3b compares\": no such tables in the paper\n- \"each method acsess the\" --> \"each method accesses the\""}, "signatures": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper285/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget", "authors": ["Anirudh Goyal", "Yoshua Bengio", "Matthew Botvinick", "Sergey Levine"], "authorids": ["anirudhgoyal9119@gmail.com", "yoshua.bengio@mila.quebec", "botvinick@google.com", "svlevine@eecs.berkeley.edu"], "keywords": ["Variational Information Bottleneck", "Reinforcement learning"], "TL;DR": "Training agents with adaptive computation based on information bottleneck can promote generalization. ", "abstract": "In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant.\nThe information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.", "pdf": "/pdf/b40be7a245f8e1c10d5e434f960b0a68288a91b5.pdf", "paperhash": "goyal|the_variational_bandwidth_bottleneck_stochastic_evaluation_on_an_information_budget", "_bibtex": "@inproceedings{\nGoyal2020The,\ntitle={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},\nauthor={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye1kTVFDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/109951b019651e7044742206321a257ae68e94a1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye1kTVFDS", "replyto": "Hye1kTVFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper285/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576076940052, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper285/Reviewers"], "noninvitees": [], "tcdate": 1570237754351, "tmdate": 1576076940063, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper285/-/Official_Review"}}}], "count": 16}