{"notes": [{"id": "rJg8TeSFDH", "original": "Syex7bZFPS", "number": 2575, "cdate": 1569439934327, "ddate": null, "tcdate": 1569439934327, "tmdate": 1583912041773, "tddate": null, "forum": "rJg8TeSFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "0lFT19rXkk", "original": null, "number": 1, "cdate": 1576798752473, "ddate": null, "tcdate": 1576798752473, "tmdate": 1576800883129, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "After the revision, the reviewers agree on acceptance of this paper.    Let's do it.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717432, "tmdate": 1576800267740, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Decision"}}}, {"id": "rJlhm_ETtH", "original": null, "number": 1, "cdate": 1571797027576, "ddate": null, "tcdate": 1571797027576, "tmdate": 1574390978458, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "%%% Update to the review %%%\nThanks for your clarification and the revision - the paper looks good! With regards to your comment on accelerating hyper-parameter search, note that there are fairly subtle issues owing to the use of SGD - refer to a recent work of Ge et al \"The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares\" (2019). \n%%%\n\nThe paper makes an interesting observation connecting the use of weight decay + normalization to training the same network (without regularization) using an exponentially increasing learning rate schedule, under an assumption of scale invariance that is satisfied by normalization techniques including batch norm, layer norm and other variants. An example is also provided where the joint use of batch norm and weight decay can lead to non-convergence to a global minimum whereas the use of one (without the other) converges to a global minimum - serving to indicate various interdependencies between the hyper-parameters that one needs to be careful about.\n\nWhile the connection of scale invariant models to novel schemes of learning rates is interesting (and novel), the paper will benefit quite a bit in its contributions through attempting a convergence analysis towards a stationary point even for solving a routine smooth non-convex stochastic optimization problem. Owing to the equivalence described in the paper, this enables us to understand the behavior of the combination of batch norm (or some scale invariance property) + weight decay + momentum (+ step decay of the learning rate), which, to my knowledge isn\u2019t present in the literature of non-convex optimization. \n\nThe paper is reasonably written (the proof of the main claim is fairly easy to follow), but needs to be carefully read through because I see typos and ill-formed sentences that should be rectified - e.g. see point 3. in appendix A.1 - some facts about equation 4, missing citation in definition 1.2 amongst others. I went over the proof of the main result and this appears to be correct. Furthermore, I find the connections to other learning rates (such as the cosine learning rate) to be rather hard to understand/interpret, in the current shape of the paper.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2575/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2575/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667698636, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2575/Reviewers"], "noninvitees": [], "tcdate": 1570237720886, "tmdate": 1575667698649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Official_Review"}}}, {"id": "rklU4eINhr", "original": null, "number": 4, "cdate": 1574359085715, "ddate": null, "tcdate": 1574359085715, "tmdate": 1574359085715, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "UPDATE TO MY EARLIER REVIEW\n===========================\n\nThe authors improved the writing of the paper substantially relative to the first version they submitted, and fixed minor issues. I am changing my rating from \"Weak accept\" to \"Strong accept\". It is now a must-read paper for anyone doing research on deep learning. \n\n\n\nMY EARLIER REVIEW\n=================\n\nThis exciting and insightful paper presents theorems (and illustrating examples and experiments) describing an equivalence of commonly used learning rate schedules and weight decay settings with an exponentially increasing learning rate schedule and no weight decay, for neural networks with scale-invariant weights. Hence, the results apply to a large set of commonly employed settings. The paper contains an interesting example of a neural network for which gradient descent converges if with batch normalization as well as with L2 regularization, but not when both are used. \n\nFrom a theory viewpoint, the paper offers new insights into Batch normalization and other normalization schemes. From a practical viewpoint, the paper suggests a way to speed up hyper-parameter search, effectively allowing to consider learning rate and weight decay as one parameter. \n\nA small gripe: this paper is a bit rough around the edges, and reads a bit like a draft (see comments on details below). \n\n\nDetailed Comments / advice / questions\n==================================\n\n- It often takes a bit of searching to figure out what proof goes with what theorem / fact. I recommend to add to each occurrence of \u201cProof.\u201d (before a proof) with a reference to the theorem or fact that is being proved, e.g. \u201cProof of Theorem 2.6\u201d. \n\n- The authors state that theorem B2 applies to \u201cgeneral deep nets\u201d. In this theorem, the limit R_\\infty could very well be zero (e.g. for networks with weights that are not scale-invariant), in which case the statement contains a division by zero. I wonder if the authors overlooked this or forgot to state an assumption in the theorem. Maybe I am missing something. Since this theorem does not appear to be critical to the main contributions of the paper, it may be easiest to remove the theorem if the division by zero is indeed a problem. \n\n\n- On page 6, if c_w(x) is independent of w, would c(x) be more suitable? \n\n- At the bottom of page 6, the authors state that \u201cAs a result, for any fixed learning rate, ||w_{t+1}|| ^4= \u2026\u201d. It appears that the authors get the expression for  ||w_{t+1}||^4 from the expression for ||w_{t+1}||^2 above (maybe by multiplying by ||w_{t+1}||^2?), but I don\u2019t see how. Could the authors explain this? Maybe authors mixed up w_t and w_{t+1} by accident? \n\n- The authors claim to \u201cconstrict better Exponential learning rate schedules\u201d. Since the authors only perform a limited evaluation of their proposed learning rate schedule on CIFAR10, I suggest qualifying this statement. \n\n- Theorem 1.1 does not introduce what \\tilde{\\gamma} is. It\u2019s somewhat obvious, but I would state it nonetheless. \n\n- The authors state that \u201c\u2026the exponent trumps the effect of initial lr very fast, which serves as another explanation of the standard wisdom that initial lr is unimportant when training with BN\u201d. I don\u2019t think that this constitutes a full explanation without further argument. I am also wondering if \u201canother\u201d is appropriate here: I am not aware of any (other) mathematically precise explanations of why initial learning rates do not matter in this set-up. If the authors are, they should cite it. \n\n- The authors often forgot spaces before \\cite commands. If you are trying to avoid a line break before the \\cite command, you can use a tilde ~ , like this \u201cGroup Normalization~\\cite{somepaper}\u201d. \n\n- Please introduce the abbreviation LR for \u201cLearning Rate\u201d, and always use the all-upper-case version (not \u201clr\u201d). \n\n- Definition 1.2 has a broken \\cite . \n\n- Theorem 2.7 should introduce \\hat{eta}_t, but it doesn\u2019t. \n\n- Appendix A.1 contains a broken sentence (\u201cas a function of\u2026\u201d)\n\n- It\u2019s odd that the proof for theorems B1 and B2 appear before theorems B1 and B2. I would restructure the appendices to improve this. \n\n- Theorem B2 contains a stray \u201cwhen\u201d, and \u201cexists\u201d should be \u201cexist\u201d\n\n- What the authors call \u201cProof of Theorem 2.4\u201d in the appendix is really a proof of the rigorous version of Theorem 2.4 - Theorem 2.7! The proof should in my opinion be labeled \u201cProof of Theorem 2.7\u201d\n\n- The typo \u201ceventsequation\u201d should be replaced with something like \u201cthe events from equations\u201d\n\n- Replace the colloquialism \u201cnets\u201d with \u201cnetworks\u201d. \n\n- Replace \u201cBatchNorm\u201d with \u201cBatch Normalization\u201d\n\n- \u201cCOvariate\u201d has casing issues\n\n- \u201cRiemmanian\u201d should be \u201cRiemannian\u201d\n\n- \u201cBNhas\u201d should be \u201cBN has\u201d\n\n- The paper\u2019s title states that the results are for batch-normalized networks, while the analysis appears to be more generally for networks with scale-invariant weights, which as the authors point out can arise from mechanisms other than batch normalization. Have the authors considered changing the paper\u2019s title to better capture what their work applies to? In terms of discoverability, the authors would do the community a service by titling the paper in such a way that it captures the set-up well. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2575/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2575/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667698636, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2575/Reviewers"], "noninvitees": [], "tcdate": 1570237720886, "tmdate": 1575667698649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Official_Review"}}}, {"id": "r1e-C3cUor", "original": null, "number": 3, "cdate": 1573461193425, "ddate": null, "tcdate": 1573461193425, "tmdate": 1573846151416, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "HkxH37tRFH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your detailed and thoughtful reviews as well as the helpful suggestions!\n\n1.  Our new revision tries to improve the writing and fixed the typos you have mentioned. We specified which theorem/lemma each proof corresponds to as you suggested. We also reorganized the sections in appendix.\n\n2. You\u2019re correct on that $R_\\infty$ could be 0, thus making the statement meaningless. When $R_\\infty= 0$, $D_\\infty$ is also equal to 0 since $D_\\infty <= 4* R_\\infty$. A quick fix is just to replace the original statement by $D_\\infty = \\frac{2\\eta\\lambda}{1+\\gamma}R_\\infty$. \n\n3. Random variable $c_w$ is actually a function mapping input data $x$ to a real number. This function indeed depends on $w$, but for different $w$, the random variable $c_w$ follows the same law(distribution). \nTo avoid confusion, we modified this argument in the new revision and now we used the fact for each w, with constant probability, $\\|\\nabla_w L\\|> \\frac{C}{\\|w\\|}$, where $C$ is some other constant. \n\n4. We indeed missed some subscription in the sentence \u201cAs a result, for any fixed learning rate, $\\|w_{t+1}\\|^4=$ \u2026\u201d.  The reason that $\\|w_{t+1}\\|^4>= \\|w_t\\|^4+2\\eta^2c_w^2$ holds is that we can expand $\\|w_{t+1}\\|^4$ as$ (\\|w_{t+1}\\|^2)^2$, which is larger than $(\\|w_t\\|^2+\\eta^2*c_w^2/\\|w_t\\|^2)^2 = \\|w_t\\|^4+2\\eta^2c_w^2 + \\eta^4*c_w^4/\\|w_t\\|^4$.\n\n5. We\u2019re sorry for the typo in the statement \u201c\u2026the exponent trumps the effect of initial LR very fast, which serves as another explanation of the standard wisdom that initial LR is unimportant when training with BN\u201d. It should be that the initial scale of initialization is unimportant, and we give further explanation for this in the new revision. We don\u2019t have a theory for this but we verified this intuition in experiments.\n\nThank you so much for pointing this out!! And actually, the initial LR does matter a lot in practice. \n\n6. As you suggested, we changed the title of the paper into \u201cAn Exponential Learning Rate Schedule for Deep Learning\u201d.\n\nThanks again for your helpful and thoughtful suggestions!! Given the new revision addressing all your concerns, would you consider raising your rating?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2575/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg8TeSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2575/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2575/Authors|ICLR.cc/2020/Conference/Paper2575/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139253, "tmdate": 1576860555679, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Official_Comment"}}}, {"id": "HygVdTqLir", "original": null, "number": 4, "cdate": 1573461355523, "ddate": null, "tcdate": 1573461355523, "tmdate": 1573461355523, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "rJlhm_ETtH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your thoughtful and valuable reviews!\n\n-We\u2019ve improved the writing and fixed the typos in the paper. Regarding your suggestion about convergence analysis for routine smooth convex stochastic optimization problem, we do know how to get such a bound for the case of SGD + momentum + constant LR, which could be seen as a generalization of the convergence result in [Arora, Li, Lyu\u20192019] where momentum is turned off. The main idea is the same, because of the norm of the weight monotone increases (as shown in this paper), we can use the objective as a potential to bound the sum of the squared norm of gradients along the trajectory. Here the assumption is that the smoothness of scale invariant loss L(w) needs to be bounded outside the unit ball.  Due to the space limit, we didn\u2019t put this result in the paper. But given your suggestion, we will try to put this result in the appendix of the next revision if time permits.\n\nHowever, it seems to be impossible to get a convergence rate for the case of SGD + momentum + constant lr + weight decay, as we\u2019ve already given a non-convergence result for the toy example. If we further allowing step decay LR schedule, then when LR is sufficiently small, the dynamics is like gradient flow and it converges to stationary points of course. \n\n-The main point about the connection to other LR schedules is that we can get rid off WD by adopting the exponential version of the original schedule, which could accelerate the hyperparameter search.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2575/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg8TeSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2575/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2575/Authors|ICLR.cc/2020/Conference/Paper2575/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139253, "tmdate": 1576860555679, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Official_Comment"}}}, {"id": "SJl1vjc8oB", "original": null, "number": 2, "cdate": 1573460823484, "ddate": null, "tcdate": 1573460823484, "tmdate": 1573460823484, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "HJxfYraZsB", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for the careful reviews. \n\nIn the new revision, we added a new section in appendix explaining the scale invariance in the modern network architectures. We also improved the writing of the proofs and fixed the typos.\n\nThanks again for your appreciation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2575/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg8TeSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2575/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2575/Authors|ICLR.cc/2020/Conference/Paper2575/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139253, "tmdate": 1576860555679, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Official_Comment"}}}, {"id": "Hyle1ic8sS", "original": null, "number": 1, "cdate": 1573460695594, "ddate": null, "tcdate": 1573460695594, "tmdate": 1573460695594, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Official_Comment", "content": {"title": "A new revision is uploaded", "comment": "Besides improving writing and fixing the typos in the paper, we made the following major changes in the new revision:\n\n1. We changed the title to \u201cAn Exponential Learning Rate Schedule for Deep Learning\u201d for better discoverability as suggested by Reviewer #3.\n\n2. We added a warm-up subsection in Section 2, proving the equivalence between exp LR and weight decay in the momentum-free case. \n\n3. We provided a new perspective for understanding the almost equivalence between Step Decay + WD and our TEXP LR schedule, which says that with additional one-time momentum correction per phase, TEXP is **exactly** equivalent to Step Decay+WD. This explanation is conceptually easier than the original one(convergence of the LR growth rate) and thus we put this explanation in the main paper and defer the original one into the appendix. \n\n4. We reorganized the experiment section and improved the writing.\n\n5. In response to the reviews of Reviewer#2, we added a new section in the appendix named \u201cScale Invariance in Modern Network Architectures\u201d explaining why various network architectures are scale invariant, including feedforward CNN and FC nets, ResNet, PreResNet. In detail, we gave an efficient algorithm for checking the sufficient condition of scale invariance and demonstrate how we could apply it onto the aforementioned architectures. The scale invariance of some architectures, like ResNet and PreResNet, is not entirely trivial to show.\n\n6. We add a section named in appendix named \u201cViewing EXP LR Via Canonical Optimization Framework\u201d, aiming at explaining why the efficacy of exponential LR in deep learning is mysterious to us, at least as viewed in the canonical framework of optimization theory.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2575/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg8TeSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2575/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2575/Authors|ICLR.cc/2020/Conference/Paper2575/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139253, "tmdate": 1576860555679, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Official_Comment"}}}, {"id": "rJxVktSXsr", "original": null, "number": 1, "cdate": 1573243099615, "ddate": null, "tcdate": 1573243099615, "tmdate": 1573243099615, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Public_Comment", "content": {"title": "An Interesting Connection", "comment": "Hi Authors,\nThank you for your interesting paper.  I noticed that your work concerning learning rates for neural networks with batch normalization is related to our paper, which shows that an alternative to weight decay, which may stabilize effective learning rate, can improve performance for networks, especially those with batch norm.[1]  Please consider mentioning the relationship with our work in your next version.\n\n[1] https://arxiv.org/abs/1910.00359"}, "signatures": ["~Micah_Goldblum1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Micah_Goldblum1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJg8TeSFDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504178308, "tmdate": 1576860588829, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2575/Authors", "ICLR.cc/2020/Conference/Paper2575/Reviewers", "ICLR.cc/2020/Conference/Paper2575/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Public_Comment"}}}, {"id": "HJxfYraZsB", "original": null, "number": 3, "cdate": 1573143930411, "ddate": null, "tcdate": 1573143930411, "tmdate": 1573143930411, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This work makes an interesting observation that it is possible to use exponentially growing learning rate schedule when training with neural networks with batch normalization. This paper provides both theoretical insights and empirical demonstration of this remarkable property. In detail, the authors prove that for stochastic gradient descent (SGD) with momentum, this exponential learning rate schedule is equivalent to constant learning rate + weight decay, for any scale invariant networks, including networks with Batch Normalization and other normalization methods. This paper also contains an interesting toy example where  gd converges when normalization or weight decay is used alone while not when normalization and weight decay are used together.\n\nPros:\n\n1. This paper gives new and important insight to the complex interplay between the tricks of network training, such as weight decay, normalization and momentum. The assumption and derivation are simple but the result is quite surprising. In classical optimization framework, it is common to keep the learning rate smaller than the 1/smoothness such that gd decreases the loss. However, the connection between exponential learning rate schedule and weight decay in common practice built by this paper suggests that the current neural net training recipe may be inherently non-smooth.\n\n2. The experiment of this paper also suggests that in practice (with normalization layer), learning rate and weight decay coefficient can be packed into a single parameter, which reduces the effort needed for hyper-parameter tuning.\n\nCons:\n\n1. Though it's obvious for the feedforward networks with normalization layers to be scale invariant, it's not the case for ResNet ( and the authors use this for experiment). And this needs to be clarified.\n2. The writing of the proofs should be imporved.\n\nTypos:\n\n1. Definition 1.2 broke citation\n2. Equation (1)  \\eta_t should be \\eta_{t-1}\n3. Some facts about Equation 4, incomplete sentence\n4 In thm B.2,R_\\infty might be 0. So the authors can just delete the last equation on page 12 and use the equation above as the statement of the lemma.\n5. In the first line of Equation (13), the appearance of ( \\beta * e^{1-\\beta} )^{ k/2 } seems to be a mistake\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2575/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2575/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667698636, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2575/Reviewers"], "noninvitees": [], "tcdate": 1570237720886, "tmdate": 1575667698649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Official_Review"}}}, {"id": "HkxH37tRFH", "original": null, "number": 2, "cdate": 1571881901034, "ddate": null, "tcdate": 1571881901034, "tmdate": 1572972320268, "tddate": null, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "invitation": "ICLR.cc/2020/Conference/Paper2575/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This exciting and insightful paper presents theorems (and illustrating examples and experiments) describing an equivalence of commonly used learning rate schedules and weight decay settings with an exponentially increasing learning rate schedule and no weight decay, for neural networks with scale-invariant weights. Hence, the results apply to a large set of commonly employed settings. The paper contains an interesting example of a neural network for which gradient descent converges if with batch normalization as well as with L2 regularization, but not when both are used. \n\nFrom a theory viewpoint, the paper offers new insights into Batch normalization and other normalization schemes. From a practical viewpoint, the paper suggests a way to speed up hyper-parameter search, effectively allowing to consider learning rate and weight decay as one parameter. \n\nA small gripe: this paper is a bit rough around the edges, and reads a bit like a draft (see comments on details below). \n\n\nDetailed Comments / advice / questions\n==================================\n\n- It often takes a bit of searching to figure out what proof goes with what theorem / fact. I recommend to add to each occurrence of \u201cProof.\u201d (before a proof) with a reference to the theorem or fact that is being proved, e.g. \u201cProof of Theorem 2.6\u201d. \n\n- The authors state that theorem B2 applies to \u201cgeneral deep nets\u201d. In this theorem, the limit R_\\infty could very well be zero (e.g. for networks with weights that are not scale-invariant), in which case the statement contains a division by zero. I wonder if the authors overlooked this or forgot to state an assumption in the theorem. Maybe I am missing something. Since this theorem does not appear to be critical to the main contributions of the paper, it may be easiest to remove the theorem if the division by zero is indeed a problem. \n\n\n- On page 6, if c_w(x) is independent of w, would c(x) be more suitable? \n\n- At the bottom of page 6, the authors state that \u201cAs a result, for any fixed learning rate, ||w_{t+1}|| ^4= \u2026\u201d. It appears that the authors get the expression for  ||w_{t+1}||^4 from the expression for ||w_{t+1}||^2 above (maybe by multiplying by ||w_{t+1}||^2?), but I don\u2019t see how. Could the authors explain this? Maybe authors mixed up w_t and w_{t+1} by accident? \n\n- The authors claim to \u201cconstrict better Exponential learning rate schedules\u201d. Since the authors only perform a limited evaluation of their proposed learning rate schedule on CIFAR10, I suggest qualifying this statement. \n\n- Theorem 1.1 does not introduce what \\tilde{\\gamma} is. It\u2019s somewhat obvious, but I would state it nonetheless. \n\n- The authors state that \u201c\u2026the exponent trumps the effect of initial lr very fast, which serves as another explanation of the standard wisdom that initial lr is unimportant when training with BN\u201d. I don\u2019t think that this constitutes a full explanation without further argument. I am also wondering if \u201canother\u201d is appropriate here: I am not aware of any (other) mathematically precise explanations of why initial learning rates do not matter in this set-up. If the authors are, they should cite it. \n\n- The authors often forgot spaces before \\cite commands. If you are trying to avoid a line break before the \\cite command, you can use a tilde ~ , like this \u201cGroup Normalization~\\cite{somepaper}\u201d. \n\n- Please introduce the abbreviation LR for \u201cLearning Rate\u201d, and always use the all-upper-case version (not \u201clr\u201d). \n\n- Definition 1.2 has a broken \\cite . \n\n- Theorem 2.7 should introduce \\hat{eta}_t, but it doesn\u2019t. \n\n- Appendix A.1 contains a broken sentence (\u201cas a function of\u2026\u201d)\n\n- It\u2019s odd that the proof for theorems B1 and B2 appear before theorems B1 and B2. I would restructure the appendices to improve this. \n\n- Theorem B2 contains a stray \u201cwhen\u201d, and \u201cexists\u201d should be \u201cexist\u201d\n\n- What the authors call \u201cProof of Theorem 2.4\u201d in the appendix is really a proof of the rigorous version of Theorem 2.4 - Theorem 2.7! The proof should in my opinion be labeled \u201cProof of Theorem 2.7\u201d\n\n- The typo \u201ceventsequation\u201d should be replaced with something like \u201cthe events from equations\u201d\n\n- Replace the colloquialism \u201cnets\u201d with \u201cnetworks\u201d. \n\n- Replace \u201cBatchNorm\u201d with \u201cBatch Normalization\u201d\n\n- \u201cCOvariate\u201d has casing issues\n\n- \u201cRiemmanian\u201d should be \u201cRiemannian\u201d\n\n- \u201cBNhas\u201d should be \u201cBN has\u201d\n\n- The paper\u2019s title states that the results are for batch-normalized networks, while the analysis appears to be more generally for networks with scale-invariant weights, which as the authors point out can arise from mechanisms other than batch normalization. Have the authors considered changing the paper\u2019s title to better capture what their work applies to? In terms of discoverability, the authors would do the community a service by titling the paper in such a way that it captures the set-up well. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2575/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2575/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhiyuanli@cs.princeton.edu", "arora@cs.princeton.edu"], "title": "An Exponential Learning Rate Schedule for Deep Learning", "authors": ["Zhiyuan Li", "Sanjeev Arora"], "pdf": "/pdf/c0a5feba02a5a8332a346bf69eb4ec194a8c9a24.pdf", "TL;DR": "We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.", "abstract": "Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)\n\u2022 Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + \u03b1) factor in every epoch for some \u03b1 > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.\n\u2022 Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.\n\u2022 A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "keywords": ["batch normalization", "weight decay", "learning rate", "deep learning theory"], "paperhash": "li|an_exponential_learning_rate_schedule_for_deep_learning", "_bibtex": "@inproceedings{\nLi2020An,\ntitle={An Exponential Learning Rate Schedule for Deep Learning},\nauthor={Zhiyuan Li and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJg8TeSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/6f1a16fdd8f783b8f9e37e23095efdcd973081d0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJg8TeSFDH", "replyto": "rJg8TeSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2575/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667698636, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2575/Reviewers"], "noninvitees": [], "tcdate": 1570237720886, "tmdate": 1575667698649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2575/-/Official_Review"}}}], "count": 11}