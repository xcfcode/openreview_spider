{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573619896, "tcdate": 1521573619896, "number": 322, "cdate": 1521573619550, "id": "Skhlky15M", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJDCPSJPM", "replyto": "BJDCPSJPM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Flexible Approach to Automated RNN Architecture Generation", "abstract": "The process of designing neural architectures requires expert knowledge and extensive trial and error.\nWhile automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\nWe propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\nThe DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\nThe resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.", "pdf": "/pdf/d4aba96c86c0f058a0289f02c378f8f4e1723fd5.pdf", "TL;DR": "We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures.", "paperhash": "schrimpf|a_flexible_approach_to_automated_rnn_architecture_generation", "_bibtex": "@misc{\nmerity2018a,\ntitle={A Flexible Approach to Automated {RNN} Architecture Generation},\nauthor={Stephen Merity, Martin Schrimpf, James Bradbury, Richard Socher},\nyear={2018},\nurl={https://openreview.net/forum?id=SkOb1Fl0Z},\n}", "keywords": ["reinforcement learning", "architecture search", "ranking function", "recurrent neural networks", "recursive neural networks"], "authors": ["Martin Schrimpf", "Stephen Merity", "James Bradbury", "Richard Socher"], "authorids": ["msch@mit.edu", "smerity@smerity.com", "james.bradbury@salesforce.com", "richard@socher.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730181707, "tcdate": 1518454735507, "number": 161, "cdate": 1518454735507, "id": "BJDCPSJPM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJDCPSJPM", "original": "SkOb1Fl0Z", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "A Flexible Approach to Automated RNN Architecture Generation", "abstract": "The process of designing neural architectures requires expert knowledge and extensive trial and error.\nWhile automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\nWe propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\nThe DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\nThe resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.", "pdf": "/pdf/d4aba96c86c0f058a0289f02c378f8f4e1723fd5.pdf", "TL;DR": "We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures.", "paperhash": "schrimpf|a_flexible_approach_to_automated_rnn_architecture_generation", "_bibtex": "@misc{\nmerity2018a,\ntitle={A Flexible Approach to Automated {RNN} Architecture Generation},\nauthor={Stephen Merity, Martin Schrimpf, James Bradbury, Richard Socher},\nyear={2018},\nurl={https://openreview.net/forum?id=SkOb1Fl0Z},\n}", "keywords": ["reinforcement learning", "architecture search", "ranking function", "recurrent neural networks", "recursive neural networks"], "authors": ["Martin Schrimpf", "Stephen Merity", "James Bradbury", "Richard Socher"], "authorids": ["msch@mit.edu", "smerity@smerity.com", "james.bradbury@salesforce.com", "richard@socher.org"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730181707, "tcdate": 1509097215895, "number": 323, "cdate": 1518730181696, "id": "SkOb1Fl0Z", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SkOb1Fl0Z", "original": "rkvZkKx0W", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "A Flexible Approach to Automated RNN Architecture Generation", "abstract": "The process of designing neural architectures requires expert knowledge and extensive trial and error.\nWhile automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\nWe propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\nThe DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\nThe resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.", "pdf": "/pdf/9873717312898f8fe2f78ebc1a27b40cf8105a8e.pdf", "TL;DR": "We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures.", "paperhash": "schrimpf|a_flexible_approach_to_automated_rnn_architecture_generation", "_bibtex": "@misc{\nschrimpf2018a,\ntitle={A Flexible Approach to Automated {RNN} Architecture Generation},\nauthor={Martin Schrimpf and Stephen Merity and James Bradbury and Richard Socher},\nyear={2018},\nurl={https://openreview.net/forum?id=SkOb1Fl0Z},\n}", "keywords": ["reinforcement learning", "architecture search", "ranking function", "recurrent neural networks", "recursive neural networks"], "authors": ["Martin Schrimpf", "Stephen Merity", "James Bradbury", "Richard Socher"], "authorids": ["msch@mit.edu", "smerity@smerity.com", "james.bradbury@salesforce.com", "richard@socher.org"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}