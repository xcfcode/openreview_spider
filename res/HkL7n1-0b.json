{"notes": [{"tddate": null, "ddate": null, "tmdate": 1523264431001, "tcdate": 1523264431001, "number": 9, "cdate": 1523264431001, "id": "Bkv3jj_sM", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "BJD-D8Ioz", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "On dropping the mutual information", "comment": "Thank you for the question!\n\nIndeed, we were not completely clear in the text. The argument applies only to the version of WAE-GAN, where the KL divergence is used instead of the JS entropy (which can b estimated in a very similar way using the adversarial training)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "tmdate": 1523111708410, "tcdate": 1523111679155, "number": 8, "cdate": 1523111679155, "id": "BJD-D8Ioz", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["~Jeasine_Ma1"], "readers": ["everyone"], "writers": ["~Jeasine_Ma1"], "content": {"title": "A question to 'simply drop the mutual information term \\mathbb{I}_Q(X,Z)'", "comment": "Thanks for your insightful work. I found a question during reading your paper. And I hope to get some guidance from you.\n\nIn Sec.3, you mentioned that compared to VAE, WAE just 'simply drop the mutual information term \\mathbb{I}_Q(X,Z) in the VAE regularizer', where this entangled regularizer has been discussed in [1]. But It seems that the proof to this evident conclusion is not straight-forward. I've tried to prove the GAN-WAE version, by regarding the GAN objective as JSD(q(z|x) || p(z)), however, it seems that the entropy term still cannot be eliminated, which means that we may still suffer from this entangled term in the regularizer you proposed(at least the GAN-WAE version). \n\nIs it possible for your to give some proof sketch on this conclusion? It will be better if the proof of both the GAN and MMD version can be provided. Thanks.\n\n[1] Matthew D. Hoffman, Matthew J. Johnson, ELBO surgery: yet another way to carve up the variational evidence lower bound, 2016"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791684940, "id": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Area_Chair"], "cdate": 1512791684940}}}, {"tddate": null, "ddate": null, "tmdate": 1519610429534, "tcdate": 1519610357116, "number": 7, "cdate": 1519610357116, "id": "SJTe9k-_f", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "forum": "HkL7n1-0b", "replyto": "BysNAdlOf", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Encoders must be random", "comment": "The last line of the proof in page 14 says: \"It remains to notice that \\mathcal{P}_{X, Z} = \\mathcal{P}(X ~ P_X, Z ~ P_Z) as stated earlier.\" That means the proof considers all possible coupling between X and Z and thus considers both random and deterministic encoders. So, the proof considers all form of Gamma(Y | X), whether random or deterministic.\n\nTheorem 1 is technically correct, but the framework's effectiveness in minimizing/estimating the WS-distance remains to be explored. We need to optimize over the set of all possible couplings between X and Z, which is impossible. Considering only Dirac or Gaussian encoders, however, might be too restrictive."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791684940, "id": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Area_Chair"], "cdate": 1512791684940}}}, {"tddate": null, "ddate": null, "tmdate": 1519582770883, "tcdate": 1519582770883, "number": 8, "cdate": 1519582770883, "id": "BysNAdlOf", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "SJOMQOguf", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "Encoders can indeed be Dirac", "comment": "Yes, in Theorem 1 the encoder Q(Z|X) is allowed to be Dirac as long as it satisfies the constrain. Of course, it is not always the case that deterministic encoders can match the prior: consider the case when the intrinsic dimensionality of the data is less than the latent space dimensionality (as discussed on page 7 and in more details in [1]).\n\nRegarding Gamma(Y|X)---the conditional part of the coupling---we assume you are referring to the sentence \"we can consider Gamma(Y|X) as a non-deterministic mapping from X to Y\" appearing in the proof of Theorem 1 on page 13. The proof never argues that Gamma(Y|X) should be necessarily random or deterministic, and does not use any of these two assumptions.\n\n[1] Rubenstein, P., Scholkopf, B., Tolstikhin, I. On the Latent Space of Wasserstein Auto-Encoders. https://arxiv.org/pdf/1802.03761.pdf\n "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "tmdate": 1519579920046, "tcdate": 1519579920046, "number": 6, "cdate": 1519579920046, "id": "SJOMQOguf", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "forum": "HkL7n1-0b", "replyto": "r1FUJDldG", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Is Q(Z|X) assumed to be Dirac?", "comment": "Thank you for the clarification. I misinterpreted: \"We also report results of VAEs. VAEs used the same latent spaces as discussed above and standard Gaussian priors P_Z = N(0; I_d). We used Gaussian encoders Q(Z|X) = N(Z;\\mu_\\phi(X); \\Sigma(X)) with mean \\mu_\\phi and diagonal covariance \\Sigma\"\n\nYou said: \"Note that as opposed to VAEs, the WAE formulation allows for non-random encoders deterministically mapping inputs to their latent codes.\" Does that mean Q(Z|X) is assumed to be Dirac measure? If that's the case, Gamma(Y|X) is deterministic but the proof of Theorem 1 considers Gamma(Y|X) as non-deterministic mapping from X to Y. Am I wrong? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791684940, "id": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Area_Chair"], "cdate": 1512791684940}}}, {"tddate": null, "ddate": null, "tmdate": 1519575579089, "tcdate": 1519573942770, "number": 5, "cdate": 1519573942770, "id": "r1kaoIgdf", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Limitation of theorem 1 should be stated clearer", "comment": "The key of the paper is Theorem 1. It says that we can estimate WS-distance between Px and Pg by optimizing over random encoders Q(Z|X). Implementation however assumes that Q(Z | X) is Gaussian with mean and covariance parametrized by deep neural nets. The discussion of Theorem 1 in the paper simply says: \"Similarly to VAE, we propose to use deep neural networks to parametrize both encoders Q and decoders G.\" It should openly discuss the limitation of this parameterization because it significantly reduces the encoder space over which the model optimizes. Otherwise, readers and reviewers would be left with the impression that \"wow, this approach avoids the min-max problem in GAN and formulate a min min problem, which is much easier to solve.\" In theory, it's true.  In practice, I suspect it's far from truth as the encoder space is significantly constrained and effectively forcing Q(Z) to match P(Z) is still an open problem."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791684940, "id": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Area_Chair"], "cdate": 1512791684940}}}, {"tddate": null, "ddate": null, "tmdate": 1519574865284, "tcdate": 1519574865284, "number": 7, "cdate": 1519574865284, "id": "r1FUJDldG", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "r1kaoIgdf", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "There are no constraints on encoders", "comment": "None of WAE implementations in our paper uses Gaussian encoders. Theorem 1 does not constrain encoders in any way and WAEs can be used with any form of random (or deterministic) encoders, including flexible implicit random encoders induced by generative architectures of GANs (and for instance used in [1]).\n\n[1] Mescheder, L. and Nowozin, S. and Geiger, A. Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks. ICML 2017."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "tmdate": 1519131140603, "tcdate": 1509125149607, "number": 517, "cdate": 1518730174738, "id": "HkL7n1-0b", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HkL7n1-0b", "original": "S1nfhyW0W", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "nonreaders": [], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "tmdate": 1518875644950, "tcdate": 1518875644950, "number": 6, "cdate": 1518875644950, "id": "rkSWN2rvf", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "SkGcPcZ-z", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "WAEs drop the mutual information term in the VAE regularizer", "comment": "Thank you for this comment. Indeed, this observation provides one more intuitively clear way to explain a difference between VAEs and WAEs. We will use your suggestion in the camera-ready version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260101220, "tcdate": 1517249191169, "number": 11, "cdate": 1517249191155, "id": "ryknMk6Hz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper proposes a new generative model that has the stability of variational autoencoders (VAE) while producing better samples. The authors clearly compare their work to previous efforts that combine VAEs and Generative Adversarial Networks with similar goals.  Authors show that the proposed algorithm is a generalization of Adversarial Autoencoder (AAE) and minimizes Wasserstein distance between model and target distribution. The paper is well written with convincing results. Reviewers agree that the algorithm is novel and practical; and close connections of the algorithm to related approaches are clearly discussed with useful insights.  Overall, the paper is strong and I recommend acceptance.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1516644640373, "tcdate": 1516644640373, "number": 4, "cdate": 1516644640373, "id": "Sy_QFsmHG", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "forum": "HkL7n1-0b", "replyto": "HyBIaDXBM", "signatures": ["~Min_Lin1"], "readers": ["everyone"], "writers": ["~Min_Lin1"], "content": {"title": "Clarification", "comment": "Let me clarify the markov chain point.\n\nIn the case Q(Z|X) is stochastic, the encode/decode chain X->Z->X' is stochastic. Namely, P(X'|X) is not a deterministic function, it is a distribution. A markov chain can be constructed if we sample X from P_X and use P(X'|X) as the transition probability.\n\nBy optimizing the Wasserstein distance between P(X') and P_X, we hope to get the parameter such that P(X') == P_X. The reconstruction term in this paper requires that X' == X, which is stronger than P(X') == P_X."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791684940, "id": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Area_Chair"], "cdate": 1512791684940}}}, {"tddate": null, "ddate": null, "tmdate": 1516629324913, "tcdate": 1516629324913, "number": 5, "cdate": 1516629324913, "id": "HyBIaDXBM", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "rJSDX-xSG", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "Regarding random / deterministic encoders in WAE", "comment": "Thank you for the question.\n\nUnfortunately, we did not quite get the point of your Markov chain example. But we would like to make it clear that the paper does not assume anything specific about the encoder Q(Z|X). As long as the aggregated posterior Qz matches the prior Pz, the encoder can be either deterministic or random. The same holds true for the WAE algorithm. We will try to emphasize it better in the updated version of the paper.\n\nThe decoder is indeed a different story: for Theorem 1 we need it to be deterministic, but a very similar result holds also for the random decoder (Supplementary B)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "tmdate": 1516405677420, "tcdate": 1516405597208, "number": 3, "cdate": 1516405597208, "id": "rJSDX-xSG", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["~Min_Lin1"], "readers": ["everyone"], "writers": ["~Min_Lin1"], "content": {"title": "Possibility of a Markov Chain instead of Reconstruction.", "comment": "Thanks for the great work. It's nice to see there is theoretical support for the (auto-encoder + constraint on Z) objective.\n\nIt seems to me the expectation over X could not be moved out in theorem 1, as this breaks the independence of Z and X.\nConsider the case Q(Z|X) is not deterministic, we can have a markov chain X_{t+1} ~ \\int_Z [ P_G(X'|Z)Q(Z|X_t) ], which has a stationary distribution same as P_X.  The algorithm in this paper gives a special case where Q(Z|X) is deterministic and X_{t+1} = X_{t}.\n\nIn supplementary B, the case where the decoder is random is discussed. It would be nice to also discuss the cases where Q(Z|X) is random vs deterministic.\n\nDo correct me if I'm wrong, thanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791684940, "id": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Area_Chair"], "cdate": 1512791684940}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642459859, "tcdate": 1511716362715, "number": 1, "cdate": 1511716362715, "id": "SJQzLO_gM", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Review", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["ICLR.cc/2018/Conference/Paper517/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "This is a well-written paper which provides a useful generalisation of some existing methods for inferring generative models.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper satisfies the following necessary conditions for\nacceptance. The writing is clear and I was able to understand the\npresented method (and its motivation) despite not being too familiar\nwith the relevant literature. Explicitly writing the auto-encoder(s)\nas pseudo-code algorithms was particular helpful. I found no technical\nerrors. The problem addressed is one worth solving - building a\ngenerative model of observed data. There is some empirical testing\nwhich show the presented method in a good light.\n\nThe authors are careful to relate the presented method with existing\nones, most notably VAE and AAE. I suppose one could argue that the\nclose connection to existing methods means that this paper is not\ninnovative enough. I think that would be unfair - most new methods\nhave close relations with existing ones - it is just that sometimes\nthe authors do not flag this up as they should.\n\nWAE is a bit oversold. The authors state that WAE generates \"samples\nof better quality\" (than VAE) without any condition being put on when\nit does this. There is no proof that it is always better, and I can't\nsee how there could be. Any method of inferring a generative model\nfrom data must make some 'inductive' assumptions. Surely one could\ndevise situations where VAE outperforms WAE. I think this issue should\nhave been examined in more depth.\n\nI found no typo or grammatical errors which is unusual - good careful\njob!\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642459766, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/AnonReviewer2", "ICLR.cc/2018/Conference/Paper517/AnonReviewer1", "ICLR.cc/2018/Conference/Paper517/AnonReviewer3"], "reply": {"forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642459766}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642459820, "tcdate": 1511970931556, "number": 2, "cdate": 1511970931556, "id": "Hk2dO8ngz", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Review", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["ICLR.cc/2018/Conference/Paper517/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Excellent tutorial papers with novel contributions and convincing results", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This very well written paper covers the span between W-GAN and VAE. For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations. The appendix are very useful, and tutorial paper material (especially A). \n\nWhile I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work. There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous. One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y.\n\n\nThis work contains plenty of novel material, which is clearly compared to previous work:\n- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1. I could not verify its novelty, but this seems to be a great contribution.\n- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max\n- The use of MMD in the context of GANs has also been tried. The authors claim that their use in the latent space makes it more practival\n\nThe experiments are very convincing, both numerically and visually.\n\nSource of confusion: in algorithm 1 and 2, \\tilde{z} is \"sampled\" from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642459766, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/AnonReviewer2", "ICLR.cc/2018/Conference/Paper517/AnonReviewer1", "ICLR.cc/2018/Conference/Paper517/AnonReviewer3"], "reply": {"forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642459766}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642459783, "tcdate": 1512256147793, "number": 3, "cdate": 1512256147793, "id": "SJncf2gWz", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Review", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["ICLR.cc/2018/Conference/Paper517/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "A well-written paper that generalizes Wasserstein distance to VAEs ", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper provides a reasonably comprehensive generalization to VAEs and Adversarial Auto-encoders through the lens of the Wasserstein metric. By posing the auto-encoder design as a dual formulation of optimal transport, the proposed work supports the use of both deterministic and random decoders under a common framework. In my opinion, this is one of the crucial contributions of this paper. While the existing properties of auto-encoders are preserved, stability characteristics of W-GANs are also observed in the proposed architecture. The results from MNIST and CelebA datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward MMD metric and potentially discuss their pros and cons. In some sense, given the challenges in evaluating and comparing closely related auto-encoder solutions, the authors could design demonstrative experiments for cases where Wassersterin distance helps and may be  its potential limitations.\n\nThe closest work to this paper is the adversarial variational bayes framework by Mescheder et.al. which also attempts at unifying VAEs and GANs. While the authors describe the conceptual differences and advantages over that approach, it will be beneficial to actually include some comparisons in the results section.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642459766, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/AnonReviewer2", "ICLR.cc/2018/Conference/Paper517/AnonReviewer1", "ICLR.cc/2018/Conference/Paper517/AnonReviewer3"], "reply": {"forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642459766}}}, {"tddate": null, "ddate": null, "tmdate": 1513679235667, "tcdate": 1513678621985, "number": 4, "cdate": 1513678621985, "id": "BkU7vv8ff", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "SkxfpL8GG", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "Assumptions clarified", "comment": "Dear Mathieu,\n\nthank you for the suggestion. We will update the paper accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "tmdate": 1513676040473, "tcdate": 1513676040473, "number": 2, "cdate": 1513676040473, "id": "SkxfpL8GG", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["~Mathieu_Blondel1"], "readers": ["everyone"], "writers": ["~Mathieu_Blondel1"], "content": {"title": "Assumptions of Theorem 1 should be clarified", "comment": "Congratulations on this nice paper. \n\nThe ability to remove one of the two marginal constraints in Theorem 1 relies on the assumption that P_G(Y|Z=z) is a Dirac. I know that you stated in the intro that you focus on deterministic maps but it would be nice to repeat the assumptions made in Theorem 1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791684940, "id": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Area_Chair"], "cdate": 1512791684940}}}, {"tddate": null, "ddate": null, "tmdate": 1513373114249, "tcdate": 1513372980414, "number": 2, "cdate": 1513372980414, "id": "BJ2VpnZff", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "Hk2dO8ngz", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "Answer to AnonReviewer1", "comment": "We thank the reviewer for the positive feedback and the kind words regarding the overview part of the paper.\n\nWe will make sure to make notations clearer and include all the details of architectures used in experiments in the updated version of the paper. Of course we will also open source the code."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "tmdate": 1513373069341, "tcdate": 1513373069341, "number": 3, "cdate": 1513373069341, "id": "BkrqpnbGG", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "SJQzLO_gM", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "Answer to AnonReviewer2", "comment": "We are pleased that the reviewer found the paper well written. \n\nWe tried to be modest in our claims, in particular we never implied that WAEs produce better samples for *all data distributions*. As noticed by the reviewer this would be indeed impossible to prove, especially because the question of how to evaluate and compare sample qualities of unsupervised generative models is still open. We will double-check that there are no bold and unsupported statements in the final version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "tmdate": 1513372936746, "tcdate": 1513372936746, "number": 1, "cdate": 1513372936746, "id": "H1bGp3bfz", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "forum": "HkL7n1-0b", "replyto": "SJncf2gWz", "signatures": ["ICLR.cc/2018/Conference/Paper517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper517/Authors"], "content": {"title": "Answer to AnonReviewer3", "comment": "We thank the reviewer for the positive feedback. \n\nComparing properties of WAE-MMD and WAE-GAN is indeed an intriguing direction and we intend to look into the details in our future research. In this paper we only report initial empirical observations, which can be concluded by saying that WAE-MMD enjoys a stable training but does not match Pz and Qz perfectly, while the training of WAE-GAN is not so stable but leads to much better matches once succeeded. \n\nIn this paper we decided that comparing to VAE was sufficient for our purposes: both VAE and AVB follow the same objective of maximizing the marginal log likelihood in contrast to the minimization of the optimal transport studied in our work. However, we do agree that in future it would be interesting to compute the FID scores of the AVB samples. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732302, "id": "ICLR.cc/2018/Conference/-/Paper517/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper517/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper517/Authors|ICLR.cc/2018/Conference/Paper517/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper517/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper517/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper517/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732302}}}, {"tddate": null, "ddate": null, "tmdate": 1512314762407, "tcdate": 1512314762407, "number": 1, "cdate": 1512314762407, "id": "SkGcPcZ-z", "invitation": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "forum": "HkL7n1-0b", "replyto": "HkL7n1-0b", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Statement about the KL divergence term in VAEs", "comment": "You state in the paper that the variational auto-encoder objective is composed of reconstruction cost plus a KL divergence term that captures how distinct the image by the encoder of each training example is from the prior p(z), and then go on to say that this KL term is not guaranteeing that the overall encoded distribution matches the prior p(z).\n\nHowever, as shown in the paper \"ELBO surgery: yet another way to carve up the variational evidence lower bound\" by Hoffman and Johnson, the KL term in the VAE objective can be decomposed into exactly this KL(q(z)||p(z)) between the average encoder distribution and the prior plus a mutual information term, and that the former is a heavy contributor towards the overall KL term. This means that VAE does indeed try to match the overall encoder distribution of q to the prior, but also includes a regularizing term that aims to minimize the mutual information between the hidden code z and the index of the observation x that encourages the VAE to have the encoder produce the same codes z for different observations.\n\nIn conclusion, it would be more accurate to state that in comparison to VAEs you simply exclude the mutual information regularisation term from the objective as formulated in the ELBO surgery paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "pdf": "/pdf/8a046910519ed75fbd072e63725d9cf2814c1a6b.pdf", "TL;DR": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "paperhash": "tolstikhin|wasserstein_autoencoders", "_bibtex": "@inproceedings{\ntolstikhin2018wasserstein,\ntitle={Wasserstein Auto-Encoders},\nauthor={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HkL7n1-0b},\n}", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"], "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791684940, "id": "ICLR.cc/2018/Conference/-/Paper517/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkL7n1-0b", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper517/Authors", "ICLR.cc/2018/Conference/Paper517/Reviewers", "ICLR.cc/2018/Conference/Paper517/Area_Chair"], "cdate": 1512791684940}}}], "count": 22}