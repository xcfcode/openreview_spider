{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730158465, "tcdate": 1509137965386, "number": 1045, "cdate": 1518730158456, "id": "ryZERzWCZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "ryZERzWCZ", "original": "ryme0fZC-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260078028, "tcdate": 1517250136697, "number": 798, "cdate": 1517250136683, "id": "BJbD8ypBG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "ryZERzWCZ", "replyto": "ryZERzWCZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper provides a constrained mutual information objective function whose Lagrangian dual covers several existing generative models. However reviewers are not convinced of the significance or usefulness of the proposed unifying framework (at least from the way results are presented currently in the paper). Authors have not taken any steps towards revising the paper to address these concerns. Improving the presentation to bring out the significance/utility of the proposed unifying framework is needed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517090994996, "tcdate": 1511817231988, "number": 3, "cdate": 1511817231988, "id": "S1ufxZqlG", "invitation": "ICLR.cc/2018/Conference/-/Paper1045/Official_Review", "forum": "ryZERzWCZ", "replyto": "ryZERzWCZ", "signatures": ["ICLR.cc/2018/Conference/Paper1045/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Not clear what specific insights exist or what problem this solves", "rating": "4: Ok but not good enough - rejection", "review": "EDIT: I have read the authors' rebuttals and other reviews. My opinion has not been changed. I recommend the authors significantly revise their work, streamlining the narrative and making clear what problems and solutions they solve. While I enjoy the perspective of unifying various paths, it's unclear what insights come from a simple reorganization. For example, what new objectives come out? Or given this abstraction, what new perspectives or analysis is offered?\n\n---\n\nThe authors propose an objective whose Lagrangian dual admits a variety of modern objectives from variational auto-encoders and generative adversarial networks. They describe tradeoffs between flexibility and computation in this objective leading to different approaches. Unfortunately, I'm not sure what specific contributions come out, and the paper seems to meander in derivations and remarks that I didn't understand what the point was.\n\nFirst, it's not clear what this proposed generalization offers. It's a very nuanced and not insightful construction (eq. 3) and with a specific choice of a weighted sum of mutual informations subject to a combinatorial number of divergence measure constraints, each possibly held in expectation (eq. 5) to satisfy the chosen subclass of VAEs and GANs; and with or without likelihoods (eq. 7). What specific insights come from this that isn't possible without the proposed generalization?\n\nIt's also not clear with many GAN algorithms that reasoning with their divergence measure in the limit of infinite capacity discriminators is even meaningful (e.g., Arora et al., 2017; Fedus et al., 2017). It's only true for consistent objectives such as MMD-GANs.\n\nSection 4 seems most pointed in explaining potential insights.  However, it only introduces hyperparameters and possible combinatorial choices with no particular guidance in mind. For example, there are no experiments demonstrating the usefulness of this approach except for a toy mixture of Gaussians and binarized MNIST, explaining what is already known with the beta-VAE and infoGAN. It would be useful if the authors could make the paper overall more coherent and targeted to answer specific problems in the literature rather than try to encompass all of them.\n\nMisc\n+ The \"feature marginal\" is also known as the aggregate posterior (Makhzani et al., 2015) and average encoding distribution (Hoffman and Johnson, 2016); also see Tomczak and Welling (2017).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642379271, "id": "ICLR.cc/2018/Conference/-/Paper1045/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1045/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1045/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1045/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1045/AnonReviewer1"], "reply": {"forum": "ryZERzWCZ", "replyto": "ryZERzWCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642379271}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515661353053, "tcdate": 1511768815907, "number": 2, "cdate": 1511768815907, "id": "SkugmHtgf", "invitation": "ICLR.cc/2018/Conference/-/Paper1045/Official_Review", "forum": "ryZERzWCZ", "replyto": "ryZERzWCZ", "signatures": ["ICLR.cc/2018/Conference/Paper1045/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Good framework for learning generative models, but significance/consequence of the results is unclear", "rating": "5: Marginally below acceptance threshold", "review": "Update after rebuttal\n==========\nThanks for your response on my questions. The stated usefulness of the method unfortunately do not answer my worry about the significance. It remains unclear to me how much \"real\" difference the presented results would make to advance the existing work on generative models. Also, the authors did not promised any major changes in the final version in this direction, which is why I have reduced my score.\n\nI do believe that this work could be useful and should be resubmitted. There are two main things to improve. First, the paper need more work on improving the clarity. Second, more work needs to be added to show that the paper will make a real difference to advance/improve existing methods.\n\n==========\nBefore rebuttal\n==========\nThis paper proposes an optimization problem whose Lagrangian duals contain many existing objective functions for generative models. Using this framework, the paper tries to generalize the optimization problems by defining computationally-tractable family which can be expressed in terms of existing objective functions. \n\nThe paper has interesting elements and the results are original. The main issue is that the significance is unclear. The writing in Section 3 is unclear for me, which further made it challenging to understand the consequences of the theorems presented in that section. \n\nHere is a big-picture question that I would like to know answer for. Do the results of sec 3 help us identify a more useful/computationally tractable model than exiting approaches? Clarification on this will help me evaluate the significance of the paper.\n\nI have three main clarification points. First, what is the importance of T1, T2, and T3 classes defined in Def. 7, i.e., why are these classes useful in solving some problems? Second, is the opposite relationship in Theorem 1, 2, and 3 true as well, e.g., is every linear combination of beta-ELBO and VMI is equivalent to a likelihood-based computable-objective of KL info-encoding family? Is the same true for other theorems?\n\nThird, the objective of section 3 is to show that \"only some choices of lambda lead to a dual with a tractable equivalent form\". Could you rewrite the theorems so that they truly reflect this, rather than stating something which only indirectly imply the main claim of the paper.\n\nSome small comments:\n- Eq. 4. It might help to define MI to remind readers.\n- After Eq. 7, please add a proof (may be in the Appendix). It is not that straightforward to see this. Also, I suppose you are saying Eq. 3 but with f from Eq. 4.\n- Line after Eq. 8, D_i is \"one\" of the following... Is it always the same D_i for all i or it could be different? Make this more clear to avoid confusion.\n- Last line in Para after Eq. 15, \"This neutrality corresponds to the observations made in..\" It might be useful to add a line explaining that particular \"observation\"\n- Def. 7, the names did not make much sense to me. You can add a line explaining why this name is chosen.\n- Def. 8, the last equation is unclear. Does the first equivalence impy the next one? \n- Writing in Sec. 3.3 can be improved. e.g., \"all linear operations on log prob.\" is very unclear, \"stated computational constraints\" which constraints?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642379271, "id": "ICLR.cc/2018/Conference/-/Paper1045/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1045/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1045/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1045/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1045/AnonReviewer1"], "reply": {"forum": "ryZERzWCZ", "replyto": "ryZERzWCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642379271}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642379380, "tcdate": 1511717117835, "number": 1, "cdate": 1511717117835, "id": "BJ8bKuOlM", "invitation": "ICLR.cc/2018/Conference/-/Paper1045/Official_Review", "forum": "ryZERzWCZ", "replyto": "ryZERzWCZ", "signatures": ["ICLR.cc/2018/Conference/Paper1045/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Contains some interesting results but the presentation is not focused", "rating": "6: Marginally above acceptance threshold", "review": "Thank you for the feedback, I have read it.\n\nI do think that developing unifying frameworks is important. But not all unifying perspective is interesting; rather, a good unifying perspective should identify the behaviour of existing algorithms and inspire new algorithms.\n\nIn this perspective, the proposed framework might be useful, but as noted in the original review, the presentation is not clear, and it's not convincing to me that the MI framework is indeed useful in the sense I described above.\n\nI think probably the issue is the lack of good evaluation methods for generative models. Test-LL has no causal relationship to the quality of the generated data. So does MI. So I don't think the argument of preferring MI over MLE is convincing.\n\nSo in summary, I will still keep my original score. I think the paper will be accepted by other venues if the presentation is improved and the advantage of the MI perspective is more explicitly demonstrated.\n\n==== original review ====\n\nThank you for an interesting read.\n\nThe paper presented a unifying framework for many existing generative modelling techniques, by first considering constrained optimisation problem of mutual information, then addressing the problem using Lagrange multipliers.\n\nI see the technical contribution to be the three theorems, in the sense that it gives a closure of all possible objective functions (if using the KL divergences). This can be useful: I'm tired of reading papers which just add some extra \"regularisation terms\" and claim they work. I did not check every equation of the proof, but it seems correct to me.\n\nHowever, an imperfection is, the paper did not provide a convincing explanation on why their view should be preferred compared to the original papers' intuition.  For example in VAE case, why this mutual information view is better than the traditional view of approximate MLE, where q is known to be the approximate posterior? A better explanation on this (and similarly for say infoGAN/infoVAE) will significantly improve the paper.\n\nContinuing on the above point, why in section 4 you turn to discuss relationship between mutual information and test-LL?  How does that relate to the main point you want to present in the paper, which is to prefer MI interpretation if I understand it correctly?\n\nTerm usage: we usually *maximize* the ELBO and *minimise* the variational free-energy (VFE). ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642379271, "id": "ICLR.cc/2018/Conference/-/Paper1045/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1045/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1045/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1045/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1045/AnonReviewer1"], "reply": {"forum": "ryZERzWCZ", "replyto": "ryZERzWCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642379271}}}, {"tddate": null, "ddate": null, "tmdate": 1515194342224, "tcdate": 1515194342224, "number": 5, "cdate": 1515194342224, "id": "B1A1_t67z", "invitation": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "forum": "ryZERzWCZ", "replyto": "SJ2PZA-XM", "signatures": ["ICLR.cc/2018/Conference/Paper1045/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1045/Authors"], "content": {"title": "Solution by Bounding Mutual Information", "comment": "Thank you for your comment. The solution proposed in our paper is to bound the mutual information rather than direct optimization of the Lagrangian multipliers. Direct maximization would lead to maximizing it to infinity for infeasible problems. Our experiments show that bounding the mutual information can solve the problem: as soon as mutual information reaches the preset bound, log likelihood starts to improve."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825724517, "id": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryZERzWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1045/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1045/Reviewers", "ICLR.cc/2018/Conference/Paper1045/Authors", "ICLR.cc/2018/Conference/Paper1045/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825724517}}}, {"tddate": null, "ddate": null, "tmdate": 1514426723849, "tcdate": 1514426723849, "number": 4, "cdate": 1514426723849, "id": "SJ2PZA-XM", "invitation": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "forum": "ryZERzWCZ", "replyto": "BycXcabXz", "signatures": ["ICLR.cc/2018/Conference/Paper1045/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1045/AnonReviewer3"], "content": {"title": "thank you for your feedback", "comment": "Thank you for your feedback.\n\nCould you add experiments that optimises the Lagrange multiplier as well? It would help strengthen the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825724517, "id": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryZERzWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1045/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1045/Reviewers", "ICLR.cc/2018/Conference/Paper1045/Authors", "ICLR.cc/2018/Conference/Paper1045/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825724517}}}, {"tddate": null, "ddate": null, "tmdate": 1514424865958, "tcdate": 1514424865958, "number": 3, "cdate": 1514424865958, "id": "BycXcabXz", "invitation": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "forum": "ryZERzWCZ", "replyto": "BJ8bKuOlM", "signatures": ["ICLR.cc/2018/Conference/Paper1045/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1045/Authors"], "content": {"title": "Clarification on Main Concerns", "comment": "We thank the reviewers for their time and valuable feedback. \n\n\u201cHowever, an imperfection is, the paper did not provide a convincing explanation on why their view should be preferred compared to the original papers' intuition.\u201d  For example in VAE case, why this mutual information view is better than the traditional view of approximate MLE, where q is known to be the approximate posterior? A better explanation on this (and similarly for say infoGAN/infoVAE) will significantly improve the paper. Continuing on the above point, why in section 4 you turn to discuss relationship between mutual information and test-LL?  How does that relate to the main point you want to present in the paper, which is to prefer MI interpretation if I understand it correctly?\u201c\n\nOur view (optimize mutual information under distribution matching constraint) provides several understandings traditional perspectives do not provide. First, several attributes of an objective are revealed by the Lagrangian form: information preference, possible optimization methods (likelihood based or likelihood free), closure (most generic form) of model family, etc. In addition Section 4 proceeds to demonstrate two applications where the Lagrangian perspective reveal problems/features that are difficult to identify from traditional perspectives.  \n\n\n1.Correct optimization of the Lagrangian dual requires maximization over the Lagrangian parameters. However, all existing methods use fixed (arbitrarily chosen) Lagrangian parameters. We show failure cases where this does not correctly optimize the primal problem. For example, when the primal objective is information maximization under constraints of distributional consistency, optimization with fixed Lagrangian parameters can maximize mutual information indefinitely without ever encouraging distributional consistency. As a result, data fit (distributional consistency) may even get worse during training (for example, resulting in lower test log likelihood) as mutual information is maximized. We show that this also happens in practice. \n\n2.The Lagrangian perspective allows us to explicitly weight (\u201cprice\u201d) different (conflicting) terms in the objective. For example, suppose the input x has more dimensions than the feature space z. Then for the same per-dimension loss, the input space is weighted more than the latent space (because it has more dimensions). We show in the paper that increasing the weight on matching marginals on z can solve the problem and leads to better performance. In general, we can write out the desired preference in Lagrangian form, and then convert it into a familiar model and optimization method (in our example, this corresponds to InfoVAE with a specific hyper-parameter choice.)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825724517, "id": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryZERzWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1045/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1045/Reviewers", "ICLR.cc/2018/Conference/Paper1045/Authors", "ICLR.cc/2018/Conference/Paper1045/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825724517}}}, {"tddate": null, "ddate": null, "tmdate": 1514424801689, "tcdate": 1514424801689, "number": 2, "cdate": 1514424801689, "id": "rkckcaWXM", "invitation": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "forum": "ryZERzWCZ", "replyto": "SkugmHtgf", "signatures": ["ICLR.cc/2018/Conference/Paper1045/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1045/Authors"], "content": {"title": "Clarification on the Main Concerns", "comment": "We thank the reviewers for their time and valuable feedback. \n\n\u201cThe main issue is that the significance is unclear.\u201d\n\nBeyond providing an organizational principle for learning objectives (highlighting their information maximization/minimization properties and trade-offs between computational requirements and flexibility) our new perspective is useful for several reasons (Sections 3 and 4):\n\n1. We are able to characterize **all** learning objectives that can be optimized under given computational constraints (likelihood based optimization; unary likelihood free optimization; binary likelihood free optimization) providing a \u201cclosure\u201d result. Even though we do not introduce a new learning objective, we show that (slightly generalized versions) of ten (already known) \u201cbase classes\u201d encompass all possible objectives in each category. Therefore, in a certain sense, we show that there do not exist \u201cnew\u201d objectives under our stated assumptions on how objectives can be constructed. \n\n2. We show that several known problems are revealed by the Lagrangian perspective and hold across the entire model family: \n\na. Correct optimization of the Lagrangian dual requires maximization over the Lagrangian parameters. However, all existing methods use fixed (arbitrarily chosen) Lagrangian parameters. We show failure cases where this does not correctly optimize the primal problem. For example, when the primal objective is information maximization under constraints of distributional consistency, optimization with fixed Lagrangian parameters can maximize mutual information indefinitely without ever encouraging distributional consistency. As a result, data fit (distributional consistency) may even get worse during training (for example, resulting in lower test log likelihood) as mutual information is maximized. We show that this also happens in practice. \n\nb. The Lagrangian perspective allows us to explicitly weight (\u201cprice\u201d) different (conflicting) terms in the objective. For example, suppose the input x has more dimensions than the feature space z. Then for the same per-dimension loss, the input space is weighted more than the latent space (because it has more dimensions). We show in the paper that increasing the weight on matching marginals on z can solve the problem and leads to better performance. In general, we can write out the desired preference in Lagrangian form, and then convert it into a familiar model and optimization method (in our example, this corresponds to InfoVAE with a specific hyper-parameter choice.)\n\n\u201cWhat is the importance of T1, T2, and T3 classes defined in Def. 7, i.e., why are these classes useful in solving some problems?\u201d\n\nIt has been observed experimentally that T1 T2 and T3 are increasingly more challenging in terms of optimization stability, sensitivity to hyper-parameters, and outcome of optimization (Arjovsky et al., 2017). In particular, T1 (likelihood based, e.g. VAE) is highly stable and converges quickly, while T2/T3 methods (such as GANs) suffer from issues such as optimization stability, non-convergence. T3 is slightly more challenging than T2 because BiGAN/ALI (Dumoulin et al., 2016a; Donahue et al., 2016) tend to suffer from inaccurate inference. \n\n\u201cIs the opposite relationship in Theorem 1, 2, and 3 true as well, e.g., is every linear combination of beta-ELBO and VMI is equivalent to a likelihood-based computable-objective of KL info-encoding family? Is the same true for other theorems?\u201d\n\n\nYes the opposite relationship is true as well. The existing objectives enumerated in Theorem 1, 2, 3 are exactly equivalent to T1/T2/T3 computably objectives respectively.\n\n\u201cThird, the objective of section 3 is to show that only some choices of lambda lead to a dual with a tractable equivalent form. Could you rewrite the theorems so that they truly reflect this, rather than stating something which only indirectly imply the main claim of the paper.\u201d\n\nThe statement we supported with Theorems 1/2/3 is: only some parameters choices lead to objectives in each computability (T1/T2/T3) classes (easy vs hard to optimize). For example, only parameters choices that correspond to beta-VAE/VMI can have a likelihood-based computable equivalent form. Most objectives cannot be equivalently transformed to become a likelihood based computable objective. We have revised the paper to make the statement more clear. \n\n\u201cSome small comments\u201d\n\nThank you. We have revised the writing according to the advice. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825724517, "id": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryZERzWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1045/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1045/Reviewers", "ICLR.cc/2018/Conference/Paper1045/Authors", "ICLR.cc/2018/Conference/Paper1045/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825724517}}}, {"tddate": null, "ddate": null, "tmdate": 1514320769673, "tcdate": 1514320769673, "number": 1, "cdate": 1514320769673, "id": "HJqtmElmf", "invitation": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "forum": "ryZERzWCZ", "replyto": "S1ufxZqlG", "signatures": ["ICLR.cc/2018/Conference/Paper1045/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1045/Authors"], "content": {"title": "Clarification on Significance", "comment": "We thank the reviewers for their time and valuable feedback. \n\n\u201cIt would be useful if the authors could make the paper overall more coherent and targeted to answer specific problems in the literature rather than try to encompass all of them.\u201d\n\nWe respectfully disagree. We strongly believe that identifying connections between existing methods and developing general frameworks and theories that encompass as many existing methods as possible is a fundamental scientific goal. Machine learning research is not only about developing new methods and beating benchmarks, but also achieving a deeper understanding of the strengths, weaknesses, and relationships of existing techniques. \n\n\n\u201cWhat specific insights come from this that isn't possible without the proposed generalization?\u201d\n\nBeyond providing an organizational principle for learning objectives (highlighting their information maximization/minimization properties and trade-offs between computational requirements and flexibility) our new perspective is useful for several reasons:\n\n1. We are able to characterize **all** learning objectives that can be optimized under given computational constraints (likelihood based optimization; unary likelihood free optimization; binary likelihood free optimization) providing a \u201cclosure\u201d result. Even though we do not introduce a new learning objective, we show that (slightly generalized versions) of ten (already known) \u201cbase classes\u201d encompass all possible objectives in each category. Therefore, in a certain sense, we show that there do not exist \u201cnew\u201d objectives under our stated assumptions on how objectives can be constructed. \n\n2. We show that several problems are revealed by the Lagrangian perspective and hold across the entire model family: \n\na. Correct optimization of the Lagrangian dual requires maximization over the Lagrangian parameters. However, all existing methods use fixed (arbitrarily chosen) Lagrangian parameters. We show failure cases where this does not correctly optimize the primal problem. For example, when the primal objective is information maximization under constraints of distributional consistency, optimization with fixed Lagrangian parameters can maximize mutual information indefinitely without ever encouraging distributional consistency. As a result, data fit (distributional consistency) may even get worse during training (for example, resulting in lower test log likelihood) as mutual information is maximized. We show that this also happens in practice. \n\nb. The Lagrangian perspective allows us to explicitly weight (\u201cprice\u201d) different (conflicting) terms in the objective. For example, suppose the input x has more dimensions than the feature space z. Then for the same per-dimension loss, the input space is weighted more than the latent space (because it has more dimensions). We show in the paper that increasing the weight on matching marginals on z can solve the problem and leads to better performance. In general, we can write out the desired preference in Lagrangian form, and then convert it into a familiar model and optimization method (in our example, this corresponds to InfoVAE with a specific hyper-parameter choice.)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling", "abstract": "A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.", "pdf": "/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf", "paperhash": "zhao|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling", "_bibtex": "@misc{\nzhao2018the,\ntitle={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\nauthor={Shengjia Zhao and Jiaming Song and Stefano Ermon},\nyear={2018},\nurl={https://openreview.net/forum?id=ryZERzWCZ},\n}", "keywords": ["Generative Models", "Variational Autoencoder", "Generative Adversarial Network"], "authors": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "authorids": ["sjzhao@stanford.edu", "tsong@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825724517, "id": "ICLR.cc/2018/Conference/-/Paper1045/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryZERzWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1045/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1045/Authors|ICLR.cc/2018/Conference/Paper1045/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1045/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1045/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1045/Reviewers", "ICLR.cc/2018/Conference/Paper1045/Authors", "ICLR.cc/2018/Conference/Paper1045/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825724517}}}], "count": 10}