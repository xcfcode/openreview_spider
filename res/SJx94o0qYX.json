{"notes": [{"id": "SJx94o0qYX", "original": "Byl-ArsdF7", "number": 23, "cdate": 1538087729643, "ddate": null, "tcdate": 1538087729643, "tmdate": 1545355427984, "tddate": null, "forum": "SJx94o0qYX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJeGiK44xN", "original": null, "number": 1, "cdate": 1544993178412, "ddate": null, "tcdate": 1544993178412, "tmdate": 1545354488056, "tddate": null, "forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Meta_Review", "content": {"metareview": "The submission proposes a strategy for quantization of neural networks with skip connections that quantizes only the convolution paths, while leaving the skip paths at full precision.  The approach can save computation through compressing the convolution kernels, while spending more on the skip connections.\nEmpirical results show improved performance at 2-bit quantization compared to a handful of competing methods.  Figure 5 provides some interpretation of why the method might be working in terms of \"smoothness\" of the loss surface (term not used in the traditional mathematical sense).\n\nThe paper seems to focus too much on selling the name \"precision highway\" rather than providing proper definitions of their strategy (a definition block would be a good first step), and there is little mathematical analysis of the consequences of the chosen approach.\nThere are concerns about the novelty of the method, specifically compared to Liu et al. (2018) and Choi et al. (2018b), which propose approximately the same strategy.  Footnote 1 claims that these works were conducted in parallel with the current submission, but it is unambiguously the case that Choi et al appeared on arXiv in May, and Liu et al. appeared in ECCV 2018 and on arXiv more than 30 days before the ICLR deadline, and can fairly be considered prior work https://iclr.cc/Conferences/2019/Reviewer_Guidelines\n\nThe reviewer scores were on aggregate borderline for the ICLR acceptance threshold.  On the balance, the paper seems to fall under the threshold due to insufficient novelty and analysis of the method.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Area chair recommendation"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper23/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353364247, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper23/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper23/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper23/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353364247}}}, {"id": "BklpLk1EJV", "original": null, "number": 3, "cdate": 1543921493497, "ddate": null, "tcdate": 1543921493497, "tmdate": 1543921493497, "tddate": null, "forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Public_Comment", "content": {"comment": "\nThanks to authors for the responses to my question. Here's my rebuttal to your answers.\n\n- It seems to be clear that the concept of Precision Highway for LSTM (at least for the LSTM cells) is very similar to the conventional quantization approaches, if not identical. In fact, the first answer of the authors alludes to their confusion in understanding [He et al., 2016]; they said \"the quantization is applied to all the outputs of tanh and sigmoid activation functions\", which is wrong. The authors claim that there is a slight difference in the decoding layer, but it is not sure how important it is since it is not discussed at all in the submitted paper but the authors brought up only at the comment.\n\n- The authors also claim Precision Highway for GRU as their novel contribution, but without thorough demonstration. To claim novelty, it should provide in-depth ablation study on it.\n\n- Similarly, the authors claim that Precision Highway for \"post-activation\" ResNet is a novel contribution. But as the authors admitted, it is an addition to the existing concept in Liu et al. (2018) and Choi et al. (2018b) on \"pre-actication ResNet (and thus the authors did not even demonstrate \"pre-activation\" in their paper). It would be more interesting to see the application of Precision Highway for drastically different network structures, such as ResNext or DenseNet, but the authors conveniently postpone it as a future work.\n\n- The key claim of the authors about novelty is an \"explicit\" consideration of the end-to-end highway precision, which implies that this concept is already presented but they are revisiting explicitly. To differentiate itself enough, the readers would expect in-depth analysis of why Precision Highway is working, with theoretical proof or rigorous numerical analysis. \n\nBut the analysis presented in this paper is VERY naive. The authors explained that their choice of high-precision skip connection is to reduce overhead, not because of in-depth theoretical study. The overhead reduction is already the motivation of prior work, Liu et al. (2018) and Choi et al. (2018b). The authors don't show theoretically why high-precision in skip-connection is essential; even in their answer, they just showed a table of convergence curve, which is another empirical outcome of Precision Highway, not the reason of why this works. \n\nThe authors also responded back the question about the loss surface, but their evaluation is very subjective. They said \"the surface of our method is much smoother\", but how much smooth is good? Does this guarantee any convergence property? The authors said, \"the sharpness very near the local minimum is important\". But why? (why sharpness very near the local minimum is more important than sharpness \"not very near\" the local minimum?)\n\n\n- After all, this paper seems to be premature; 1) the main concept of Precision Highway is borrowed from prior works, 2) extension of the idea is very misleading (e.g., misunderstanding on prior LSTM quant) or weak (GRU or \"pre-activation\" ResNet without demonstration), and 3) the analysis is very naive (does not explain high precision in shortcut is essential). It is true that Precision Highway is an important topic, but the way this paper addresses it currently is quite concerning as the readers expect more in-depth research on it. \n\n\n\n", "title": "Rebuttal to the authors' responses"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311936989, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SJx94o0qYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311936989}}}, {"id": "Byx2RgOZ1E", "original": null, "number": 7, "cdate": 1543762131953, "ddate": null, "tcdate": 1543762131953, "tmdate": 1543762131953, "tddate": null, "forum": "SJx94o0qYX", "replyto": "r1g8RtfxyE", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "content": {"title": "Clarification of the answer ", "comment": "\nWe really appreciate the comments. It helped clarify our contribution with respect to [He, et al.]. In summary, our method is different from [He, et al.] by exploiting the precision highway in (1) fine-tuning for LSTM and (2) improving the cell structure for GRU as follows.\n\nIn LSTM, both [He, et al.] and our methods have the same inter/intra-time step operation as mentioned in the new comments. However, We exploited the precision highway in training the quantized LSTM model as follows. First, we trained it in full precision. After then, we quantized the LSTM model incrementally in multiple steps. During one of the step, we quantized only the LSTM cells while keeping, in high precision, the rest of the network, and applied fine-tuning. We exploited high-precision h_t by not quantizing the output of LSTM cells which is the input of the decoder layer. It is different from [He, et al.] since the un-quantized h_t is provided to the decoder layer. This training pipeline with the high-precision h_t offers significant improvements in accuracy compared with the conventional quantization where high-precision h_t is not exploited. We will clarify this in the final version of the paper.\n\nSecond, in GRU, our precision highway is structurally different from [He, et al.]. It is because h_t is quantized in [He, et al.] while our precision highway keeps it in high precision, which makes us form the end-to-end high-precision flow across time steps. We will prepare new experiments of quantizing GRU models to compare [He, et al.] and ours and present them in the final version of the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615920, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJx94o0qYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper23/Authors|ICLR.cc/2019/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615920}}}, {"id": "r1g8RtfxyE", "original": null, "number": 2, "cdate": 1543674317870, "ddate": null, "tcdate": 1543674317870, "tmdate": 1543678823931, "tddate": null, "forum": "SJx94o0qYX", "replyto": "HkxdNLHaCm", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Public_Comment", "content": {"comment": "\nNote that in page 7 of [He et al., 2016]'s paper, it is clearly mentioned that only xt, W, ht (which will become an operand of matmul) are quantized, as highlighted in this capture (https://www.dropbox.com/s/20zr3wklqtrco18/tmp.png?dl=0). \n\nThis way of quantization is identical to what is shown in Fig 2(b): ht and xt are stacked and quantized, then multiplied with quantized W. Thus this work already shows the impact of \"end to end flow\" \n\n(Rebuttals to the other answers will soon be followed.)", "title": "Authors' answer to 1.a) seems to be wrong"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311936989, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SJx94o0qYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311936989}}}, {"id": "Hkx1PUS6AQ", "original": null, "number": 6, "cdate": 1543489111513, "ddate": null, "tcdate": 1543489111513, "tmdate": 1543489111513, "tddate": null, "forum": "SJx94o0qYX", "replyto": "HkxdNLHaCm", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "content": {"title": "The response for the anonymous reviewer - 2", "comment": "\n2.a) \nWe think there are two issues to address here, why to select e in the two error terms in Eq (1) and the convergence of training. \n\nRegarding the selection, we chose to remove the error term e in Eq (1), i.e., keep the skip connection in high precision in order to reduce the overhead of high precision. The computation overhead of keeping high precision on precision highway is small as mentioned in Section 3.2 as follows.\n\n\u201cthe overhead of this high-precision element-wise multiplications is negligible compared with the matrix multiplication\u201d \n\n\nWe also reported the energy overhead of precision highway in the experiments of Section 5.5 as follows.\n\n\u201cIn the 2-bit case where the overhead of precision highway is the largest, the precision highway incurs only 3.9 % additional energy consumption due to the high-precision data while offering 4.1 % better accuracy than the case that precision highway is not adopted.\u201d\n\n\nRegarding the second issue of convergence in training, we have an evidence that precision highway gives better convergence (i.e., validation error vs epochs) during training. The following table shows the additional validation error of ResNet-50 (with respect to the full precision model) obtained during training for the 2-bit quantization. Note that, we reduce bit precision every 15 epochs by following gradual quantization [Zhuang et al.], and adopt knowledge distillation with a teacher of ResNet-101. Thus, the negative error, i.e., better accuracy than the full precision ResNet-50 model, is due to the teacher effect. As the table shows, the precision highway consistently outperforms No Highway during the entire training. More importantly, it offers better results as the training continues, e.g., 0.72% at 45 epochs to 1.86% at 150 epochs. We will include these results into the final version of the paper.\n\n# epochs           15       30       45        60       75      90     105    120      135    150      \nHighway         -0.61  -0.84   -0.74    -0.21   1.60   1.30   1.12   1.20     1.59   2.45   \nNo highway    0.34   -0.32   -0.02     0.61   3.41   2.85   2.48   2.75     3.13   4.31\n\n\n2. b)\nWe think sharpness is a rather relative concept and the sharpness very near the local minimum is important. In the figure, we wanted to demonstrate that, at the local minimum, i.e., zero point in the figure, the loss surface of our method is much smoother than that of the existing method. We will clarify this in the final version of the paper.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615920, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJx94o0qYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper23/Authors|ICLR.cc/2019/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615920}}}, {"id": "HkxdNLHaCm", "original": null, "number": 5, "cdate": 1543489072282, "ddate": null, "tcdate": 1543489072282, "tmdate": 1543489072282, "tddate": null, "forum": "SJx94o0qYX", "replyto": "r1gEj8y3AQ", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "content": {"title": "The response for the anonymous reviewer", "comment": "\nWe appreciate the comments and, for each issue, present our response as follows\n\n1.a)\nIn the followings, we present our response to this issue for each related paper mentioned in the comments.\nAccording to the following excerpt from [He et al.], the quantization is applied to all the outputs of tanh and sigmoid activation functions including Ht. Only Ct is not quantized just in order to avoid clipping.\n\n\u201cDifferent from GRU, Ct can not be easily quantized, since the value is unbounded by not using activation function like tanh and the sigmoid function. This difficulty comes from structure design and can not be alleviated without introducing extra facility to clip value ranges. But it can be noted that the computations involving Ct are all element-wise multiplications and additions, which may take much less time than computing matrix products. For this reason, we leave Ct to be in floating point form.\u201d\n\nIn case of GRU, all internal signals including Ct are quantized. Thus, we think the method in [He et al.] was designed without considering the effect of end-to-end high precision flow. \n\nOur work is conceptually different from [He et al.] in that ours shows that the end-to-end flow is critical in low precision and, thus, aims at keeping signals on the end-to-end flow in high precision while [He et al.] applies quantization to all the outputs of activation functions without considering the importance of end-to-end flow. In addition, we also present an improved weight quantization based on Laplace distribution model to offer further improvements.\n\n\nIn [Kapur et al., 2017], the authors mention that they followed [He et al.] as shown in the following excerpt.\n \u201cWe use a quantization method very similar to that proposed in [6]\u201d \n\n\nIn [Hubara et al., 2016], which is one of the first papers on binary quantization, the authors did not apply the notion of end-to-end high precision flow since they used AlexNet and GoogLeNet (without skip connection). Regarding the RNN, they quantized both activations and weights as shown in the following excerpt.\n\n\u201cHere we report on the first attempt to quantize both weights and activations by trying to evaluate the accuracy of quantized recurrent models trained on the Penn Treebank dataset.\u201d\n\nWe can also guess that they did not apply the concept of end-to-end high precision flow to the RNN since their quantized RNNs suffer from significant accuracy loss.\n\n\nIn [Shin et al., 2016], the authors clarified that they did not apply the notion of end-to-end high precision flow according to the following excerpt. \n\n\u201cThe output ranges of the sigmoid and the tanh are limited by 0 to 1 and -1 to 1, respectively. The quantization step size \u2206 is determined by the quantization level M. For example, if the signal word-length is two bits (M is four), the quantization points are 0/3, 1/3, 2/3, and 3/3 for the sigmoid and -1/1, 0/1, and 1/1 for the tanh. However signals of linear units are not bounded and their quantization range should be determined empirically. In our phoneme recognition example, each component of the input data is normalized to have zero mean and a unit variance over the training set. The input range is chosen to be from -3 to 3. One hot encoding is used for the input linear units in the language model example.\u201d \n\nThey applied quantization to all the signals, even the inputs.\n\n\nIn [Lee et al., 2016], the authors mentioned that all the weights and activations are quantized to 8 and 6 bits as shown in the following excerpt. \n\n\u201cIn our work, the weights and the internal signals are quantized to 6 and 8 bits, respectively.\u201d \n\nSince they quantized all the internal signals of RNN, we think they did not apply the notion of end-to-end high precision flow. \n\nIn summary, the papers mentioned in the comments did not apply the notion of end-to-end high precision flow in the quantization. \n\n\n1.b)\nWe think considering the notion of end-to-end high precision flow on both pre and post-activation makes a difference that ours can also be applied to other types of networks including LSTM and GRU as well as CNNs with skip connections. That is why we could extend our idea to LSTM networks as shown in the paper.\n\nWe\u2019d like to stress again that, compared with other works mentioned in the comments, an \u2018explicit\u2019 consideration of end-to-end high precision information flow is our key difference, which enables very low-precision quantization in RNNs (LSTMs and GRUs) as well as CNNs (with skip connections).\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615920, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJx94o0qYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper23/Authors|ICLR.cc/2019/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615920}}}, {"id": "r1gEj8y3AQ", "original": null, "number": 1, "cdate": 1543399068137, "ddate": null, "tcdate": 1543399068137, "tmdate": 1543399161701, "tddate": null, "forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Public_Comment", "content": {"comment": "This paper has serious concerns about novelty in its contributions. Although the authors already acknowledged that \u201cprecision highway\u201d is not a new idea, they claimed that their main contributions \u2013 generalization of the idea to post-activation Resnet and LSTM as well as the in-depth analysis \u2013 are significant. However, their generalization is almost trivial or already done before, and their analysis is misleading or contradicting to the previous well-established analysis.\n\n1) Lack of novelty in the claims about generalization\n1.a) The authors claim that applying \u201cprecision highway\u201d for RNNs is their novel contribution. But what it does is simply quantizing only inputs of matrix multiplication and not quantizing inputs to the cell-state (ct) in LSTM, which is the de-facto standard quantization scheme for LSTM inference. To name a few, [Hubara et al., 2016], [He et al., 2016b]* and [Kapur et al., 2017] quantize only input (weight and activation) of matrix multiplication, and [Shin et al., 2016] and [Lee et al., 2016] even have some preliminary study/observation on the sensitivity of cell-state to the quantization. \n* Note that [He et al., 2016b] is already cited in this paper for accuracy comparison. This comparison is very misleading; the authors claim that their work achieves \u201cmuch better\u201d accuracies than [He et al., 2016b] since \u201cprecision highway provides more gain for a more aggressive quantization\u201d. But it is clearly specified in [He et al., 2016] that only inputs to the matrix multiplication (i.e., W, ht, xt) are quantized (i.e., it also uses \u201cprecision highway\u201d!). \n[Hubara et al., 2016]: https://arxiv.org/pdf/1609.07061.pdf\n[He et al., 2016b]: https://arxiv.org/pdf/1611.10176.pdf\n[Kapur et al., 2017]: https://arxiv.org/pdf/1710.07706.pdf\n[Shin et al., 2016]: https://arxiv.org/pdf/1512.01322.pdf\n[Lee et al., 2016]: https://arxiv.org/pdf/1610.00552.pdf\n\n1.b) The authors also claim that the idea of using full-precision data for the skip-connections has been demonstrated only for \u201cpre-activation\u201d Resnet, thus extending it to \u201cpost-activation\u201d is a significant progress. But there is little innovation added in this extension; the principle of not quantizing data used in the skip connection is the same for both \u201cpre-\u201c and \u201cpost-\u201c activation, and there\u2019s little additional care needed than following the same principle to identify the location of quantization. \n\n2) Misleading or contradicting outcome from the quantitative analysis\n2.a) Analysis of accumulated quantization error is based on the comparison of eq(1) and eq(2). But this is a mere observation that not quantizing skip-connection can remove one of the error terms, leading to smaller quantization error. In other words, this analysis is mainly focusing on the number of quantized elements. (If the number of error terms is what matters most, one can think of not quantizing other parts of the network, e.g., one of the residual passes, so that overall quantization error is further reduced.) Fig 4 shows a natural outcome; quantizing less number of data elements results in smaller quantization error. Unfortunately, this analysis does not shed light on understanding which of the error terms -- error in the residual-path (e_r) vs the skip-connection (e) -- influences more on the convergence of training. \n\n2.b) The second analysis is based on the visualization of the loss function, but the interpretation of the plots is contradicting to the well-established analysis. The authors claim that \u201cprecision highway gives better loss surface\u201d so that the proposed method would help SGD to \u201cquickly converge\u201d and offer \u201cbetter accuracy\u201d. However, in Fig 5 (d), \u201cwith PH\u201d shows sharper loss curve than \u201cw/o PH\u201d, which (according to [Li et al., 2018]) would indicate sharper minima. There are many analysis papers such as [Keskar et al. 2017] that proposed a strong correlation between sharp minima and the generalization gap leading to accuracy degradation. Based on these papers, therefore, the plots can be interpreted as \"with PH\" might suffer larger generalization gap than \"w/o PH\" due to sharper minima, which is conflicting with the authors' interpretation. \n[Li et al., 2018] http://arxiv.org/abs/1609.04836\n[Keskar et al. 2017] http://arxiv.org/abs/1609.04836\n\nAlthough there are several pros in this paper, such as clarification of the location of quantization for \u201cpost-activation\u201d Resnets, and the ablation study on the impact of \u201chighway precision\u201d with normal and wide Resnets, the above concerns on the major contributions of this paper raise a question if it has sufficient novelty for ICLR publication.  ", "title": "Serious concerns about novelty of this paper"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311936989, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SJx94o0qYX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311936989}}}, {"id": "HkeWxbrxR7", "original": null, "number": 2, "cdate": 1542635753418, "ddate": null, "tcdate": 1542635753418, "tmdate": 1542785794863, "tddate": null, "forum": "SJx94o0qYX", "replyto": "BylJ0G-03X", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "content": {"title": "The response for Reviewer 2", "comment": "\n1. The answer to question 1\n\nWhen the \u201chighway\u201d box is unchecked, the skip connection is branched after the quantization. This case corresponds to the conventional quantization where the quantization is combined with ReLU and, thus, skip connection is quantized before the branch. When the \u201cteacher\u201d box is unchecked, we use the conventional cross-entropy loss for training. We will clarify this in the revision.\n\n\n\n2. The answer to question 2\n\nWe re-implemented Zhuang\u2019s baseline, but final accuracy is different due to the minor difference of implementation details on input augmentation and teacher-student methods. According to Zhuang\u2019s paper, their implementation shows 70.8 /88.3 % of Top-1/Top-5 accuracy for 2-bit ResNet-50, while our implementation shows 70.48 / 89.93 % of Top1-/Top-5 accuracy. \n\n\n\n3. The comments about Figure 4\n\nWe appreciate the comments. We\u2019d like to first explain how we had obtained Figure 4 and how we performed again new experiments to clarify the phenomenon of accumulated quantization error in the revision. \nIn order to obtain Figure 4, we first obtained a fully trained full-precision network. Then, we applied 4-bit weight/activation quantization to the network while having two cases of skip connection, 4-bit one (Zhuang\u2019s in the figure) and 32-bit one (Proposed in the figure). Since they are from the same fully trained full-precision network, we think that the difference between the two graphs in Figure 4 represents the effect of high-precision skip connection.\n\nIn order to account for the reviewer\u2019s comments and give a more direct comparison, we did new experiments where we prepared, from the same initial condition, two activation-quantized networks (one with precision highway and the other with low precision skip connection) where weights are not modified and only activations are quantized to 4 bits. The new experiments give a similar result to Figure 4 while the difference in accumulated quantization errors gets slightly reduced, possibly, due to the removal of the quantization error of weights.  In order to clarify the phenomenon of accumulated quantization error, we will use the new experimental results in the revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615920, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJx94o0qYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper23/Authors|ICLR.cc/2019/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615920}}}, {"id": "rkeJ-QBl07", "original": null, "number": 3, "cdate": 1542636278847, "ddate": null, "tcdate": 1542636278847, "tmdate": 1542636278847, "tddate": null, "forum": "SJx94o0qYX", "replyto": "Syego60FhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "content": {"title": "The response for Reviewer 1 ", "comment": "\n1. The  importance of precision highway\n\nPrecision highway helps reduce the accumulated quantization error. In ResNet, the difference between Equations (1) and (2) explains how the precision highway reduces quantization error. Without precision highway, the output of residual block has additional quantization error, \u2018e\u2019 in Equation (1) while the precision highway removes it as shown in Equation (2). \nSection 3.2 describes how the precision highway reduces the accumulation of quantization error in the LSTM as follows.\n\u201cSpecifically, when calculating ct, the inputs are not quantized, which reduces the accumulation of quantization error on ct. The computation of ht can also reduce the accumulation of quantization error by utilizing high-precision inputs. The construction of such a precision highway allows us to propagate high-precision information, i.e., cell states ct and outputs ht, across time steps\u201d\nThe result of low-precision computation is in high precision before quantization. We perform elementwise operations (additions in ResNet and multiplications in LSTM/GRU) between the precision highway and the high-precision result of low-precision computation. In other words, the elementwise computations in ResNet and LSTM/GRU are performed in high precision as mentioned in the original manuscript as follows.\n\n\u201cWe keep high-precision activation only on the skip connection and utilize it only for the element-wise addition. \u201d in Section 3.1.\n\u201cIn our proposed method, all of the element-wise multiplications in Equations 3e and 3f are performed in high precision.\u201d in Section 3.2.\n\n\n\n2. The novelty of the proposed method compared to Bi-Real Net\n\nPlease refer to our response to reviewer 3.\n\n\n\n3. Laplace distribution approximation\n\nPlease note that the y-axis is in log-scale while x-axis in linear scale. The histogram decreases linearly in the plot, which is well modeled by Laplace distribution. The jitter at the ends is due to the fact that the number of samples is small, and the range is in log-scale. We performed the same quantization adopting other distributions including Gaussian and triangle distributions, and the Laplace distribution showed marginally better results than the others. \n\n\n\n4. What kind of activation quantization is used? \n\nWe use the conventional quantization method used in DoReFa-net, and the method is also adopted in Zhuang\u2019s work. After clipping the activation to a pre-defined value, typically 1, the linear quantization is applied to the activation. We will clarify this in the revision.\n\n\n\n5. when is the cosine similarity between the quantized and full-precision networks computed? \n\nPlease refer to our response to reviewer 2.\n\n\n\n6.  What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?\n\nWe appreciate the comments. It helped clarify the loss surface analysis in Figure 5. In order to obtain Figure 5, we applied Hao Li\u2019s method as mentioned in the paper. In short, each figure represents loss surface seen from the local minimum we obtained from the training, i.e., the weight vector of the final trained model. In order to obtain two-dimensional view, we utilize two base vectors, u1 and u2, each of which corresponds to the axis of the figure. The base vector is a randomly generated vector having the same dimension of the weight vector. According to (Li et al., 2017), two randomly generated high-dimensional vectors tend to be orthogonal to each other. The origin of the figure at (0, 0) corresponds to the weight vector of the local minimum. The z-axis corresponds to the loss. In order to obtain a point, e.g., (0.25, 0.5) in the figure, we scale the two base vectors, i.e., 0.25*u1 and 0.5*u2, and add them to the local minimum weight vector corresponding to the origin. Then, we obtain the loss for the new weight vector, which is depicted at the point, (0.25, 0.5) on Figure 5. Since the figure is a loss surface near the local minimum, we tend to have a single local minimum in the figure unless we have another local minimum near the obtained one. The figure does not represent the relationship between loss and training epochs. We will clarify how we obtained Figure 5 in the revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615920, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJx94o0qYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper23/Authors|ICLR.cc/2019/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615920}}}, {"id": "Bkl1SxreR7", "original": null, "number": 1, "cdate": 1542635574911, "ddate": null, "tcdate": 1542635574911, "tmdate": 1542635623015, "tddate": null, "forum": "SJx94o0qYX", "replyto": "BJeYeCw-aQ", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "content": {"title": "The response for Reviewer 3", "comment": "\n1. The difference between the proposed method and Bi-Real Net\n\nWe\u2019d like to let you know that this study was conducted in parallel with Bi-Real Net. This work was submitted to another conference and re-submitted to ICLR2019 after adding additional extensive experiments including loss surface analysis and hardware cost estimation. We respect the research outcome of Bi-Real Net and refer to it in the original manuscript. \n\nAs mentioned in the original manuscript, both Bi-Real Net and PACTv2 apply quantization on pre-activation style residual net, the basic module of which is composed of batch-normalization (BN) \u2013 ReLU \u2013 convolution. Meanwhile, the end-to-end precision highway is a generalized \u201cnetwork-level structural\u201d method applicable to not only pre-activation style network but also post-activation style network, having the conv-BN-ReLU module as stated in the original manuscript as follows.\n\n\u201cin the case of feed-forward networks with identity path, our precision highway idea is applicable regardless of pre-activation or post-activation structure\u201d\n\nIn addition, it is a general method also applicable to recurrent network including LSTM and GRU. We described how the precision highway can be applied to LSTM in Section 3.2 and showed it significantly outperforms the existing quantization method on a language model. \nSince it is a novel structural method, it raises new challenges for further improvements as mentioned in Section 3.3 of the original manuscript as follows. \n\n\u201cIn the case of networks with multiple candidates for the precision highway, e.g., DenseNet, which has multiple parallel skip connections (Huang et al., 2017), we need to address a new problem of selecting skip connections to form a precision highway, which is left for future work.\u201d\n\nWe think the precision highway opened a new space of mixed precision neural network design where the precision of data representation, previously ignored, can now be jointly optimized with that of computation for further improvements of quantized networks.\n\n\n\n2. 1-bit quantization result\n\nWe performed 1-bit activation/weight quantization for the post-act style ResNet-18. For a fair comparison, we didn\u2019t apply the teacher-student and progressive quantization method and instead adopted BN-retraining proposed in Bi-Real Net. Our 1-bit activation/weight ResNet-18 gives 56.73 / 80.11 % of Top-1/Top-5 accuracy, which is by 0.33 / 0.61 % higher than the result of Bi-Real Net, respectively. According to our observation, the final accuracy is degraded when adopting a tight approximation of the derivative of the non-differentiable sign function proposed by Bi-Real Net. Instead, the conventional quantization method proposed by DoReFa-net can improve results. This difference seems to result from the difference of the network structure and activation quantization function. We will add this result and analysis to the revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615920, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJx94o0qYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper23/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper23/Authors|ICLR.cc/2019/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers", "ICLR.cc/2019/Conference/Paper23/Authors", "ICLR.cc/2019/Conference/Paper23/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615920}}}, {"id": "BJeYeCw-aQ", "original": null, "number": 3, "cdate": 1541664240920, "ddate": null, "tcdate": 1541664240920, "tmdate": 1541664240920, "tddate": null, "forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Review", "content": {"title": "An OK paper but need more evaluation.", "review": "This paper investigates the problem of neural network quantization. The main idea is to employ an end-to-end precision highway to reduce the accumulated quantization error and meanwhile enable ultra-low precision in deep neural networks.  The experimental results on the 3- and 2-bit quantizations of ResNet-18/50 and 2-bit quantization of an LSTM model demonstrate the effectiveness of the proposed method. \n\nThis paper is well written and organized. The idea of utilizing a high-precision information flow to reduce the accumulated quantization error is technically sound. The empirical studies on accumulated quantization error, loss surface analysis, model performance, and hardware cost are quite thorough and solid. \n\nThe idea of precision highway, however, is quite similar to the skip connections used in Bi-Real Net. Therefore, it may be a good idea to provide a thorough discussion over these two different methods so as to make the distinction.\n\nIn Table 2, the results of Bi-Real Net is based upon 1 bit activation/weight quantization, while the proposed method uses 2 bit activation/weight quantization. To give a fair comparison, it may be better to provide 1 bit activation/weight quantization results of the proposed method.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Review", "cdate": 1542234555481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631691, "tmdate": 1552335631691, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BylJ0G-03X", "original": null, "number": 2, "cdate": 1541440199262, "ddate": null, "tcdate": 1541440199262, "tmdate": 1541534352819, "tddate": null, "forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Review", "content": {"title": "A paper with good ideas and solid results, but some overlap with the literature", "review": "This paper studies methods to improve the performance of quantized neural networks.  The paper is largely centered around the idea of \"precision highways\" (full-precision residual connections) that run in parallel to fully-quantized convolutions.  However, the paper also throws in a toolbox of other methods like distillation from a teacher network, a quantization method based on the Laplace distribution, and a fine tuning scheme.\n\nThe paper reports performance for the resulting networks that is impressive but still believable.   They also do very extensive experiments, including an ablation study in Table 1 that I really liked, and a study of how the precision of the skip connections impacts overall performance.   I also like the visualizations of how quantization impacts the loss surface.\n\nMy main concern about this paper is that is has conceptual overlap with other approaches.  The authors are not the first to quantize resnets, and other papers have looked at teacher training and distillation as a method of refinement.  The authors are fairly upfront about this though, and I think this paper is the first to do a really thorough investigation of the impacts of skip connections in their own right.    Realistically, fully binarizing neural nets without modification is unlikely to lead to good performance.  The idea of leaving the skip connections with higher precision is a good compromise that achieves hardware friendliness along with strong performance, so I think it's worth having a paper like this that takes a closer look at this approach.\n\nA few questions I had:\n1)  I can't tell exactly what methods are being used in Table 1.  When the \"highway\" box is unchecked, does this mean the skip connection is absent?  Or that it exists but with full precision?  Or maybe that the skip connection branched after the quantization instead of before?   Also, what fine-tuning methods is used when the \"teacher\" box is un-checked?\n\n2) You implemented your own version of Zhuang's method.  However, I'd like to know how your numbers compare to the original reported numbers in Zhuang's paper.\n\nOne other minor criticism - When you fine-tune a modified network, the activations and weights will change.  It could be that the networks is modifying its parameters to account for (i.e., cancel out) the quantization errors.  For this reason I don't interpret Figure 4 as evidence for accumulation of error.  Perhaps this type of behavior would exists if you fine-tuned two full-precision networks using different random seeds, or different teacher networks.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Review", "cdate": 1542234555481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631691, "tmdate": 1552335631691, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Syego60FhQ", "original": null, "number": 1, "cdate": 1541168535909, "ddate": null, "tcdate": 1541168535909, "tmdate": 1541534352615, "tddate": null, "forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "invitation": "ICLR.cc/2019/Conference/-/Paper23/Official_Review", "content": {"title": "This paper proposes to keep a high activation/gradient flow in two special kinds of networks structures, namely ResNet and LSTM. For ResNet, the skip connections are made high-precision by adding the skip connection before quantization. For LSTM, the cell and hidden state computations are of high precision.", "review": "The proposed method is advantageous in that it only requires changes to some parts of the original ResNet or LSTM, without having to significantly change the network structure or training algorithm. It also reports empirical success of using high-precision skip connections in ResNet and cell/hidden state updates in LSTMs.\n\nHowever, it is unclear why it is necessary to keep a high-precision activation/gradient flow. What is the problem with existing quantized networks that do not have these high-precision-flow? Also, how does the high-precision flow interact with the rest of the network (with low-precision operations)?\n\nMoreover, the proposed method has limited novelty as the use of full-precision skip connections has been proposed in Bi-Real (Liu et al. 2018).\n\nMinor:\n- It is hard to tell that the weight histogram in Figure 3 is similar to a Laplacian distribution. It can also be approximated by other distributions (such as Gaussian or piecewise-linear distributions).\n- What kind of activation quantization is used?\n- In the experiments, when is the cosine similarity between the quantized and full-precision networks computed? after training or on an intermediate training step?\n- What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper23/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Precision Highway for Ultra Low-precision Quantization", "abstract": "Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.", "paperhash": "park|precision_highway_for_ultra_lowprecision_quantization", "keywords": ["neural network", "quantization", "optimization", "low-precision", "convolutional network", "recurrent network"], "authorids": ["canusglow@gmail.com", "dongyoungkim42@gmail.com", "sungjoo.yoo@gmail.com", "vajdap@fb.com"], "authors": ["Eunhyeok Park", "Dongyoung Kim", "Sungjoo Yoo", "Peter Vajda"], "TL;DR": "precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ", "pdf": "/pdf/913affc95816166b7eec5806490f76c0d1fd66d4.pdf", "_bibtex": "@misc{\npark2019precision,\ntitle={Precision Highway for Ultra Low-precision Quantization},\nauthor={Eunhyeok Park and Dongyoung Kim and Sungjoo Yoo and Peter Vajda},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx94o0qYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper23/Official_Review", "cdate": 1542234555481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJx94o0qYX", "replyto": "SJx94o0qYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper23/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631691, "tmdate": 1552335631691, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper23/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 14}