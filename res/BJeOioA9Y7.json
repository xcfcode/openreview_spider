{"notes": [{"id": "rJetJYMplr", "original": null, "number": 1, "cdate": 1562351841120, "ddate": null, "tcdate": 1562351841120, "tmdate": 1562351970643, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Public_Comment", "content": {"comment": "Hi, authors,\n\nThanks for this interesting work. I have a question about the size of matrix Q. One Q's dimension is (the size of a teacher's feature map)x(the size of student's feature map). For ImageNet, one intermediate feature map might be 512x14x14= 100352. Is Q becomes a 100352 x 100352 matrix? If so, it would be too large to run.\n\nIn addition, would you mind to release codes?\n\nBest Regards,", "title": "The additional memory cost of large deep network."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311788448, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJeOioA9Y7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311788448}}}, {"id": "BJeOioA9Y7", "original": "rkggZhjFFX", "number": 638, "cdate": 1538087840459, "ddate": null, "tcdate": 1538087840459, "tmdate": 1550894495515, "tddate": null, "forum": "BJeOioA9Y7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1g_pSjGg4", "original": null, "number": 1, "cdate": 1544889792504, "ddate": null, "tcdate": 1544889792504, "tmdate": 1545354494125, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Meta_Review", "content": {"metareview": "The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard-coded annealing of coeffiecients. The authors have done thorough experiments and the value of the approach seems clear, especially compared against progressive nets and pathnets. The paper is well-written and interesting, and the approach is novel. The reviewers have discussed the paper in detail and agree, with the AC, that it should be accepted.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper638/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353142972, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper638/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353142972}}}, {"id": "Bkl3Lcd80m", "original": null, "number": 2, "cdate": 1543043668419, "ddate": null, "tcdate": 1543043668419, "tmdate": 1543280365323, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "Byx-vLP5hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "content": {"title": "Response to AnonReviewer3:", "comment": "Updated: Changed section numbers to fit latest revision.\n---------------------------------------------------------------------------\nWe thank the reviewer for time and feedback.\n\nRe 1: Use teachers with different architectures from the student. \nIn additional experiments, following the suggestion of the reviewer, we use architectures for the teacher which differ from the student model. The results are summarized in Fig. 10 and discussed in Sec. 7.4. We observed that learning with teachers, whose architecture differs from the student, to have similar performance as learning with teachers which have the same architecture. Consider Fig.10 (a) as an example, where the target task is KungFu Master, and the teachers are experts for Seaquest and Riverraid. At the end of training, learning with teachers of different architectures achieves an average reward of 37520, and learning with teachers of the same architecture achieves an average reward of 35012. More results are shown in Figs. 10 (b, c). The results illustrate that Knowledge Flow can benefit from the knowledge of teachers, and thus achieve higher rewards, even if the teachers and the student architectures differ. \n\nRe 2: Importance of KL term. \nThe KL term prevents the student\u2019s output distribution over actions or labels to change too much when the teachers\u2019 influence is decreasing. To investigate the importance of the KL term, we conduct an ablation study where the KL coefficient (\\lambda2) is set to zero. The results are summarized in Fig. 9 and discussed in Sec. 7.3.2. Considering Fig. 9 (a) as an example, where the target task is MsPacman and the teachers are Riverraid and Seaquest experts. Without the KL term the rewards drop drastically when the teacher\u2019s influence decreases. In contrast, we don\u2019t observe this performance drop with a KL term. At the end of training, learning with a KL term achieves an average reward of 2907 and learning without the KL term achieves an average reward of 1215. More results are presented in Figs. 9 (b, c), which show that training with the KL term achieves higher reward than training without the KL term. \n\nRe 3: Use of an average network as \\theta_{old}.\nAn average network, i.e., exponential averaging can be used to obtain \\theta_{old}. To investigate how usage of an average network for \\theta{old} affects the performance, we conduct an experiment setting \\theta_{old} to be the exponential running average of the model weight. More specifically, \\theta_{old} is updated as follows: \\theta_{old} \\leftarrow \\alpha * \\theta_{old} + (1 - \\alpha) * \\theta, where \\alpha = 0.9. The results are summarized in Fig. 11 and discussed in Sec. 7.5. We observed that using an exponential average to compute \\theta_{old} results in very similar performance to using a single model. Consider Fig.11 (a) as an example, where the target task is Boxing and the teacher is a Riverraid expert. At the end of training, computing \\theta_{old} via an exponential average achieves an average reward of 96.2 and using a single parameter to set \\theta_old achieves an average reward of 96.0. More results on using an exponential average to compute \\theta_{old} are shown in Figs. 11 (b, c).\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607458, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeOioA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper638/Authors|ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607458}}}, {"id": "rklBpc_LC7", "original": null, "number": 3, "cdate": 1543043773484, "ddate": null, "tcdate": 1543043773484, "tmdate": 1543280219308, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "rkgVy1xs2m", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "content": {"title": "Response to AnonReviewer1:", "comment": "Updated: Changed section numbers to fit latest revision.\n---------------------------------------------------------------------------\nWe thank the reviewer for time and feedback.\n\nRe 1: plot p_w values for C10/C100 dataset. \nIn the newly added Fig. 4 and the corresponding discussion (Sec. 4.2), we plot the weight (p_w) for teachers and the student in the C10/C100 experiment, where C100 and SVHN experts are teachers. As expected and intuitively, the C100 teacher should have higher p_w value than the SVHN based teacher, because C100 is more relevant to C10. The plot verifies this intuition, p_w of the C100 teacher is higher than that of the SVHN teacher during the entire training. Both teachers\u2019 normalized weights approach zero at the end of training. \n\nRe 2: verify Knowledge Flow is not just NAS. \nAs the reviewer pointed out, one key difference between NAS and Knowledge Flow is that a student in Knowledge Flow benefits from teachers\u2019 knowledge. To verify that the student really benefits from the knowledge of teachers, we conduct the ablation study suggested by the reviewer. In the newly added experiment, discussed in Sec. 7.3.1 and summarized in Fig. 8, teachers are models that haven\u2019t been trained at all. Intuitively, learning with untrained teachers should have worse performance than learning with knowledgeable teachers. Our experiments verify this intuition. Considering Fig. 8 (a), where the target task is hero, learning with untrained teachers achieves an average reward of 15934, while learning with knowledgeable teachers (experts of Seaquest and Riverraid) achieves an average reward of 30928. Consistently with all other experiments we average over five runs. More results are presented in Fig. 8 (b, c). The results show that Knowledge Flow achieves higher rewards than NAS in different environments and teacher-student settings.\n\nRe 3: training teacher networks jointly. \nWe did try to train teachers jointly with students. However, as the reviewer mentioned, the memory usage is large and training is very slow. Up until now we didn\u2019t observe any improvements. \n\nRe 4: memory requirement for matrices Q. \nThe upper bound of the number of Q matrices in our framework is O(L*M*T). In practice, we don\u2019t link a student\u2019s layer to every layer of a teacher network. For example, we observed that linking a teachers\u2019 bottom layer to a student\u2019s top layer generally doesn\u2019t yield improvements. Intuitively, a teachers\u2019 bottom layer features are very likely irrelevant to a student\u2019s top layer features. Therefore, in practice, we recommend to link one teacher layer to one or two student layers, in which case the space complexity is O(L*T).\n\nRe 5: Captions of Table 1 and Table 2.\nWe updated the caption of Table 1 and Table 2. \n\nRe 6: Shorten paragraph 2 and paragraph 3.\nWe felt shortening paragraph 2 and 3 would remove the motivation of this work. Shortening the related work section wouldn\u2019t do justice to our peers. Therefore at this point we prefer to maintain the current writing unless the majority of the reviewers and the AC feel strongly about shortening.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607458, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeOioA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper638/Authors|ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607458}}}, {"id": "r1liuBG50X", "original": null, "number": 10, "cdate": 1543279987398, "ddate": null, "tcdate": 1543279987398, "tmdate": 1543279987398, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "BJlDEqbKA7", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "content": {"title": "Response to AnonReviewer1:", "comment": "Thanks a lot for additional time and feedback.\n\nRe 4: We added the comment regarding space complexity to Sec. 3.2 of the main paper.\n\nRe 6: We moved the detailed treatment of related work to the appendix and provide a shortened version in the main paper. We also moved Fig. 7 and the corresponding text to Sec. 4.2 of the main paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607458, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeOioA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper638/Authors|ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607458}}}, {"id": "r1lPW7iI07", "original": null, "number": 5, "cdate": 1543054078568, "ddate": null, "tcdate": 1543054078568, "tmdate": 1543249708676, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "SkgAysOIRm", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "content": {"title": "Multiple-task learning for DNN", "comment": "2 and 3 are the same.\n\nMultiple-task learning approaches are rife in this area (see e.g. https://en.wikipedia.org/wiki/Multi-task_learning, and citations therein). This huge body of work establishes that using a proper regularisation scheme is central. The intuition in the present paper seems to align with those ideas. But since those are so standard by now, the authors can be  expected to make the connection explicit. Note that the idea of 'lifelong learning' as cited does acknowledge this connection.\n\nMulti-task learning for DNN is a standard theme (especially in this conference), and it is not clarified how this work relates/improves over this body of work. One way to address this issue is to report empirical results on a standard benchmark (as MNIST).\n\nThe introductory text (ch, 2) is not quite correct (especially the RL needs care), but can be patched up by citing relevant introductory texts (what is random etc..) and adhering to their notation.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper638/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607458, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeOioA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper638/Authors|ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607458}}}, {"id": "BkgiFx9nnQ", "original": null, "number": 3, "cdate": 1541345410890, "ddate": null, "tcdate": 1541345410890, "tmdate": 1543249654942, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Review", "content": {"title": "Multiple task learning for NNs", "review": "This paper proposes a new set of heuristics for learning a NN for generalising a set of NNs trained for more specific tasks. This particular recipe might be reasonable, but the semi-formal flavour is distracting. The issue of model selection (clearly the main issue here) is not addressed. A quite severe issue with this report is that the authors don't report relevant learning results from before (+-) 2009, and empirical comparisons are only given w.r.t. other recent heuristics. This makes it for me not possible to advice publication as is.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Review", "cdate": 1542234413981, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335769529, "tmdate": 1552335769529, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJlDEqbKA7", "original": null, "number": 7, "cdate": 1543211566874, "ddate": null, "tcdate": 1543211566874, "tmdate": 1543211566874, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "rklBpc_LC7", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "content": {"title": "Nice revision!", "comment": "The new additions to the paper are very welcome, and definitely make the paper stronger in my opinion.\n\nRe 4: I recommend the authors include this statement somewhere in the paper/appendix.\n\nRe 6: If the authors feel paragraphs 2 & 3 are critical to motivate this work, then I still think you can instead shorten the parts of the related work section to make the information there less redundant.  In my opinion, the main text would be better if you made space for Fig 7 from the appendix and the relevant text description, by reducing the redundancy in descriptions of the alternative methods and their shortcomings."}, "signatures": ["ICLR.cc/2019/Conference/Paper638/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper638/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607458, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeOioA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper638/Authors|ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607458}}}, {"id": "ByeCKHTOA7", "original": null, "number": 6, "cdate": 1543193989602, "ddate": null, "tcdate": 1543193989602, "tmdate": 1543193989602, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "r1lPW7iI07", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "content": {"title": "Response to AnonReviewer2:", "comment": "Thanks a lot for additional time, feedback and clarifications.\n\nRe 1: Multi-task learning. \nNote that the challenge we address differs from multi-task learning. In multi-task learning, multiple tasks are addressed at the same time. In contrast, `Knowledge Flow\u2019 focuses on a single task. Hence, common for multi-task learning and `Knowledge Flow\u2019 is a transfer of information. However, in multi-task learning, information extracted from different tasks are shared to boost performance, while, in `Knowledge Flow,\u2019 the information of multiple teachers is leveraged to help a student learn better a single, new, previously unseen task. We updated Section 5 to clarify the connection and differences.\n\n\nRe 2: Notation of Section 2. \nWe follow the notation of Mnih et al. (2016), i.e., the expectation is taken with respect to a trajectory \\tau = ({x_t, a_t, r_t}, {x_{t+1}, a_{t+1}, r_{t+1}}, ...) generated by following the policy \\pi. We clarified this and updated Section 2 and 3. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607458, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeOioA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper638/Authors|ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607458}}}, {"id": "SkgAysOIRm", "original": null, "number": 4, "cdate": 1543043814229, "ddate": null, "tcdate": 1543043814229, "tmdate": 1543043814229, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "BkgiFx9nnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "content": {"title": "Response to AnonReviewer2:", "comment": "We thank the reviewer for time and feedback. We think the questions aren\u2019t precise enough for us to act upon:\n1. We\u2019d appreciate if the reviewer can point out the parts that are according to the reviewer\u2019s opinion `semi-formal\u2019? We are more than happy to revise the text but are currently left guessing, particularly since another reviewer points out that the paper is `well written.\u2019 \n2. We compare to recent baselines, in particular state-of-the-art methods like PNN and PathNet. If the reviewer would specify which papers from before 2009 we should compare to, we are very happy to include a statement, assuming that PNN and/or PathNet or their predecessors haven\u2019t compared to those already. \n3. To the best of our knowledge, the two baselines (PNN and PathNet) we compare with are the state-of-the-art RL transfer frameworks.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607458, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeOioA9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper638/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper638/Authors|ICLR.cc/2019/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers", "ICLR.cc/2019/Conference/Paper638/Authors", "ICLR.cc/2019/Conference/Paper638/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607458}}}, {"id": "rkgVy1xs2m", "original": null, "number": 2, "cdate": 1541238492384, "ddate": null, "tcdate": 1541238492384, "tmdate": 1541533817846, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Review", "content": {"title": "Intriguing idea, strong performance, but missing empirical results to validate intuition", "review": "This paper proposes to feed the representations of various external \"teacher\" neural networks of a particular example as inputs to various layers of a student network. \nThe idea is quite intriguing and performs very well empirically, and the paper is also well written.  While I view the performance experiments as extremely thorough, I believe the paper could possibly use some additional ablation-style experiments just to verify the method actually operates as one intuitively thinks it should.   \n\nOther Comments:\n\n- Did you verify that in Table 3, the p_w values for the teachers trained on the more-relevant C10/C100 dataset are higher than the p_w value for the teacher trained on the SVHN data?  It would be interesting to see the plots of these p_w over the course of training (similar to Fig 1c) to verify this method actually operates as one intuitively believes it should.\n\n- Integrating the teacher-network representations into various hidden layers of the student network might also be considered some form of neural architecture search (NAS)  (by including parts of the teacher network into the student architecture). \nSee for example the DARTS paper: https://arxiv.org/abs/1806.09055\nwhich similarly employs mixtures of potential connections.  \nUnder this NAS perspective, the dependence loss subsequently distills the optimal architecture network back into the student network architecture.\n\nHave you verified that this method is not just doing NAS, by for example, providing a small student network with a few teacher networks that haven't been trained at all? (i.e. should not permit any knowledge flow)\n\n- Have the authors considered training the teacher networks jointly with the student? This could be viewed as teachers learning how to improve their knowledge flow (although might require large amounts of memory depending on the size of the teacher networks).\n\n- Suppose we have an L-layer student network and T M-layer teacher networks.\nDoes this imply we have to consider O(L*M*T) additional weight matrices Q?\nCan you comment on the memory requirements?\n\n- The teacher-student setup should be made more clear in Tables 1 and 2 captions (took me some time to comprehend).\n\n- The second and third paragraphs are redundant given the Related Work section that appears later on. I would like to see these redundancies minimized and the freed up  space used to include more results from the Appendix in the main text. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Review", "cdate": 1542234413981, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335769529, "tmdate": 1552335769529, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Byx-vLP5hQ", "original": null, "number": 1, "cdate": 1541203544616, "ddate": null, "tcdate": 1541203544616, "tmdate": 1541533817643, "tddate": null, "forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper638/Official_Review", "content": {"title": "interesting approach in combining multiple trained models for transfer", "review": "This paper presents a method for distilling multiple teacher networks into a student, by linearly combining feature representations from all networks at multiple intermediate layers, and gradually forcing the student to \"take over\" the learned combination.  Networks to be used as teachers are first pretrained on various initial tasks.  A student network is then trained on a target task (possibly different from any teacher task), by combining corresponding hidden layers from each teacher using learned linear remappings and weighted combinations.  Learning this combination allows the system to find appropriate teachers for the target task; eventually, a penalty on the combination weights forces all weight onto the student network, resulting in the distillation.\n\nApplications to both reinforcement learning (atari game) and supervised image classification (cifar, svhn) are evaluated.  The reinforcement learning application is particularly fitting, since combining tasks together is less straightforward in this domain.\n\nI wonder whether any experiments were performed where the layers correspondence between teacher models was less clear --- say, using teachers with different architectures.  Figure 1(a) (different teacher archs) as well as the text (\"candidate set\" on p.4) indicate this is possible, but experiment details describe combinations of same-architecture teachers only.\n\nIn addition, I would have liked to see some further exploration of the KL term and use of \"theta_old\".  This seems potentially important, and also has ties to self-ensembling through teachers with exponential weight averaging.  Could an average network also be used here?  And how important is this term in linking student to teachers as the weights change?\n\nOverall I find this a very interesting approach.  Rather than training a large joint model on multiple tasks simultaneously as a transfer initialization, this approach uses models already fully trained for different tasks.  This results in a potentially advantageous trade-off:  One no longer needs to carefully calibrate the different tasks and common task components in a joint model, but at the cost of requiring inference through multiple teachers when training the student.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper638/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Flow: Improve Upon Your Teachers", "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods.\n\n", "keywords": ["Transfer Learning", "Reinforcement Learning"], "authorids": ["iliu3@illinois.edu", "jianpeng@illinois.edu", "aschwing@illinois.edu"], "authors": ["Iou-Jen Liu", "Jian Peng", "Alexander Schwing"], "TL;DR": "\u2018Knowledge Flow\u2019 trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).", "pdf": "/pdf/2e4273e25e1642c25b08ec7bb063d798ea63af2b.pdf", "paperhash": "liu|knowledge_flow_improve_upon_your_teachers", "_bibtex": "@inproceedings{\nliu2018knowledge,\ntitle={Knowledge Flow: Improve Upon Your Teachers},\nauthor={Iou-Jen Liu and Jian Peng and Alexander Schwing},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeOioA9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper638/Official_Review", "cdate": 1542234413981, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeOioA9Y7", "replyto": "BJeOioA9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper638/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335769529, "tmdate": 1552335769529, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper638/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}