{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124472868, "tcdate": 1518454423382, "number": 158, "cdate": 1518454423382, "id": "BJkjUrJwz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJkjUrJwz", "signatures": ["~Wenjun_Bai1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Circumventing negative transfer via cross generative initialisation", "abstract": "\nNegative transfer \u2013 a special type of transfer learning \u2013 refers to the interference of the previous knowledge with new learning. In this research, through an empirical study, we demonstrate the futile defence to the negative transfer via conventional neural network based transfer techniques, i.e., mid-level feature extraction and knowledge distillation. Under a finer specification of transfer learning, we speculate the real culprits of negative transfer are the incongruence on task and model complexity and the ordering of learning. Based on this speculation, we propose a tentative transfer learning technique, i.e., cross generative initialisation, to sidestep the negative transfer. The effectiveness of cross generative initialisation was evaluated empirically.", "paperhash": "bai|circumventing_negative_transfer_via_cross_generative_initialisation", "keywords": ["transfer learning", "negative transfer"], "_bibtex": "@misc{\n  bai2018circumventing,\n  title={Circumventing negative transfer via cross generative initialisation},\n  author={Wenjun Bai and Changqin Quan and Zhi-Wei Luo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJkjUrJwz}\n}", "authorids": ["bwj@cs11.cs.kobe-u.ac.jp", "qcq@cs11.cs.kobe-u.ac.jp", "lzw@cs11.cs.kobe-u.ac.jp"], "authors": ["Wenjun Bai", "Changqin Quan", "Zhi-Wei Luo"], "TL;DR": "The proposed cross generative initialisation is superior to the conventional transfer learning techniques in sidestepping the negative transfer.", "pdf": "/pdf/92c0ce494c8e269ea142803b99ca28b5dbc7a8ba.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582787517, "tcdate": 1520630509361, "number": 1, "cdate": 1520630509361, "id": "B1BgjuxKG", "invitation": "ICLR.cc/2018/Workshop/-/Paper158/Official_Review", "forum": "BJkjUrJwz", "replyto": "BJkjUrJwz", "signatures": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer1"], "content": {"title": "Official Review", "rating": "4: Ok but not good enough - rejection", "review": "Summary: This paper studies the topical problem of transfer learning. Th focus is on proposing a transfer learning method that is more robust to negative transfer (transfer outcome being worse than train from scratch without any transfer), which is a risk in the case of many existing transfer learning methods. The proposed \u201cCross-Generative\u201d initialisation method uses bootstrapping, and a weight-GMM transferred from the source model. The results show more robust transfer (no negative transfer) on an MNIST classification task using an MLP.\n\nNovelty: Unclear (poorly written/explained, see next)\nClarity: In general, this paper is poorly written and hard to follow, and the proposed methodology is poorly explained. The core methodology is explained in just about 8 lines at the end of Pg 2, which is not enough to assess it. The Algorithm 1 is completely uninformative being mostly composed of control flow statements. Weak English throughout doesn't help.\nSignificance: The strength of the empirical result is unclear. It\u2019s based on one experiment, and not clear if it generalises. The outcome of this kind of TL experiment depends on all sorts of things like learning rate, etc, so its hard to be convinced that the negative transfer problem shown in Tab 1 is for real, or just the result of bad tuning. Also the ubiquitous pre-train/fine-tune baseline for transfer learning seems not to be compared, which is a fatal gap in a TL paper. (* Unless this is what is meant by \u201cmid-level feature extraction\u201d: but this baseline is not explained, which is another problem). \nQuality: Many questionable statements are made. EG:  PG1: \u201cnegative transfer can only be circumvented in which the congruence on model and task complexity is high\u2026\u201d this might be the result in Tab 1, but there is definitely not enough evidence/theory to make an absolutely general claim like this. PG 3: \u201cAs a special case of transfer learning, negative transfer has been rarely studied\u201d. Negative transfer is more accurately characterised as the failure cases of transfer learning. And it has been studied, many papers try to make TL methods that are more robust to negative transfer. \n\nAssessment: Weak writing, particularly unclearly explained method, and questionable statements. There may be novel ideas in there somewhere, but the current execution is substandard for ICLR Workshop.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Circumventing negative transfer via cross generative initialisation", "abstract": "\nNegative transfer \u2013 a special type of transfer learning \u2013 refers to the interference of the previous knowledge with new learning. In this research, through an empirical study, we demonstrate the futile defence to the negative transfer via conventional neural network based transfer techniques, i.e., mid-level feature extraction and knowledge distillation. Under a finer specification of transfer learning, we speculate the real culprits of negative transfer are the incongruence on task and model complexity and the ordering of learning. Based on this speculation, we propose a tentative transfer learning technique, i.e., cross generative initialisation, to sidestep the negative transfer. The effectiveness of cross generative initialisation was evaluated empirically.", "paperhash": "bai|circumventing_negative_transfer_via_cross_generative_initialisation", "keywords": ["transfer learning", "negative transfer"], "_bibtex": "@misc{\n  bai2018circumventing,\n  title={Circumventing negative transfer via cross generative initialisation},\n  author={Wenjun Bai and Changqin Quan and Zhi-Wei Luo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJkjUrJwz}\n}", "authorids": ["bwj@cs11.cs.kobe-u.ac.jp", "qcq@cs11.cs.kobe-u.ac.jp", "lzw@cs11.cs.kobe-u.ac.jp"], "authors": ["Wenjun Bai", "Changqin Quan", "Zhi-Wei Luo"], "TL;DR": "The proposed cross generative initialisation is superior to the conventional transfer learning techniques in sidestepping the negative transfer.", "pdf": "/pdf/92c0ce494c8e269ea142803b99ca28b5dbc7a8ba.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582787286, "id": "ICLR.cc/2018/Workshop/-/Paper158/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper158/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper158/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper158/AnonReviewer3"], "reply": {"forum": "BJkjUrJwz", "replyto": "BJkjUrJwz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper158/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper158/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582787286}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582662150, "tcdate": 1520761367839, "number": 2, "cdate": 1520761367839, "id": "Hyg7quMYz", "invitation": "ICLR.cc/2018/Workshop/-/Paper158/Official_Review", "forum": "BJkjUrJwz", "replyto": "BJkjUrJwz", "signatures": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer2"], "content": {"title": "not fully clear", "rating": "3: Clear rejection", "review": "This paper tackles the problem of negative transfer, a problem that rises every time knowledge transfer\nis applied between tasks that are only marginally related among each other. Here the authors show that\nstandard transfer learning solutions like extracting and re-using cnn-mid-level features or distillation\ntechniques do not have any safe guard against the negative transfer issue. As an alternative solution, this\nwork proposes to interconnect the source and target task networks in a new way where their intermediate\nparameters are used to train a GMM that then is abe to generate new and transferrable parameters to \nfine-tune the target task network without negative transfer.\n\nThere are some points in the paper that need more attention and a more detailed explanation\n\n1) From table 1 it can be observed that when M1 is in source knowledge distillation works well regardless of starting\nfrom T1 or T2. This is not surprising since knowledge distillation is designed for a transfer from a complex teacher\nnetwork to a simple student network. The performance drops if M2 is in source. On the other side, the mid-level feature\nextraction strongly depends on which is the source task since it is designed for passing from a complex large task\nto a smaller and simpler one. This is to say that the underlying issue is not just negative transfer related to how much\nthe observed categories in source and target are similar, but the fact that existing methods are applied in cases out \nof their original design conditions.\n\n2) I find also a bit confusing the idea that the source and target network can exchange information among each\nother while both are in training. The original setting for transfer learning supposes that the source model is already\ntrained and the target model is learned only in a second step. If both source and target data are available at the\nsame time, why not putting the data together to define an overall task or why not attempting a co-training process?\nAgain it seems that the deigned coditions for transfer learning are not respected.\n\n3) I'm not convinced by the results in table 2. First of all the advantage of the proposed method with respect to the\nno-transfer baseline is not significant in the T2M1->T1M2 case and is anyway less than 1% in the T1M1->T2M2 and\nT1M2->T2M1 cases. Moreover, it is not clear the comparison with the results presented in table 1: the baseline results\nchange in the two tables so there is no clear reference that we can use to benchmark and the results of the \ncompeting distillation and feature transfer approaches seems also higher in table 1 for some cases with respect to those\nof the proposed method in table 2.\n\nMinor:\nIn different points D1 is used instead of M1 creating some confusion\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Circumventing negative transfer via cross generative initialisation", "abstract": "\nNegative transfer \u2013 a special type of transfer learning \u2013 refers to the interference of the previous knowledge with new learning. In this research, through an empirical study, we demonstrate the futile defence to the negative transfer via conventional neural network based transfer techniques, i.e., mid-level feature extraction and knowledge distillation. Under a finer specification of transfer learning, we speculate the real culprits of negative transfer are the incongruence on task and model complexity and the ordering of learning. Based on this speculation, we propose a tentative transfer learning technique, i.e., cross generative initialisation, to sidestep the negative transfer. The effectiveness of cross generative initialisation was evaluated empirically.", "paperhash": "bai|circumventing_negative_transfer_via_cross_generative_initialisation", "keywords": ["transfer learning", "negative transfer"], "_bibtex": "@misc{\n  bai2018circumventing,\n  title={Circumventing negative transfer via cross generative initialisation},\n  author={Wenjun Bai and Changqin Quan and Zhi-Wei Luo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJkjUrJwz}\n}", "authorids": ["bwj@cs11.cs.kobe-u.ac.jp", "qcq@cs11.cs.kobe-u.ac.jp", "lzw@cs11.cs.kobe-u.ac.jp"], "authors": ["Wenjun Bai", "Changqin Quan", "Zhi-Wei Luo"], "TL;DR": "The proposed cross generative initialisation is superior to the conventional transfer learning techniques in sidestepping the negative transfer.", "pdf": "/pdf/92c0ce494c8e269ea142803b99ca28b5dbc7a8ba.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582787286, "id": "ICLR.cc/2018/Workshop/-/Paper158/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper158/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper158/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper158/AnonReviewer3"], "reply": {"forum": "BJkjUrJwz", "replyto": "BJkjUrJwz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper158/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper158/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582787286}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582595847, "tcdate": 1521034406982, "number": 3, "cdate": 1521034406982, "id": "H1Jn4s8KM", "invitation": "ICLR.cc/2018/Workshop/-/Paper158/Official_Review", "forum": "BJkjUrJwz", "replyto": "BJkjUrJwz", "signatures": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer3"], "content": {"title": "An heuristic method to solve the negative transfer problem in CNN training", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposed the problem of negative transfer for CNN training, which has not been well studied.\nThe paper further propose a heuristic method to solve the so-called negative transfer problem.\nThe problem is interesting, but I don't think the negative transfer problem has been well defined and justified. It seems that the results is not better, then we call it negative transfer. However, in the setting of the deep learning, even for the fine-tune procedure, there are many parameters and methods that can be tried. It is difficult to justify whether the poor performance is due to improper choosing of the algorithms or the problem does have the issue of negative transfer.\nWhat's more, the authors just use the MNIST as examples, which is quite difficult to claim the general negative transfer problem. The proposed approach is also heuristic, and lacks solid theoretical supporting.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Circumventing negative transfer via cross generative initialisation", "abstract": "\nNegative transfer \u2013 a special type of transfer learning \u2013 refers to the interference of the previous knowledge with new learning. In this research, through an empirical study, we demonstrate the futile defence to the negative transfer via conventional neural network based transfer techniques, i.e., mid-level feature extraction and knowledge distillation. Under a finer specification of transfer learning, we speculate the real culprits of negative transfer are the incongruence on task and model complexity and the ordering of learning. Based on this speculation, we propose a tentative transfer learning technique, i.e., cross generative initialisation, to sidestep the negative transfer. The effectiveness of cross generative initialisation was evaluated empirically.", "paperhash": "bai|circumventing_negative_transfer_via_cross_generative_initialisation", "keywords": ["transfer learning", "negative transfer"], "_bibtex": "@misc{\n  bai2018circumventing,\n  title={Circumventing negative transfer via cross generative initialisation},\n  author={Wenjun Bai and Changqin Quan and Zhi-Wei Luo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJkjUrJwz}\n}", "authorids": ["bwj@cs11.cs.kobe-u.ac.jp", "qcq@cs11.cs.kobe-u.ac.jp", "lzw@cs11.cs.kobe-u.ac.jp"], "authors": ["Wenjun Bai", "Changqin Quan", "Zhi-Wei Luo"], "TL;DR": "The proposed cross generative initialisation is superior to the conventional transfer learning techniques in sidestepping the negative transfer.", "pdf": "/pdf/92c0ce494c8e269ea142803b99ca28b5dbc7a8ba.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582787286, "id": "ICLR.cc/2018/Workshop/-/Paper158/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper158/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper158/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper158/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper158/AnonReviewer3"], "reply": {"forum": "BJkjUrJwz", "replyto": "BJkjUrJwz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper158/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper158/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582787286}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573603816, "tcdate": 1521573603816, "number": 255, "cdate": 1521573603482, "id": "rkhkkJy9M", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJkjUrJwz", "replyto": "BJkjUrJwz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Circumventing negative transfer via cross generative initialisation", "abstract": "\nNegative transfer \u2013 a special type of transfer learning \u2013 refers to the interference of the previous knowledge with new learning. In this research, through an empirical study, we demonstrate the futile defence to the negative transfer via conventional neural network based transfer techniques, i.e., mid-level feature extraction and knowledge distillation. Under a finer specification of transfer learning, we speculate the real culprits of negative transfer are the incongruence on task and model complexity and the ordering of learning. Based on this speculation, we propose a tentative transfer learning technique, i.e., cross generative initialisation, to sidestep the negative transfer. The effectiveness of cross generative initialisation was evaluated empirically.", "paperhash": "bai|circumventing_negative_transfer_via_cross_generative_initialisation", "keywords": ["transfer learning", "negative transfer"], "_bibtex": "@misc{\n  bai2018circumventing,\n  title={Circumventing negative transfer via cross generative initialisation},\n  author={Wenjun Bai and Changqin Quan and Zhi-Wei Luo},\n  year={2018},\n  url={https://openreview.net/forum?id=BJkjUrJwz}\n}", "authorids": ["bwj@cs11.cs.kobe-u.ac.jp", "qcq@cs11.cs.kobe-u.ac.jp", "lzw@cs11.cs.kobe-u.ac.jp"], "authors": ["Wenjun Bai", "Changqin Quan", "Zhi-Wei Luo"], "TL;DR": "The proposed cross generative initialisation is superior to the conventional transfer learning techniques in sidestepping the negative transfer.", "pdf": "/pdf/92c0ce494c8e269ea142803b99ca28b5dbc7a8ba.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}