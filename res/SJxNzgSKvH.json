{"notes": [{"id": "SJxNzgSKvH", "original": "HyxjZGeFwB", "number": 2169, "cdate": 1569439755761, "ddate": null, "tcdate": 1569439755761, "tmdate": 1577168269721, "tddate": null, "forum": "SJxNzgSKvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Selective sampling for accelerating  training of deep neural networks", "authors": ["Berry Weinstein", "Shai Fine", "Yacov Hel-Or"], "authorids": ["berry.weinstein@post.idc.ac.il", "shai.fine@idc.ac.il", "toky@idc.ac.il"], "keywords": [], "abstract": "We present a selective sampling method designed to accelerate the training of deep neural networks. To this end,  we introduce a novel measurement,  the {\\it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched.   For multi-class linear classification,  the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting.  In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks.  The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection.\nFinally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.", "code": "https://github.com/paper-submissions/mms-select", "pdf": "/pdf/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "paperhash": "weinstein|selective_sampling_for_accelerating_training_of_deep_neural_networks", "original_pdf": "/attachment/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "_bibtex": "@misc{\nweinstein2020selective,\ntitle={Selective sampling for accelerating  training of deep neural networks},\nauthor={Berry Weinstein and Shai Fine and Yacov Hel-Or},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxNzgSKvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XcWSXnGZZ", "original": null, "number": 1, "cdate": 1576798742302, "ddate": null, "tcdate": 1576798742302, "tmdate": 1576800893918, "tddate": null, "forum": "SJxNzgSKvH", "replyto": "SJxNzgSKvH", "invitation": "ICLR.cc/2020/Conference/Paper2169/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a method to speed up training of deep nets by re-weighting samples based on their distance to the decision boundary. However, they paper seems hastily written and the method is not backed by sufficient experimental evidence.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Selective sampling for accelerating  training of deep neural networks", "authors": ["Berry Weinstein", "Shai Fine", "Yacov Hel-Or"], "authorids": ["berry.weinstein@post.idc.ac.il", "shai.fine@idc.ac.il", "toky@idc.ac.il"], "keywords": [], "abstract": "We present a selective sampling method designed to accelerate the training of deep neural networks. To this end,  we introduce a novel measurement,  the {\\it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched.   For multi-class linear classification,  the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting.  In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks.  The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection.\nFinally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.", "code": "https://github.com/paper-submissions/mms-select", "pdf": "/pdf/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "paperhash": "weinstein|selective_sampling_for_accelerating_training_of_deep_neural_networks", "original_pdf": "/attachment/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "_bibtex": "@misc{\nweinstein2020selective,\ntitle={Selective sampling for accelerating  training of deep neural networks},\nauthor={Berry Weinstein and Shai Fine and Yacov Hel-Or},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxNzgSKvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJxNzgSKvH", "replyto": "SJxNzgSKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725390, "tmdate": 1576800277271, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2169/-/Decision"}}}, {"id": "B1lvRyksiB", "original": null, "number": 2, "cdate": 1573740495220, "ddate": null, "tcdate": 1573740495220, "tmdate": 1573740495220, "tddate": null, "forum": "SJxNzgSKvH", "replyto": "HkgDJXI-5B", "invitation": "ICLR.cc/2020/Conference/Paper2169/-/Official_Comment", "content": {"title": "Answer", "comment": "Dear reviewer #3:\n\nWe would like to thank you for the feedback and will answer the questions raised:\n\n1) We used CIFAR10 and CIFAR100 to prove our main concept which aims to reduce the number of training steps. We didn't have enough time to include other datasets for the deadline but we plan to add ImageNet to the paper.\n\n2) Figure 5 and Table 1 show that for CIFAR10 we reached the final accuracy (with a minor drop of 0.25%) after 28% of required training steps using our method. For CIFAR100 we reached the final accuracy (with a minor drop of 0.07%) after 51% of required training steps using our method. This is a very substantial speed up with a very minor drop in final accuracy. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2169/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2169/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Selective sampling for accelerating  training of deep neural networks", "authors": ["Berry Weinstein", "Shai Fine", "Yacov Hel-Or"], "authorids": ["berry.weinstein@post.idc.ac.il", "shai.fine@idc.ac.il", "toky@idc.ac.il"], "keywords": [], "abstract": "We present a selective sampling method designed to accelerate the training of deep neural networks. To this end,  we introduce a novel measurement,  the {\\it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched.   For multi-class linear classification,  the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting.  In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks.  The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection.\nFinally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.", "code": "https://github.com/paper-submissions/mms-select", "pdf": "/pdf/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "paperhash": "weinstein|selective_sampling_for_accelerating_training_of_deep_neural_networks", "original_pdf": "/attachment/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "_bibtex": "@misc{\nweinstein2020selective,\ntitle={Selective sampling for accelerating  training of deep neural networks},\nauthor={Berry Weinstein and Shai Fine and Yacov Hel-Or},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxNzgSKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxNzgSKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2169/Authors", "ICLR.cc/2020/Conference/Paper2169/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2169/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2169/Reviewers", "ICLR.cc/2020/Conference/Paper2169/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2169/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2169/Authors|ICLR.cc/2020/Conference/Paper2169/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145327, "tmdate": 1576860539748, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2169/Authors", "ICLR.cc/2020/Conference/Paper2169/Reviewers", "ICLR.cc/2020/Conference/Paper2169/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2169/-/Official_Comment"}}}, {"id": "H1lCXxqqsB", "original": null, "number": 1, "cdate": 1573720102393, "ddate": null, "tcdate": 1573720102393, "tmdate": 1573739791322, "tddate": null, "forum": "SJxNzgSKvH", "replyto": "rJglmTn5tS", "invitation": "ICLR.cc/2020/Conference/Paper2169/-/Official_Comment", "content": {"title": "Training vs. inference and other comments", "comment": "Dear reviewer #2:\n\nWe would like to thank you for the feedback and the effort involved in running the performance example you presented. We will answer the questions and reply to the comments raised:\n\n1) As for the answer regarding our central premise. In order to select the samples using our MMS scheme, we leverage inference concepts that are entirely different from training. Some of the prominent ideas are low precision arithmetic operations when applying quantization, layers fusion like Convolution-BatchNorm (applying the BN running statistics into the convolutions and eliminating the need of performing BN) and weight compression. These concepts are not theoretical as they are being used when building specialized hardware accelerators as T4 (by Nvidia), Goya (by Habana) and TPU (by Google), allowing these devices to be ~10X faster at inference than running training step on a modern GPU. A detailed explanation and performance charts can be seen in Google\u2019s TPU paper \u201cIn-Datacenter Performance Analysis of a Tensor Processing Unit\u201d.\n\nAdditionally, for distributed training on large-scale hardware, the advantage of the inference devices is even greater. As the training instances must wait to a gradient reduction across all instances, the inference devices can perform forward passes on multiple instances in full parallelism (i.e. inference is embarrassingly parallel), without the need to wait to any other instance in the system.  Thus, it can potentially select more offline examples for our MMS scheme. \n\nFinally, more performance benchmarks can be found when referring to Habana\u2019s site (https://habana.ai/inference/ and https://habana.ai/training/) as the training vs. inference throughput on their hardware is 1650 vs. 15453 images/sec.\n\n2) As for \"In Figure 2 all methods seem to have similar final performance.\". Our main goal is not to improve final accuracy but rather to train less steps than the vanilla training regime. For that purpose, we introduced the early LR drop regime (as seen in Figure 5). We presented the plots in Figure 2 in order to show the relatively large deviation in the validation error as well as the training error. The validation error decrease using our MMS method implies a faster convergence and that a faster regime can be used. We further accept the comment and will move these plots to the appendix as well as mention their purpose in the paper.\n\n3) As for cutting CIFAR100 validation error for the early LR drop regime. We decided to end this experiment when the error reaches a sufficient performance w.r.t the baseline training (red). Moreover, we explicitly stated the final accuracy of the baseline and our MMS method in Table 1, showing a drop of 0.07% with almost halving the baseline required steps (from 156K to 80K steps).\n\n4) We kindly accept your comment regarding the entropy experiment and will include it with the other methods plot.\n\n5) We couldn't run many experiments due to time limitations, but we will make the effort to add STD bars.\n6) We will expand Table 1 as suggested with the entropy experiment and will add more relevant step information for clarity.\n7) We will add the ImageNet experiment. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2169/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2169/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Selective sampling for accelerating  training of deep neural networks", "authors": ["Berry Weinstein", "Shai Fine", "Yacov Hel-Or"], "authorids": ["berry.weinstein@post.idc.ac.il", "shai.fine@idc.ac.il", "toky@idc.ac.il"], "keywords": [], "abstract": "We present a selective sampling method designed to accelerate the training of deep neural networks. To this end,  we introduce a novel measurement,  the {\\it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched.   For multi-class linear classification,  the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting.  In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks.  The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection.\nFinally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.", "code": "https://github.com/paper-submissions/mms-select", "pdf": "/pdf/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "paperhash": "weinstein|selective_sampling_for_accelerating_training_of_deep_neural_networks", "original_pdf": "/attachment/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "_bibtex": "@misc{\nweinstein2020selective,\ntitle={Selective sampling for accelerating  training of deep neural networks},\nauthor={Berry Weinstein and Shai Fine and Yacov Hel-Or},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxNzgSKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxNzgSKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2169/Authors", "ICLR.cc/2020/Conference/Paper2169/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2169/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2169/Reviewers", "ICLR.cc/2020/Conference/Paper2169/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2169/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2169/Authors|ICLR.cc/2020/Conference/Paper2169/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145327, "tmdate": 1576860539748, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2169/Authors", "ICLR.cc/2020/Conference/Paper2169/Reviewers", "ICLR.cc/2020/Conference/Paper2169/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2169/-/Official_Comment"}}}, {"id": "rJglmTn5tS", "original": null, "number": 1, "cdate": 1571634455602, "ddate": null, "tcdate": 1571634455602, "tmdate": 1572972374133, "tddate": null, "forum": "SJxNzgSKvH", "replyto": "SJxNzgSKvH", "invitation": "ICLR.cc/2020/Conference/Paper2169/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "### Summary of contributions\n\nThis paper aims to accelerate the training of deep networks using a selective sampling. \nThey adapt ideas from active learning (which use some form of uncertainty estimation about the class of the label) to selectively choose samples on which to perform the backward pass. Specifically, they use the minimal margin score (MMS). \nTheir algorithm works by computing the forward pass over a batch of size B (which is much larger than the regular batch of size b), compute the uncertainty measure for each sample, and only perform the backward pass over the b samples with the highest uncertainty. The motivation is that the backward pass is more expensive than the forward pass, and that by only performing this pass on a subset of samples, computations are saved. \n\n\n### Recommendation\n\nReject. The central premise of the paper is unclear, the writing/presentation needs improvement, and the experiments are not convincing. \n\n\n### Detailed comments/improvements: \n\n\nThere is a central premise of the paper that I don't understand: that the forward pass is much cheaper than the backward pass. \nThis is claimed in the intro by referring to charts that hardware manufacturers publish (but there are no references included), but I don't see why this should be the case. \nFor a linear network with weights W, the forward pass is given by the matrix-matrix product (rows of X are minibatch samples):\nY = XW^T\n\nand the backward pass is given by the two matrix-matrix products:\ndL/dX = dL/dY*dY/dX = dL/dY*W\ndL/dW = dL/dY*dY/dW = dL/dY*X^T\n\nSimilarly the two operations in the backward pass for convolutional layers are given by a convolution of the output gradients with the transposed weigtht kernels and the input image respectively. \n\nPoint being, I don't see why the backward pass should be more than 3x more expensive than the forward pass. A simple experiment in PyTorch confirms this: the code snippet pasted at the bottom shows that the backward pass takes only around 2.6x longer than the forward pass.  \n\nfprop: 0.009286s\nbprop: 0.0240s\nbprop/fprop: 2.5893x\n\nIn algorithm 1, it is assumed that b << B. For this to be effective the forward pass would have to be *much* faster than the backward pass for this method to yield an improvement in computation. Can the authors comment on where this justification comes from?\n\nI am unclear on what the purpose of Section 4.1 is. This shows that the MMS of the proposed method is lower than the other two, but this should be completely expected since that is exactly the quantity being minimized. \nThere are also several unsubstantiated claims: \"Lower MMS scores resemble a better...batch of samples\", \"the batches selected by our method provide a higher value for the training procedure vs. the HNM samples.\", \"Evidently, the mean MMS provides a clearer perspective...and usefulness of the selected samples\". What does higher value, usefulness, clearer perspective mean?\n\nMore generally, it is unclear if there is really any improvement in the final performance from using the proposed method.\nIn Figure 2, all methods seem to have similar final performance. \nIn Figure 5, is there a reason why the curve for MMS is cut off? How does its final performance compare to that of the baseline method in red? It looks like the baseline might be better, but it's hard to tell from the figure. \n\nWhy are the experiments with the entropy measure in a seperate section? Please include them along with the other methods in the same plot, i.e merge Figure 2 and Figure 4. \n\nMy suggestions for improving the experimental section are as follows:\n- include all methods together in all the plots/tables\n- repeat experiments multiple times with different seeds to get error bars. Include these both in the learning curves and in the tables. \n- It's hard to see small differences in the learning curves, so including tables as well is important. Include best performance for all the methods in the tables. \n\nFinally, in 2019 CIFAR alone is not longer a sufficient dataset to report experiments on. Please report results on ImageNet as well. \n\nOne of the central premises of the paper is acceleration in terms of compute/time. To make this point, there should also be results in terms of walltime and floating-point operations. Please include these results in the paper.  \n    \n\n\n\n### Code snippet timing forward/backward passes\n\n\nimport torch, torch.nn as nn, time\n\nmodel =\tnn.Sequential(nn.Linear(784, 1000),\n                      nn.ReLU(),\n                      nn.Linear(1000, 1000),\n                      nn.ReLU(),\n                      nn.Linear(1000, 10),\n                      nn.LogSoftmax())\n\ndata = torch.randn(128, 784)\nlabels = torch.ones(128).long()\nt = time.time()\npred = model.forward(data)\nloss = nn.functional.nll_loss(pred, labels)\nfprop_time = time.time() - t\nt = time.time()\nloss.backward()\nbprop_time = time.time() - t\nprint('fprop: {:.4}s'.format(fprop_time))\nprint('bprop: {:.4f}s'.format(bprop_time))\nprint('bprop/fprop: {:.4f}x'.format(bprop_time / fprop_time))\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2169/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2169/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Selective sampling for accelerating  training of deep neural networks", "authors": ["Berry Weinstein", "Shai Fine", "Yacov Hel-Or"], "authorids": ["berry.weinstein@post.idc.ac.il", "shai.fine@idc.ac.il", "toky@idc.ac.il"], "keywords": [], "abstract": "We present a selective sampling method designed to accelerate the training of deep neural networks. To this end,  we introduce a novel measurement,  the {\\it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched.   For multi-class linear classification,  the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting.  In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks.  The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection.\nFinally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.", "code": "https://github.com/paper-submissions/mms-select", "pdf": "/pdf/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "paperhash": "weinstein|selective_sampling_for_accelerating_training_of_deep_neural_networks", "original_pdf": "/attachment/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "_bibtex": "@misc{\nweinstein2020selective,\ntitle={Selective sampling for accelerating  training of deep neural networks},\nauthor={Berry Weinstein and Shai Fine and Yacov Hel-Or},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxNzgSKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxNzgSKvH", "replyto": "SJxNzgSKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2169/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2169/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576392250910, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2169/Reviewers"], "noninvitees": [], "tcdate": 1570237726710, "tmdate": 1576392250927, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2169/-/Official_Review"}}}, {"id": "BJeWmNn3KS", "original": null, "number": 2, "cdate": 1571763225134, "ddate": null, "tcdate": 1571763225134, "tmdate": 1572972374096, "tddate": null, "forum": "SJxNzgSKvH", "replyto": "SJxNzgSKvH", "invitation": "ICLR.cc/2020/Conference/Paper2169/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a minimal margin score (MMS) criterion to speed up the training of the deep networks.\n\nI would vote for a clear rejection of this paper. This submission is a clearly unfinished one. The two biggest problems are as follows\n\n1. Lack of a comprehensive discussion on rules for sampling section, please see \"Automated Curriculum Learning for Neural Networks\". Why previous methods are worse than the proposed one is not clear.\n\n2. All experiments are only compared with baseline approaches. In some experiments, the improvements are really marginal (e.g., Figure 2). In these cases, the STD of these curves is not shown, it is not clear whether the improvements are significant or not."}, "signatures": ["ICLR.cc/2020/Conference/Paper2169/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2169/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Selective sampling for accelerating  training of deep neural networks", "authors": ["Berry Weinstein", "Shai Fine", "Yacov Hel-Or"], "authorids": ["berry.weinstein@post.idc.ac.il", "shai.fine@idc.ac.il", "toky@idc.ac.il"], "keywords": [], "abstract": "We present a selective sampling method designed to accelerate the training of deep neural networks. To this end,  we introduce a novel measurement,  the {\\it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched.   For multi-class linear classification,  the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting.  In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks.  The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection.\nFinally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.", "code": "https://github.com/paper-submissions/mms-select", "pdf": "/pdf/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "paperhash": "weinstein|selective_sampling_for_accelerating_training_of_deep_neural_networks", "original_pdf": "/attachment/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "_bibtex": "@misc{\nweinstein2020selective,\ntitle={Selective sampling for accelerating  training of deep neural networks},\nauthor={Berry Weinstein and Shai Fine and Yacov Hel-Or},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxNzgSKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxNzgSKvH", "replyto": "SJxNzgSKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2169/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2169/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576392250910, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2169/Reviewers"], "noninvitees": [], "tcdate": 1570237726710, "tmdate": 1576392250927, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2169/-/Official_Review"}}}, {"id": "HkgDJXI-5B", "original": null, "number": 3, "cdate": 1572066014769, "ddate": null, "tcdate": 1572066014769, "tmdate": 1572972374052, "tddate": null, "forum": "SJxNzgSKvH", "replyto": "SJxNzgSKvH", "invitation": "ICLR.cc/2020/Conference/Paper2169/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "A new approach is proposed to speed up training in deep models.\n\nThe idea is to select sample batches when back propagating the error based on the distance of the prediction foe the sample from the decision boundary. Specifically, we pick points closer to the boundary, i.e., ones that we are less confident about for backpropagation.\n\nExperiments are performed comparing the method with Hard negative sampling (HNM) , entropy-based sample selection as well as regular training. Experiments are performed on Cifar10 and Cifar100 datasets. \nWhy only two datasets, the method is general so there should be more datasets to verify its performance.\n\nThe results on Cifar100 in Fig 5 c seems to show that we cannot reach the training accuracy using the proposed method as compared to the other methods. What is the intuition here as to why it happens? In general though since the main goal is to speed up training I do not see very convincing evidence of this in the limited evaluation which seems to be the main weakness here."}, "signatures": ["ICLR.cc/2020/Conference/Paper2169/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2169/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Selective sampling for accelerating  training of deep neural networks", "authors": ["Berry Weinstein", "Shai Fine", "Yacov Hel-Or"], "authorids": ["berry.weinstein@post.idc.ac.il", "shai.fine@idc.ac.il", "toky@idc.ac.il"], "keywords": [], "abstract": "We present a selective sampling method designed to accelerate the training of deep neural networks. To this end,  we introduce a novel measurement,  the {\\it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched.   For multi-class linear classification,  the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting.  In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks.  The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection.\nFinally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.", "code": "https://github.com/paper-submissions/mms-select", "pdf": "/pdf/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "paperhash": "weinstein|selective_sampling_for_accelerating_training_of_deep_neural_networks", "original_pdf": "/attachment/2f6172635fd4b5b82ca61b88cfca796fee570ab6.pdf", "_bibtex": "@misc{\nweinstein2020selective,\ntitle={Selective sampling for accelerating  training of deep neural networks},\nauthor={Berry Weinstein and Shai Fine and Yacov Hel-Or},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxNzgSKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxNzgSKvH", "replyto": "SJxNzgSKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2169/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2169/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576392250910, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2169/Reviewers"], "noninvitees": [], "tcdate": 1570237726710, "tmdate": 1576392250927, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2169/-/Official_Review"}}}], "count": 7}