{"notes": [{"id": "ryekdoCqF7", "original": "r1l7-n9YKX", "number": 316, "cdate": 1538087782815, "ddate": null, "tcdate": 1538087782815, "tmdate": 1545355440175, "tddate": null, "forum": "ryekdoCqF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Incremental training of multi-generative adversarial networks", "abstract": "Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.", "keywords": ["GAN", "Incremental training", "Information projection", "Mixture distribution"], "authorids": ["thunderingtan@gmail.com", "kenshinping@gmail.com", "xuke@tsinghua.edu.cn", "emersonswr@gmail.com", "songzuo.z@gmail.com"], "authors": ["Qi Tan", "Pingzhong Tang", "Ke Xu", "Weiran Shen", "Song Zuo"], "TL;DR": "We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.", "pdf": "/pdf/246d9668c96f57f2198604b74af957498c063386.pdf", "paperhash": "tan|incremental_training_of_multigenerative_adversarial_networks", "_bibtex": "@misc{\ntan2019incremental,\ntitle={Incremental training of multi-generative adversarial networks},\nauthor={Qi Tan and Pingzhong Tang and Ke Xu and Weiran Shen and Song Zuo},\nyear={2019},\nurl={https://openreview.net/forum?id=ryekdoCqF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1ep4nF8lN", "original": null, "number": 1, "cdate": 1545145397084, "ddate": null, "tcdate": 1545145397084, "tmdate": 1545354477206, "tddate": null, "forum": "ryekdoCqF7", "replyto": "ryekdoCqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper316/Meta_Review", "content": {"metareview": "The reviewers and the AC acknowledge the paper contains interesting ideas on using an incremental sequence of multiple generators to capture the diversity of the examples. However, the reviewers and the AC also note that the potential drawback of the paper is the lack of evaluation with other metrics such as inception score, FID score, etc. Therefore the paper is not quite ready for acceptance right now, but the AC encourages the authors to submit to other top venues with more thorough experiments. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper316/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper316/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental training of multi-generative adversarial networks", "abstract": "Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.", "keywords": ["GAN", "Incremental training", "Information projection", "Mixture distribution"], "authorids": ["thunderingtan@gmail.com", "kenshinping@gmail.com", "xuke@tsinghua.edu.cn", "emersonswr@gmail.com", "songzuo.z@gmail.com"], "authors": ["Qi Tan", "Pingzhong Tang", "Ke Xu", "Weiran Shen", "Song Zuo"], "TL;DR": "We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.", "pdf": "/pdf/246d9668c96f57f2198604b74af957498c063386.pdf", "paperhash": "tan|incremental_training_of_multigenerative_adversarial_networks", "_bibtex": "@misc{\ntan2019incremental,\ntitle={Incremental training of multi-generative adversarial networks},\nauthor={Qi Tan and Pingzhong Tang and Ke Xu and Weiran Shen and Song Zuo},\nyear={2019},\nurl={https://openreview.net/forum?id=ryekdoCqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper316/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353260032, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryekdoCqF7", "replyto": "ryekdoCqF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper316/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper316/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper316/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353260032}}}, {"id": "BJlLjYqBAm", "original": null, "number": 4, "cdate": 1542986141827, "ddate": null, "tcdate": 1542986141827, "tmdate": 1542986141827, "tddate": null, "forum": "ryekdoCqF7", "replyto": "H1xn-w1Op7", "invitation": "ICLR.cc/2019/Conference/-/Paper316/Official_Comment", "content": {"title": "Respond to Reviewer4", "comment": "We thank the reviewer for careful reading and helpful comments.\n\n1. We propose incremental training because of its flexible. A jointly training algorithm is limited by its number of components. As [1] shows, due to the limit of GPU memory, they can only get mix+GAN with components T<=5. As in our algorithm, we don\u2019t need to store all the parameters in the memory. In our new loss function, we could store a sets consisted of samples generated from each generator, and then get batches from these sample sets to approximate the expectation of each generator, i.e., E_{x~g_i}[D(x)]. We argue that we could also add the generators group by group using the jointly training algorithm to search for a complementary mixture distribution instead of one by one, and these may not be limited by the GPU memory.\n\n2. We use the architectures resemble to DC-GAN in all our experiments except the Gaussian mixture sets experiments. In Gaussian mixture experiment, we use generators contain 2 hidden layers of size 128 with a relu activation fully connected network, followed by a linear projection to 2 dimensions. \nFor the second question, we don\u2019t need to put all generators within the GPU memory, after i-th iteration, we store a new folder consisted of samples generated by the new generator, and make a pipeline to read the batches to approximate the expectations of E_{x~g_i}[D(x)], so we could add multiple generators into the generator group. \nWe reduce the latent variable to dimensional 1 so that the condition of figure 1(c) is likely met, and we also test our algorithm by using a lager dimension z, the only difference between different dimensions is that a lager dimension z can get a smaller distance of start point, and the convergence distance is also smaller than a small dimension z, so we could also obtain a better solution through a combination of generator using the larger dimension z.\n\n3. We are trying to implement these experiments and we will measure our method on CIFAR and LSUN to test the performance of our algorithm. We will give the results soon.\n\nMinor points:\n1. Through the WGAN, the loss function W(P_r, P_g)=sup_{||f||_L<=1}{E_{x~P_r}[f(x)]-E{x~P_g}[f(x)]} is a elegant lower bound to \"Wasserstein distance\", when we fix the input data distribution for both the real data and the generated data, then train an auxiliary discriminator to maximize the loss function, we can get an approximate \"Wasserstein distance\".\n\n2. Based on the same input distribution and the same architecture for each generator, we assume that the ability of expression for each generator is identical in the target distribution space, so it\u2019s reasonable to set the corresponding weight as the same number. Moreover, in theory, we further prove that when we set the weight to 1, the mixed distribution can converge to the optimal distribution we can get in the convex hull under such specifical settings.\n\n3. We fix these typos.\n\n4. We fix these expressions.\n\n5. Thank you for your thorough thinking, we think this argument is meaningful. First, consider a set consisted of the endpoints of a simplex, under this condition, a mixture of 2 endpoints can only get the point on the boundary, and we should make combination of all endpoints to get each point within the simplex. Since the distribution range is complex, a mixture of 2 generators may not enough to get the optimal one in the convex hull.\n\n____________________________________________________________________________________________\n\n[1]. Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 224\u2013232. PMLR, 2017.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper316/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper316/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental training of multi-generative adversarial networks", "abstract": "Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.", "keywords": ["GAN", "Incremental training", "Information projection", "Mixture distribution"], "authorids": ["thunderingtan@gmail.com", "kenshinping@gmail.com", "xuke@tsinghua.edu.cn", "emersonswr@gmail.com", "songzuo.z@gmail.com"], "authors": ["Qi Tan", "Pingzhong Tang", "Ke Xu", "Weiran Shen", "Song Zuo"], "TL;DR": "We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.", "pdf": "/pdf/246d9668c96f57f2198604b74af957498c063386.pdf", "paperhash": "tan|incremental_training_of_multigenerative_adversarial_networks", "_bibtex": "@misc{\ntan2019incremental,\ntitle={Incremental training of multi-generative adversarial networks},\nauthor={Qi Tan and Pingzhong Tang and Ke Xu and Weiran Shen and Song Zuo},\nyear={2019},\nurl={https://openreview.net/forum?id=ryekdoCqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper316/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615812, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryekdoCqF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference/Paper316/Reviewers", "ICLR.cc/2019/Conference/Paper316/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper316/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper316/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper316/Authors|ICLR.cc/2019/Conference/Paper316/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper316/Reviewers", "ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference/Paper316/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615812}}}, {"id": "HJxt4YLHC7", "original": null, "number": 3, "cdate": 1542969649381, "ddate": null, "tcdate": 1542969649381, "tmdate": 1542969649381, "tddate": null, "forum": "ryekdoCqF7", "replyto": "BJe_W-n-TX", "invitation": "ICLR.cc/2019/Conference/-/Paper316/Official_Comment", "content": {"title": "Respond to Reviewer1", "comment": "We thank the reviewer for the careful reading and insightful comments.\n\n1. It\u2019s a promising future work to find the optimal weight (i.e., w_i) for each generator, and this approach can accelerate the convergence. E.g., one can imagine a special case that the range set is a minor arc lies in a plane and the target points lies on the chord corresponding to the arc (just like the sketch map below). \n\n___A\n|\n|\n| C  . p_data\n|\n|___B\n\nThe optimal combination is easy to get: w_A * A+w_B * B. But using a greedy algorithm, we may first find the solution that is nearest to p_data, i.e., C. After that, the target distribution for next generator is shifting to 2*p_data \u2013 C, a point lies on the reverse extension line of line p_data-C. After several iterations, we can find the point A and B as the solution for the generator (At this time, A and B is the nearest point to the target point n*p_data-(n-1)*p_pre.). In this extreme case, we need to repeat infinite times to offset the horizontal shifting, while if we can optimize the weights, the optimal combination can be obtained in the finite iterations.\n\n2. Increase the number of parameters may enlarge the distribution range of each generator, and thus we could get a better solution to approximate real data distribution through a single generator. But these increments may change the distribution range for each generator, however, in each iteration, we are indeed in search of the solution in the different distribution sets, so in theory, we can\u2019t guarantee the combination of the generators is better than before.\n\n3. We fix this typo.\n\n4. We further train up to 10 generators to approximate the real data distribution, and we didn\u2019t project the data to the interval (-1, 1), so the difference is more significant, the results show below:\n---------------------------------------------------------------------------------------------------------------------------------------\n   Method         |G|=1      |G|=2     |G|=3      |G|=4      |G|=5      |G|=6     |G|=7 \n----------------------------------------------------------------------------------------------------------------------------------------\nIncremental GAN  52003.589  7655.897  7494.769   14156.5    4016.2288  2600.899  23281.205\nOriginal GAN   52003.589   57966.67  27102.465  32559.604  31377.982  31643.3   30829.93\n----------------------------------------------------------------------------------------------------------------------------------------\n\n----------------------------------------------------------------------\n   Method         |G|=8      |G|=9     |G|=10    \n----------------------------------------------------------------------\nIncremental GAN  17003.852   9159.07    9118.682   \nOriginal GAN    35698.953   32310.938  44249.152  \n----------------------------------------------------------------------\nThe oscillation is because the generator converges to some local optimal, thus influence the whole performance. Furthermore, our algorithm can fast recover from the deviation, and get closer to it again in the successive iterations.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper316/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper316/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental training of multi-generative adversarial networks", "abstract": "Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.", "keywords": ["GAN", "Incremental training", "Information projection", "Mixture distribution"], "authorids": ["thunderingtan@gmail.com", "kenshinping@gmail.com", "xuke@tsinghua.edu.cn", "emersonswr@gmail.com", "songzuo.z@gmail.com"], "authors": ["Qi Tan", "Pingzhong Tang", "Ke Xu", "Weiran Shen", "Song Zuo"], "TL;DR": "We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.", "pdf": "/pdf/246d9668c96f57f2198604b74af957498c063386.pdf", "paperhash": "tan|incremental_training_of_multigenerative_adversarial_networks", "_bibtex": "@misc{\ntan2019incremental,\ntitle={Incremental training of multi-generative adversarial networks},\nauthor={Qi Tan and Pingzhong Tang and Ke Xu and Weiran Shen and Song Zuo},\nyear={2019},\nurl={https://openreview.net/forum?id=ryekdoCqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper316/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615812, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryekdoCqF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference/Paper316/Reviewers", "ICLR.cc/2019/Conference/Paper316/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper316/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper316/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper316/Authors|ICLR.cc/2019/Conference/Paper316/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper316/Reviewers", "ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference/Paper316/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615812}}}, {"id": "BJxrYBLr0m", "original": null, "number": 2, "cdate": 1542968701385, "ddate": null, "tcdate": 1542968701385, "tmdate": 1542968701385, "tddate": null, "forum": "ryekdoCqF7", "replyto": "S1lT0m9n2m", "invitation": "ICLR.cc/2019/Conference/-/Paper316/Official_Comment", "content": {"title": "Respond to Reviewer3\uff1a", "comment": "We thank the reviewer for the careful reading and recognizing our contributions\n\n1. We discuss these problems for multiple times, and we agree that a perfect discriminator can obtain the equality as the lower bound become the virtual \u2018distance\u2019. Furthermore, using combination of discriminators can also enlarge the functional space. Another question is how to combine the discriminator results. For discriminators, it needn\u2019t to be a distribution, so we can use affine combination or other combination methods to improve its performance. In a word, it\u2019s a promising work, we would further investigate these theories in the future. \n\n2. We thank the reviewer for the helpful comments and we redraw the corresponding results in the experiment for clarity. In figure 3, we use different colors as well as corresponding numbers to describe the data points generated by different generators.\n\n3. Based on the same input distribution and the same architecture for each generator, we assume that the ability of expression for each generator is identical in the target distribution space, so it\u2019s reasonable to set the corresponding weight as the same number. Moreover, in theory, we further prove that when we set the weight to 1, the mixed distribution can converge to the optimal distribution we can get in the convex hull under such specifical settings. \n\n4. We fix this typo.\n\n5. The Original GAN means we train each generator using the original target distribution as before, i.e., p_data, and for comparison, the training method we use in the experiment for the three are identical. We rewrite the footnote to make this interpretation more clearly.\n\n6. We tried this approach under a small sized condition. Heuristically, a small sized generator means a weak fitting capacity, and in our experiments, due to the continuous latent variable, a simple architecture may generate more samples fall into the clearance between different modes, thus obtain a worse performance.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper316/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper316/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental training of multi-generative adversarial networks", "abstract": "Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.", "keywords": ["GAN", "Incremental training", "Information projection", "Mixture distribution"], "authorids": ["thunderingtan@gmail.com", "kenshinping@gmail.com", "xuke@tsinghua.edu.cn", "emersonswr@gmail.com", "songzuo.z@gmail.com"], "authors": ["Qi Tan", "Pingzhong Tang", "Ke Xu", "Weiran Shen", "Song Zuo"], "TL;DR": "We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.", "pdf": "/pdf/246d9668c96f57f2198604b74af957498c063386.pdf", "paperhash": "tan|incremental_training_of_multigenerative_adversarial_networks", "_bibtex": "@misc{\ntan2019incremental,\ntitle={Incremental training of multi-generative adversarial networks},\nauthor={Qi Tan and Pingzhong Tang and Ke Xu and Weiran Shen and Song Zuo},\nyear={2019},\nurl={https://openreview.net/forum?id=ryekdoCqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper316/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615812, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryekdoCqF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference/Paper316/Reviewers", "ICLR.cc/2019/Conference/Paper316/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper316/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper316/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper316/Authors|ICLR.cc/2019/Conference/Paper316/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper316/Reviewers", "ICLR.cc/2019/Conference/Paper316/Authors", "ICLR.cc/2019/Conference/Paper316/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615812}}}, {"id": "H1xn-w1Op7", "original": null, "number": 3, "cdate": 1542088451553, "ddate": null, "tcdate": 1542088451553, "tmdate": 1542088451553, "tddate": null, "forum": "ryekdoCqF7", "replyto": "ryekdoCqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper316/Official_Review", "content": {"title": "Incremental training of GANs", "review": "The paper introduces an incremental training method for GAN's for capturing the diversity of the input space. The paper demonstrate that the proposed method allows smaller distances between the true and generated distribution. I find the idea interesting, but fear that the 60-100 small ensemble models could be replaced by a larger model.\n\nI am curious about why we need incremental training when it seems like we could directly train all the networks jointly. The corresponding generative model is simply stronger so all the convergence arguments would still hold. Is the statistical distance a reasonable estimate for you to determine whether you need an additional generator for incremental training?\n\nAlso what are the generator architectures for the experiments? How can you put 60-100 generators within the GPU memory? The latent variable dimension seem to be only 1 for each of your generator? That seems to be seriously handicapping the capacity of each individual generator (to just some data points), so the ensemble distribution might be obtained simply by using a larger dimension z?\n\nThere are also other measurements that are used by the GAN community, such as inception score, FID score and samples. It seems also reasonable to verify the effectiveness of this method on CIFAR or LSUN datasets, where the method would have a greater improvement because the data distributions are more complex.\n\nMinor points:\n- How do you measure the \"Wasserstein distance\" for high-dimensional distributions? \n- What not set $\\omega_i$ to be always 1? The subsampling process introduced in Algorithm 2 seem to enforce this, and you do this for all the experiments.\n- Fix citation typos.\n- Fix \\mathbf for vector quantities, such as x and z.\n- Since the generative models have the same architecture, does the non-convex argument becomes moot when you have a mixture of 2 generators?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper316/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental training of multi-generative adversarial networks", "abstract": "Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.", "keywords": ["GAN", "Incremental training", "Information projection", "Mixture distribution"], "authorids": ["thunderingtan@gmail.com", "kenshinping@gmail.com", "xuke@tsinghua.edu.cn", "emersonswr@gmail.com", "songzuo.z@gmail.com"], "authors": ["Qi Tan", "Pingzhong Tang", "Ke Xu", "Weiran Shen", "Song Zuo"], "TL;DR": "We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.", "pdf": "/pdf/246d9668c96f57f2198604b74af957498c063386.pdf", "paperhash": "tan|incremental_training_of_multigenerative_adversarial_networks", "_bibtex": "@misc{\ntan2019incremental,\ntitle={Incremental training of multi-generative adversarial networks},\nauthor={Qi Tan and Pingzhong Tang and Ke Xu and Weiran Shen and Song Zuo},\nyear={2019},\nurl={https://openreview.net/forum?id=ryekdoCqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper316/Official_Review", "cdate": 1542234489206, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryekdoCqF7", "replyto": "ryekdoCqF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper316/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335696968, "tmdate": 1552335696968, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper316/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJe_W-n-TX", "original": null, "number": 2, "cdate": 1541681408439, "ddate": null, "tcdate": 1541681408439, "tmdate": 1541681408439, "tddate": null, "forum": "ryekdoCqF7", "replyto": "ryekdoCqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper316/Official_Review", "content": {"title": "A sequential training of GANs and some theory associated with it", "review": "This paper proposes a method for ensembling GAN\u2019s for capturing diversity in the target space. This done by a convex combination of GAN\u2019s that are sequentially trained by trying to approximate the real distribution by fixing the previous generators. The paper theoretically shows that this approach converges to the optimal theoretical distribution.\n\nComments \n\n1)  What will be the performance of Original GAN and Incremental GAN by finding optimal weighting ($w_i$) parameters for each of them?\n2)  Can you increase the number of parameters of the generator by no of generators used in the incremental GAN\u2019s and compare the performance?\n3) The abstract first line you have written \u2018possibly distribution\u2019 instead of \u2018probability distribution\u2019\n4) Table 1 \u2018Incremental GAN\u2019 doesn\u2019t show consistently improved performance in comparison \u2018Original GAN\u2019. Can you train a few more generators and verify it?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper316/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental training of multi-generative adversarial networks", "abstract": "Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.", "keywords": ["GAN", "Incremental training", "Information projection", "Mixture distribution"], "authorids": ["thunderingtan@gmail.com", "kenshinping@gmail.com", "xuke@tsinghua.edu.cn", "emersonswr@gmail.com", "songzuo.z@gmail.com"], "authors": ["Qi Tan", "Pingzhong Tang", "Ke Xu", "Weiran Shen", "Song Zuo"], "TL;DR": "We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.", "pdf": "/pdf/246d9668c96f57f2198604b74af957498c063386.pdf", "paperhash": "tan|incremental_training_of_multigenerative_adversarial_networks", "_bibtex": "@misc{\ntan2019incremental,\ntitle={Incremental training of multi-generative adversarial networks},\nauthor={Qi Tan and Pingzhong Tang and Ke Xu and Weiran Shen and Song Zuo},\nyear={2019},\nurl={https://openreview.net/forum?id=ryekdoCqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper316/Official_Review", "cdate": 1542234489206, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryekdoCqF7", "replyto": "ryekdoCqF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper316/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335696968, "tmdate": 1552335696968, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper316/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lT0m9n2m", "original": null, "number": 1, "cdate": 1541346260870, "ddate": null, "tcdate": 1541346260870, "tmdate": 1541534098817, "tddate": null, "forum": "ryekdoCqF7", "replyto": "ryekdoCqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper316/Official_Review", "content": {"title": "Using multiple generators to properly capture the target (data) distribution", "review": "In this work, the authors propose to use multiple generators to estimate the target distribution. Especially, it assumes the case that the range of generators is non-convex and the target distribution doesn't fall into it. To solve this issue, the multiple generators are convex combined to do better approximation and an incremental training process is proposed to train multiple generators one by one.\n\n1) Using multiple generators seems reasonable based on the authors assumption (non-convex of the range of generators), but is  this assumption based on having a perfect discriminator? Could you assume a similar case for the discriminator?\n\n2) In figure 3, it is shown each generator tries to improve the estimated target distribution. However, it is not clear what generator generates what samples. It would be better to use different colors for different generators. If I assume that the red samples are from the first generator, why the second image (top right) shows slightly shifted samples compared to the first image (top left). As far as I understand, the first generator is fixed after it is converged.\n\n3) It is shown that the (convex) weights for generators are fixed to 1, is there any reason to fixed it?\n\n4) On page 3, the equation in section 2.1 looks like missing $w_{n+1}$, could you confirm this?\n\n5) is the Original GAN exactly the Ian's original GAN or WGAN?\n\n6) Have you tried this approach using small sized generator (having  small number of parameters)?\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper316/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental training of multi-generative adversarial networks", "abstract": "Generative neural networks map a standard, possibly distribution to a complex high-dimensional distribution, which represents the real world data set. However, a determinate input distribution as well as a specific architecture of neural networks may impose limitations on capturing the diversity in the high dimensional target space. To resolve this difficulty, we propose a training framework that greedily produce a series of generative adversarial networks that incrementally capture the diversity of the target space. We show theoretically and empirically that our training algorithm converges to the theoretically optimal distribution, the projection of the real distribution onto the convex hull of the network's distribution space.", "keywords": ["GAN", "Incremental training", "Information projection", "Mixture distribution"], "authorids": ["thunderingtan@gmail.com", "kenshinping@gmail.com", "xuke@tsinghua.edu.cn", "emersonswr@gmail.com", "songzuo.z@gmail.com"], "authors": ["Qi Tan", "Pingzhong Tang", "Ke Xu", "Weiran Shen", "Song Zuo"], "TL;DR": "We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.", "pdf": "/pdf/246d9668c96f57f2198604b74af957498c063386.pdf", "paperhash": "tan|incremental_training_of_multigenerative_adversarial_networks", "_bibtex": "@misc{\ntan2019incremental,\ntitle={Incremental training of multi-generative adversarial networks},\nauthor={Qi Tan and Pingzhong Tang and Ke Xu and Weiran Shen and Song Zuo},\nyear={2019},\nurl={https://openreview.net/forum?id=ryekdoCqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper316/Official_Review", "cdate": 1542234489206, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryekdoCqF7", "replyto": "ryekdoCqF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper316/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335696968, "tmdate": 1552335696968, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper316/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}