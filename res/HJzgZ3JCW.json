{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519091051816, "tcdate": 1517267395044, "number": 7, "cdate": 1517267395044, "id": "Byipt7pBz", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "forum": "HJzgZ3JCW", "replyto": "HJzgZ3JCW", "signatures": ["ICLR.cc/2018/Conference/Paper168/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper168/Authors"], "content": {"title": "New Version", "comment": "We uploaded a new version of the paper. We also open-source our code at https://github.com/xingyul/Sparse-Winograd-CNN. The arXiv version is at https://arxiv.org/abs/1802.06367."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738200, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJzgZ3JCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper168/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper168/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper168/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers", "ICLR.cc/2018/Conference/Paper168/Authors", "ICLR.cc/2018/Conference/Paper168/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738200}}}, {"tddate": null, "ddate": null, "tmdate": 1519018238260, "tcdate": 1509044457680, "number": 168, "cdate": 1518730186674, "id": "HJzgZ3JCW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HJzgZ3JCW", "original": "rJ-xWhk0-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260100461, "tcdate": 1517249225045, "number": 41, "cdate": 1517249225028, "id": "Hy-RG1THG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HJzgZ3JCW", "replyto": "HJzgZ3JCW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper presents a modification of the Winograd convolution algorithm that reduces the number of multiplications in a forward pass of a CNN with minimal loss of accuracy. The reviewers brought up the strong results, the readability of the paper, and the thoroughness of the experiments. One concern brought up was the applicability to deeper network structures. This was acknowledged by the authors to be a subject of future work. Another issue raised was the question of theoretical vs. actual speedup. Again, this was acknowledged by the authors to be an eventual goal but subject to further systems work and architecture optimizations. The reviewers were consistent in their support of the paper. I follow their recommendation: Accept.\n", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1515777446554, "tcdate": 1515777446554, "number": 6, "cdate": 1515777446554, "id": "Hk0i6DUVz", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "forum": "HJzgZ3JCW", "replyto": "BkOMrBeGM", "signatures": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer1"], "content": {"title": "Good work", "comment": "Thanks for the clarifications! I think the score is still appropriate."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738200, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJzgZ3JCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper168/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper168/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper168/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers", "ICLR.cc/2018/Conference/Paper168/Authors", "ICLR.cc/2018/Conference/Paper168/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738200}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642402945, "tcdate": 1511585002087, "number": 1, "cdate": 1511585002087, "id": "SyMeSO8ef", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Review", "forum": "HJzgZ3JCW", "replyto": "HJzgZ3JCW", "signatures": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Good paper with thorough experiments", "rating": "7: Good paper, accept", "review": "This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network. Specifically, ReLU nonlinearity was moved after Winograd transformation to increase the dynamic sparsity in the Winograd domain, while an additional pruning on low magnitude weights and re-training procedure based on pruning is used to increase static sparsity of weights, which decreases computational demand. The resulting Winograd-ReLU\nCNN shows strong performance in three scenarios (CIFAR10 with VGG, CIFAR100 with ConvPool-CNN-C, and ImageNEt with ResNet-18). The proposed method seems to improve over the two baseline approaches (Winograd and sparsity, respectively).\n\nOverall, the paper is well-written and the experiments seems to be quite thorough and clear. Note that I am not an expert in this field and I might miss important references along this direction. I am leaving it to other reviewers to determine its novelty. \n\nPutting ReLU in the Winograd domain (or any transformed domain, e.g., Fourier) seems to be an interesting idea, and deserves some further exploration. Also, I am curious about the performance after weight pruning but before retraining).", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642401788, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer2", "ICLR.cc/2018/Conference/Paper168/AnonReviewer3", "ICLR.cc/2018/Conference/Paper168/AnonReviewer1"], "reply": {"forum": "HJzgZ3JCW", "replyto": "HJzgZ3JCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642401788}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642401849, "tcdate": 1511844682292, "number": 2, "cdate": 1511844682292, "id": "rJMLjDqeM", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Review", "forum": "HJzgZ3JCW", "replyto": "HJzgZ3JCW", "signatures": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "A promising Method, though with some limitations", "rating": "7: Good paper, accept", "review": "This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. The resultant CNN can achieve ~10x theoretical speedup with little performance loss.\n\nThe paper is well-written. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy. Rather than strictly keeping the architecture of ordinary CNNs, the proposed method applied ReLU to the transform domain, which is interesting.  \n\nThe results on Cifar-10 and ImageNet are promising. In particular, the pruned model in the Winograd domain performs comparably to the state-of-the-art dense neural networks and shows significant theoretical speedup. \nThe results on ImageNet using ResNet-18 architecture are also promising. However, no results are provided for deeper networks, so it is unclear how this method can benefit the computation of very deep neural networks \n\nA general limitation of the proposed method is the network architecture inconsistency with the ordinary CNNs. Due to the location change of ReLUs, it is unclear how to transform a pretrained ordinary CNNs to the new architectures accurately. It seems training from scratch using the transformed architectures is the simplest solution. \n\nThe paper does not report the actual speedup in the wall clock time. The actual implementation is what matters in the end. \n\nIt will be more informative to present Figure 2,3,4 with respect to the workload in addition to the weight density. \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642401788, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer2", "ICLR.cc/2018/Conference/Paper168/AnonReviewer3", "ICLR.cc/2018/Conference/Paper168/AnonReviewer1"], "reply": {"forum": "HJzgZ3JCW", "replyto": "HJzgZ3JCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642401788}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642401806, "tcdate": 1512016718176, "number": 3, "cdate": 1512016718176, "id": "HJ8UsZ6gM", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Review", "forum": "HJzgZ3JCW", "replyto": "HJzgZ3JCW", "signatures": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Well-written paper introducing a novel method of reducing multiplications in CNNs with very minor loss in accuracy", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Summary: \nThe paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy. \nThis modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way:\n- weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity\n- the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations\n\nThis way sparse multiplication can be performed. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. The authors highlight that a dimension increase in weights and ReLU activations provide a more powerful representation and that stable dynamic activation densities over layer depths benefit the representational power of ReLU layers.\n\nReview:\nThe paper shows good results using the proposed method and the description is easy to follow. I particularly like Figure 1. \nI only have a couple of questions/comments:\n1) I\u2019m not familiar with the term m-specific (\u201cMatrices B, G and A are m-specific.\u201d) and didn\u2019t find anything that seemed related in a very quick google search. Maybe it would make sense to add at least an informal description.\n2) Although small filters are the norm, you could add a note, describing up to what filter sizes this method is applicable. Or is it almost exactly the same as for general Winograd CNNs?\n3) I think it would make sense to mention weight and activation quantization in the intro as well (even if you leave a combination with quantization for future work), e.g. Rastegari et al. (2016), Courbariaux et al. (2015) and Lin et al. (2015)\n4) Figure 5 caption has a typo: \u201cacrruacy\u201d\n\nReferences:\nCourbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. \"Binaryconnect: Training deep neural networks with binary weights during propagations.\" In Advances in Neural Information Processing Systems, pp. 3123-3131. 2015.\nLin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. \"Neural networks with few multiplications.\" arXiv preprint arXiv:1510.03009 (2015).\nRastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. \"Xnor-net: Imagenet classification using binary convolutional neural networks.\" In European Conference on Computer Vision, pp. 525-542. Springer International Publishing, 2016.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642401788, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer2", "ICLR.cc/2018/Conference/Paper168/AnonReviewer3", "ICLR.cc/2018/Conference/Paper168/AnonReviewer1"], "reply": {"forum": "HJzgZ3JCW", "replyto": "HJzgZ3JCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642401788}}}, {"tddate": null, "ddate": null, "tmdate": 1515536625435, "tcdate": 1515536625435, "number": 5, "cdate": 1515536625435, "id": "rktxb6fNM", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "forum": "HJzgZ3JCW", "replyto": "ryvYuSgzf", "signatures": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer2"], "content": {"title": "Thanks for the response. ", "comment": "Thanks for the additional comments. I keep the rating. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738200, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJzgZ3JCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper168/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper168/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper168/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers", "ICLR.cc/2018/Conference/Paper168/Authors", "ICLR.cc/2018/Conference/Paper168/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738200}}}, {"tddate": null, "ddate": null, "tmdate": 1513976032181, "tcdate": 1513976032181, "number": 4, "cdate": 1513976032181, "id": "BJdkWgiGz", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "forum": "HJzgZ3JCW", "replyto": "B171DHlfG", "signatures": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper168/AnonReviewer3"], "content": {"title": "Thanks for the response", "comment": "Thanks for the response. I hold a positive opinion on this paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738200, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJzgZ3JCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper168/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper168/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper168/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers", "ICLR.cc/2018/Conference/Paper168/Authors", "ICLR.cc/2018/Conference/Paper168/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738200}}}, {"tddate": null, "ddate": null, "tmdate": 1513277567422, "tcdate": 1513277567422, "number": 3, "cdate": 1513277567422, "id": "ryvYuSgzf", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "forum": "HJzgZ3JCW", "replyto": "SyMeSO8ef", "signatures": ["ICLR.cc/2018/Conference/Paper168/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper168/Authors"], "content": {"title": "Response", "comment": "Thanks for your comments. \nWe agree that placing activation functions in other domains (e.g. Fourier) could hold more promise than we've uncovered so far.\nAs far as accuracy before re-training, since we used iterative pruning and re-training, we provide top-5 accuracy drop for each pruning step of Winograd-ReLU CNN on ImageNet:\n\noriginal density | pruned density | original accuracy | pruned accuracy (without re-training)\n100%                  |                           | 87.43%                 | \n70%                    | 60%                   | 87.456%               | 87.338% \n60%                    | 50%                   | 87.424%               | 87.202% \n50%                    | 40%                   | 87.406%               | 86.672%\n40%                    | 35%                   | 87.406%               | 86.784%\n35%                    | 30%                   | 87.358%               | 86.286%\n30%                    | 25%                   | 87.228%               | 85.692%\n25%                    | 20%                   | 86.898%               | 84.466%\n20%                    | 15%                   | 86.570%               | 80.430%\n15%                    | 12%                   | 86.246%               | 79.246%\n12%                    | 10%                   | 85.916%               | 77.128%\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738200, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJzgZ3JCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper168/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper168/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper168/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers", "ICLR.cc/2018/Conference/Paper168/Authors", "ICLR.cc/2018/Conference/Paper168/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738200}}}, {"tddate": null, "ddate": null, "tmdate": 1513277147419, "tcdate": 1513277147419, "number": 2, "cdate": 1513277147419, "id": "B171DHlfG", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "forum": "HJzgZ3JCW", "replyto": "rJMLjDqeM", "signatures": ["ICLR.cc/2018/Conference/Paper168/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper168/Authors"], "content": {"title": "Response", "comment": "We appreciate your comments and questions; thank you. Let us address each in turn:\n1) We agree, more work is warranted for deeper networks; we plan to explore this in the future.\n2) It is true that the Winograd-ReLU CNN network architecture is not equivalent to an ordinary Winograd CNN. However, training a Winograd-ReLU network from scratch is a fairly simple solution. In fact there's no transformation from ordinary CNN weights to Winograd-ReLU CNN weights: the ReLU layer sizes are different. This cannot be compensated by any weight transformation.\n3) While a reduction in wall clock time is the eventual goal, we focus here on a novel network type that reduces the theoretical number of operations needed, rather than the systems work needed to accelerate it.  This will need careful design with attention to architecture optimizations and tradeoffs, and we leave this as future work.\n4) We'll try to find a clear way to present both density and workload in these figures, thanks for the suggestion."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738200, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJzgZ3JCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper168/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper168/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper168/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers", "ICLR.cc/2018/Conference/Paper168/Authors", "ICLR.cc/2018/Conference/Paper168/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738200}}}, {"tddate": null, "ddate": null, "tmdate": 1513276688869, "tcdate": 1513276688869, "number": 1, "cdate": 1513276688869, "id": "BkOMrBeGM", "invitation": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "forum": "HJzgZ3JCW", "replyto": "HJ8UsZ6gM", "signatures": ["ICLR.cc/2018/Conference/Paper168/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper168/Authors"], "content": {"title": "Response", "comment": "Thank you for your questions and comments; please allow us to address them here.\n1) You are right to be confused - this should have been \"p-specific,\" meaning the values of B, G, and A depend on p.  We'll correct this in a future version.\n2) In general, our approach can be used wherever general Winograd convolutions can be used.  B, G, and A will be different for different patch sizes and filter sizes, and of course, we leave finding these and experimenting with larger sizes as future work.\n3) Quantization approaches could fit well in the introduction; we'll try to find a way to make it clear that it may be orthogonal to pruning and Winograd convolutions.\n4) Thanks for catching this typo.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined \u2014 applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "pdf": "/pdf/577cc25450693c482d3f2922681e407b24c98a93.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "_bibtex": "@inproceedings{\nliu2018efficient,\ntitle={Efficient Sparse-Winograd Convolutional Neural Networks},\nauthor={Xingyu Liu and Jeff Pool and Song Han and William J. Dally},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJzgZ3JCW},\n}", "keywords": ["deep learning", "convolutional neural network", "pruning"], "authors": ["Xingyu Liu", "Jeff Pool", "Song Han", "William J. Dally"], "authorids": ["xyl@stanford.edu", "jpool@nvidia.com", "songhan@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738200, "id": "ICLR.cc/2018/Conference/-/Paper168/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJzgZ3JCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper168/Authors|ICLR.cc/2018/Conference/Paper168/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper168/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper168/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper168/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper168/Reviewers", "ICLR.cc/2018/Conference/Paper168/Authors", "ICLR.cc/2018/Conference/Paper168/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738200}}}], "count": 12}