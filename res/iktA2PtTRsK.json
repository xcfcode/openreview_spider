{"notes": [{"id": "iktA2PtTRsK", "original": "fwXNVJT_AuV", "number": 1235, "cdate": 1601308138215, "ddate": null, "tcdate": 1601308138215, "tmdate": 1614985656201, "tddate": null, "forum": "iktA2PtTRsK", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "authorids": ["~Daniel_Gordon1", "~Kiana_Ehsani1", "~Dieter_Fox1", "~Ali_Farhadi3"], "authors": ["Daniel Gordon", "Kiana Ehsani", "Dieter Fox", "Ali Farhadi"], "keywords": ["Representation Learning", "Unsupervised Learning", "Video Analytics"], "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.", "one-sentence_summary": "Using unlabeled videos, we improve over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks.", "pdf": "/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gordon|watching_the_world_go_by_representation_learning_from_unlabeled_videos", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1hxAZGK7Ik", "_bibtex": "@misc{\ngordon2021watching,\ntitle={Watching the World Go By: Representation Learning from Unlabeled Videos},\nauthor={Daniel Gordon and Kiana Ehsani and Dieter Fox and Ali Farhadi},\nyear={2021},\nurl={https://openreview.net/forum?id=iktA2PtTRsK}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "qBsn8Ia3_BB", "original": null, "number": 1, "cdate": 1610040506388, "ddate": null, "tcdate": 1610040506388, "tmdate": 1610474113624, "tddate": null, "forum": "iktA2PtTRsK", "replyto": "iktA2PtTRsK", "invitation": "ICLR.cc/2021/Conference/Paper1235/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper was a difficult decision. Overall it seems to be a quality paper, well written and with many experiments, in particular evaluating learned representations across various tasks and datasets. The authors were also quite courteous in their replies which is appreciated. I really like the point the paper makes about video as a natural augmentation and I find that novel amid the recent NCE surge, where most papers rely critically on augmentation. R4 was also very positive about the paper overall concept.\n\nIn terms of paper weaknesses two of the reviewers voted for rejection because the paper ignores existing work on contrastive learning from videos. The authors rebuttal is that they are the first evaluating on images, not on videos. All reviewers also point out limited technical novelty, which the authors acknowledge. Finally, R1 is not very confident about the experiments.\n\nOverall, and after calibration, the appropriate recommendation seems to be rejection.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "authorids": ["~Daniel_Gordon1", "~Kiana_Ehsani1", "~Dieter_Fox1", "~Ali_Farhadi3"], "authors": ["Daniel Gordon", "Kiana Ehsani", "Dieter Fox", "Ali Farhadi"], "keywords": ["Representation Learning", "Unsupervised Learning", "Video Analytics"], "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.", "one-sentence_summary": "Using unlabeled videos, we improve over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks.", "pdf": "/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gordon|watching_the_world_go_by_representation_learning_from_unlabeled_videos", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1hxAZGK7Ik", "_bibtex": "@misc{\ngordon2021watching,\ntitle={Watching the World Go By: Representation Learning from Unlabeled Videos},\nauthor={Daniel Gordon and Kiana Ehsani and Dieter Fox and Ali Farhadi},\nyear={2021},\nurl={https://openreview.net/forum?id=iktA2PtTRsK}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iktA2PtTRsK", "replyto": "iktA2PtTRsK", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040506375, "tmdate": 1610474113607, "id": "ICLR.cc/2021/Conference/Paper1235/-/Decision"}}}, {"id": "0bTRSiqYLQS", "original": null, "number": 3, "cdate": 1605390650141, "ddate": null, "tcdate": 1605390650141, "tmdate": 1605405461914, "tddate": null, "forum": "iktA2PtTRsK", "replyto": "L7Sjr9-lIb", "invitation": "ICLR.cc/2021/Conference/Paper1235/-/Official_Comment", "content": {"title": "Clarity and clarifications", "comment": "Thank you for your thorough and encouraging comments. We appreciate that you have taken into account the significance of our findings in your decision. \n\n1. Clarity: Frankly, we are embarrassed to see these failures. When worrying about page limits, one often tests removing various sections without losing the core essence of the work. Clearly here we were not careful enough to ensure the proper flow of ideas. We have gone back through the paper thoroughly and made sure all ideas flow naturally and completely. We strive to deliver the ideas of our paper in an intuitive fashion without overwhelming the reader with unnecessary details (retaining completeness through appendices), and we appreciate that for the most part you think we were successful in this.\n\n2. Removing anchor/positive from the same image pair: This was actually a conscious choice to keep in as it ensures that our method is a strict superset of existing techniques like MoCo. Because we construct our dataset automatically, there may be videos which have poor temporal consistency and are simply too hard to find valuable signal from across different images. This allows us to still use these videos effectively in a single image fashion.\n\n3. Shift to different scene: We don't, but we have tuned various data collection parameters to attempt to avoid this pitfall. Specifically, the images we choose are separated by 5 seconds each, providing more visual diversity than neighboring frames, but less possibility of total visual independency than 1 minute apart. Furthermore the nature of the potential video pool is somewhat conducive to gathering videos on single subjects. Each video comes from searching a single keyword, and not using videos over 4 minutes in length, so generally they are limited to a single subject matter. We have found this to be true across a small (~1000 videos) but representative set of frames visually inspected by hand. A random sample of frames is available in the appendix.\n\n4. Direct experiment on natural transformations: This is a great suggestion. We attempted this via the tracking experiment, but could have been more thorough in our exploration of the failure modes of the various models. Robustness to occlusion and appearance change is fundamental to tracking, and as such, many videos in the tracking dataset contain significant examples of these. We will make this point more directly in the paper. The OTB videos can be individually downloaded at http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html if you are interested in seeing the actual examples."}, "signatures": ["ICLR.cc/2021/Conference/Paper1235/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "authorids": ["~Daniel_Gordon1", "~Kiana_Ehsani1", "~Dieter_Fox1", "~Ali_Farhadi3"], "authors": ["Daniel Gordon", "Kiana Ehsani", "Dieter Fox", "Ali Farhadi"], "keywords": ["Representation Learning", "Unsupervised Learning", "Video Analytics"], "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.", "one-sentence_summary": "Using unlabeled videos, we improve over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks.", "pdf": "/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gordon|watching_the_world_go_by_representation_learning_from_unlabeled_videos", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1hxAZGK7Ik", "_bibtex": "@misc{\ngordon2021watching,\ntitle={Watching the World Go By: Representation Learning from Unlabeled Videos},\nauthor={Daniel Gordon and Kiana Ehsani and Dieter Fox and Ali Farhadi},\nyear={2021},\nurl={https://openreview.net/forum?id=iktA2PtTRsK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iktA2PtTRsK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1235/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1235/Authors|ICLR.cc/2021/Conference/Paper1235/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862045, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1235/-/Official_Comment"}}}, {"id": "cgTIowXy9BY", "original": null, "number": 4, "cdate": 1605401955342, "ddate": null, "tcdate": 1605401955342, "tmdate": 1605401955342, "tddate": null, "forum": "iktA2PtTRsK", "replyto": "XWYGO_DNtx-", "invitation": "ICLR.cc/2021/Conference/Paper1235/-/Official_Comment", "content": {"title": "Correcting a few misconceptions", "comment": "Thank you for your comments on the paper. We appreciate that you think the idea is straightforward. We view this as a strength of our approach (as did SimCLR). We would like to address several of your comments and correct a few misconceptions.\n\n1. Lack of novelty: True, our paper lacks a large amount of novelty, but we believe the impact shown from our method outperforming other recent popular methods like MoCo V2 gives our paper significant scientific merit.\n\n2. Multi Modalities and missing citations: You are correct that multi-modal learning has its merits and should be addressed. We will add a short section on multi-modal learning to our related works section. However, multi-modal learning has currently not shown the same performance that simpler single image techniques like MoCo V2 and SimCLR have. Secondly, those specific papers you mentioned only show results on video datasets whereas we show improvements on both single image and video datasets. For more on this, see our response to R3.\n\n3. Missing Experiments: We chose to compare against two very strong baselines. 1) MoCo V2 which incorporates the MLP suggestion of SimCLR and subsequently outperforms it, and fully supervised pretrained ImageNet weights. Comparing against every new unsupervised method (especially those which have not published pretrained models or code) is a daunting task which we simply did not have the computational resources to do. As noted by the other reviewers, our experiments are actually quite thorough and cover a wide variety of domains in both single image and video tasks."}, "signatures": ["ICLR.cc/2021/Conference/Paper1235/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "authorids": ["~Daniel_Gordon1", "~Kiana_Ehsani1", "~Dieter_Fox1", "~Ali_Farhadi3"], "authors": ["Daniel Gordon", "Kiana Ehsani", "Dieter Fox", "Ali Farhadi"], "keywords": ["Representation Learning", "Unsupervised Learning", "Video Analytics"], "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.", "one-sentence_summary": "Using unlabeled videos, we improve over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks.", "pdf": "/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gordon|watching_the_world_go_by_representation_learning_from_unlabeled_videos", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1hxAZGK7Ik", "_bibtex": "@misc{\ngordon2021watching,\ntitle={Watching the World Go By: Representation Learning from Unlabeled Videos},\nauthor={Daniel Gordon and Kiana Ehsani and Dieter Fox and Ali Farhadi},\nyear={2021},\nurl={https://openreview.net/forum?id=iktA2PtTRsK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iktA2PtTRsK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1235/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1235/Authors|ICLR.cc/2021/Conference/Paper1235/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862045, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1235/-/Official_Comment"}}}, {"id": "QzqYLIwJ3pp", "original": null, "number": 2, "cdate": 1605389055429, "ddate": null, "tcdate": 1605389055429, "tmdate": 1605389055429, "tddate": null, "forum": "iktA2PtTRsK", "replyto": "G2MJmqKXIbk", "invitation": "ICLR.cc/2021/Conference/Paper1235/-/Official_Comment", "content": {"title": "Novelty and comparison with other video methods", "comment": "Thank you for your insightful comments. We appreciate you noting that our experiments were thorough and extensive. \n\n1. Lack of novelty: Unsupervised learning is a well-explored problem, and the NCE approach has been used successfully in many other works. However prior work with NCE has been limited to single image analysis. We are the first to adapt the NCE approach to video inputs which for the use of single image analysis.\n\n2. No comparison of other video-based approaches: This is a gap in our experiments, however not as large as you might think. The vast majority of unsupervised video techniques have only been tested on downstream video tasks, and some techniques are not even applicable on single image tasks. Furthermore, with the recent popularity in the NCE approach on single images, a large body of work has come out which essentially states that single images are enough to learn good representations. We refute this claim by showing that even on single image downstream tasks, using videos for pretraining gives a benefit over using single image techniques. This is the primary conceit of our paper, and as such is the primary focus of our experiments. We do appreciate the suggestion to incorporate more existing unsupervised video techniques, and there may indeed be some out there that outperform our method on various tasks, but a limited computational budget makes this unfeasible."}, "signatures": ["ICLR.cc/2021/Conference/Paper1235/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "authorids": ["~Daniel_Gordon1", "~Kiana_Ehsani1", "~Dieter_Fox1", "~Ali_Farhadi3"], "authors": ["Daniel Gordon", "Kiana Ehsani", "Dieter Fox", "Ali Farhadi"], "keywords": ["Representation Learning", "Unsupervised Learning", "Video Analytics"], "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.", "one-sentence_summary": "Using unlabeled videos, we improve over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks.", "pdf": "/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gordon|watching_the_world_go_by_representation_learning_from_unlabeled_videos", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1hxAZGK7Ik", "_bibtex": "@misc{\ngordon2021watching,\ntitle={Watching the World Go By: Representation Learning from Unlabeled Videos},\nauthor={Daniel Gordon and Kiana Ehsani and Dieter Fox and Ali Farhadi},\nyear={2021},\nurl={https://openreview.net/forum?id=iktA2PtTRsK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iktA2PtTRsK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1235/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1235/Authors|ICLR.cc/2021/Conference/Paper1235/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862045, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1235/-/Official_Comment"}}}, {"id": "XWYGO_DNtx-", "original": null, "number": 1, "cdate": 1602708876670, "ddate": null, "tcdate": 1602708876670, "tmdate": 1605024494942, "tddate": null, "forum": "iktA2PtTRsK", "replyto": "iktA2PtTRsK", "invitation": "ICLR.cc/2021/Conference/Paper1235/-/Official_Review", "content": {"title": "Review", "review": "This paper proposes an idea to use NCE for videos where positive/negative training pairs are created by temporally sampling different frames in the video.\n\nThe idea is fairly straightforward, there is not anything particularly novel about the approach, other than how the training pairs are created. I think it would have been more interesting and insightful if more ways to generate these pairs were proposed and evaluated. For example, video has many modalities (e.g., RGB, audio, optical flow, etc) in addition to the spatial and temporal dimensions. Exploring ways of creating pairs using multi-modal data would have been more interesting.\n\nExperimentally, there aren't really any comparisons to previous self-supervised learning methods. This is a pretty major weakness as it makes it difficult to understand how well this task is doing. Methods like SimCLR provide >70% accuracy on ImageNet and others do well on video tasks (see missing related works below). Currently, I'm not convinced by the experiments.\n\nThe studies comparing different pretraining data and multi-frame vs. single frame are interesting and show the potential of the approach. \n\nThere are some missing related works, for example:\n- \"Cooperative learning of audio and video models from self-supervised synchronization\", NeurIPS'18\n- \"Audio-visual scene analysis with self-supervised multisensory features\", ECCV'18\n- \"Evolving Losses for Unsupervised Video Representation Learning\", CVPR'20 \n\nThese works all provide strong performance on unsupervised video representation learning, yet are not mentioned or compared to.\n\nOverall, I think the proposed idea is not especially novel and the experiments aren't strong enough to show that the simple idea is good. I think there is some potential in the paper, but needs more to be convincing.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1235/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "authorids": ["~Daniel_Gordon1", "~Kiana_Ehsani1", "~Dieter_Fox1", "~Ali_Farhadi3"], "authors": ["Daniel Gordon", "Kiana Ehsani", "Dieter Fox", "Ali Farhadi"], "keywords": ["Representation Learning", "Unsupervised Learning", "Video Analytics"], "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.", "one-sentence_summary": "Using unlabeled videos, we improve over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks.", "pdf": "/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gordon|watching_the_world_go_by_representation_learning_from_unlabeled_videos", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1hxAZGK7Ik", "_bibtex": "@misc{\ngordon2021watching,\ntitle={Watching the World Go By: Representation Learning from Unlabeled Videos},\nauthor={Daniel Gordon and Kiana Ehsani and Dieter Fox and Ali Farhadi},\nyear={2021},\nurl={https://openreview.net/forum?id=iktA2PtTRsK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iktA2PtTRsK", "replyto": "iktA2PtTRsK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1235/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123368, "tmdate": 1606915801847, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1235/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1235/-/Official_Review"}}}, {"id": "L7Sjr9-lIb", "original": null, "number": 2, "cdate": 1603757984781, "ddate": null, "tcdate": 1603757984781, "tmdate": 1605024494865, "tddate": null, "forum": "iktA2PtTRsK", "replyto": "iktA2PtTRsK", "invitation": "ICLR.cc/2021/Conference/Paper1235/-/Official_Review", "content": {"title": "Learning representations from video sequences, using the Noise Contrastive Estimation proves to yield useful representations", "review": "The idea of learning representations from video rather than single images is an appealing one with many favorable properties to allow a system to get direct signal on appearance of objects under various natural transformations (occlusion, lighting, etc). Combining instance discrimination ideas of loss based on unlabelled images for which it is known whether they are similar or not, with the idea of curating the images from video is hypothesized to yield learned representations that capture properties enabling improved performance across a variety of single image tasks. The authors create a dataset based on video with positive pairs for noise contrastive estimation, conduct fairly comprehensive experiments and promise to make their newly constructed dataset available. The experiments showcase this type of learned representation outperform alternatives not based on videos on a variety of tasks. \n\nQuality : this seems like a solid paper offering a good intuitive idea with well supported experimental section to show case its relevance. \n\nClarity : the paper is quite clearly written for the most part. The dataset section 3.1 seems to have an omitted paragraph, please see point 1 below. Section 3.2 is quite lean and does not stand on its own, but relies heavily on previous work omitting much of the essence. I would recommend spending a bit more time on ensuring it is more rigorously written. See for example my comment 2 below. \n\nOriginality : the paper is modestly original. It combines two existing ideas - that of using discrimination loss for unsupervised learning of image features, and that of using video based data to allow for rich example of the same data that takes into account real world type transformations. Thus, it is hard to claim more than moderate originality. However,  \n\nSignificance : the improvements over existing baselines are solid, though I would not categorize them as dramatic. Given the originality is also solid, the overall significance is moderate. \n\nComments:\n1. The dataset generation section is strange, did you omit too much? \"We use the following fast and automated procedure to generate the images in our dataset. Using this procedure,...\" it almost seems like a few sentences were dropped between the first and second sentences. While the information exists in the appendix, a sentence or two seem to be warranted in the main test. \n2. \"Gradients flow through the positive pairs\" - at this point in the text you have only introduced a loss. The sentence warrants the question of gradients with respect to what? for rigor, and clarity of exposition, this intuition related statement should come after you talk of what is the parameterized aspect of eq (1) wrt which gradients are taken (so after eq 2 is introduced and the idea that the feature representations are captured through learned method, and in particular some reference to the NN you are using. This makes it a bit more complete as a description of the method and presented in a more methodical order. \n3. Why would you not remove the option of choosing an anchor and positive as the same image? seems easy to avoid\n4. how do you know if the video doesn't contain a shift to a different scene, with different content, thus making the positive actually very different? \n5. Since the representation you provide is supposed to learn representations that are somehow more informed about natural transformations (occlusion, lighting) - is there an experiment you can conceive of to test this specific hypothesis? i think it would both be interesting, and also give insight on whether this is indeed what is being learned. This might also make this representation useful for other types of tasks that are not looked at in the paper that I would encourage the authors to explore. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1235/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "authorids": ["~Daniel_Gordon1", "~Kiana_Ehsani1", "~Dieter_Fox1", "~Ali_Farhadi3"], "authors": ["Daniel Gordon", "Kiana Ehsani", "Dieter Fox", "Ali Farhadi"], "keywords": ["Representation Learning", "Unsupervised Learning", "Video Analytics"], "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.", "one-sentence_summary": "Using unlabeled videos, we improve over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks.", "pdf": "/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gordon|watching_the_world_go_by_representation_learning_from_unlabeled_videos", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1hxAZGK7Ik", "_bibtex": "@misc{\ngordon2021watching,\ntitle={Watching the World Go By: Representation Learning from Unlabeled Videos},\nauthor={Daniel Gordon and Kiana Ehsani and Dieter Fox and Ali Farhadi},\nyear={2021},\nurl={https://openreview.net/forum?id=iktA2PtTRsK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iktA2PtTRsK", "replyto": "iktA2PtTRsK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1235/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123368, "tmdate": 1606915801847, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1235/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1235/-/Official_Review"}}}, {"id": "G2MJmqKXIbk", "original": null, "number": 3, "cdate": 1603890138517, "ddate": null, "tcdate": 1603890138517, "tmdate": 1605024494805, "tddate": null, "forum": "iktA2PtTRsK", "replyto": "iktA2PtTRsK", "invitation": "ICLR.cc/2021/Conference/Paper1235/-/Official_Review", "content": {"title": "Initial review from R3", "review": "This paper incorporate the popular contrastive with unsupervised learning from video. Specifically, multiple frames from the same video is used as positive pairs and frames from different videos is viewed as negative pair.  The author also proposed a simple and effective ways to collect class-balanced and diverse video frame dataset from Youtube.  The author conducted extensive evaluation experiments on both video recognition and image recognition downstream tasks. Extensive ablation experiments demonstrated the effectiveness  of utilizing multiple frames and the balanced data collection algorithms. \n\nMy concerns:\n1.  Lack of novelty.  The idea of mapping different frames in one video closer,  predicting other frames in one videos, or maximizing mutual informations of embedding of different frames in one video is widely explored.  And adopting contrastive learning in video unsupervised learning scenario has been done before. (Mentioned in the related work section of your paper too)\n2. No comparison against other video based unsupervised learning algorithms.  From my viewpoint, improve over single framed based contrastive learning only proves that your algorithm successfully utilized temporal information encoded in the data and provide limited insights for exploiting more useful information from videos.  \n3. If you can demonstrate your way of incorporating contrastive learning into video based unsupervised learning offers a non-trivial improvement or have a significant difference with other video based unsupervised learning, the impact of your work will be larger. \n\nOverall, I think this paper is interesting but its contribution is limited. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1235/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1235/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "authorids": ["~Daniel_Gordon1", "~Kiana_Ehsani1", "~Dieter_Fox1", "~Ali_Farhadi3"], "authors": ["Daniel Gordon", "Kiana Ehsani", "Dieter Fox", "Ali Farhadi"], "keywords": ["Representation Learning", "Unsupervised Learning", "Video Analytics"], "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.", "one-sentence_summary": "Using unlabeled videos, we improve over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks.", "pdf": "/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gordon|watching_the_world_go_by_representation_learning_from_unlabeled_videos", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1hxAZGK7Ik", "_bibtex": "@misc{\ngordon2021watching,\ntitle={Watching the World Go By: Representation Learning from Unlabeled Videos},\nauthor={Daniel Gordon and Kiana Ehsani and Dieter Fox and Ali Farhadi},\nyear={2021},\nurl={https://openreview.net/forum?id=iktA2PtTRsK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iktA2PtTRsK", "replyto": "iktA2PtTRsK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1235/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123368, "tmdate": 1606915801847, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1235/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1235/-/Official_Review"}}}], "count": 8}