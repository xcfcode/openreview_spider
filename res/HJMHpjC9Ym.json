{"notes": [{"id": "HJMHpjC9Ym", "original": "Hkg4DXnYYQ", "number": 800, "cdate": 1538087869154, "ddate": null, "tcdate": 1538087869154, "tmdate": 1550026532043, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJgg51PHxN", "original": null, "number": 1, "cdate": 1545068423762, "ddate": null, "tcdate": 1545068423762, "tmdate": 1545354482037, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper800/Meta_Review", "content": {"metareview": "This paper propose a novel CNN architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. reviewers generally arrived at a consensus on accept.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Simple and effective"}, "signatures": ["ICLR.cc/2019/Conference/Paper800/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper800/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper800/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353082494, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper800/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper800/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper800/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353082494}}}, {"id": "Syll-AfcTQ", "original": null, "number": 4, "cdate": 1542233592107, "ddate": null, "tcdate": 1542233592107, "tmdate": 1542233592107, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper800/Official_Comment", "content": {"title": "updated the pdf", "comment": "We updated the pdf to address the comments from the reviewers. (the revised parts are highlighted in blue.)"}, "signatures": ["ICLR.cc/2019/Conference/Paper800/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper800/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper800/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620200, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJMHpjC9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference/Paper800/Reviewers", "ICLR.cc/2019/Conference/Paper800/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper800/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper800/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper800/Authors|ICLR.cc/2019/Conference/Paper800/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper800/Reviewers", "ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference/Paper800/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620200}}}, {"id": "BJxO_af9pQ", "original": null, "number": 3, "cdate": 1542233456498, "ddate": null, "tcdate": 1542233456498, "tmdate": 1542233456498, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": "B1egeFYOom", "invitation": "ICLR.cc/2019/Conference/-/Paper800/Official_Comment", "content": {"title": "feedback", "comment": "We thank the reviewer for the positive comments on our approach. We have included in Table 11 (Page 18) the results of bL-ResNet-50 and bL-ResNet-101 with alpha and beta both set to be 1. Not surprisingly, both models achieve the best accuracy, but they also become most costly in computation and are parameter heavy.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper800/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper800/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper800/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620200, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJMHpjC9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference/Paper800/Reviewers", "ICLR.cc/2019/Conference/Paper800/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper800/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper800/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper800/Authors|ICLR.cc/2019/Conference/Paper800/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper800/Reviewers", "ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference/Paper800/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620200}}}, {"id": "Hye0Uaf96X", "original": null, "number": 2, "cdate": 1542233429528, "ddate": null, "tcdate": 1542233429528, "tmdate": 1542233429528, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": "BJx2-Tcv3m", "invitation": "ICLR.cc/2019/Conference/-/Paper800/Official_Comment", "content": {"title": "feedback", "comment": "We thank the reviewer for the constructive comments. \n\n- Transfer capability of bLNet:\nWe used bLNet as a backbone network for feature extraction in the Faster RCNN + FPN detector.\nThe detection results on PASCAL VOC and COCO datasets are included in Table 10 in Appendix A6.\nOur bLNet achieves comparable or better accuracy than the baseline detectors while reducing FLOPs by about 1.5 times.\nPlease refer to Table 10 in Appendix A6 for more detail.\n\n- Memory requirements of bLNet:\nWe benchmarked the GPU memory consumption in runtime at both the training and test phases for all the models evaluated in Fig. 3.\nThe results are shown in Fig. 5 in Appendix A7. The batch size was set to 8, which is the largest number allowed for NASNet on a P100 GPU card (16 GiB memory). The image size for any model in this benchmark experiment is the same as that used in the experiment reported in Fig. 3. For bLNet, the input image size is 224x224 in training and 256x256 in test.\n\nFrom Fig. 5, we can see that bLNet is the most memory-efficient for training among all the approaches. \nIn test, bL-ResNeXt consumes more memory than inception-resnet-v2 and inception-v4 at the same accuracy, \nbut bL-SEResNeXt outperforms all the approaches. Note that NASNet and PNASNet are not memory friendly.\nThis is largely because they are trained on a larger image size (331x331) and these models are composed of many layers.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper800/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper800/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper800/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620200, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJMHpjC9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference/Paper800/Reviewers", "ICLR.cc/2019/Conference/Paper800/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper800/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper800/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper800/Authors|ICLR.cc/2019/Conference/Paper800/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper800/Reviewers", "ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference/Paper800/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620200}}}, {"id": "B1xmXpMqpm", "original": null, "number": 1, "cdate": 1542233370927, "ddate": null, "tcdate": 1542233370927, "tmdate": 1542233370927, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": "BygYyRtc3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper800/Official_Comment", "content": {"title": "feedback", "comment": "We thank the reviewer for the positive comments on our approach. We have revised the manuscript to clarify our contributions in the introduction. For the parameters alpha and beta in bLNet, although they could be tuned for each layer, we fixed them (alpha=2 and beta=4) in all our experiments except in the ablation study. We found that this universal setting in general leads to good tradeoffs between accuracy and computation cost among all the models consistently. In the future, we are interested in exploring reinforcement learning to search for optimal alpha and beta to achieve a better tradeoff.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper800/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper800/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper800/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620200, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJMHpjC9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference/Paper800/Reviewers", "ICLR.cc/2019/Conference/Paper800/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper800/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper800/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper800/Authors|ICLR.cc/2019/Conference/Paper800/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper800/Reviewers", "ICLR.cc/2019/Conference/Paper800/Authors", "ICLR.cc/2019/Conference/Paper800/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620200}}}, {"id": "BygYyRtc3Q", "original": null, "number": 3, "cdate": 1541213664818, "ddate": null, "tcdate": 1541213664818, "tmdate": 1541533680435, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper800/Official_Review", "content": {"title": "Simple way to gain performance and computation", "review": "This paper presents a novel multi-scale architecture that achieves a better trade-off speed/accuracy than most of the previous models. The main idea is to decompose a convolution block into multiple resolutions and trade computation for resolution, i.e. low computation for high resolution representations and higher computation for low resolution representations. In this way the low resolution can focus on having more layers and channels, but coarsely, while the high resolution can keep all the image details, but with a smaller representation. The branches (normally two) are merged at the end of each block with linear combination at high resolution. Results for image classification on ImageNet with different network architectures and for speech recognition on Switchboard show the accuracy and speed of the proposed model.\n\nPros:\n- The idea makes sense and it seems GPU friendly in the sense that the FLOPs reduction can be easily converted in a real speed-up\n- Results show that the joint use of two resolution can provide better accuracy and lower computational cost, which is normally quite difficult to obtain\n- The paper is well written and experiments are well presented.\n- The appendix shows many interesting additional experiments\n\nCons:\n- The improvement in performance and speed is not exceptional, but steady on all models.\n- Alpha and beta seem to be two hyper-parameters that need to be tuned for each layer.\n\nOverall evaluation:\nGlobally the paper seems well presented, with an interesting idea and many thorough experiments that show the validity of the approach. In my opinion this paper deserves to be published.\n\n\nAdditional comments:\n- - In the introduction (top of pag. 2) and in the contributions, the advantages of this approach are explained in a different manner that can be confusing. More precisely in the introduction the authors say that bL-Net yeald 2x computational saving with better accuracy. In the contributions they say that the savings in computation can be up to 1/2 with no loss in accuracy.  \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper800/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper800/Official_Review", "cdate": 1542234374245, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper800/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335805286, "tmdate": 1552335805286, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper800/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJx2-Tcv3m", "original": null, "number": 2, "cdate": 1541020932027, "ddate": null, "tcdate": 1541020932027, "tmdate": 1541533680233, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper800/Official_Review", "content": {"title": "paper review", "review": "The authors propose a new CNN architecture and show results on object and speech recognition. In particular, they propose a multi-scale CNN module that processes feature maps at various scales. They show compelling results on IN and a reduction of compute complexity\n\nPros:\n(+) The paper is well written\n(+) The method is elegant and reproducible\n(+) Results are compelling and experimentation is thorough\nCons:\n(-) Transfer to other visual tasks, beyond IN, is missing\n(-) Memory requirements are not mentioned, besides FLOPs, speed and parameters\n\nOverall, the proposed approach is elegant and clear. The impact of the multi-scale module is evident, in terms of FLOPs and performance. While their approach performs a little worse than NASNet, both in terms of FLOP efficiency and top1-error, it is simpler and easier to train. I'd like for the authors to also discuss memory requirements for training and testing the network. \n\nFinally, various papers have appeared over the recent years showing improvements over baselines on ImageNet. However, most of these papers are not impactful, because they do not show any impact to other visual tasks, such as detection. On the contrary, methods that do transfer get adopted very fast. I would be much more convinced of this approach, if the authors showed similar performance gains (both in terms of complexity and metrics) for COCO detection. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper800/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper800/Official_Review", "cdate": 1542234374245, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper800/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335805286, "tmdate": 1552335805286, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper800/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1egeFYOom", "original": null, "number": 1, "cdate": 1540032743726, "ddate": null, "tcdate": 1540032743726, "tmdate": 1541533680033, "tddate": null, "forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper800/Official_Review", "content": {"title": "extension of multi-scale network, and expected good results", "review": "The big-little module is an extension of the multi-scale module. Different scales takes different complexities: higher complexity for low-scale, and lower complexity for high scale. Two schemes of merging two branches are also discussed, and the linear combination is empirically better. \n\nAs expected, the results are better than ResNets, ResNexts, SEResNexts. I do not have  comments except ablation study is needed to show the results for more choices of alpha, beta, e.g., alpha =1, beta =1.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper800/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition", "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy.  Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.", "keywords": ["CNN", "multi-scale", "efficiency", "object recognition", "speech recognition"], "authorids": ["chenrich@us.ibm.com", "qfan@us.ibm.com", "neil.r.mallinar@ibm.com", "tom.sercu1@ibm.com", "rsferis@us.ibm.com"], "authors": ["Chun-Fu (Richard) Chen", "Quanfu Fan", "Neil Mallinar", "Tom Sercu", "Rogerio Feris"], "pdf": "/pdf/f8d29f7c14d1021ae591f4ea44813918e7317e7f.pdf", "paperhash": "chen|biglittle_net_an_efficient_multiscale_feature_representation_for_visual_and_speech_recognition", "_bibtex": "@inproceedings{\nchen2018biglittle,\ntitle={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\nauthor={Chun-Fu (Richard) Chen and Quanfu Fan and Neil Mallinar and Tom Sercu and Rogerio Feris},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMHpjC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper800/Official_Review", "cdate": 1542234374245, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJMHpjC9Ym", "replyto": "HJMHpjC9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper800/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335805286, "tmdate": 1552335805286, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper800/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}