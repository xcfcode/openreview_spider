{"notes": [{"id": "bEoxzW_EXsa", "original": "E5A6AX0F9sB", "number": 2987, "cdate": 1601308331113, "ddate": null, "tcdate": 1601308331113, "tmdate": 1615670393038, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "PEeIsf7mqYM", "original": null, "number": 1, "cdate": 1610040492182, "ddate": null, "tcdate": 1610040492182, "tmdate": 1610474098196, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "\nThe reviewers have  different views on the papers but agreed that the paper can be accepted. However, they suggested\nsome points of improvements including the writing (clarity and style) and experiments showing strong improvements\ncompared to WGAN."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040492168, "tmdate": 1610474098181, "id": "ICLR.cc/2021/Conference/Paper2987/-/Decision"}}}, {"id": "yRMmEAZB2Q8", "original": null, "number": 1, "cdate": 1603935468901, "ddate": null, "tcdate": 1603935468901, "tmdate": 1606778312838, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Review", "content": {"title": "Official Review", "review": "This paper proposes Wasserstein-2 Generative Networks (W2GNs) which is an optimal transport framework for learning generative models. Unlike minimax problems of Wasserstein GANs, the proposed approach which is based on minimizing the 2-Wasserstein distance reduces to a single-level optimization problem. The paper numerically shows that the new approach enjoys faster convergence and improves upon the performance scores of Wasserstein GANs and other optimal transport baselines. While the paper's idea on applying optimal transport tools for training generative models seems interesting, the discussed theoretical and numerical results are not supportive enough to show that the proposed approach indeed improves upon WGANs. Also, the theoretical sections have been written in a convoluted way with several weakly supported claims and the final algorithm has not been stated clearly. I, therefore, do not recommend the paper for acceptance.\n\nTo further explain my concerns, let me start with section 3 which reviews the dual formulation to 2-Wasserstein distance in Eq. (8) and also the connection to the convex conjugate optimization in Eq. (9). Here, the paper vaguely mentions that the optimization problem for computing the convex conjugate is \"convex and very complex\" followed by a brief explanation which I do not find satisfactory. Specifically, the paper's way of reasoning does not convince me why it is necessary to switch from Eq. (8)  to the paper's formulation in (12). Both the issues mentioned for problems (8) and (9) also apply to the formulation in (12) as (12) still requires taking the gradient of the convex conjugate \\psi_w. Also, I highly recommend replacing the terms \"very complex\" and \"impossible\" with more solid theoretically or numerically supported statements. \n\nNext, let me refer to the final sentences of the first paragraph of section 4.1: \" Yet such a problem is still minimax. Thus, it suffers from typical problems such as convergence to local saddle points, instabilities during training and usually requires non-trivial hyperparameters choice.\" These sentences argue that every minimax problem suffers from instability and convergence to local solutions. However, the paper's own formulation in (12) also leads to a non-convex optimization problem for which the authors show no convergence guarantees to a global solution. The paper should either remove these sentences or precisely explain why the proposed non-convex problem enjoys better convergence properties than the minimax problems. Let me also add that while Eq. (12) states an optimization problem, it still does not completely characterize a learning algorithm, because it is unclear how one wants to take the gradient of \\psi_w's convex conjugate. I think the paper should include an algorithm clearly stating the steps of learning the generative model.\n\nFinally, the theoretical guarantees in Theorem 4.1 and 4.2 do not analyze the algorithm's performance for the class of input convex neural nets. Theorem 4.1 connects the optimization error to the closeness of the generative and underlying distributions. Yet, it does not provide any guarantee on how large the optimization error could be for convex neural nets. The result of Theorem 4.2 also immediately follows from the assumptions and offers little understanding of the algorithm's performance with convex networks, since it considers the set of all differentiable functions instead. The theoretical guarantees should somehow analyze the algorithms' convergence and approximation properties for convex neural nets rather than all differentiable convex functions. Overall, the paper seems to carry several nice ideas, but the theoretical discussion needs to be significantly improved.  \n\n*****\nReview update: I thank the authors for their response and for revising the paper based on the comments. The revision addresses several of my concerns. I still think the theoretical guarantees should be stronger and therefore change my score to borderline 5.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538084618, "tmdate": 1606915798039, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2987/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Review"}}}, {"id": "kkia4K447V8", "original": null, "number": 11, "cdate": 1606127019532, "ddate": null, "tcdate": 1606127019532, "tmdate": 1606127019532, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "RWsJ97gqK_T", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Thank you for considering the updated paper", "comment": "Dear reviewer, thank you very much for your valuable comments and feedback. We really appreciate that you even raised the grade and you are positive about our results. Following your advice, we will additionally mention in the paper that Section 4.2 (in particular, Theorem 4.2) does not take into account the optimization error."}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}, {"id": "NrCemgimoKR", "original": null, "number": 3, "cdate": 1604576663903, "ddate": null, "tcdate": 1604576663903, "tmdate": 1606119696865, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Review", "content": {"title": "Reasonable approach but writing to be improved", "review": "The paper proposes a new method for learning an optimal pushforward (for the quadratic cost) from a distribution to another distribution based on samples of both distributions. The optimal transport problem is first written equivalently as a minimax problem over set of convex functions, as in Makkuva et al. 19. Then, the convex functions are parametrized as Input Convex Neural Networks (ICNN). To my understanding, the novelty of the approach is to replace the minimax problem over the parameters of the ICNN by an easier to solve a regularized minimization problem. The authors prove the consistency of their approach: loosely speaking, they show that true minimizers of their approximate problem are epsilon minimizers of the original optimal transport problem. They also provide promising numerical experiments.\n\n\n\nThe paper proposes a simple fix to drawbacks of previous works of on the topic, which e.g. propose minimax approaches. Moreover, the consistency of the approach is rigorously proven.\nOn the other hand, one could argue that the novelty is marginal (replacing a minimax problem by a regularized minimization). Moreover, the consistency result assumes access to a true minimizer of the regularized problem (which is highly non convex). Finally, the writing of the paper can be improved.\n\n    \nI rate the paper as marginally below.\n\n    \nRegarding the novelty of the approach. This can be addressed by SOTA numerical experiments, but this is not the case (although the numerical experiments are well explained and detailed). \nRegarding the consistency result. The authors provide an approximation result that does not take into account the optimization of the regularized problem. It is a difficult a problem but the authors could at least mention this difficulty. \nRegarding the overall quality of the paper. \n- The structure of the introduction is strange. Several paragraphs are used to explain points which cannot be understood at this stage or provide too much details. Several paragraphs are used to explain that we look for well structured pushforward mappings. Why not directly mention the equivalent optimal transport problem ? \nIn dimension one all continuous invertible mappings sending P to Q are monotone? I am not sure. What is a maximal monotone mapping (eq 2)? Considerations about the dimension of the space should not be in the intro in my opinion (the reader has the feeling that the authors are defending themselves). Considerations on minimax vs non minimax and end to end are too difficult to understand at this stage (same in Section 2).\n-Section 2. The notation for the Fenchel transform is misleading. It should be with a star...\n-Section 3. Why do we need a positive density? Optimal pushforward mappings exist (in both directions) if both distributions admit a density wrt Lebesgue measure. What is the point of the footnote 4? The set \"Convex\" is not defined. Convex potentials are called discriminators. Why this name? I guess that there is a connection with GANs but I don't see (actually this work is about fitting an optimal pushforward mapping, which is something independent from the GAN setup). \n-Section 4. Check Eq 10 (symbol = and alignment). Consideration on why non minimax is better than minimax should be explained before.  \"Fully-connected ICNNs satisfy universal approximation property\". What does it imply for the assumptions of Th 4.2? Are they satisfied or not?\n\n\n\nMINOR:\n- Question: Are gradients wrt to x of the ICNN easy to compute?\n- parenthesis in the equation after Forward Generative Property\n- \"diffirentiable\" (several times)\n- I have seen similar considerations (using ICNN to fit gradient of convex functions) in https://openreview.net/pdf?id=rklx-gSYPS", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538084618, "tmdate": 1606915798039, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2987/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Review"}}}, {"id": "RWsJ97gqK_T", "original": null, "number": 10, "cdate": 1606119306030, "ddate": null, "tcdate": 1606119306030, "tmdate": 1606119534637, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "3w1i4YXmG5z", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Thanks", "comment": "OK, I thank the authors for their reply and I am raising my score based on their answers.\nI encourage the authors to mention that Section 4.2 does not take into account the optimization properties."}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}, {"id": "CI5vndLT0m9", "original": null, "number": 9, "cdate": 1606079886262, "ddate": null, "tcdate": 1606079886262, "tmdate": 1606079886262, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "T9GE6ceWu-", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Thank you for considering the updated paper", "comment": "Dear reviewer, thank you very much for your valuable comments and feedback. We really appreciate that you even raised the grade and are very positive about our results."}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}, {"id": "S4LferGWJrC", "original": null, "number": 2, "cdate": 1604013526936, "ddate": null, "tcdate": 1604013526936, "tmdate": 1605643805500, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Review", "content": {"title": "An end-to-end algorithm with a non-minimax objective for training a OT map for quadratic cost", "review": "From my perspective, this is a much needed and love-to-see work for the line of neural generative modeling. Previous approaches were dominated by GAN based approaches which require solving a minimax optimization which has technical hurdles in practice. This paper reviews the literature of OT theory, and provides a comprehensive explanation about the relation between W_2 and cyclical monotonic map. In particular, it motivates the opportunity to approximate Eq. (8) with Eq.(12), as which a non-minimax formulation exist. \n\nPros:\n\n- a non-minimax formulation was proposed for neural generative modeling\n- theoretical properties are derived for the proposed formulation Eq.(12)\n- experiments are preliminary but promising. \n\nCons:\n\n- The paper misses a discussion about how gradient-based optimization is done for Eq.(12) in implementation. For example, it is not clear what functions are supposed to be parameterized as a neural network. Is it \\varphi_\\theta and \\bar{\\varphi}_\\omega? If so, how the gradient is calculated for \\Delta \\bar{\\varphi_\\omega}? Would it involve high order auto-differentiation? \n- The authors have not outlined how computational feasible their approach is? This makes me less confident about their approach regarding processing more difficult datasets or tasks. \n- I strongly suggest the authors to re-organize their presentations, and focus more on what was actually calculated in practice. This allows others to follow their work and reproducing their experiments. Some mathematical introduction is nice, but they have to be directly related to the approach of this paper. \n\n\n\n---------------\npost revision:\nI read authors' revision, and it is indeed an improved version with more readabilities. Since I am familiar with the OT literature, the mathematical presentation is clear enough for me to follow. The motivation of this paper is clear. The presentation of algorithm is also clear enough in the revision. But I can not say much for wider audience this paper may target. \n\nGiven the significance and popularity of GAN related work, this paper seems a big deal to the community. This's why I intend to give 8 instead of 7 for the revised paper.\n\n\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538084618, "tmdate": 1606915798039, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2987/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Review"}}}, {"id": "4cC-NEYbitQ", "original": null, "number": 8, "cdate": 1605634313292, "ddate": null, "tcdate": 1605634313292, "tmdate": 1605634313292, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "YSEXyEu3H_5", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Rebuttal Revision", "comment": "Dear reviewers, we have revised the paper according to your comments. Please consider the updated submission. \n\nThe edits are highlighted by the blue color in the revised version of the paper. The main edits are listed below.\n\n**(R1, R2)** We added the detailed numerical optimization algorithm to Section 4.1.\n\n**(R1, R2, R5)** We devoted Section C.2 to the discussion of the computation complexity of our training procedure.\n\n**(R1)** We added the comparison with the minimax approach by Taghvaei and Jalali (2019) to Section 5.1 (Table 1, Figure 2) and Appendix C.4 (Figure 12).\n\n**(R1)** In several relevant places of the paper (paragraphs with contributions in Introduction, the algorithm Section 4.1, etc.), we additionally highlighted the key feasible advantage w.r.t. minimax optimization, i.e. faster convergence.\n\n**(R5)** We changed the \"discriminators\" notation to \"potentials\" notation.\n\n**(R5)** We removed the (unnecessary) requirement of positive density from the theorems.\n\nIf there are any additional changes you suppose we should perform, please kindly suggest."}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}, {"id": "T9GE6ceWu-", "original": null, "number": 7, "cdate": 1605631439917, "ddate": null, "tcdate": 1605631439917, "tmdate": 1605631439917, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "hMeYR1X2XI", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Look forward to your updated version.", "comment": "I will calibrate my score based on the revision. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}, {"id": "3w1i4YXmG5z", "original": null, "number": 4, "cdate": 1605445328807, "ddate": null, "tcdate": 1605445328807, "tmdate": 1605527259458, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "NrCemgimoKR", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Answers to AnonReviewer5", "comment": "Thank you for your valuable feedback. Please find above (in our reply to all the Reviewers) the answers to your questions common with other reviews. Please below find below our answers to your questions that do not overlap with those of other Reviewers.\n\n**[1] Every 1D continuous and invertible function is monotone**\n\nThis is true, please check the discussion here\nhttps://math.stackexchange.com/questions/2476832/continuous-function-is-invertible-only-if-it-is-strictly-monotonic\n\n**[2] The notation of Fenchel transform**\n\nWe use overline notation rather that the superscript (*) to avoid the notation with multiple superscripts (e.g. dagger superscripts are already used to denote the approximating functions). If you can kindly suggest some other notation, please recommend.\n\n**[3] Positive Density**\n\nYou noted that it is not needed to require positive density everywhere. We completely agree --- in all our theoretical results this requirement can be removed. \n\n**[4] Potentials/Discriminators Notation**\n\nWe call functions $\\psi$ as discriminators in order to emphasize that they play the role of the critic between a pair of distributions. Following you advice, we will replace the \"discriminator\" notation by the \"potential\" one.\n\n**[5] The Universal Approximation Property and Theorem 4.2**\n\nTheorem 4.2 demonstrates that if the approximating classes of convex functions $\\Psi_X$, $\\overline{\\Psi_Y}$ (e.g. ICNNs) contain transport maps that are $\\epsilon_{X}$-- and $\\epsilon_{Y}$--close to the true ones, then as the result of the optimization (assuming that we achieve the optimum within those classes), we obtain $(\\psi^{\\dagger},\\overline{\\psi^{\\ddagger}})$ for which the value of the objective is $O(\\epsilon_{X}+\\epsilon_{Y})$-optimum. Next, by using our Theorem 4.1, we immediately obtain that transport maps $\\nabla\\psi^{\\dagger}$, $\\nabla\\overline{\\psi^{\\ddagger}}$ are $O(\\epsilon_{X}+\\epsilon_{Y})$-close to the optimal forward $\\nabla\\psi^{\\star}$ and inverse $\\nabla\\overline{\\psi^{\\star}}$  maps respectively in $W_{2}^2$ sense. The approximation classes $\\Psi_X$, $\\overline{\\Psi_Y}$ can be made rich enough ($\\epsilon_{X}, \\epsilon_{Y}\\rightarrow 0$) by considering a large ICNN since fully connected ICNNs satisfy the universal approximation property. Thus, we can fit the transport map with any desired error (see the end of Appendix A.2 for additional details). Note that this is not the case with entropy-based regularization of Seguy et. al. (2017) since their solution will always be biased from the true one. \n\nWe also emphasize that for the min-max method there is no analogous theoretical result. Theorem 3.5 of Makkuva et.al. (2020) provides an upper bound on the closeness of the fitted forward transport map $\\nabla\\psi^{\\dagger}$ to the true $\\nabla\\psi^{\\star}$. The upper bound is linear in the sum of the maximization gap and the minimization gap of their min-max problem. However, it remains unclear whether the optimal solution of min-max problem within some restricted classes $\\Psi_X$, $\\overline{\\Psi_Y}$ actually provides small values of the sum of mentioned gaps, even when $\\Psi_X$, $\\overline{\\Psi_Y}$ are rich enough."}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}, {"id": "gGgvTmhlas2", "original": null, "number": 6, "cdate": 1605445622730, "ddate": null, "tcdate": 1605445622730, "tmdate": 1605511208716, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "yRMmEAZB2Q8", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Answers to AnonReviewer1", "comment": "Thank you for your valuable feedback. Please find above (in our reply to all the Reviewers) the answers to your questions common with other reviews. Please find below our answers to your questions that do not overlap with those of other Reviewers.\n\n**[1] Conjugacy of neural networks**\n\nFirst we note, that in our method, analogously to Makkuva et. al. (2020), the optimal discriminators $\\psi^{\\star}$ and $\\overline{\\psi^{\\star}}$ are approximated by two separate neural networks $\\psi_{\\theta}$ and $\\overline{\\psi_{\\omega}}$ respectively. In particular, $\\overline{\\psi_{\\omega}}$ is itself a neural network (not a convex conjugate function to some neural network $\\psi_{\\omega}$).  The approximate forward and inverse transport maps are the gradients of these networks, i.e. $\\nabla\\psi_{\\theta}$ and $\\nabla\\overline{\\psi_{\\omega}}$. During optimization, there is no need to compute the conjugate functions of $\\psi_{\\theta}$, $\\overline{\\psi_{\\omega}}$ or the inverse functions for $\\nabla\\psi_{\\theta}$, $\\nabla\\overline{\\psi_{\\omega}}$. Thus, in constrast to Makkuva et.al. (2020) and Taghvaei et.al. (2019), in our method there are no additional optimization subproblems.\n\n**[2] Why non-minimax is better than minimax**\n\nIn the experimental section, we compare a feasible property of non-minimax vs. minimax optimization, i.e. convergence speed. We demonstrate that our non-minimax method for computing W2 optimal transport maps converges much faster than its closest alternative, i.e. minimax approach by Makkuva et. al. (2020). In Figure 2 (Section 5.1) and Figure 2 (Appendix C.3), we show that our non-minimax setup converges up to 10x faster in high dimensions. As we explain in Section 5.1, this naturally follows from the fact that our optimization does not require solving inner optimization subproblem.\n\nYou also noted that it is unclear why one should use our method (Eq. (12)) instead of the minimax method by Taghvaei et.al. (2019) (represented by Eq. (8) in our paper). For brevity, let us name this minimax method by MM-1 and its successor method of Makkuva et.al. (2020) [discussed in the previous paragraph] by MM-2. MM-1 method uses only one ICNN (discriminator) and solves an additional convex subproblem to compute the value of the conjugate discriminator. We initially did not include any comparison with the MM-1 method, since Makkuva et.al. (2020) [Remark 3.5] has already explained why their MM-2 method (which we outperform) is superior to MM-1 method.\n\nNevertheless, we are planning to add MM-1 method to our comparison in Section 5.1 in the next revision to demonstrate that MM-1 performs even worse than MM-2 w.r.t. the computational time. \n\nPlease tell us whether this comparison is needed and will help to convince you that MM-1 method has poor performance (in terms of the computational time).\n\n**[3] Theoretical Results**\n\nIn this paper, we do not theoretically analyze the optimization error (or provide convergence guarantees), but analyze approximation error assuming that there is no optimization error.\n\nWe demonstrate (Theorem 4.2) that if the approximating class of convex functions (e.g. ICNNs) is rich enough, then the minimizer of eq. (12) is a good approximator of the OT map. On the other hand, the approximation class can actually be made rich enough by considering a large ICNN because fully connected ICNNs satisfy the universal approximation property. The question how large ICNN architecture should be to provide a required approximation is a complicated question which depends on the many additional aspects: the parameterization, the complexity of the pair of distributions, etc. Answering this question in the context of W2 transport by ICNNs  might be the avenue for a future research.\n\nPlease note that we analyse the performance of our method in some arbitrary given classes $\\Psi_X$ and $\\overline{\\Psi_Y}$ of differentiable convex functions, e.g. represented by ICNNs. The ICNNs which we use are actually differentiable, since we use the CELU activation function instead of ReLU (all technical details on the implementation are given in Appendix B)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}, {"id": "hMeYR1X2XI", "original": null, "number": 5, "cdate": 1605445431176, "ddate": null, "tcdate": 1605445431176, "tmdate": 1605511156177, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "S4LferGWJrC", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Answers to AnonReviewer2", "comment": "Thank you for your valuable feedback. Please find above (in our reply to all the Reviewers) the answers to your questions common with other reviews. Please below find our answer to your question not which is not common with the other reviews.\n\n**Q: Would it [the optimization procedure] involve high order auto-differentiation?**\n\nAs we noted in the general answer to all the reviewers, the computational time is linear w.r.t. the computation time for a single forward pass for $\\psi_{\\theta}(x)$. While it might seem that the full second order derivative $\\frac{\\partial^{2}}{\\partial (\\theta,\\omega)\\partial (x,y)}$ (a part of the Hessian H of objective w.r.t. $(\\theta,\\omega,x,y)$) is needed, actually only its product Hv with some direction v is used (known as second directional derivative). Its computation is as fast as computation of the gradient, see http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.6143"}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}, {"id": "YSEXyEu3H_5", "original": null, "number": 3, "cdate": 1605444999758, "ddate": null, "tcdate": 1605444999758, "tmdate": 1605511116130, "tddate": null, "forum": "bEoxzW_EXsa", "replyto": "bEoxzW_EXsa", "invitation": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment", "content": {"title": "Answers to shared questions", "comment": "Dear reviewers, thanks for your insightful comments! We are currently working on improving the paper according to your comments and will soon submit the updated version. Please, find the answers to your shared questions below.\n\n**[1] Details of the Numerical Optimization Procedure: Reviewers 1 and 2**\n\n To address this concern, we are planning to add a step-by-step detailed description of the numerical procedure used to optimize our main objective Eq. (12). Please note that the architecture, the implementation and the training details are  described in Appendices B, C of the initial submission.\n\n**[2] Computational Complexity: Reviewers 1, 2 and 5**\n\nAs we note in Appendix C.1, to compute the gradients of the objective w.r.t. parameters of the ICNNs (for SGD steps, we use the automatic differentiation provided by the PyTorch framework. We emphasize that computing the forward and the backward passes of Eq. (12) is NOT difficult, see below.\n\nThe time complexity is comparable (up to a constant factor) to that of a single forward pass through the discriminator $\\psi_{\\theta}(x)$.  This claim follows from the well-known fact that gradient evaluation $\\nabla_\\theta h_\\theta(x)$ of $h_\\theta: \\mathbb{R}^{D} \\to \\mathbb{R}$, when parameterized as a neural network, requires time proportional to the size of the computational graph. Hence gradient computation requires computational time proportional to the time for evaluating the function $h_\\theta(x)$ itself. The same holds true when computing the derivative with respect to $x$. Thus, the number of operations required to compute different terms in Eq. (12), e.g. $\\nabla \\overline{\\psi_{\\omega}}(y)$,  $\\psi_{\\theta}\\big(\\nabla \\overline{\\psi_{\\omega}}(y)\\big)$ and $\\nabla\\psi_{\\theta}\\circ\\nabla \\overline{\\psi_{\\omega}}(y)$, is also linear w.r.t. the computation time of $\\psi_{\\theta}(x)$ or, equivalently, $\\overline{\\psi_{\\omega}}(x)$. As a consequence, the time required for the forward pass of Eq. (12) is larger than the forward pass for $\\psi_{\\theta}(x)$ only up to a constant factor. Thus, the backward pass for Eq. (12) with respect to parameters of ICNNs $\\theta$ and $\\omega$ is also linear in the computation time of $\\psi_{\\theta}(x)$. We empirically measured that for our DenseICNN discriminators, the computation of gradient of Eq. (12) w.r.t. parameters $\\theta,\\omega$ requires roughly 8-12x more time than the computation of $\\psi_{\\theta}(x)$. \n\nPlease note that for our model in the initial submission, we provide the convergence time in the experiments with the Gaussian setting. In Figure 2, we demonstrate that the method converges in less than 10 minutes for the highest considered dimension 4096. In Figure 12 of Appendix, we also provide wall clock times for smaller dimensions. The wall clock times for other experiments are as follows (not reported in the initial submission): $<2$ minutes for color transfer and domain adaptation, several hours (roughly $<4$) for latent space mass transport and style transfer."}, "signatures": ["ICLR.cc/2021/Conference/Paper2987/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein-2 Generative Networks", "authorids": ["~Alexander_Korotin2", "~Vage_Egiazarian1", "~Arip_Asadulaev1", "a.safin@skoltech.ru", "~Evgeny_Burnaev1"], "authors": ["Alexander Korotin", "Vage Egiazarian", "Arip Asadulaev", "Alexander Safin", "Evgeny Burnaev"], "keywords": ["wasserstein-2 distance", "optimal transport maps", "non-minimax optimization", "cycle-consistency regularization", "input-convex neural networks"], "abstract": "We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "korotin|wasserstein2_generative_networks", "one-sentence_summary": "We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.", "supplementary_material": "/attachment/78b1dd79725be47bc24a36e1524243dff6abb15f.zip", "pdf": "/pdf/dbe3a9934dc8bb605cdc8c67d7e68c0a54cf4d38.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkorotin2021wasserstein,\ntitle={Wasserstein-2 Generative Networks},\nauthor={Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander Safin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bEoxzW_EXsa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bEoxzW_EXsa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2987/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2987/Authors|ICLR.cc/2021/Conference/Paper2987/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2987/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842381, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2987/-/Official_Comment"}}}], "count": 14}