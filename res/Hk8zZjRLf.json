{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573616133, "tcdate": 1521573616133, "number": 306, "cdate": 1521573615790, "id": "rJ_x1JJqz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Hk8zZjRLf", "replyto": "Hk8zZjRLf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION", "abstract": "Stochastic Gradient Descent or SGD is the most popular optimization algorithm for large-scale problems. SGD estimates the gradient by uniform sampling with sample size one. There have been several other works that suggest faster epoch wise convergence by using weighted non-uniform sampling for better gradient estimates. Unfortunately, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient. As a result, the false impression of faster convergence in iterations leads to slower convergence in time, which we call a chicken-and-egg loop. In this paper, we break this barrier by providing the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to that of the uniform sampling. Such an algorithm is possible due to the sampling view of Locality Sensitive Hashing (LSH), which came to light recently. As a consequence of superior and fast estimation, we reduce the running time of all existing gradient descent algorithms. We demonstrate the benefits of our proposal on both SGD and AdaGrad.", "pdf": "/pdf/bd9378ecbd459ce0f4a1ac3514240a136fef161b.pdf", "TL;DR": "We improve the running of all existing gradient descent algorithms.", "paperhash": "chen|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation", "_bibtex": "@misc{\nchen2018lshsampling,\ntitle={{LSH}-{SAMPLING} {BREAKS} {THE} {COMPUTATIONAL} {CHICKEN}-{AND}-{EGG} {LOOP} {IN} {ADAPTIVE} {STOCHASTIC} {GRADIENT} {ESTIMATION}},\nauthor={Beidi Chen, Yingchen Xu, Anshumali Shrivastava},\nyear={2018},\nurl={https://openreview.net/forum?id=SyVOjfbRb},\n}", "keywords": ["Stochastic Gradient Descent", "Optimization", "Sampling", "Estimation"], "authors": ["Beidi Chen", "Yingchen Xu", "Anshumali Shrivastava"], "authorids": ["beidi.chen@rice.edu", "yingchen.xu@rice.edu", "anshumali@rice.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730160656, "tcdate": 1518412046780, "number": 88, "cdate": 1518412046780, "id": "Hk8zZjRLf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Hk8zZjRLf", "original": "SyVOjfbRb", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION", "abstract": "Stochastic Gradient Descent or SGD is the most popular optimization algorithm for large-scale problems. SGD estimates the gradient by uniform sampling with sample size one. There have been several other works that suggest faster epoch wise convergence by using weighted non-uniform sampling for better gradient estimates. Unfortunately, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient. As a result, the false impression of faster convergence in iterations leads to slower convergence in time, which we call a chicken-and-egg loop. In this paper, we break this barrier by providing the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to that of the uniform sampling. Such an algorithm is possible due to the sampling view of Locality Sensitive Hashing (LSH), which came to light recently. As a consequence of superior and fast estimation, we reduce the running time of all existing gradient descent algorithms. We demonstrate the benefits of our proposal on both SGD and AdaGrad.", "pdf": "/pdf/bd9378ecbd459ce0f4a1ac3514240a136fef161b.pdf", "TL;DR": "We improve the running of all existing gradient descent algorithms.", "paperhash": "chen|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation", "_bibtex": "@misc{\nchen2018lshsampling,\ntitle={{LSH}-{SAMPLING} {BREAKS} {THE} {COMPUTATIONAL} {CHICKEN}-{AND}-{EGG} {LOOP} {IN} {ADAPTIVE} {STOCHASTIC} {GRADIENT} {ESTIMATION}},\nauthor={Beidi Chen, Yingchen Xu, Anshumali Shrivastava},\nyear={2018},\nurl={https://openreview.net/forum?id=SyVOjfbRb},\n}", "keywords": ["Stochastic Gradient Descent", "Optimization", "Sampling", "Estimation"], "authors": ["Beidi Chen", "Yingchen Xu", "Anshumali Shrivastava"], "authorids": ["beidi.chen@rice.edu", "yingchen.xu@rice.edu", "anshumali@rice.edu"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730160656, "tcdate": 1509137261557, "number": 939, "cdate": 1518730160645, "id": "SyVOjfbRb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SyVOjfbRb", "original": "r1rwoGZAW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION", "abstract": "Stochastic Gradient Descent or SGD is the most popular optimization algorithm for large-scale problems. SGD estimates the gradient by uniform sampling with sample size one. There have been several other works that suggest faster epoch wise convergence by using weighted non-uniform sampling for better gradient estimates. Unfortunately, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient. As a result, the false impression of faster convergence in iterations leads to slower convergence in time, which we call a chicken-and-egg loop. In this paper, we break this barrier by providing the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to that of the uniform sampling. Such an algorithm is possible due to the sampling view of Locality Sensitive Hashing (LSH), which came to light recently. As a consequence of superior and fast estimation, we reduce the running time of all existing gradient descent algorithms. We demonstrate the benefits of our proposal on both SGD and AdaGrad.", "pdf": "/pdf/8e9ff307abb6c03d5f9613f697b6fbd4f7435692.pdf", "TL;DR": "We improve the running of all existing gradient descent algorithms.", "paperhash": "chen|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation", "_bibtex": "@misc{\nchen2018lshsampling,\ntitle={{LSH}-{SAMPLING} {BREAKS} {THE} {COMPUTATIONAL} {CHICKEN}-{AND}-{EGG} {LOOP} {IN} {ADAPTIVE} {STOCHASTIC} {GRADIENT} {ESTIMATION}},\nauthor={Beidi Chen and Yingchen Xu and Anshumali Shrivastava},\nyear={2018},\nurl={https://openreview.net/forum?id=SyVOjfbRb},\n}", "keywords": ["Stochastic Gradient Descent", "Optimization", "Sampling", "Estimation"], "authors": ["Beidi Chen", "Yingchen Xu", "Anshumali Shrivastava"], "authorids": ["beidi.chen@rice.edu", "yingchen.xu@rice.edu", "anshumali@rice.edu"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}