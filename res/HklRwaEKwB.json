{"notes": [{"id": "HklRwaEKwB", "original": "H1l8IFowPH", "number": 615, "cdate": 1569439078032, "ddate": null, "tcdate": 1569439078032, "tmdate": 1583912045059, "tddate": null, "forum": "HklRwaEKwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "authors": ["Sifan Liu", "Edgar Dobriban"], "authorids": ["sfliu@stanford.edu", "dobribanedgar@gmail.com"], "keywords": ["ridge regression", "sketching", "random matrix theory", "cross-validation", "high-dimensional asymptotics"], "TL;DR": "We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching.", "abstract": "We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. \nWe study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.", "pdf": "/pdf/455671549ea589bd0c09c5457c52062150923fca.pdf", "code": "https://github.com/liusf15/RidgeRegression", "paperhash": "liu|ridge_regression_structure_crossvalidation_and_sketching", "_bibtex": "@inproceedings{\nLiu2020Ridge,\ntitle={Ridge Regression: Structure, Cross-Validation, and Sketching},\nauthor={Sifan Liu and Edgar Dobriban},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklRwaEKwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/93023a7889d9100ddba9f147df77e93d85b4dc78.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "AKVb60TfqP", "original": null, "number": 1, "cdate": 1576798701430, "ddate": null, "tcdate": 1576798701430, "tmdate": 1576800934562, "tddate": null, "forum": "HklRwaEKwB", "replyto": "HklRwaEKwB", "invitation": "ICLR.cc/2020/Conference/Paper615/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The paper studies theoretical properties of ridge regression, and in particular how to correct for the bias of the estimator. \n\nThe reviewers appreciated the contribution and the fact that you updated the manuscript to make it clearer.\n\nI however advise the authors to think about the best way to maximize impact for the ICLR audience, perhaps by providing relevant examples from the ML literature.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "authors": ["Sifan Liu", "Edgar Dobriban"], "authorids": ["sfliu@stanford.edu", "dobribanedgar@gmail.com"], "keywords": ["ridge regression", "sketching", "random matrix theory", "cross-validation", "high-dimensional asymptotics"], "TL;DR": "We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching.", "abstract": "We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. \nWe study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.", "pdf": "/pdf/455671549ea589bd0c09c5457c52062150923fca.pdf", "code": "https://github.com/liusf15/RidgeRegression", "paperhash": "liu|ridge_regression_structure_crossvalidation_and_sketching", "_bibtex": "@inproceedings{\nLiu2020Ridge,\ntitle={Ridge Regression: Structure, Cross-Validation, and Sketching},\nauthor={Sifan Liu and Edgar Dobriban},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklRwaEKwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/93023a7889d9100ddba9f147df77e93d85b4dc78.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HklRwaEKwB", "replyto": "HklRwaEKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722184, "tmdate": 1576800273430, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper615/-/Decision"}}}, {"id": "BkgrXByTYB", "original": null, "number": 2, "cdate": 1571775773073, "ddate": null, "tcdate": 1571775773073, "tmdate": 1573748507030, "tddate": null, "forum": "HklRwaEKwB", "replyto": "HklRwaEKwB", "invitation": "ICLR.cc/2020/Conference/Paper615/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper deals with 3 theoretical properties of ridge regression. First, it proves that the ridge regression estimator is equivalent to a specific representation which is useful as for instance it can be used to derive the training error of the ridge estimator. Second, it provides a bias correction mechanism for ridge regression and finally it provides proofs regarding the accuracy of several sketching algorithms for ridge regression.\n \nThe paper addresses an important problem and puts itself nicely in context of previous work. However, it comes across as three papers stapled together, that were submitted to some journal and have now been put into ICLR format. Most of the important results are in the appendix. The main body of the paper is just a smattering of theorems with some text flowing around them and too much notation. There is little or no intuition provided for the proofs in the paper. Moreover, the connection between the 3 theoretical properties of ridge regression studied is also unclear. There could very well be 2 or 3 conference papers written out of this one paper. \n\nAs far as the technical merit is concerned, I checked some theory and it appears correct, however some things were unclear. For example, the Theorem 2.1 is proven under a random design setting; how would the proven ridge representation look under a fixed design setting? Or can we even prove something in that case?\n\nI think the paper definitely has some merit but the presentation makes it hard to assess it. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper615/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper615/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "authors": ["Sifan Liu", "Edgar Dobriban"], "authorids": ["sfliu@stanford.edu", "dobribanedgar@gmail.com"], "keywords": ["ridge regression", "sketching", "random matrix theory", "cross-validation", "high-dimensional asymptotics"], "TL;DR": "We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching.", "abstract": "We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. \nWe study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.", "pdf": "/pdf/455671549ea589bd0c09c5457c52062150923fca.pdf", "code": "https://github.com/liusf15/RidgeRegression", "paperhash": "liu|ridge_regression_structure_crossvalidation_and_sketching", "_bibtex": "@inproceedings{\nLiu2020Ridge,\ntitle={Ridge Regression: Structure, Cross-Validation, and Sketching},\nauthor={Sifan Liu and Edgar Dobriban},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklRwaEKwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/93023a7889d9100ddba9f147df77e93d85b4dc78.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklRwaEKwB", "replyto": "HklRwaEKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper615/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper615/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575495576475, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper615/Reviewers"], "noninvitees": [], "tcdate": 1570237749564, "tmdate": 1575495576492, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper615/-/Official_Review"}}}, {"id": "Hkg5V-bIor", "original": null, "number": 4, "cdate": 1573421362214, "ddate": null, "tcdate": 1573421362214, "tmdate": 1573421362214, "tddate": null, "forum": "HklRwaEKwB", "replyto": "rygXNl-qtr", "invitation": "ICLR.cc/2020/Conference/Paper615/-/Official_Comment", "content": {"title": "Reply to Referee #2", "comment": "We thank the referee for the many helpful comments. We provide some answers below.\n\n\n\"This paper presents a theoretical study of ridge regression, focusing on the practical problems of correcting for the bias of the cross-validation based estimate of the optimal regularisation parameter, and quantification of the asymptotic risk of sketching algorithms for ridge regression, both in the p / n -> gamma in (0, 1) regime (n = # data points, p = # dimensions). The authors derive most of their results exploiting their (AFAICT) new asymptotic characterisation of the ridge regression estimator which may be of independent interest. The whole study is complemented by a series of numerical experiments.\"\n\n- Thanks for that summary. We would like to clarify that the asymptotic characterization (while new), can only be used to derive part of our results. Namely, the representation characterizes linear functionals of the estimator, and thus can be use to derive the bias of the estimator. However, it can not directly be used to derive the variance, and in fact this seems to require some major advances in random matrix theory. We have added a comment about this after the theorem. \n\nMoreover, we would like to clarify that we allow gamma>1 everywhere in the paper. This is possible because we work with a positive lambda>0 regularization parameter, and so ridge regression is always well defined.\n\n\n\"I am recommending this paper to be accepted for publication at ICLR. The paper is clearly written, makes several solid theoretical contributions, and recommends a simple and practical bias correction for CV-based estimates of the optimal ridge regulariser. While ICLR is biased towards deep learning focused publications, the work of Belkin, Hsu, Ma, Bartlett, Hastie, Montanari, Rakhlin, Liang and others (apologies to everyone whose name was omitted---it is for the sole reason of brevity) has recently shown that we can learn non-negligible amount about neural networks from study of linear models.\"\n\n\nComments:\n\n\"- Most of the examples in the paper focus on the regime gamma < 1. Would you expect the observed behaviours to be significantly different when gamma > 1? I am asking specifically because of [1] which has found that the bias of the risk estimate obtained via cross-validation is often most extreme when p >> n, which makes me wonder about what would an experiment like those in fig.2 look like in the p >> n regime?\"\n\n- As we mentioned above, our theory covers gamma>1. For Figure 2, the particular datasets we used happen to have n>p (ie gamma<1). We have added some additional simulations for the p>>n case to the already existing simulations, see Figure 7 in Section A.5. However, in this particular case, while CV indeed has a very large bias for the test error, our correction does not significantly improve the test error. Looking at the figure, it seems we would need a stronger bias correction; however, it is unclear to us at the moment how to od this in a principled way.\n\n\"- I was somewhat confused when I first read the statement of thm.2.1. In particular, the definition of asymptotic equivalence requires (roughly speaking) that any series of projections of the difference between the two random vectors converges to zero a.s. However, within the theorem, you introduce Z without much explanation which confused me because not every Z ~ standard normal would be asymptotically equivalent. I needed to look at the proof to understand how Z is \u201ccoupled\u201d with hat(beta), which I think should not be necessary. If possible, I would either say that there exists a (series of) Z (all standard normal) s.t. the asymptotic equivalence holds, or add some other clarification (possibly in the form of a footnote).\"\n\n- We have clarified the statement and mentioned (right after the theorem) that the noise is coupled with the estimator. However, we don't think that the result would hold for an independent noise vector Z, because that would require the two noise vectors Z,Z' to be asymptotically equivalent, while we think that they cannot be (e.g., the difference between the first coordinates is O(1) and does not vanish.)\n\n\n\"- When you are citing a book, please consider citing exact pages or at least chapters/sections (e.g., when citing the exact shortcut from Hastie et al. (2019)).\"\n\n- Actually the cited reference Hastie et al 2019 is a paper (\"surprises in interpolation\"). We already had the reference to page 243 of Hastie et al 2009 (the ESL book).\n\n\"- In the definition of asymptotic equivalence (starting at the bottom of p.2), did you mean to assume limsup ||w|| < \\infty a.s. (or is limsup not needed here)?\"\n\nGood point, we added the limsup."}, "signatures": ["ICLR.cc/2020/Conference/Paper615/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper615/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "authors": ["Sifan Liu", "Edgar Dobriban"], "authorids": ["sfliu@stanford.edu", "dobribanedgar@gmail.com"], "keywords": ["ridge regression", "sketching", "random matrix theory", "cross-validation", "high-dimensional asymptotics"], "TL;DR": "We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching.", "abstract": "We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. \nWe study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.", "pdf": "/pdf/455671549ea589bd0c09c5457c52062150923fca.pdf", "code": "https://github.com/liusf15/RidgeRegression", "paperhash": "liu|ridge_regression_structure_crossvalidation_and_sketching", "_bibtex": "@inproceedings{\nLiu2020Ridge,\ntitle={Ridge Regression: Structure, Cross-Validation, and Sketching},\nauthor={Sifan Liu and Edgar Dobriban},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklRwaEKwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/93023a7889d9100ddba9f147df77e93d85b4dc78.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklRwaEKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper615/Authors", "ICLR.cc/2020/Conference/Paper615/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper615/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper615/Reviewers", "ICLR.cc/2020/Conference/Paper615/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper615/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper615/Authors|ICLR.cc/2020/Conference/Paper615/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168817, "tmdate": 1576860545962, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper615/Authors", "ICLR.cc/2020/Conference/Paper615/Reviewers", "ICLR.cc/2020/Conference/Paper615/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper615/-/Official_Comment"}}}, {"id": "HJecJWZ8iH", "original": null, "number": 3, "cdate": 1573421281645, "ddate": null, "tcdate": 1573421281645, "tmdate": 1573421307773, "tddate": null, "forum": "HklRwaEKwB", "replyto": "BkgrXByTYB", "invitation": "ICLR.cc/2020/Conference/Paper615/-/Official_Comment", "content": {"title": "Reply to Reviewer #1", "comment": "We thank the referee for their careful reading of the manuscript and for the many helpful suggestions. We have made several changes to try and address the issues they raised. We reproduce their comments and our replies below. We are more than happy to work with the referee to make sure we address all comments within our ability.\n\n\n\"This paper deals with 3 theoretical properties of ridge regression. First, it proves that the ridge regression estimator is equivalent to a specific representation which is useful as for instance it can be used to derive the training error of the ridge estimator. Second, it provides a bias correction mechanism for ridge regression and finally it provides proofs regarding the accuracy of several sketching algorithms for ridge regression.\"\n\n- We thank the reviewer for this summary.\n \n\"The paper addresses an important problem and puts itself nicely in context of previous work. However, it comes across as three papers stapled together, that were submitted to some journal and have now been put into ICLR format.\"\n\n- We thank the reviewer for acknowledging the importance of the problem we address. However, we emphasize that we have *not* submitted this paper anywhere (neither to a journal nor to a conference) before, and so this is perhaps why the structure is less than perfect. However, we believe that the unifying theme of our paper is that we study ridge regression in a common high-dimensional asymptotic framework, and also several results rely on having a random effects model for the regression coefficients beta. We think that this gives the paper a unifying theme.\n\n\"Most of the important results are in the appendix.\"\n\n- We thank the reviewer for raising this concern. We have moved one of the results from the appendix to the main body, namely the result on the MSE and training error of ridge (now Theorem 2.2). We think that most of the other results in the appendix are now merely supporting results, and the main results are already in the paper. However, if the referee thinks there are some remaining results which should be moved to the main paper, we are happy to do so (please let us know which ones.)\n\n\"The main body of the paper is just a smattering of theorems with some text flowing around them and too much notation. There is little or no intuition provided for the proofs in the paper.\" \n\n- We have attempted to add more text in the main body, and in particular increased the number of pages from 8 to 10. We have added paragraph headings to many places, and we hope that this will make the paper easier to navigate. We have also added intuitive descriptions (and high-level steps) to the proofs, right after the statements. We hope that this will make the paper more readable, and please let us know what else we should improve.\n\n\"Moreover, the connection between the 3 theoretical properties of ridge regression studied is also unclear. There could very well be 2 or 3 conference papers written out of this one paper. \"\n\n- We agree that there are a few separate topics addressed here. However, as we mentioned above, we think that they are tied together by the common high-dimensional asymptotic framework. In terms of structuring, we hope that this structure will make one strong paper, instead of two-three not so strong ones. However, if the referee insists, we can try to restructure our paper (e.g., move sketching to a separate paper).\n\n\"As far as the technical merit is concerned, I checked some theory and it appears correct, however some things were unclear. For example, the Theorem 2.1 is proven under a random design setting; how would the proven ridge representation look under a fixed design setting? Or can we even prove something in that case?\"\n\n- Thank you for that insightful question. The representation indeed depends on the random design setting. For a fixed design, as far as we can tell, there is not much more than one can say beyond the obvious decomposition (which follows directly from the definition)\n\n$(X^TX+l*I)^{-1}(X^TX)*\\beta+(X^TX+l*I)^{-1}X^T\\epsilon$\n\n- We have added a comment about this before the theorem. We also mentioned that 'However, for a random design, we can find a representation that depends on the true covariance $\\Sigma$, which may be simpler when $\\Sigma$ is simple, e.g., when $\\Sigma=I_p$ is isotropic.'\n\n\"I think the paper definitely has some merit but the presentation makes it hard to assess it.\" \n\nWe hope that our edits improved the presentation, and that this will make the merits of our paper easier to asses. Please let us know if you have any other feedback and we will do our best to address those issues.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper615/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper615/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "authors": ["Sifan Liu", "Edgar Dobriban"], "authorids": ["sfliu@stanford.edu", "dobribanedgar@gmail.com"], "keywords": ["ridge regression", "sketching", "random matrix theory", "cross-validation", "high-dimensional asymptotics"], "TL;DR": "We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching.", "abstract": "We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. \nWe study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.", "pdf": "/pdf/455671549ea589bd0c09c5457c52062150923fca.pdf", "code": "https://github.com/liusf15/RidgeRegression", "paperhash": "liu|ridge_regression_structure_crossvalidation_and_sketching", "_bibtex": "@inproceedings{\nLiu2020Ridge,\ntitle={Ridge Regression: Structure, Cross-Validation, and Sketching},\nauthor={Sifan Liu and Edgar Dobriban},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklRwaEKwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/93023a7889d9100ddba9f147df77e93d85b4dc78.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklRwaEKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper615/Authors", "ICLR.cc/2020/Conference/Paper615/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper615/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper615/Reviewers", "ICLR.cc/2020/Conference/Paper615/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper615/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper615/Authors|ICLR.cc/2020/Conference/Paper615/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168817, "tmdate": 1576860545962, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper615/Authors", "ICLR.cc/2020/Conference/Paper615/Reviewers", "ICLR.cc/2020/Conference/Paper615/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper615/-/Official_Comment"}}}, {"id": "rygXNl-qtr", "original": null, "number": 1, "cdate": 1571586090763, "ddate": null, "tcdate": 1571586090763, "tmdate": 1572972573175, "tddate": null, "forum": "HklRwaEKwB", "replyto": "HklRwaEKwB", "invitation": "ICLR.cc/2020/Conference/Paper615/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a theoretical study of ridge regression, focusing on the practical problems of correcting for the bias of the cross-validation based estimate of the optimal regularisation parameter, and quantification of the asymptotic risk of sketching algorithms for ridge regression, both in the p / n -> gamma in (0, 1) regime (n = # data points, p = # dimensions). The authors derive most of their results exploiting their (AFAICT) new asymptotic characterisation of the ridge regression estimator which may be of independent interest. The whole study is complemented by a series of numerical experiments.\n\nI am recommending this paper to be accepted for publication at ICLR. The paper is clearly written, makes several solid theoretical contributions, and recommends a simple and practical bias correction for CV-based estimates of the optimal ridge regulariser. While ICLR is biased towards deep learning focused publications, the work of Belkin, Hsu, Ma, Bartlett, Hastie, Montanari, Rakhlin, Liang and others (apologies to everyone whose name was omitted---it is for the sole reason of brevity) has recently shown that we can learn non-negligible amount about neural networks from study of linear models.\n\n\nComments:\n\n- Most of the examples in the paper focus on the regime gamma < 1. Would you expect the observed behaviours to be significantly different when gamma > 1? I am asking specifically because of [1] which has found that the bias of the risk estimate obtained via cross-validation is often most extreme when p >> n, which makes me wonder about what would an experiment like those in fig.2 look like in the p >> n regime?\n\n- I was somewhat confused when I first read the statement of thm.2.1. In particular, the definition of asymptotic equivalence requires (roughly speaking) that any series of projections of the difference between the two random vectors converges to zero a.s. However, within the theorem, you introduce Z without much explanation which confused me because not every Z ~ standard normal would be asymptotically equivalent. I needed to look at the proof to understand how Z is \u201ccoupled\u201d with hat(beta), which I think should not be necessary. If possible, I would either say that there exists a (series of) Z (all standard normal) s.t. the asymptotic equivalence holds, or add some other clarification (possibly in the form of a footnote).\n\n- When you are citing a book, please consider citing exact pages or at least chapters/sections (e.g., when citing the exact shortcut from Hastie et al. (2019)).\n\n- In the definition of asymptotic equivalence (starting at the bottom of p.2), did you mean to assume limsup ||w|| < \\infty a.s. (or is limsup not needed here)?\n\n\nReferences:\n\n[1] Tibshirani, R. J., & Tibshirani, R. (2009). A bias correction for the minimum error rate in cross-validation. The Annals of Applied Statistics, 822-829."}, "signatures": ["ICLR.cc/2020/Conference/Paper615/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper615/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "authors": ["Sifan Liu", "Edgar Dobriban"], "authorids": ["sfliu@stanford.edu", "dobribanedgar@gmail.com"], "keywords": ["ridge regression", "sketching", "random matrix theory", "cross-validation", "high-dimensional asymptotics"], "TL;DR": "We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching.", "abstract": "We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. \nWe study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.", "pdf": "/pdf/455671549ea589bd0c09c5457c52062150923fca.pdf", "code": "https://github.com/liusf15/RidgeRegression", "paperhash": "liu|ridge_regression_structure_crossvalidation_and_sketching", "_bibtex": "@inproceedings{\nLiu2020Ridge,\ntitle={Ridge Regression: Structure, Cross-Validation, and Sketching},\nauthor={Sifan Liu and Edgar Dobriban},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklRwaEKwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/93023a7889d9100ddba9f147df77e93d85b4dc78.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklRwaEKwB", "replyto": "HklRwaEKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper615/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper615/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575495576475, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper615/Reviewers"], "noninvitees": [], "tcdate": 1570237749564, "tmdate": 1575495576492, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper615/-/Official_Review"}}}], "count": 6}