{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396446712, "tcdate": 1486396446712, "number": 1, "id": "ryw73MIOx", "invitation": "ICLR.cc/2017/conference/-/paper228/acceptance", "forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "all reviewers agree that the paper is not convincing enough at this stage but needs more work to be ready for ICLR (e.g. missing comparisons to other existing methods)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396447206, "id": "ICLR.cc/2017/conference/-/paper228/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396447206}}}, {"tddate": null, "tmdate": 1482749193536, "tcdate": 1482749193536, "number": 2, "id": "BybMB_AEl", "invitation": "ICLR.cc/2017/conference/-/paper228/public/comment", "forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Possible relevant work not discussed", "comment": "This paper misses discussion of 'Diversity Networks' published in ICLR 2016 (https://arxiv.org/abs/1511.05077). This work also looks at correlations among neural activations.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287675134, "id": "ICLR.cc/2017/conference/-/paper228/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJIY0E9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper228/reviewers", "ICLR.cc/2017/conference/paper228/areachairs"], "cdate": 1485287675134}}}, {"tddate": null, "tmdate": 1482340681845, "tcdate": 1482340681845, "number": 3, "id": "B1zUKEd4x", "invitation": "ICLR.cc/2017/conference/-/paper228/official/review", "forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "signatures": ["ICLR.cc/2017/conference/paper228/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper228/AnonReviewer3"], "content": {"title": "Good proof of concept, but more experimental evidence needed", "rating": "5: Marginally below acceptance threshold", "review": "Summary:\nIn this paper, the authors introduce NoiseOut, a way to reduce parameters by pruning neurons from a network. \nThey do this by identifying pairs of neurons produce the most correlated outputs, and replacing the pair by one neuron, and then appropriately adjusting weights.\nThis technique relies on neurons having high correlations however, so they introduce an additional output neuron -- a noise output, which results in the network trying to predict the mean of the noise distribution.\nAs this is a constant, it increases correlation between neurons.\nExperiments test this out on MNIST and SVHN\n\nComments:\nThis is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well. \n\nEquation (5) seems to be very straightforwards?\n\nIt seems like that for larger datasets, more noise outputs might have to be added to ensure higher correlations? Is there a downside to this in terms of the overall accuracy?\n\nThe paper is presented clearly, and was definitely interesting to read, so I encourage the authors to continue this line of work.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655952, "id": "ICLR.cc/2017/conference/-/paper228/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper228/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper228/AnonReviewer1", "ICLR.cc/2017/conference/paper228/AnonReviewer2", "ICLR.cc/2017/conference/paper228/AnonReviewer3"], "reply": {"forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655952}}}, {"tddate": null, "tmdate": 1482183240894, "tcdate": 1482183240894, "number": 2, "id": "SybLfCSEe", "invitation": "ICLR.cc/2017/conference/-/paper228/official/review", "forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "signatures": ["ICLR.cc/2017/conference/paper228/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper228/AnonReviewer2"], "content": {"title": "Interesting results and the start of a good paper", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes and tests two ideas. (1) a method of pruning networks by identifying highly correlated neuron pairs, pruning one of the pair, and then modifying downstream weights to compensate for the removal (which works well if the removed neurons were highly correlated). (2) a method, dubbed NoiseOut, for increasing neuron correlation by adding auxiliary noise target outputs to the network during training.\n\n\nThe first idea (1) is fairly straightforward, and it is not clear if it has been tried before. It does seem to work.\n\n\nThe second idea (2) is of unclear value and seems to this reviewer that it may merely add a regularizing effect. Comments in this direction:\n - In Fig 4 (right), the constant and Gaussian treatments seem to produce the same effect in both networks, right? And the Binomial effect seems the same as No_Noise. If this is true, can we conclude that the NoiseOut targets are simply serving to regularize the network, that is, to reduce its capacity slightly?\n - To show whether this effect is true, one would need to compare to other methods of reducing the network capacity, for example: by reducing the number of neurons, by applying L2 regularization of various values, or by applying Dropout of various strengths. Fig 7 makes an attempt at this direction, but critically misses several comparison treatments: \u201cPruned without any regularization\u201d, \u201cPruned with only L2\u201d, and \u201cPruned with only DropOut\u201d. Have these experiments been run? Can their results be included and used to produce plots like Fig 5 and Fig 7?\n\nWithout these comparisons, it seems impossible to conclude that NoiseOut does anything but provide similar regularization to DropOut or L2.\n\n\nThe combined ideas (1) + (2) DO produce a considerable reduction in parameters, but sadly the experiments and exposition are somewhat too lacking to really understand what is going on. With a little more work the paper could be quite interesting, but as is it should probably not be accepted.\n\n\nAdditional comments:\n - Section 4 states: \u201cIn all of these experiments, the only stop criteria is the accuracy decay of the model. We set the threshold for this criteria to match the original accuracy; therefore all the compressed network have the same accuracy as the original network.\u201d Is this accuracy the train accuracy or test accuracy? If train, then test accuracy needs to be shown (how much test performance is lost when pruning?). If test, then this would typically be referred to as \u201ccheating\u201d and so the choice needs to be very clearly stated and then defended.\n - Lowercase rho is used to indicate correlation but this is never actually specified, which is confusing for. Just state once that it indicates correlation.\n - How do these results compare to other pruning methods? No numerical comparison is attempted.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655952, "id": "ICLR.cc/2017/conference/-/paper228/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper228/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper228/AnonReviewer1", "ICLR.cc/2017/conference/paper228/AnonReviewer2", "ICLR.cc/2017/conference/paper228/AnonReviewer3"], "reply": {"forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655952}}}, {"tddate": null, "tmdate": 1482182848562, "tcdate": 1482182848562, "number": 2, "id": "ryu6gABEe", "invitation": "ICLR.cc/2017/conference/-/paper228/pre-review/question", "forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "signatures": ["ICLR.cc/2017/conference/paper228/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper228/AnonReviewer2"], "content": {"title": "Train vs. Test acc?", "question": "Section 4 states: \u201cIn all of these experiments, the only stop criteria is the accuracy decay of the model. We set the threshold for this criteria to match the original accuracy; therefore all the compressed network have the same accuracy as the original network.\u201d Is this accuracy the train accuracy or test accuracy? If train, then test accuracy needs to be shown (how much test performance is lost when pruning?). If test, then this would typically be referred to as \u201ccheating\u201d and so the choice needs to be very clearly stated and then defended.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482182849268, "id": "ICLR.cc/2017/conference/-/paper228/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper228/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper228/AnonReviewer1", "ICLR.cc/2017/conference/paper228/AnonReviewer2"], "reply": {"forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482182849268}}}, {"tddate": null, "tmdate": 1481977638734, "tcdate": 1481977638734, "number": 1, "id": "By14J3MNx", "invitation": "ICLR.cc/2017/conference/-/paper228/official/review", "forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "signatures": ["ICLR.cc/2017/conference/paper228/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper228/AnonReviewer1"], "content": {"title": "Intriguing idea, but theory and experiments are lacking.", "rating": "3: Clear rejection", "review": "The paper proposes to prune a neural network by removing neurons whose operation is highly correlated with other neurons. The idea is nice and somewhat novel - most pruning methods concentrate on removal of individual weights, however I haven't done a through research on this topic. However, the experimental and theoretical justification of this method need to be improved before publication:\n\n1. Experiments. The authors do not report accuracy degradation while pruning in the tables, laconically stating that the networks did not degrade. This is not convincing. The only details are given in Figure 5, however this Figure disagrees with Table 2: in the Table, the number of parameters ranges from 40k-600k, while the Figure pictures the range 12k-24k. Unless more details are provided, simply claiming that a network can remove 50% neurons with no number on the degradation of accuracy is not convincing.\n\n2. Theory. The proofs do not match the experimental conditions and make unreasonable assumptions. The proofs show that in the absence of biases a network with a constant output will have two correlated neurons that generate the output offset. However, this is exactly why networks have biases and doesn't explain why noise injection helps (the proof suggests that all should be fine with deterministic auxiliary neuron). My interpretation is that the noisy output injects gradient noise (see e.g. the concurrent ICLR submission https://openreview.net/forum?id=rkjZ2Pcxe). As such the proof muddies the picture more than it helps in understanding what is happening.\n\nVerdict:\nReject and resubmit. \nThe pruning idea has potential, however its efficiency must be more soundly demonstrated (please provide network accuracies at various pruning levels, the method removes one neuron at a time, this allows making of nice plots) rather than laconically stating that a degradation on mnist from 97% accuracy to 92% is not significant (Figure 5.). Please provide Figures and Tables that agree with the text in terms of numbers provided.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655952, "id": "ICLR.cc/2017/conference/-/paper228/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper228/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper228/AnonReviewer1", "ICLR.cc/2017/conference/paper228/AnonReviewer2", "ICLR.cc/2017/conference/paper228/AnonReviewer3"], "reply": {"forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655952}}}, {"tddate": null, "tmdate": 1480698586688, "tcdate": 1480698586683, "number": 1, "id": "ry71jQ1Xg", "invitation": "ICLR.cc/2017/conference/-/paper228/public/comment", "forum": "HJIY0E9ge", "replyto": "B1AthyaMx", "signatures": ["~Mohammad_Babaeizadeh1"], "readers": ["everyone"], "writers": ["~Mohammad_Babaeizadeh1"], "content": {"title": "Clarifications", "comment": "Thank you for the question.\n\nAs the reviewer indicated, the proof assumes the absence of biases to avoid the trivial solution of all zeros. However, in practice, the algorithm does not rely on this assumption and provides a mechanism for removing the neurons with biases. All the models in our experiments had biases and analyzing the trained weights indicated that the trivial solution barely happens, if ever (actually, it didn't occur in any of the tests which contain over 1K runs on different networks and datasets).\n\nWe used weight decay only for the experiments in section 4.3 to explore and demonstrate its effect on pruning with NoiseOut. As it can be seen in Figure 7, adding regularization has negative effects on the pruning. We are currently in the process of analysis this effect in depth, but early investigations indicate that it is related to the trivial solution with biases discussed above. Nevertheless, confirming this requires more detailed experiments and further analysis."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287675134, "id": "ICLR.cc/2017/conference/-/paper228/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJIY0E9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper228/reviewers", "ICLR.cc/2017/conference/paper228/areachairs"], "cdate": 1485287675134}}}, {"tddate": null, "tmdate": 1480551558039, "tcdate": 1480551558034, "number": 1, "id": "B1AthyaMx", "invitation": "ICLR.cc/2017/conference/-/paper228/pre-review/question", "forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "signatures": ["ICLR.cc/2017/conference/paper228/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper228/AnonReviewer1"], "content": {"title": "Do you use bias terms?", "question": "The proof of the effectiveness of noisy units relies on the absence of biases. Otherwise, a simple solution for the noisy outputs has the bias equal to the expected output value and zero weights. Please clarify if your networks use bias terms, and if they use weight decay (which should remove weights incoming to the noisy units). \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482182849268, "id": "ICLR.cc/2017/conference/-/paper228/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper228/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper228/AnonReviewer1", "ICLR.cc/2017/conference/paper228/AnonReviewer2"], "reply": {"forum": "HJIY0E9ge", "replyto": "HJIY0E9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper228/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482182849268}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478282103082, "tcdate": 1478278781680, "number": 228, "id": "HJIY0E9ge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJIY0E9ge", "signatures": ["~Mohammad_Babaeizadeh1"], "readers": ["everyone"], "content": {"title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks\nand datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "pdf": "/pdf/90c57fe986190c8e6b49b4756b273506318bc814.pdf", "TL;DR": "Pruning neural networks by adding output neurons with fully random targets and removing strongly correlated neurons.", "paperhash": "babaeizadeh|a_simple_yet_effective_method_to_prune_dense_layers_of_neural_networks", "keywords": ["Deep learning"], "conflicts": ["illinois.edu"], "authors": ["Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell"], "authorids": ["mb2@illinois.edu.edu", "paris@illinois.edu.edu", "rhc@illinois.edu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}