{"notes": [{"tddate": null, "nonreaders": null, "tmdate": 1493647576833, "tcdate": 1493647329459, "number": 3, "id": "SkYylTNk-", "invitation": "ICLR.cc/2017/workshop/-/paper103/public/comment", "forum": "Bkiqt3Ntg", "replyto": "Bkiqt3Ntg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Interesting but short ;)", "comment": "Interesting topic but a little \"short\" in the sense of the potential impact of such a full model (in this form) should at least in \"theory\" be incredible!\n\nShould be something that can potentially unify a lot of mathematical approaches and \"human/brain inspired behaviour\" in ML.\n\nDo it exist a \"longer\" more mathematically version of the paper? Also a picture/sketch of the set-up would further enhance the paper. \n\n\n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Kernel Machines via the Kernel Reparametrization Trick", "abstract": "While deep neural networks have achieved state-of-the-art performance on many tasks across varied domains, they still remain black boxes whose inner workings are hard to interpret and understand. In this paper, we develop a novel method for efficiently capturing the behaviour of deep neural networks using kernels. In particular, we construct a hierarchy of increasingly complex kernels that encode individual hidden layers of the network. Furthermore, we discuss how our framework motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.\n", "pdf": "/pdf/7b90d23ba0edef1f28fba71755438811368f3d45.pdf", "TL;DR": "Encoding deep neural networks using a hierarchy of increasingly complex kernels. This motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.", "paperhash": "mitrovic|deep_kernel_machines_via_the_kernel_reparametrization_trick", "conflicts": ["ox.ac.uk"], "keywords": ["Theory", "Deep learning"], "authors": ["Jovana Mitrovic", "Dino Sejdinovic", "Yee Whye Teh"], "authorids": ["mitrovic@stats.ox.ac.uk", "dino.sejdinovic@stats.ox.ac.uk", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487354259094, "tcdate": 1487354259094, "id": "ICLR.cc/2017/workshop/-/paper103/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper103/reviewers"], "reply": {"forum": "Bkiqt3Ntg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487354259094}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028601292, "tcdate": 1490028601292, "number": 1, "id": "BkbruKTig", "invitation": "ICLR.cc/2017/workshop/-/paper103/acceptance", "forum": "Bkiqt3Ntg", "replyto": "Bkiqt3Ntg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Kernel Machines via the Kernel Reparametrization Trick", "abstract": "While deep neural networks have achieved state-of-the-art performance on many tasks across varied domains, they still remain black boxes whose inner workings are hard to interpret and understand. In this paper, we develop a novel method for efficiently capturing the behaviour of deep neural networks using kernels. In particular, we construct a hierarchy of increasingly complex kernels that encode individual hidden layers of the network. Furthermore, we discuss how our framework motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.\n", "pdf": "/pdf/7b90d23ba0edef1f28fba71755438811368f3d45.pdf", "TL;DR": "Encoding deep neural networks using a hierarchy of increasingly complex kernels. This motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.", "paperhash": "mitrovic|deep_kernel_machines_via_the_kernel_reparametrization_trick", "conflicts": ["ox.ac.uk"], "keywords": ["Theory", "Deep learning"], "authors": ["Jovana Mitrovic", "Dino Sejdinovic", "Yee Whye Teh"], "authorids": ["mitrovic@stats.ox.ac.uk", "dino.sejdinovic@stats.ox.ac.uk", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028601908, "id": "ICLR.cc/2017/workshop/-/paper103/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bkiqt3Ntg", "replyto": "Bkiqt3Ntg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028601908}}}, {"tddate": null, "tmdate": 1489657355052, "tcdate": 1489657355052, "number": 2, "id": "Sk7MCCPix", "invitation": "ICLR.cc/2017/workshop/-/paper103/public/comment", "forum": "Bkiqt3Ntg", "replyto": "Bkz97slil", "signatures": ["~Jovana_Mitrovic1"], "readers": ["everyone"], "writers": ["~Jovana_Mitrovic1"], "content": {"title": "Rebuttal", "comment": "Our proposed method is the first approach, to the best of our knowledge, that enables the construction of arbitrarily deep infinite-width neural networks. We use the kernel reparametrization trick to do that. In particular, we sample weights from a Gaussian processes, where the covariance function is constructed using kernels inferred from the infinite-width neural network. In summary, we introduce a new neural network architecture, describe a formalism to reason about it and apply the insights gained from this construction on standard finite-width neural networks in the form of a novel initialization method.\n\nIn order to perform initialization of a finite-width neural network with our method, we need to determine for each layer l the points {\\xi_{im}^{(l)}}_{m} that are used for constructing the weights as\n\nu_{l, i} = \\sum_{m = 1}^{M_{l}} \\alpha_{im}\\hat{k}_{l}(\\cdot, \\xi_{im}^{(l)})  with \\alpha_{i} \\sim N(0, \\frac{1}{M_{l}} I). \n\nWe choose these points from the training set according to supervised signal. In particular, for each neuron i, we assigned it a class and choose the {\\xi_{im}^{(l)}}_{m} from that particular class.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Kernel Machines via the Kernel Reparametrization Trick", "abstract": "While deep neural networks have achieved state-of-the-art performance on many tasks across varied domains, they still remain black boxes whose inner workings are hard to interpret and understand. In this paper, we develop a novel method for efficiently capturing the behaviour of deep neural networks using kernels. In particular, we construct a hierarchy of increasingly complex kernels that encode individual hidden layers of the network. Furthermore, we discuss how our framework motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.\n", "pdf": "/pdf/7b90d23ba0edef1f28fba71755438811368f3d45.pdf", "TL;DR": "Encoding deep neural networks using a hierarchy of increasingly complex kernels. This motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.", "paperhash": "mitrovic|deep_kernel_machines_via_the_kernel_reparametrization_trick", "conflicts": ["ox.ac.uk"], "keywords": ["Theory", "Deep learning"], "authors": ["Jovana Mitrovic", "Dino Sejdinovic", "Yee Whye Teh"], "authorids": ["mitrovic@stats.ox.ac.uk", "dino.sejdinovic@stats.ox.ac.uk", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487354259094, "tcdate": 1487354259094, "id": "ICLR.cc/2017/workshop/-/paper103/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper103/reviewers"], "reply": {"forum": "Bkiqt3Ntg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487354259094}}}, {"tddate": null, "tmdate": 1489656716059, "tcdate": 1489656716059, "number": 1, "id": "Bk4qoCwjx", "invitation": "ICLR.cc/2017/workshop/-/paper103/public/comment", "forum": "Bkiqt3Ntg", "replyto": "H12YaKeje", "signatures": ["~Jovana_Mitrovic1"], "readers": ["everyone"], "writers": ["~Jovana_Mitrovic1"], "content": {"title": "Rebuttal", "comment": "The method presented in our paper is related to (Hazan and Jaakkola 2015), but is quite different from the one (Daniely at el 2016) present. In particular, the approach proposed in (Hazan and Jaakkola 2015) can only accommodate up to two infinite-width layers, while our approach is the first one, to the best of our knowledge, that enables a construction on arbitrarily deep infinite-width neural networks. In particular, we achieve this with the kernel reparametrization trick. Furthermore, the work on (Daniely et al. 2016) deals with finite-width neural networks and uses kernels to analyze them, while we examine infinite-width neural networks and construct a novel architecture and derive a whole new formalism for reasoning about it. \n\nThe advantage of a supervised initialization can be easily seen from the classification accuracies right after initialization. In particular, the very high initial classification accuracy of our method clearly indicates that our initialization method is successful at disentangling factors of variation as it takes into account the structure of the data through the supervised information. In particular, this makes the networks constructed using our initialization method useful already from the initialization stage requiring less training, thus making it useful in settings where we have only a limited computational budget, for example. \n\nConcerning the experimental results, these are state-of-the-art results on this architecture when no data augmentation is used. As the experiments were about considering the advantages of our initialization method over standard ones, we isolated the contribution of the initialization to the final classification accuracy by not using any fine-tuning beyond that necessary to make the learning converge on this architecture.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Kernel Machines via the Kernel Reparametrization Trick", "abstract": "While deep neural networks have achieved state-of-the-art performance on many tasks across varied domains, they still remain black boxes whose inner workings are hard to interpret and understand. In this paper, we develop a novel method for efficiently capturing the behaviour of deep neural networks using kernels. In particular, we construct a hierarchy of increasingly complex kernels that encode individual hidden layers of the network. Furthermore, we discuss how our framework motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.\n", "pdf": "/pdf/7b90d23ba0edef1f28fba71755438811368f3d45.pdf", "TL;DR": "Encoding deep neural networks using a hierarchy of increasingly complex kernels. This motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.", "paperhash": "mitrovic|deep_kernel_machines_via_the_kernel_reparametrization_trick", "conflicts": ["ox.ac.uk"], "keywords": ["Theory", "Deep learning"], "authors": ["Jovana Mitrovic", "Dino Sejdinovic", "Yee Whye Teh"], "authorids": ["mitrovic@stats.ox.ac.uk", "dino.sejdinovic@stats.ox.ac.uk", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487354259094, "tcdate": 1487354259094, "id": "ICLR.cc/2017/workshop/-/paper103/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper103/reviewers"], "reply": {"forum": "Bkiqt3Ntg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487354259094}}}, {"tddate": null, "tmdate": 1489183626216, "tcdate": 1489183626216, "number": 2, "id": "Bkz97slil", "invitation": "ICLR.cc/2017/workshop/-/paper103/official/review", "forum": "Bkiqt3Ntg", "replyto": "Bkiqt3Ntg", "signatures": ["ICLR.cc/2017/workshop/paper103/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper103/AnonReviewer2"], "content": {"title": "review: interesting but need stronger experimental evidence", "rating": "5: Marginally below acceptance threshold", "review": "The paper discusses the distribution of function mappings governed by deep infinitely wide neural networks and, in theory, how to sample the weights using the canonical feature mappings (or the so-called kernel reparameterisation trick) and map the data points to features. \n\nThe paper then proposes an initialization scheme for finite-width neural networks, inspired by the above analysis. I don't think I quite understand the procedure you use. Can you please clarify: \n\ni) what do you mean by \"guide by the idea of ... in a supervised fashion\", what is supervised here? Perhaps it's mean you use the inputs to initialise the weights, compared to other techniques that only use the size of the layers?\n\nii) the procedure you use to initialise weights, say for a single layer net.\n\nWhile the contribution is theoretically interesting, I'm afraid it doesn't reach the level required (i.e. late-breaking work) for the workshop.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Kernel Machines via the Kernel Reparametrization Trick", "abstract": "While deep neural networks have achieved state-of-the-art performance on many tasks across varied domains, they still remain black boxes whose inner workings are hard to interpret and understand. In this paper, we develop a novel method for efficiently capturing the behaviour of deep neural networks using kernels. In particular, we construct a hierarchy of increasingly complex kernels that encode individual hidden layers of the network. Furthermore, we discuss how our framework motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.\n", "pdf": "/pdf/7b90d23ba0edef1f28fba71755438811368f3d45.pdf", "TL;DR": "Encoding deep neural networks using a hierarchy of increasingly complex kernels. This motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.", "paperhash": "mitrovic|deep_kernel_machines_via_the_kernel_reparametrization_trick", "conflicts": ["ox.ac.uk"], "keywords": ["Theory", "Deep learning"], "authors": ["Jovana Mitrovic", "Dino Sejdinovic", "Yee Whye Teh"], "authorids": ["mitrovic@stats.ox.ac.uk", "dino.sejdinovic@stats.ox.ac.uk", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489183626840, "id": "ICLR.cc/2017/workshop/-/paper103/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper103/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper103/AnonReviewer1", "ICLR.cc/2017/workshop/paper103/AnonReviewer2"], "reply": {"forum": "Bkiqt3Ntg", "replyto": "Bkiqt3Ntg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper103/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489183626840}}}, {"tddate": null, "tmdate": 1489177988318, "tcdate": 1489177988318, "number": 1, "id": "H12YaKeje", "invitation": "ICLR.cc/2017/workshop/-/paper103/official/review", "forum": "Bkiqt3Ntg", "replyto": "Bkiqt3Ntg", "signatures": ["ICLR.cc/2017/workshop/paper103/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper103/AnonReviewer1"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "The paper is an attempt to gain a better understanding of deep learning through kernels. I think the paper does not add much to the previous works such as (Hazan and Jaakkola 2015) and (Daniely et al. 2016). The suggested trick to construct weights is interesting but I don't understand why such initialization should be useful. In particular, what is the value of supervised initialization? The experimental results show that the supervised initialization cannot improve the final results. Moreover, the reported test errors are far worse than what is being reported for single layer network of similar size.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Kernel Machines via the Kernel Reparametrization Trick", "abstract": "While deep neural networks have achieved state-of-the-art performance on many tasks across varied domains, they still remain black boxes whose inner workings are hard to interpret and understand. In this paper, we develop a novel method for efficiently capturing the behaviour of deep neural networks using kernels. In particular, we construct a hierarchy of increasingly complex kernels that encode individual hidden layers of the network. Furthermore, we discuss how our framework motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.\n", "pdf": "/pdf/7b90d23ba0edef1f28fba71755438811368f3d45.pdf", "TL;DR": "Encoding deep neural networks using a hierarchy of increasingly complex kernels. This motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.", "paperhash": "mitrovic|deep_kernel_machines_via_the_kernel_reparametrization_trick", "conflicts": ["ox.ac.uk"], "keywords": ["Theory", "Deep learning"], "authors": ["Jovana Mitrovic", "Dino Sejdinovic", "Yee Whye Teh"], "authorids": ["mitrovic@stats.ox.ac.uk", "dino.sejdinovic@stats.ox.ac.uk", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489183626840, "id": "ICLR.cc/2017/workshop/-/paper103/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper103/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper103/AnonReviewer1", "ICLR.cc/2017/workshop/paper103/AnonReviewer2"], "reply": {"forum": "Bkiqt3Ntg", "replyto": "Bkiqt3Ntg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper103/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489183626840}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487354258564, "tcdate": 1487354258564, "number": 103, "id": "Bkiqt3Ntg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "Bkiqt3Ntg", "signatures": ["~Jovana_Mitrovic1"], "readers": ["everyone"], "content": {"title": "Deep Kernel Machines via the Kernel Reparametrization Trick", "abstract": "While deep neural networks have achieved state-of-the-art performance on many tasks across varied domains, they still remain black boxes whose inner workings are hard to interpret and understand. In this paper, we develop a novel method for efficiently capturing the behaviour of deep neural networks using kernels. In particular, we construct a hierarchy of increasingly complex kernels that encode individual hidden layers of the network. Furthermore, we discuss how our framework motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.\n", "pdf": "/pdf/7b90d23ba0edef1f28fba71755438811368f3d45.pdf", "TL;DR": "Encoding deep neural networks using a hierarchy of increasingly complex kernels. This motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization.", "paperhash": "mitrovic|deep_kernel_machines_via_the_kernel_reparametrization_trick", "conflicts": ["ox.ac.uk"], "keywords": ["Theory", "Deep learning"], "authors": ["Jovana Mitrovic", "Dino Sejdinovic", "Yee Whye Teh"], "authorids": ["mitrovic@stats.ox.ac.uk", "dino.sejdinovic@stats.ox.ac.uk", "y.w.teh@stats.ox.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 7}