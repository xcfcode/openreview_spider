{"notes": [{"id": "HyGIdiRqtm", "original": "H1ejKjYqYX", "number": 360, "cdate": 1538087790488, "ddate": null, "tcdate": 1538087790488, "tmdate": 1550464252528, "tddate": null, "forum": "HyGIdiRqtm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJgWDhVBeE", "original": null, "number": 1, "cdate": 1545059416998, "ddate": null, "tcdate": 1545059416998, "tmdate": 1545354475397, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Meta_Review", "content": {"metareview": "\nThe paper investigates mixed-integer linear programming methods for neural net robustness verification in presence of adversarial attckas. The paper addresses and important problem, is well-written, presents a novel approach and demonstrates empirical improvements; all reviewers agree that this is a solid contribution to the field.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Important problem, solid contribution"}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper360/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353243503, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353243503}}}, {"id": "SylMABVPyE", "original": null, "number": 5, "cdate": 1544140234011, "ddate": null, "tcdate": 1544140234011, "tmdate": 1544140234011, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "H1lIZjvUyE", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "content": {"comment": "Thanks for the explanation and the additional experimental data. \n\nIt seems for the 6x100 undefended networks, the method does not really improve over state of the art, which on top of that is an incomplete verifier (the main benefit of complete verifiers is precision gain for smaller networks). The approach is also slow, e.g., it times out for around 46% of cases for eps=0.02. It appears it would timeout even more for the 9x200 network and may prove less than the incomplete verifier. For the 100K net, the standard Interval analysis seems sufficient for 99% of cases.\n\nOverall, I do like the direction, but the authors need to show a use case which clearly surpasses prior work on networks beyond 3x20.\n\n\n", "title": "Use case still unclear"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311858315, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGIdiRqtm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311858315}}}, {"id": "H1lIZjvUyE", "original": null, "number": 12, "cdate": 1544088318305, "ddate": null, "tcdate": 1544088318305, "tmdate": 1544088318305, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "ByxMEVJ90Q", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Verifying undefended networks, and impact of presolve step", "comment": "Thank you for your comments.\n\nVerifying undefended networks: We would like to begin by clarifying that the runtimes in Figure 1 and the rest of our submission are not directly comparable. Figure 1 presents results on determining the _closest_ adversarial example; the rest of our submission presents results on finding _some_ adversarial example among perturbations with l-infinity norm bound \u03b5 (or proving that no adversarial example exists among those perturbations). Determining the closest adversarial example is always expected to take more time since it is a strictly more difficult task.\n\nTo determine how well our verifier performs when determining the robustness of larger undefended networks to perturbations with bounded l-infinity norm \u03b5, we verified the 6x100 network found here https://github.com/eth-sri/eran#experimental-results on a range of values of \u03b5. The results are over the first 500 samples [1], and we report a timeout if solve time for a sample exceeds 120s.\n\n\t\t| Verified \t| Verified \t| Total\n         \u03b5   \t| Robust \t| Vulnerable\t| Verified\n-----------\t| ---------------\t| ---------------\t| -----------\n     0.005\t|          0.966\t|          0.034\t|      1.000\n     0.010\t|          0.910\t|          0.046 \t|      0.956\n     0.015\t|          0.756\t|          0.056\t|      0.812\n     0.020\t|          0.466\t|          0.072\t|      0.538\n     0.025\t|          0.238\t|          0.082\t|      0.320\n     0.030\t|          0.080\t|          0.098\t|      0.178\n\nThese results are comparable to or better than the best results reported for this network (via DeepPoly).\n\nPresolve step: During the presolve step, a significant fraction of nodes only require interval arithmetic to compute bounds. (This is precisely what allows progressive bounds tightening to reduce the overall time spent computing bounds). \n\nIn terms of the impact of using LP to compute bounds, we find that it does not significantly affect _median_ solve times; however, it _does_ make a big impact for samples that are complex and take a long time to verify. For example, for the 100K network [2], 108 (out of 10000) samples take more than 120s when using only interval arithmetic to compute bounds. Of these samples, 33 can be resolved within 120s when using LP to compute bounds.\n\n[1] The standard error rate on the first 500 samples was 2.8%.\n[2] We refer to this network in our submission as the CIFAR-Resnet network."}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "ByxMEVJ90Q", "original": null, "number": 4, "cdate": 1543267370186, "ddate": null, "tcdate": 1543267370186, "tmdate": 1543267370186, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "HylmeSpd07", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "content": {"comment": "Thank you for your response. \n\nIt seems the method will not work well for undefended networks, but I am wondering whether there is a benefit over simple Interval analysis with defended networks?\n\nFor undefended, in Figure 1, the 3x20 undefended net can have a maximum of 60 unstable units but your verifier seems to take longer on this than on the 100K one with 1906 unstable units. Thus, for undefended nets, the method will likely not scale for the 9x200 net I mentioned.\n\nFor defended nets, Table 5 suggests the solver branches very rarely (not more than 3 times on 95% of the images) on the 100K net and thus even if there are 1906 branches, they are rarely explored which may explain your timings.\n\nDuring the presolve step, what percentage of nodes use only Interval arithmetic for their bounds computation? In particular, what would the results look like for the 100K net if one only uses simple Interval Arithmetic with no LP/MILP at all? \n\nThanks in advance.\n", "title": "Benefits of the proposed method over Interval arithmetic?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311858315, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGIdiRqtm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311858315}}}, {"id": "HylmeSpd07", "original": null, "number": 10, "cdate": 1543193834985, "ddate": null, "tcdate": 1543193834985, "tmdate": 1543193834985, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "Hkl4h-WLRX", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Scaling to larger networks and number of unstable units", "comment": "Thank you for your comment.\n\nWe have not encountered a network trained to be robust where many cases required more than 24 hours to solve. This is the case even for the LPd-RES CIFAR classifier, where the mean number of unstable units is 1906.15, but the mean solve time for this classifier is only 15.23s. [1] \n\nThere may be two other reasons why verification takes a long time for the network you selected.\n\nFirstly, as discussed in section H in the appendix on Sparsification and Verifiability, having parameter values close to zero can lead to significantly increased verification time [2].  Two fixes are possible if this is the case. A) Without access to the training procedure, Table 7 shows that it is possible to modify the network to significantly improve verifiability at a small cost to test error, by setting some fraction of weights to zero. B) Alternatively, with access to the training procedure, adopting a principled sparsification approach could improve verifiability even further at a lower cost to test error.\n\nSecondly, attempting to verify a network not trained to be robust, such as the 9x200 network available in the repository, can lead to significantly increased verification time. We have observed that regular training (without a robustness objective) leads to networks where almost all ReLUs are unstable, even for input domains of modest size (such as an l-\u221e ball of radius 0.1); in contrast, all robust training procedures we had access to produced networks where a significant fraction of ReLUs were provably stable over input domains of the same size. Fortunately, when working with a network not trained to be robust, the distance to the closest adversarial example for any input sample is typically small. When this is the case, you can reduce solve times by first attempting to find an adversarial example within a smaller input domain (such as an l-\u221e ball of radius 0.01), and searching over larger input domains only when no adversarial example can be found within the smaller input domain.\n\nA quick note on stability of ReLUs: for the robust networks we verified, very few ReLUs were always provably stable for all test samples; instead, the set of possibly unstable ReLUs changes significantly between test samples.\n\n[1] We have also updated the paper so that results on the numbers of ReLUs that are provably stable are reported for all networks (either in Table 3 or in Table 6). Thank you for suggesting this!\n[2] We found that networks trained simply to minimize cross-entropy loss exhibit this behavior."}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "BkgoiNTOCm", "original": null, "number": 9, "cdate": 1543193762940, "ddate": null, "tcdate": 1543193762940, "tmdate": 1543193762940, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Submission Revised", "comment": "Thank you for your comments and suggestions! We have updated the submission with revisions based on them."}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "B1xhqcM8CQ", "original": null, "number": 6, "cdate": 1543019156251, "ddate": null, "tcdate": 1543019156251, "tmdate": 1543168036712, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "r1eSMishhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Review Response, Part II", "comment": "(This is the second part of our response to the reviewer.)\n\nComparison to Wong et al. [b]: The reviewer mentions that the results in our submission do not outperform all of the latest results in terms of upper bounds on adversarial error on MNIST and CIFAR classifiers. In particular, the reviewer was interested to see a comparison on our results with those in Wong et al. at https://arxiv.org/pdf/1805.12514.pdf [3].\n\nDuring the discussion period, we were able to run our verifier on all but two of the networks [4] presented in Wong et al. Results are presented below.\n\n|               \t|         \t|       \t|        \t|    Certified Bounds on Adv. Error \t|  Mean \t|\n|               \t|         \t|       \t| Test   \t|   Lower Bound\t|   Upper Bound \t|  Time\t|\n| Dataset\t| Net\t|       \u03b5   \t| Error  \t|   PGD  \t|  Ours \t| SOA[5]\t|  Ours \t|  / s   \t|\n|----------------\t|----------\t|----------\t|----------\t|----------------------\t|----------\t|----------\t|----------\t|\n| MNIST\t\t| Small\t| 0.1\t|  1.21%\t|  3.05%\t|  3.22%\t|  5.06%\t|  3.22%\t |   2.55 \t|\n|               \t| Large\t| 0.1   \t|  1.19%\t|  2.62%\t|  2.73%\t|  4.45%\t|  2.74%\t|  46.33\t|\n|               \t| Small\t| 0.3   \t|14.77%\t|24.99%\t|28.37%\t|43.79%\t|28.37%\t|    3.71\t|\n|               \t| Large\t| 0.3   \t|11.16%\t|19.70%\t|24.12%\t|41.98%\t|24.19%\t|  98.79 \t|\n| CIFAR10\t| Small\t| 2/255 \t|39.14%\t|48.23%\t|49.84%\t|53.59%\t|50.20%\t|  22.41 \t|\n|               \t| Small\t| 8/255 \t|72.40%\t|77.36%\t|78.71%\t|79.46%\t|78.71%\t|    0.91  \t|\n|               \t| Large\t| 8/255 \t|80.99%\t|82.66%\t|83.54%\t|83.97%\t|83.55%\t|    6.01  \t|\n|               \t| Resnet\t| 8/255 \t|72.93%\t|76.51%\t|77.29%\t|78.52%\t|77.60%\t|  15.23\t|\n\nFor all of the networks we verify, we improve upon the upper bound on adversarial error provided by the certificate in Wong et al., and also improve on the lower bound provided by PGD. We also have better overall results compared to Wong et al. over all single-model networks [6] for MNIST at \u03b5=0.1 (2.74% vs. 3.67%), MNIST at \u03b5=0.3 (24.19% vs. 43.10%), and CIFAR10 at \u03b5=8/255 (77.29% vs 78.22%). We perform worse only for CIFAR10 at \u03b5=2/255 (50.20% vs 46.11%); this is a result of us only being able to verify the `Small` network for CIFAR10 at \u03b5=2/255, which has worse underlying robustness.\n\nFinally, in response to the reviewer's question: the \"restricted domain\" contribution is as described --- we use the tightest possible bounds on the perturbed input, combining the fact that the inputs to the classifier are normalized to a given range and that they are no more than \u03b5 away from the nominal input. Though simple, our results in Table 1 show that using this makes a large difference in the performance of our verifier.\n\n[3] While the paper was available before the ICLR deadline, none of the networks described (other than the `Resnet` model for CIFAR10 at \u03b5=8/255) were available until the end of October, and we were thus unable to evaluate the performance of our verifier on these more robust networks in our initial submission. The networks are now available here: https://github.com/locuslab/convex_adversarial/tree/master/models_scaled\n[4] We were not able to verify the `Large` and `Resnet` networks for the CIFAR10 dataset at \u03b5=2/255 due to memory issues in our implementation when determining upper and lower bounds.\n[5] We note that these SOA bounds are not the same as the robust single-model errors reported in https://arxiv.org/pdf/1805.12514.pdf, since the networks were trained with a different seed.\n[6] The full \"cascade\" of networks that Wong et al. present in Table 2 of their paper is not currently available for verification.\n\n[b] Eric Wong et al. \"Scaling provable adversarial defenses.\" https://arxiv.org/pdf/1805.12514.pdf"}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "HJeQNsfI0Q", "original": null, "number": 8, "cdate": 1543019306904, "ddate": null, "tcdate": 1543019306904, "tmdate": 1543019306904, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "rJlzefhupX", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Asymmetric bounds", "comment": "Thank you for your comment. The formulation presented in Ehlers [c] for the ReLU does correspond to our formulation when the integer constraint on a is relaxed from a\u2208{0,1} to 0\u2264a\u22641. We will update our submission to reflect this, but we believe that binarizing the formulation in Ehlers to obtain our formulation is not trivial. \n\nFurthermore, viewing things from a MIP perspective can be insightful: for example, for the maximum function, relaxing the integrality constraints on the indicator variables produces a set of linear constraints complementary to those presented for the maximum function in Ehlers that is tighter when the input values x_i are closer to their upper bounds u_i.\n\n[c] R\u00fcdiger Ehlers. \"Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks.\" https://arxiv.org/pdf/1705.01320.pdf"}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "rJxCCcM8CQ", "original": null, "number": 7, "cdate": 1543019222392, "ddate": null, "tcdate": 1543019222392, "tmdate": 1543019222392, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "BygGl7Gva7", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Asymmetric Bounds", "comment": "Thank you for your comment clarifying the point on asymmetric bounds.\n\nTo answer your question, when removing asymmetric bounds, we use M = max(-l, u) for all ReLUs, not just those that are unstable. In principle, a solver might still be able to identify all ReLUs that are stable, eliminating the associated binary variables from consideration. In practice, solves take significantly longer (mean of 0.08s vs 133.03s), and many more nodes are explored (mean of 2.05 vs. 1498.35), suggesting that not all these extraneous binary variables (added for stable ReLUs) are eliminated. "}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "rJg-8cfU07", "original": null, "number": 5, "cdate": 1543019081251, "ddate": null, "tcdate": 1543019081251, "tmdate": 1543019081251, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "r1eSMishhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Review Response, Part I", "comment": "Thank you for your review; your comments will help us in revising the paper.\n\nComparison to Bunel et al. [a]: We consider the ideas in our paper and those in Bunel et al. to be complementary. Both our verifier and that of Bunel et al. rely on a branch-and-bound approach, and begin by solving an LP that corresponds to the MIP we formulate, but with all integrality constraints removed. In our work, branching occurs only when we split on an unstable ReLU, producing two sub-MIPs where that ReLU is fixed as active and inactive respectively. Bunel et al. observe that it is also possible to split on the _input domain_, producing two sub-MIPs where the input in each sub-MIP is restricted to be from a half of the input domain. Splitting on the input domain could be useful when tight bounds on the perturbed input are not available (as in the problems studied in the ACAS system mentioned by the reviewer), particular where the split selected tightens bounds sufficiently to significantly reduce the number of unstable ReLUs that need to be considered.\n\nWe have also reached out to the authors and are working on running their verifier on the networks for which we report results in this paper, and will provide an update as soon as one is available.\n\nSolver used: We understand the reviewer's concern about having to use a commercial solver like Gurobi. While we were unable to run a comparison on the SCIP solver suggested, we were able to run a comparison on the Cbc [1] and GLPK [2] solvers, two open-source mixed integer programming solvers. Verification is run on the MNIST classifier network LPd-CNN, with \u03b5=0.1. The results are as follows:\n\n|                  \t\t|       Adv. Error\t| Mean\t|\n|                  \t\t|  Lower\t|  Upper\t| Time \t|\n| Approach         \t| Bound\t|Bound \t| / s   \t|\n|----------------------\t|----------------------\t|----------\t|\n| Ours w/ Gurobi\t|  4.38%\t|  4.38%\t|   3.52 \t|\n| Ours w/ Cbc     \t|  4.30%\t|  4.82%\t| 18.92 \t|\n| Ours w/ GLPK  \t|  3.50%\t|  7.30%\t| 35.78 \t|\n| PGD / SOA        \t|  4.11%\t|  5.82%\t|     --   \t|\n\nWhen we use GLPK as the solver, our performance is significantly worse than when using Gurobi, with the solver timing out on almost 4% of samples. While we time out on some samples with Cbc, our verifier still provides a lower bound better than PGD and an upper bound significantly better than the state-of-the-art for this network. The performance of our verifier is affected by the underlying MIP solver used, but we are still able to improve on existing bounds using non-commercial solvers.\n\nWe will add this table to the appendix of the paper.\n\n[1] Coin-or branch and cut (https://projects.coin-or.org/Cbc)\n[2] GNU Linear Programming Kit (https://www.gnu.org/software/glpk/). The results presented are estimates computed from 1,000 samples.\n\n[a] Rudy Bunel et al. \"A Unified View of Piecewise Linear Neural Network Verification.\" https://arxiv.org/pdf/1711.00455.pdf"}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "SylXnSG8AQ", "original": null, "number": 4, "cdate": 1543017898955, "ddate": null, "tcdate": 1543017898955, "tmdate": 1543017898955, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "H1egVwcihm", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Review Response", "comment": "Thank you for your review. We are glad that you found our paper easy to read! \n\nAddressing the bottlenecks in the scalability of the MIP solver was key in making the verification problem tractable. We look forward to utilizing other ideas from the Operations Research community (such as computing cutting planes that exploit our knowledge of the structure of our network) to further improve performance."}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "rygywrfIRm", "original": null, "number": 3, "cdate": 1543017814837, "ddate": null, "tcdate": 1543017814837, "tmdate": 1543017814837, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "S1eVvi_9hm", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "content": {"title": "Review Response", "comment": "Thank you for your positive feedback!"}, "signatures": ["ICLR.cc/2019/Conference/Paper360/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612383, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGIdiRqtm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper360/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper360/Authors|ICLR.cc/2019/Conference/Paper360/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612383}}}, {"id": "Hkl4h-WLRX", "original": null, "number": 3, "cdate": 1543012780196, "ddate": null, "tcdate": 1543012780196, "tmdate": 1543012780196, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "content": {"comment": "I have a question in regards to the > 100,000 neurons claim, which I hope the authors can clarify.\n\nI tried the method from your paper on the publicly available networks from https://github.com/eth-sri/eran and observed that in many cases the MILP solver does not finish even after 24 hours on networks much smaller than the 100,000 neurons network, say a 9x200 network. This usually happens when the number of unstable ReLU units (with both + and - values)  by the presolve algorithm is  > 1000. \n\nIndeed, as observed in your experimental section, the runtime of the MILP solver is determined by the number of unstable ReLU units and *not* the total number of ReLU units in the network. \n\nDoes it mean the approach will only work for networks where the presolve algorithm can determine that only a very small fraction of the ReLU units are unstable?  Could you please report the number of unstable units on the > 100,000 network (they are not in the paper now)? \n\nThanks in advance.\n", "title": "Scalability to > 100,000 neurons due to fewer unstable units?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311858315, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGIdiRqtm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311858315}}}, {"id": "rJlzefhupX", "original": null, "number": 2, "cdate": 1542140393828, "ddate": null, "tcdate": 1542140393828, "tmdate": 1542140393828, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "BygGl7Gva7", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "content": {"comment": "To be fair, the asymmetric bounds were first (to the best of my knowledge) used in https://arxiv.org/pdf/1705.01320.pdf. The formulation in this current paper is simply a binarized version of the same. It's a little surprising that the paper above is not cited in this context.", "title": "Asymmetric bounds before this paper"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311858315, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGIdiRqtm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311858315}}}, {"id": "BygGl7Gva7", "original": null, "number": 1, "cdate": 1542034153743, "ddate": null, "tcdate": 1542034153743, "tmdate": 1542034153743, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "r1eSMishhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "content": {"comment": "To add a datapoint of information with regards to the \"Originality\" section of the review, especially discussing the \"Asymmetric bounds\" contribution:\n\nI'm one of the authors of the paper that is being asked to compare to. Our first version (https://arxiv.org/pdf/1711.00455v1.pdf on Arxiv in November 2017) didn't have asymmetric bounds and we included them in a subsequent update, after reading about the idea in a previous version of the paper under review (which we cite, and highlight the difference in appendix https://arxiv.org/pdf/1711.00455v3.pdf ). Asking the authors to discuss the difference with our use of asymmetric bounds is therefore difficult, because it's their idea which we made use of.\n\nWhile on the subject of asymmetric bounds, would it be possible to clarify what the results of the ablation study means? When removing asymmetric bounds and instead using M = max(-l, u), could you confirm that this is only done for ReLUs that are unstable?\n\n\n", "title": "Asymmetric bounds"}, "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rudy_R_Bunel1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311858315, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGIdiRqtm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper360/Authors", "ICLR.cc/2019/Conference/Paper360/Reviewers", "ICLR.cc/2019/Conference/Paper360/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311858315}}}, {"id": "r1eSMishhQ", "original": null, "number": 3, "cdate": 1541352204958, "ddate": null, "tcdate": 1541352204958, "tmdate": 1541534059774, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Review", "content": {"title": "Strong well written paper, some improvement possible in experimental section", "review": "The authors perform a careful study of mixed integer linear programming approaches for verifying robustness of neural networks to adversarial perturbations. They propose three enhancements to MILP formulations of neural network verification: Asymmetric bounds, restricted domain and progressive bound tightening, which lead to significantly more scalable verification algorithms vis-a-vis prior work. They study the effectiveness of MILP solvers both in terms of verifying robustness (compared to other complete/incomplete verifiers) and generating adversarial attacks (compared to PGD attacks) and show that their approach compares favorable across a number of architectures on MNIST and CIFAR-10. They perform careful ablation studies to validate the importance of the \n\nQuality: The paper is very well written and organized. The problem is certainly of great interest to the deep learning community, given the difficulty of properly evaluating (and then improving) defenses against adversarial attacks. The experiments are done carefully with convincing ablation studies.\n\nClarity: The authors explain the relevant concepts carefully and all the experimental results are clearly written and explained.\n\nOriginality: The authors propose conceptually simple but practically significant enhancements to MILP formulations of neural network verification. However, the novelty wrt https://arxiv.org/pdf/1711.00455.pdf is not discussed carefully in my view (the  asymmetric bounds were already studied in this paper, as well as a novel branch and bound strategy). The progressive bound tightening is a novel idea as far as I can see - however, the ablation experiments show that this idea is not significant in terms of performance improvement. In terms of experiments, the authors indeed obtain strong results on verified adversarial error rates and generate attacks that PGD is unable to - however, again the results do not outperform latest results (in terms of the  best achievable upper bounds on verified error rates) available well before the ICLR deadline - https://arxiv.org/pdf/1805.12514.pdf . It would be great if the authors addressed these issues in a revised version of the paper.\n\nSignificance: The work does establish a strong algorithm for complete verification of neural networks along with several ideas that are critical to obtain strong performance with this approach. \n\nQuestion:\n1. I am unclear on the \"restricted domain\" contribution claimed in the paper - is this just exploiting the fact that the inputs to the classifier are normalized to a given range, in addition to being no more than eps away from the nominal input? \n\nCons\n1. The authors do not compare their approach to that of https://arxiv.org/pdf/1711.00455.pdf , both in terms of conceptual novelty and in terms of experimental results. In particular, it is not clear to me whether the authors' approach remains superior on domains where tight bounds on the neural networks inputs are not available, like the problems studied in the ACAS system in the ReLuPlex paper.\n\n2. The authors' MILP solution approach relies on having access to the state of the art commercial MILP solver Gurobi. While Gurobi is free for academic research use, for large scale neural network verification applications, this does restrict use of the approach (particularly due to limited licenses being available). It would be interesting to see a comparison that uses a freely available MILP solver (like scip.zib.de) to see how critical the approach's scalability depends on the quality of the MILP solver.\n\n3. The authors do not outperform the latest SOA numbers in terms of verified adversarial error rates on MNIST and CIFAR classifers. It would be good to see a comparison on results from https://arxiv.org/pdf/1711.00455.pdf  (I believe the training code and trained networks are available online).", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper360/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Review", "cdate": 1542234478798, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335706962, "tmdate": 1552335706962, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1egVwcihm", "original": null, "number": 2, "cdate": 1541281575778, "ddate": null, "tcdate": 1541281575778, "tmdate": 1541534059522, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Review", "content": {"title": "A strong contribution", "review": "This paper studies a Mixed Integer Linear Programming (MILP) approach to verifying the robustness of neural networks with ReLU activations. The main contribution of the paper is a progressive bound tightening approach that results in significantly faster MILP solving. This in turn allows for verifying the robustness of larger networks than previously studied, and even larger datasets such as CIFAR-10.\n\nThis paper is a solid contribution and should be accepted to ICLR. It is quite well-written, addresses an important problem using a principled method, and achieves strong experimental results that were previously elusive, despite the large body of work in adversarial learning. In particular, the paper has the following strengths:\n\n- Clarity: the paper is well-written and easy to read. Tables, figures and pseudocode are nice and easy to understand.\n- Methodology: the authors take care of a number of bottlenecks in the scalability of MIP solvers for the verification problem. This is the standard approach in the Operations Research (OR) community, and I am really glad to see it in an ICLR submission!\n- Results: the efficiency of the MIP on the tightened model, and the improvements in the bounds on the adversarial error as compared to very recent methods from the literature are both very strong points in favor of the paper.\n\nI do not have any further questions for the authors - good job!", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper360/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Review", "cdate": 1542234478798, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335706962, "tmdate": 1552335706962, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eVvi_9hm", "original": null, "number": 1, "cdate": 1541208923910, "ddate": null, "tcdate": 1541208923910, "tmdate": 1541534059309, "tddate": null, "forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "invitation": "ICLR.cc/2019/Conference/-/Paper360/Official_Review", "content": {"title": "good paper", "review": "This paper presents a mixed integer programming technique for verification of piecewise linear neural networks. This work uses progressive bounds tightening approach to determine bounds for inputs to units. The authors also show that this technique speeds up the bound determination by orders of magnitude as compared to other complete and incomplete verifiers. They also compare the advercerial accuracies on MNIST and CIFAR and improve on the lower bounds as compared to PGD and upper bounds as compared to SOA. The paper is well written and presents a valuable technique for evaluating robustness of classifiers to adversarial attacks. \n", "rating": "7: Good paper, accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper360/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.", "keywords": ["verification", "adversarial robustness", "adversarial examples", "deep learning"], "authorids": ["vtjeng@mit.edu", "kaix@mit.edu", "russt@mit.edu"], "authors": ["Vincent Tjeng", "Kai Y. Xiao", "Russ Tedrake"], "TL;DR": "We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.", "pdf": "/pdf/1f3223345aee606497f20dc66f62714eb9b83b18.pdf", "paperhash": "tjeng|evaluating_robustness_of_neural_networks_with_mixed_integer_programming", "_bibtex": "@inproceedings{\ntjeng2018evaluating,\ntitle={Evaluating Robustness of Neural Networks with Mixed Integer Programming},\nauthor={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGIdiRqtm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper360/Official_Review", "cdate": 1542234478798, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyGIdiRqtm", "replyto": "HyGIdiRqtm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper360/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335706962, "tmdate": 1552335706962, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper360/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 19}