{"notes": [{"id": "BklfR3EYDH", "original": "BJgg0SIEvB", "number": 255, "cdate": 1569438921699, "ddate": null, "tcdate": 1569438921699, "tmdate": 1577168277166, "tddate": null, "forum": "BklfR3EYDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "yAZrLt9M7d", "original": null, "number": 1, "cdate": 1576798691578, "ddate": null, "tcdate": 1576798691578, "tmdate": 1576800943714, "tddate": null, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Decision", "content": {"decision": "Reject", "comment": "The paper is interesting in video prediction, introducing a hierarchical approach: keyframes are first predicted, then intermediate frames are generated. While it is acknowledge the authors do a step in the right direction, several issues remain: (i) the presentation of the paper could be improved (ii) experiments are not convincing enough (baselines, images not realistic enough, marginal improvements) to validate the viability of the proposed approach over existing ones.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718985, "tmdate": 1576800269559, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper255/-/Decision"}}}, {"id": "rklBAZ8AYH", "original": null, "number": 1, "cdate": 1571869133169, "ddate": null, "tcdate": 1571869133169, "tmdate": 1574449965946, "tddate": null, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "The paper introduces a model trained for video prediction hierarchically: a series of significant frames called \u201ckeyframes\u201d in the paper are first predicted and then intermediate frames between keyframes couples are generated. The training criterion is maximum likelihood with a variational approximation. Experiments are performed on 3 different video datasets and the evaluation is performed for 3 tasks: keyframe detection, frame prediction and planning in robot videos.\nThe idea of generating an abstraction or a summary of a video via a sequence of important frames is attractive and could probably be used in different contexts. The proposed model is new and the authors introduce some clever ideas in order to train it. The evaluation work is important and the authors propose different settings for this evaluation. \nThe paper also present weaknesses. First the motivation for keyframes generation should be better developed: the model does not perform better than baselines for video frames prediction so that keyframes generation should be motivated by other applications. Planning as proposed by the authors could be one, but in this case it should be more developed. The main weakness is however the technical presentation which is painful to follow. When it is possible to get a general picture of what is done, it is quite difficult to figure out exactly how the model works. A global rewriting and maybe a better focus are required for publication. The probabilistic model (section 3.1) is relatively clear, even if it could be improved. It seems that the generation of a keyframe and the prediction of the corresponding time (tau^n)  are independent (eq. 3). This could be commented. Also it seems that in eq. 3 the log(K|z..) term should be inside an expectation. Section 4 was difficult to decipher for me. My understanding is that instead of sampling from a multinomial during training, you bypass this non differentiable operation by using what you call \u201csoft targets\u201d thus obtaining a differentiable objective (eq. 4). Is that true? In any case, the procedure should be made a lot clearer. The \u201cintermediate frame\u201d passage also remained confuse for me.\nConsidering the experiments, the authors make an important effort in order to evaluate different aspects of their model. In a fisrt step, they evaluate the ability of the model to generate significant keyframes using a detection setting.  It is not clear how they define ground truth frames for this evaluation. Those ground truth frames are defined as the frames where the movement in the image changes, which is easy on the Brownian movement dataset but what about the others? Also the baselines used in this comparison are weak. In the paper of Denton, they suggest some way to detect surprise and apparently this is not what you used. This should be justified/ commented. For keyframe modeling the proposed model behaves similarly to the baselines and even performs worse than the simpler \u201cjumpy\u201d model. Concerni g the paragraph about the selection of the number of predicted keyframes, it is not clear what is the reference (ground truth) number of target keyframes.\n The planning experiments are interesting, but difficult to follow at least from the main text.\nOverall, I think that there are several interesting ideas and realizations. They should be better put in perspective and explained.\n\n----- post rebuttal -----------\n\nThanks for the detailed answer. The paper is largely improved both for the style and the comparisons. But still requires further improvements. I will keep my score.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper255/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575679136591, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper255/Reviewers"], "noninvitees": [], "tcdate": 1570237754774, "tmdate": 1575679136608, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Review"}}}, {"id": "Hygl1RwhjB", "original": null, "number": 10, "cdate": 1573842391995, "ddate": null, "tcdate": 1573842391995, "tmdate": 1573842391995, "tddate": null, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment", "content": {"title": "All comments addressed and waiting for feedback", "comment": "We have yet to hear back from the reviewers, and our window to reply is rapidly closing. We appreciate any additional comments you have. Thank you!"}, "signatures": ["ICLR.cc/2020/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklfR3EYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper255/Authors|ICLR.cc/2020/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174098, "tmdate": 1576860536659, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment"}}}, {"id": "BkxTgd-YsS", "original": null, "number": 9, "cdate": 1573619701116, "ddate": null, "tcdate": 1573619701116, "tmdate": 1573619842293, "tddate": null, "forum": "BklfR3EYDH", "replyto": "rklBAZ8AYH", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment", "content": {"title": "Performed experiment with surprise baseline, keyframe modeling, improved presentation, clarified motivation", "comment": "We thank the reviewer for the helpful comments and suggestions. We have made several changes to the presentation of the technical section as well as the description of the experiments and results to address the reviewer's concerns (updates in red). We have also added comparisons to alternative \u201csurprise\u201d baselines as well as experiments on keyframe modeling. We hope the responses below address the reviewer\u2019s concerns and we are happy to make additional changes if those are required. \n\n== Motivation should be better developed ==\nWe thank the reviewer for pointing out a possible confusion about the motivation of the paper. We tried to highlight in the original submission that the goal of this work is not to improve video prediction quality but instead to discover meaningful temporal structure in sequences. As the reviewer notes, one possible application for the temporal structure discovered by our model is predicting subgoals for efficient long-horizon planning, and we now expanded the introduction to discuss this. We note that planning is known to be a challenging task [1-4] and we show that our model is able to outperform strong baselines using the learned temporal abstraction.\n\n== Keyframe detection baseline weak, use Denton&Fergus\u201918 ==\n We thank the reviewer for proposing this alternative comparison. In the original submission we used a measure of surprise based on the KL divergence (see Sec. 6.3, details in appendix, Sec. F). To support the strength of this baseline we have now added two baseline evaluations using alternative definitions of surprise. The first uses the method of Denton&Fergus\u201918 that the reviewer proposed. The second uses the lower bound on the log-likelihood -log(p) instead of just the KL divergence to measure surprise. In Tab 3,4, we find that the KL-based baseline reported in the submission consistently performs on par with these alternative formulations, and that our method outperforms all baselines. \n\n== No performance gain on keyframe modeling ==\nWe understand that the reviewer is referring to the results evaluating image sequence prediction quality (as opposed to keyframe prediction quality). It is true that our method does not improve performance on video modeling, but we emphasize that it is able to perform on par with recent work (Denton&Fergus\u201918) while additionally discovering the keyframe structure of the sequence. Please refer to our answer to the first question about motivation. We additionally performed an experiment showing that our model\u2019s ability to model keyframes (i.e. the most important frames) is superior to the baseline methods that do not discover temporal structure in Tab. 6. We further clarified the emphasis of the paper with an additional sentence in Sec 6.2.\n\n== Soft targets for obtaining differentiable objective? ==\n This interpretation is correct: we introduce the relaxed formulation to bypass the sampling step from p(tau|z,I_co) and make the formulation fully differentiable. The original submission stated this in Sec. 4. We thank the reviewer for pointing out the need for further clarifications. We restructured Sec. 4 to more clearly motivate and derive the soft relaxation objective. We additionally improved Fig. 3 to better illustrate the procedure.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklfR3EYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper255/Authors|ICLR.cc/2020/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174098, "tmdate": 1576860536659, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment"}}}, {"id": "r1g3Cw-FiH", "original": null, "number": 8, "cdate": 1573619667716, "ddate": null, "tcdate": 1573619667716, "tmdate": 1573619820273, "tddate": null, "forum": "BklfR3EYDH", "replyto": "rklBAZ8AYH", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment", "content": {"title": "Part 2 - additional improvements / clarifications", "comment": "Below we respond to additional reviewer\u2019s comments regarding clarity. We hope that this addresses the reviewer\u2019s points and would be happy to incorporate any further suggestions for clarification.\n\n== Not clear how ground truth keyframes are defined ==\n As the reviewer points out, we define keyframes as the points of motion change in the Stochastic Brownian Motion dataset. For the robot pushing dataset, keyframes are those frames when the robot lifts its arm to transition between pushes. For the gridworld dataset, frames in which the agent interacts with an object are defined as keyframes. This is explained in the third paragraph of Sec. 6.1 in the submission. We refer the reviewer to the supplementary website with visual examples of the trajectories. To further clarify this, we expanded the description in the manuscript.\n\n== Keyframe and \\tau distributions independent ==\n The keyframe and the \\tau distributions are indeed modeled as independent, given the latent variable. This is desirable as we want the latent variable to describe the dependencies between these variables. We added a footnote to Eq. 3 for clarification.\n\n== Missing expectation around keyframe likelihood ==\n We thank the reviewer for pointing out this typo, the log(K|z,I_co) term in Eq. 3 should indeed be inside the expectation over q(z), we corrected this.\n\n== Planning experiments hard to follow ==\n We thank the reviewer for pointing out the need for further clarification. In the submission we described the details of the planning procedure in Sec. K of the appendix and in Alg. 2,3. To help clarify the planning experiments in the main text we included several sentences in Sec. 6.4 detailing the employed planning procedures.\n\nWe hope that the changes to the manuscript address all points raised in the review. Please let us know if there are any other points that should be addressed and otherwise please consider updating your review.\n\n[1] Self-supervised visual planning with temporal skip connections, Ebert et al., CoRL 2017\n[2] Visual foresight, Ebert et al., 2018\n[3] Time-Agnostic Prediction, Jayaraman et al., ICLR 2019\n[4] Dynamics-Learning with Cascaded Variational Inference for Multi-Step Manipulation, Fang et al., CoRL 2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklfR3EYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper255/Authors|ICLR.cc/2020/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174098, "tmdate": 1576860536659, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment"}}}, {"id": "B1xEGS-Yir", "original": null, "number": 4, "cdate": 1573618956183, "ddate": null, "tcdate": 1573618956183, "tmdate": 1573619547936, "tddate": null, "forum": "BklfR3EYDH", "replyto": "SJl6wpNVcS", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment", "content": {"title": "Performed an experiment with gaussian noise", "comment": "We thank the reviewer for the helpful comments and suggestions. We made the following changes to the submission to address the reviewer's remarks and answer the posed questions.\n\n== robustness in noisy settings ==\nWe thank the reviewer for drawing attention to the importance of robustness to noise of the kind that exists in real-world domains. We added an experiment (Tab. 7 and Fig 11) showing that KeyIn keyframe detection performance is robust to Gaussian image noise, a noise characteristic that is commonly found in real camera sensors [5]. We believe this experiment provides some initial evidence that KeyIn can learn representations that are robust to noise. We note that comparable prior work uses environments with little noise and few distractors [1,2,3,4], but we hope that future work will be able to investigate this direction further once the video modeling community matures to the point of using complex real-world data with diverse background activity of the kind the reviewer suggests. \n\n== Modest precision-recall numbers ==\nThe reported precision-recall numbers indeed look modest, however, this is largely due to the inherent ambiguity when defining \u201ctrue\u201d keyframes. For example: in the grid-world environment, is the frame where the agent reaches an object or where it interacts with the object the better keyframe? While we chose one definition of a keyframe, we find that the method often predicts keyframes consistent with another definition, leading to off-by-one errors that are severely penalized by precision-recall metrics. To address this, we added an evaluation that measures minimum temporal distance to the true keyframe in Tab 4, which shows that KeyIn places keyframes closer to the annotated keyframes than all baselines. We also point out that the planning experiments provide a more objective evaluation metric for the quality of keyframes, and we find that KeyIn improves planning performance over all baselines.\n\n== Human annotation baseline ==\nWe thank the reviewer for this suggestion. We agree that on the current environments the human annotations would not be much different from the ones used to evaluate the models. However, for future work that extends KeyIn to more complex environments where it is even harder to define \u201cobjective\u201d keyframes, crowdsourced human annotations will likely be necessary for proper evaluation.\n\n[1] Time-Agnostic Prediction, Jayaraman et al., ICLR 2019\n[2] Stochastic Video Generation with a Learned Prior, Denton&Fergus, ICML 2018\n[3] Learning latent dynamics for planning from pixels, Hafner et al., ICML 2019\n[4] Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning, Ebert et al., 2018\n[5] https://en.wikipedia.org/wiki/Image_noise\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklfR3EYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper255/Authors|ICLR.cc/2020/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174098, "tmdate": 1576860536659, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment"}}}, {"id": "S1lMWv-Yjr", "original": null, "number": 7, "cdate": 1573619449801, "ddate": null, "tcdate": 1573619449801, "tmdate": 1573619509977, "tddate": null, "forum": "BklfR3EYDH", "replyto": "SklIUCrW5B", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment", "content": {"title": "Provided clarifications, updated presentation, performed experiment with surprisal baseline", "comment": "We thank the reviewer for the useful comments and suggestions. We understand that the main concerns of the reviewer are that 1) we only predict J intermediate frames, 3) the inference procedure is unclear, and 5) the surprisal baseline is not the same as in Kipf\u201919. There appear to be several critical points of misunderstanding, which we will try to clarify. While we made our best attempt to address the reviewer\u2019s concerns, we hope the reviewer can engage with us during the discussion period to clarify their concerns and help us understand how we can address them. We updated and clarified parts of the technical section based on the reviewer\u2019s feedback (updates in red).\n\nWe next address the reviewer\u2019s concerns, numbered by the order in which they appear in the review, starting with the most crucial.\n\n==  1, 7. Predict J intermediate frames? ==\nThe number of frames between two keyframes is not fixed - it is given by the variable tau as explained in Sec. 3.1, eqs (1, 2). However, in practice, the number of frames between the keyframes is indeed bounded by J (for computational reasons). We note that at training time, we always generate all J frames as the timestep distribution tau in general has non-zero mass at each timestep. However, at test time we only predict argmax(tau) intermediate frames. \n\n== 3. Missing details of inference? Planning needed for inference? ==\nWe interpret the question as asking about the test time procedure of the model (rather than the variational inference procedure we use). At test time, we can use our model in three different ways, to (1) generate a sequence given conditioning frames, as described in Sec 3.1, (2) infer the keyframes of a sequence, by using the inference network described in Sec 3.2, or (3) perform planning to reach a goal as described in Sec 6.4. We suspect the reviewer\u2019s question is about (1), which is performed as follows. We sample the latents from the prior distribution p(z|I_co), generate the corresponding keyframes K and their indices tau, and use the inpainter network to generate the rest of the sequence. We note that the planning algorithm described in the appendix is used for motion planning as part of (3) and is not a part of the training. Planning is not required for (1) or (2).\n\n== 5. Why this surprisal metric? ==\nAs the reviewer points out, our original surprisal metric is not the same as the traditional log(p). While a lower bound on log(p) can be computed by summing the reconstruction and the KL-divergence loss, we only used the KL-divergence part. We have now added a version of the baseline that computes the full lower bound as well as the version from Denton&Fergus\u201918 suggested by R1. In Tabs. 3, 4, we find that these new versions of the surprise baseline perform comparably to the one we originally reported, and our method outperforms all of them. "}, "signatures": ["ICLR.cc/2020/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklfR3EYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper255/Authors|ICLR.cc/2020/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174098, "tmdate": 1576860536659, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment"}}}, {"id": "HJg39U-tor", "original": null, "number": 6, "cdate": 1573619348148, "ddate": null, "tcdate": 1573619348148, "tmdate": 1573619415571, "tddate": null, "forum": "BklfR3EYDH", "replyto": "SklIUCrW5B", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment", "content": {"title": "Part 2 - additional improvements / clarifications", "comment": "== 1. Equation 3 typo? ==\nThe expectation in Eq 3 is indeed taken with respect to the current and next keyframe indices. We apologize for the typo where only the current keyframe index was specified: we have corrected this in the manuscript. \n\n== 2. Why normalize over T frames/all keyframes in first T? ==\nWe only use the first T frames output by the network as the prediction, and we do not enforce that all of the keyframes predicted by the network fall in the first T frames. Keyframes predicted after the first T total frames are discarded. Our network thus can avoid using its full keyframe budget by putting keyframes after the predicted sequence. In practice, we observe that this feature allows the network to discard extraneous keyframes when a sequence is well modeled with a fewer number of keyframes, such as in the bouncing ball experiments in Fig 9.\n\n== 4. Why two-stage training when the model is fully differentiable? ==\nWe thank the reviewer for this insightful question. In our initial experiments we observed that without the two-stage training the network failed to predict sequences that are long enough. As described in Sec 5.2, we hypothesize that pre-training the inpainting network aids optimization as it learns inpainting strategies for a variety of different inputs, allowing it to generate longer sequences. We note that end-to-end differentiability is important as we utilize it in the second stage of the training to backpropagate the error of the keyframe predictor through the inpainter.\n\n== 6. Cite Sauer; Kipf concurrent? ==\nSauer\u201919 (Tracking Holistic Object Representations) does not appear to perform keyframe analysis. The work of Kipf\u201919 was conducted in parallel to us as evidenced by preprint version of our paper (which we do not link for the purposes of double-blind review) that was cited by Kipf\u201919 as concurrent work. \n\n== 7. Try to jointly predict current, next timestep? ==\nThe indices for the timesteps of the current and the next keyframe are indeed predicted by the same network as described in Sec 5.1. \n\n== 8. Autoregressive video models. ==\nWe thank the reviewer for pointing this out and we have modified the original statement. While it is possible to adapt models like HMRNN to videos by using autoregressive prediction, as stated in our related work summary, autoregressive models for video prediction suffer from slow inference, and take minutes to generate a video even for the fastest models. We thus believe this approach to keyframe modeling would be impractical.\n\nWe hope that we were able to address all points raised by the reviewer. Please let us know if there are any other points that should be addressed and otherwise please consider updating your review.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklfR3EYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper255/Authors|ICLR.cc/2020/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174098, "tmdate": 1576860536659, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment"}}}, {"id": "ryldwNWYjB", "original": null, "number": 3, "cdate": 1573618784390, "ddate": null, "tcdate": 1573618784390, "tmdate": 1573618784390, "tddate": null, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment", "content": {"title": "Updated presentation of the approach, performed experiments with noise, surprise, keyframe modeling.", "comment": "We thank the reviewers for their feedback. We note that the reviewers found the addressed problem of keyframe discovery interesting and well-motivated and the proposed method novel. At the same time, the reviewers requested clarifications, suggested further analysis and improvements to the presentation. We performed additional experiments and otherwise updated the manuscript according to the suggestions (updates in red). We summarize the comments we believe are most crucial and respond to them here. \n\nR3: An evaluation on a dataset with noise from background activity would strengthen the paper.\nA: We added an experiment showing robustness of our method to background noise activity on Pushing and Gridworld data in Tab 7. We believe this quantitative evaluation elucidates some properties of the method, and we hope that future work will be able to investigate this further once the video modeling community matures to the point of using complex real-world data with diverse background activity of the kind the reviewer suggests.\n\nR1,2: Some prior work uses a different definition of surprisal than the one we use as a baseline.\nA: Our original submission contained a version of a surprisal baseline measuring an approximation to log(p(s)). We have now added a version of this baseline of the same form as the one in Kipf'19 suggested by R2 as well as the version from Denton&Fergus\u201918 suggested by R1. We find that these new versions of the surprise baseline perform comparably to the one we originally reported, and our method outperforms all of them in Tab 3,4. \n\nR1: Including more details about the planning experiment in the main paper would make it easier to follow.\nA:  We added further details to the planning section in the main body of the paper. We note that this section spans almost a page and for the reasons of space we left certain details of the procedure in the supplement. We will happily move specific parts of the supplement to the main paper if they are deemed necessary to understand this section.\n\nR1: The proposed method does not beat prior methods when evaluated on video prediction, other motivation is needed.\nA: Indeed, we motivate the proposed method for keyframing the future as 1) being able to build a compact representation of the sequence just in terms of discovered keyframes on three datasets, and 2) being further able to utilize such representation for planning, empirically surpassing strong prior work. Additionally, we added an experiment showing that our model outperforms baselines on *keyframe* modeling, showing that it is better than prior methods on predicting the important frames in the sequence (while being on par for sequence modeling) in Tab 6.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklfR3EYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper255/Authors|ICLR.cc/2020/Conference/Paper255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174098, "tmdate": 1576860536659, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper255/Authors", "ICLR.cc/2020/Conference/Paper255/Reviewers", "ICLR.cc/2020/Conference/Paper255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Comment"}}}, {"id": "SklIUCrW5B", "original": null, "number": 2, "cdate": 1572064845676, "ddate": null, "tcdate": 1572064845676, "tmdate": 1572972618803, "tddate": null, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a variational objective to train a model which can jointly select keyframes of a video and generate the intervening frames to produce a resultant video. The model is provided an initial set of frames as context. At training time the model always learns to produce N*J frames, where N is the number of keyframes and J is a fixed number of frames to generate for each keyframe. The authors compare their method for selecting informative keyframes on a number of baselines and show an improvement over these baselines. The problem is interesting and well-motivated, but I have some concerns with the proposed approach and experiments. As such, I am a weak reject.\n\ncomments / questions:\n- Equation 3 lacks context. Initially, when looking at the authors' objective it seems that the inner expectation should be taken with respect to the joint time indices for the current and next keyframe. Only later after equation 4 do they mention that they always predict a fixed number of frames J. \n- The need for normalizing over the first T timesteps in equation 4 seems quite messy. Is it guaranteed that all of the needed keyframes will actually be within the first T timesteps? How does this work in practice?\n- Many important details of the inference procedure are relegated to the appendix. For example, there are no details for extracting which of the 60 keyframes that were trained for a sequence (due to the fixed length sequences) should be selected at test time. Looking at the appendix, it is clear that the approach requires an extensive planning algorithm at inference time, which seems like an important component.\n- The authors prominently highlight that their method is fully differentiable, yet they train in two stages while freezing weights. Why isn't the model trained end-to-end? The stated reason for doing so is that this \"simple\" two-stage procedure improves optimization. What exactly happens if you don't do this two stage training process? Does it fail to learn? Some experimental numbers would be nice to see. \n- The authors do not compare their method to any strong keyframe prediction baselines. Considering there is existing work in keyframe prediction, it seems important to highlight the difference between other competing models, rather than relying on simple baselines. Why don't they use self-information/surprisal as a baseline i.e., by training an autoregressive model on the frames and then picking the N frames with the largest -log(p)? This is a metric that has been investigated frequently and has better interpretability than defining a new measure of surprise. Note that Kipf et al. (2019) uses this notion of surprisal as well.\n- Sauer et al. (BMVC 2019) should likely be cited as it does very similar keyframe analysis. Also, as the ICML 2019 conference had already concluded by the ICLR submission deadline, is it really fair to state the work with Kipf et al. (2019) was conducted in parallel?\n- Why does the model trained to learn a fixed number of timesteps for the intermediate frames? Did they investigate jointly predicting the indices for the current and next timesteps? It seems like it would greatly simplify their inference scheme if they did this. If they tried that approach and it failed, maybe that should be mentioned in the paper (with an explanation as to why it fails).\n- In the literature review, when discussing hierarchical temporal structure, the authors state: \"However, these models rely on autoregressive techniques for text generation and are not applicable to structured data, such as videos.\" Autoregressive techniques have been investigated in relation to videos; in fact, the authors later describe papers that have used autoregressive techniques for modeling videos.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper255/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575679136591, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper255/Reviewers"], "noninvitees": [], "tcdate": 1570237754774, "tmdate": 1575679136608, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Review"}}}, {"id": "SJl6wpNVcS", "original": null, "number": 3, "cdate": 1572257125110, "ddate": null, "tcdate": 1572257125110, "tmdate": 1572972618758, "tddate": null, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "invitation": "ICLR.cc/2020/Conference/Paper255/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors address the problem of discovering and predicting with hierarchical structure in data sequences of relevance to planning. Starting with the kinds of data that have been used recently in video prediction, the authors aim at learning a sequence of keyframes (i.e., subsets of frames forming the overall sequence) that in a suitable sense \"summarize\" the overall trace. As they rightly note, many alternate models struggle with making good long term predictions in part because they focus on all levels of prediction equally.\n\nThe technical approach is to pose the problem as one of inferring the temporal location of each of these key frames and then to interpolate with a model to generate intermediate frames. One could try to make either step sophisticated - the authors choose to make the keyframe selection more sophisticated and interpolation simpler. The paper first described the KeyIn model in terms of a probabilistic model of jointly finding the Ks and then the inpainted Is. This can become delicate, so the authors propose a relaxation that is more forgiving when the keyframe locations are being searched for. Learning is driven by a reconstruction loss of finding the approximate location, locally interpolating and then seeing if this accords with the training data. This is all implemented with an LSTM based NN architecture which seems sound to me.\n\nI feel the paper is taking on the right kinds of questions, looking for ways to inject the right kind of structure. I do have some concerns about the overall formulation:\n\n1. Much of the paper is focussed on rather clean images where nothing extraneous is happening. In reality, the backgrounds of real images is not so benign and other extraneous dynamics might interfere. While I understand this is a step towards the long term goal, I wonder if the end result is a bit too incremental in the absence of some attempt to explore this source of (lack of) robustness.\n\n2. In \u00a76.3, the authors try to demonstrate that the number of keyframes parameter can be wrong by a little bit but these are still small ranges. In realistic images it is likely that the total number of keyframes selected by such an algorithm is much larger due to extraneous events. This is why a proper robustness study is crucial on more realistic input. As it, in anything other than the trivial dot on black background, the precision-recall numbers are fairly modest. This will likely degenerate into noise in most camera-based images of the kind seen by a real robot. So, how much confidence should we expect to have in the approach's generality?\n\n3. For the baselines, the true good baseline might have been a human annotation that tells us how people really conceptualise the structure. With data such as pushing, this might not be so different from the simple visual inspection, but again with real data this will vary. The paper would really be much stronger if these were addressed.\n\n\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper255/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper255/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction", "authors": ["Karl Pertsch", "Oleh Rybkin", "Jingyun Yang", "Konstantinos G. Derpanis", "Kostas Daniilidis", "Joseph J. Lim", "Andrew Jaegle"], "authorids": ["pertsch@usc.edu", "oleh@seas.upenn.edu", "jingyuny@usc.edu", "kosta@ryerson.ca", "kostas@seas.upenn.edu", "limjj@usc.edu", "ajaegle@upenn.edu"], "keywords": ["representation learning", "variational inference", "video generation", "temporal hierarchy"], "TL;DR": "We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.", "abstract": "To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them.  We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "pdf": "/pdf/545de03f5bb128edcd4d61688150c053da64b2fa.pdf", "paperhash": "pertsch|keyframing_the_future_discovering_temporal_hierarchy_with_keyframeinpainter_prediction", "original_pdf": "/attachment/9cd514c79ede14b593d260f4cc619bb3108ffcf6.pdf", "_bibtex": "@misc{\npertsch2020keyframing,\ntitle={Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction},\nauthor={Karl Pertsch and Oleh Rybkin and Jingyun Yang and Konstantinos G. Derpanis and Kostas Daniilidis and Joseph J. Lim and Andrew Jaegle},\nyear={2020},\nurl={https://openreview.net/forum?id=BklfR3EYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklfR3EYDH", "replyto": "BklfR3EYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575679136591, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper255/Reviewers"], "noninvitees": [], "tcdate": 1570237754774, "tmdate": 1575679136608, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper255/-/Official_Review"}}}], "count": 12}