{"notes": [{"id": "6isfR3JCbi", "original": "CR7QWoAPUvJ", "number": 3161, "cdate": 1601308350767, "ddate": null, "tcdate": 1601308350767, "tmdate": 1615888030855, "tddate": null, "forum": "6isfR3JCbi", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5Z_7HMzo-tR", "original": null, "number": 1, "cdate": 1610040392684, "ddate": null, "tcdate": 1610040392684, "tmdate": 1610473987239, "tddate": null, "forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "invitation": "ICLR.cc/2021/Conference/Paper3161/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper provides a privacy-preserving method to boost the sample quality after training a GAN. The reviewers were unanimous that this paper should be presented at ICLR, with an important contribution to privacy-preserving GANs."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "tags": [], "invitation": {"reply": {"forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040392671, "tmdate": 1610473987223, "id": "ICLR.cc/2021/Conference/Paper3161/-/Decision"}}}, {"id": "eKWxtq5UJF", "original": null, "number": 1, "cdate": 1603825652695, "ddate": null, "tcdate": 1603825652695, "tmdate": 1606415012370, "tddate": null, "forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "invitation": "ICLR.cc/2021/Conference/Paper3161/-/Official_Review", "content": {"title": "A qualitative comparison with private generative models on MNIST is needed", "review": "Summary: This paper studies the differential private synthetic dataset generation. Unlike previous DP based GAN models, this paper aims to boost the sample quality of after the training stage. In particular, the final synthetic dataset is sampled from the sequence of generators obtained during GAN training. The distribution is obtained by a private two-player game between the privately selected discriminator and a sampler from the mixture of generators. The results are demonstrated on gaussian data and tabular data.\n\n\nPros: 1. The sample quality of private generative models is known to be not as good as non-private models. This paper provides a practical private post gan boosting algorithm to improve the sample quality. \n\nCons:1. My main concern is on experiments. It is known that private generative models have bad sample quality on image data. Prior works on private synthetic generation papers usually show results on MNIST. It would be better if the authors could compare private PGB on MNIST dataset. \n\n2. It would be better to have an ablation study of the proposed PGB and discriminator rejection sampling. For example, in Figure~1, the baselines for both non-private gan and private gan are too bad. I am wondering whether the gain is from rejection sampling or the proposed PGB algorithm.\n\n\nQuestions:\n\n1. I am curious about how to split epsilon for gan training and the post gan boosting. Are there any principled reasons for the split?\n\n2. How do you calculate the sensitivity for the exponential mechanism?", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3161/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081118, "tmdate": 1606915770416, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3161/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3161/-/Official_Review"}}}, {"id": "Wk2dW2Ljm0A", "original": null, "number": 6, "cdate": 1606230290183, "ddate": null, "tcdate": 1606230290183, "tmdate": 1606244253904, "tddate": null, "forum": "6isfR3JCbi", "replyto": "eKWxtq5UJF", "invitation": "ICLR.cc/2021/Conference/Paper3161/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you very much for your helpful comments. We uploaded an updated version of the manuscript. We hope this addresses your concerns. \n\n*1. My main concern is on experiments. It is known that private generative models have bad sample quality on image data. Prior works on private synthetic generation papers usually show results on MNIST. It would be better if the authors could compare private PGB on MNIST dataset.*\n\nIn the updated manuscript we included an evaluation of Private PGB on the MNIST data. Our results show that our proposed Private PGB method improves the quality of synthetic samples. We included these results on page 8 of the updated manuscript. Uncurated samples of DP MNIST digits can be found in the Appendix.\n\n*2. It would be better to have an ablation study of the proposed PGB and discriminator rejection sampling. For example, in Figure~1, the baselines for both non-private gan and private gan are too bad. I am wondering whether the gain is from rejection sampling or the proposed PGB algorithm.*\n\nWe included an ablation study to directly compare the samples from the last Generator, DRS, PGB and the combination of PGB and DRS on both the toy dataset of 25 gaussians, as well as on the MNIST data (without and with differential privacy). The results show that the improvements mainly come from our proposed PGB method. We included these results in Figure 1 on page 7 of the updated manuscript and in the respective sections describing the results of the toy data experiments and the MNIST experiments.\n\n*Q: I am curious about how to split epsilon for gan training and the post gan boosting. Are there any principled reasons for the split?*\n\nOur principle is to allocate the majority of the privacy budget to the DP GAN training, and a much smaller budget for our PGB method. Our intuition is that the DP GAN training is doing the \"heavy lifting\": providing a good \"basis\" for PGB requires a substantial privacy expenditure in training DP GAN. Without a good basis, PGB cannot provide good quality synthetic data. We address this in Footnote 2 on page 6 of the manuscript. Assessing the impact of the privacy budget allocation between DP GAN training and PGB is definitely worthwhile and something we want to address in future work.\n\n*Q: How do you calculate the sensitivity for the exponential mechanism?*\n\nRegarding the sensitivity for the exponential mechanism, as stated in Section 2.1 on page 3, for Private PGB we assume the discriminator scores to be probabilities.  Since the score function takes an average over n probabilities (one for each private example), the sensitivity is 1/n. We added footnote 1 in Section 3.2 (page 5) to highlight this."}, "signatures": ["ICLR.cc/2021/Conference/Paper3161/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6isfR3JCbi", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3161/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3161/Authors|ICLR.cc/2021/Conference/Paper3161/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840513, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3161/-/Official_Comment"}}}, {"id": "tnONjlXbt82", "original": null, "number": 5, "cdate": 1606230244321, "ddate": null, "tcdate": 1606230244321, "tmdate": 1606243974460, "tddate": null, "forum": "6isfR3JCbi", "replyto": "2uSEIDwfm9Y", "invitation": "ICLR.cc/2021/Conference/Paper3161/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you very much for your positive and helpful review. We updated the manuscript and hope that the updated version addresses the points you raised.\n\n*Q: However, I think it might be helpful to do more experiments on more difficult datasets (like those usually used to evaluate non-private GAN) with various epsilons (even with very large epsilon), so that readers and future researchers can understand the limitations of the current sota.* \n\nIn the updated manuscript we included an evaluation of Private PGB on the MNIST data. Our results show that our proposed Private PGB method improves the quality of synthetic samples. We included these results on page 8 of the updated manuscript. Uncurated samples of DP MNIST digits can be found in the Appendix.\n\n*Q: The privacy aspect of the algorithm, e.g. which part needs privacy protection and what is the sensitivity of the score function, can be elaborated more.*\n\nAs stated in Section 2.1 on page 3, for Private PGB we assume the discriminator scores to be probabilities.  Since the score function takes an average over n probabilities (one for each private example), the sensitivity is 1/n. We added footnote 1 in Section 3.2 (page 5) to highlight this."}, "signatures": ["ICLR.cc/2021/Conference/Paper3161/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6isfR3JCbi", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3161/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3161/Authors|ICLR.cc/2021/Conference/Paper3161/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840513, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3161/-/Official_Comment"}}}, {"id": "xrBNzEUbgfg", "original": null, "number": 3, "cdate": 1606203776736, "ddate": null, "tcdate": 1606203776736, "tmdate": 1606238016682, "tddate": null, "forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "invitation": "ICLR.cc/2021/Conference/Paper3161/-/Official_Comment", "content": {"title": "Reply to Reviewers | Private Post-GAN Boosting", "comment": "We thank the three reviewers for their positive and helpful reviews.\n\nIn the following we will address the points raised by Reviewers 1 & 2.\n\nWe agree with reviewers 1 & 2 that an evaluation of Private PGB on an image data set could highlight the advantage of our proposed method. In the updated manuscript we included an evaluation of Private PGB on the MNIST data as suggested by Reviewer 2 (and implied by Reviewer 1). Our results show that our proposed Private PGB method improves the quality of synthetic samples. We included these results on page 8 of the updated manuscript. Uncurated samples of DP MNIST digits can be found in the Appendix.\n\nYet, ultimately, our goal with differentially private synthetic data is mainly to generate tabular data to facilitate research with privacy guarantees. Working with noisy statistics released under differential privacy requires training. With differentially private synthetic data researchers could potentially keep their established workflows, while still protecting privacy. The focus of our manuscript was, therefore, on applied downstream tasks as shown in the experiments with the American Census data and the Titanic data.\n\nAddressing the second point of Reviewer 2, we included an ablation study to directly compare the samples from the last Generator, DRS, PGB and the combination of PGB and DRS on both the toy dataset of 25 gaussians, as well as on the MNIST data (without and with differential privacy). The results show that the improvements mainly come from our proposed PGB method. We included these results in Figure 1 on page 7 of the updated manuscript and in the respective sections describing the results of the toy data experiments and the MNIST experiments.\n\n*Q:  How do you calculate the sensitivity for the exponential mechanism?/ The privacy aspect of the algorithm, e.g. which part needs privacy protection and what is the sensitivity of the score function, can be elaborated more.*\n\nA: Regarding the sensitivity of the score function/exponential mechanism, as stated in Section 2.1 on page 3, for Private PGB we assume the discriminator scores to be probabilities. Since the score function takes an average over n probabilities (one for each private example), the sensitivity is 1/n. We added footnote 1 in Section 3.2 (page 5) to highlight this.\n\n*Q: I am curious about how to split epsilon for gan training and the post gan boosting. Are there any principled reasons for the split?*\n\nA: Our principle is to allocate the majority of the privacy budget to the DP GAN training, and a much smaller budget for our PGB method. Our intuition is that the DP GAN training is doing the \u201cheavy lifting\u201d: providing a good \u201cbasis\u201d for PGB requires a substantial privacy expenditure in training DP GAN. Without a good basis, PGB cannot provide good quality synthetic data. We address this in Footnote 2 on page 6 of the manuscript.\nAssessing the impact of the privacy budget allocation between DP GAN training and PGB is definitely worthwhile and something we want to address in future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3161/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6isfR3JCbi", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3161/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3161/Authors|ICLR.cc/2021/Conference/Paper3161/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840513, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3161/-/Official_Comment"}}}, {"id": "RTgAIYEVwj", "original": null, "number": 4, "cdate": 1606230112374, "ddate": null, "tcdate": 1606230112374, "tmdate": 1606230112374, "tddate": null, "forum": "6isfR3JCbi", "replyto": "KHkHQzqktEd", "invitation": "ICLR.cc/2021/Conference/Paper3161/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you very much for your positive review."}, "signatures": ["ICLR.cc/2021/Conference/Paper3161/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6isfR3JCbi", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3161/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3161/Authors|ICLR.cc/2021/Conference/Paper3161/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840513, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3161/-/Official_Comment"}}}, {"id": "2uSEIDwfm9Y", "original": null, "number": 2, "cdate": 1604008552065, "ddate": null, "tcdate": 1604008552065, "tmdate": 1605024056425, "tddate": null, "forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "invitation": "ICLR.cc/2021/Conference/Paper3161/-/Official_Review", "content": {"title": "Good progress in differentially private GAN", "review": "This paper proposes an algorithm to process the sequence of generators and discriminators produced by any differentially private GAN training, in order to produce synthetic data of better quality. The paper also presents some empirical evaluations. \nThere have been a few works on differentially private GAN, yet the noised required by differential privacy usually causes significant degradation in utility. So an algorithm for improving the results can be quite valuable. The evaluation was mainly on simple datasets, which is understandable as differentially private GAN can be quite hard. However, I think it might be helpful to do more experiments on more difficult datasets (like those usually used to evaluate non-private GAN) with various epsilons (even with very large epsilon), so that readers and future researchers can understand the limitations of the current sota.\n\nThe presentation is clear. The privacy aspect of the algorithm, e.g. which part needs privacy protection and what is the sensitivity of the score function, can be elaborated more.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3161/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081118, "tmdate": 1606915770416, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3161/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3161/-/Official_Review"}}}, {"id": "KHkHQzqktEd", "original": null, "number": 3, "cdate": 1604276040788, "ddate": null, "tcdate": 1604276040788, "tmdate": 1605024056362, "tddate": null, "forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "invitation": "ICLR.cc/2021/Conference/Paper3161/-/Official_Review", "content": {"title": "Sound theory, good results", "review": "The paper proposes a method of improving the generated samples of differential-private synthetic dataset using GANs by boosting them post training. They support their proposed method using theory, and then empirically show that it works on 3 types of machine learning tasks.\n\nThis paper presents a novel way of utilizing the sequence of generators and discriminators during training as they are already part of the privacy budget. So it significantly improves the quality of GAN-generated samples for different experiments under the same privacy budget.\n\nThe experiments provide evidence of the utility of the proposed method in all three tasks. The community can definitely benefit from this paper.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3161/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3161/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Private Post-GAN Boosting", "authorids": ["~Marcel_Neunhoeffer1", "~Steven_Wu1", "~Cynthia_Dwork2"], "authors": ["Marcel Neunhoeffer", "Steven Wu", "Cynthia Dwork"], "keywords": [], "abstract": "Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neunhoeffer|private_postgan_boosting", "pdf": "/pdf/9af34be61229e9ded84048009befadeb57d1957d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nneunhoeffer2021private,\ntitle={Private Post-{\\{}GAN{\\}} Boosting},\nauthor={Marcel Neunhoeffer and Steven Wu and Cynthia Dwork},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6isfR3JCbi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6isfR3JCbi", "replyto": "6isfR3JCbi", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081118, "tmdate": 1606915770416, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3161/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3161/-/Official_Review"}}}], "count": 9}