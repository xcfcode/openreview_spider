{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487682540622, "tcdate": 1478008165763, "number": 29, "id": "S1RP6GLle", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1RP6GLle", "signatures": ["~Casper_Kaae_S\u00f8nderby1"], "readers": ["everyone"], "content": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396315046, "tcdate": 1486396315046, "number": 1, "id": "HJXsif8_l", "invitation": "ICLR.cc/2017/conference/-/paper29/acceptance", "forum": "S1RP6GLle", "replyto": "S1RP6GLle", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All the reviewers agreed that the paper is original, of high quality, and worth publishing.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396315656, "id": "ICLR.cc/2017/conference/-/paper29/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1RP6GLle", "replyto": "S1RP6GLle", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396315656}}}, {"tddate": null, "tmdate": 1483284174721, "tcdate": 1483284174721, "number": 7, "id": "rkPAR58rl", "invitation": "ICLR.cc/2017/conference/-/paper29/public/comment", "forum": "S1RP6GLle", "replyto": "SywnTh9Ng", "signatures": ["~Casper_Kaae_S\u00f8nderby1"], "readers": ["everyone"], "writers": ["~Casper_Kaae_S\u00f8nderby1"], "content": {"title": "Re: Comment", "comment": "Thanks for the comment! We haven\u2019t experimented with CRF models for modelling the output layer, although that is definitely an interesting option. I\u2019m not familiar with the computational complexity for variational inference in CRFs for 2D data, but maybe the number of classes is an issue (at least fwd-bwd inference scales with the square of classes for 1D sequences) ?\n\nThanks for pointing out the typo!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287758516, "id": "ICLR.cc/2017/conference/-/paper29/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1RP6GLle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper29/reviewers", "ICLR.cc/2017/conference/paper29/areachairs"], "cdate": 1485287758516}}}, {"tddate": null, "tmdate": 1483283652896, "tcdate": 1483283652896, "number": 6, "id": "Syaph5UHl", "invitation": "ICLR.cc/2017/conference/-/paper29/public/comment", "forum": "S1RP6GLle", "replyto": "By9WjkwNl", "signatures": ["~Casper_Kaae_S\u00f8nderby1"], "readers": ["everyone"], "writers": ["~Casper_Kaae_S\u00f8nderby1"], "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for the comments, please find our response below.\n\nWe agree that the general algorithm for training GANs is not our contribution. However, the specific variant used here that minimizes the KL divergence between data and model distribution, eq 10 in the paper, is new (to our knowledge this was first discussed in a blog post by Ferenc Huszar - one of the coauthors of this submission, and has since been built upon in independent work by Arjovsky & Buttou and Mohamed & Lakshminarayanan - both parallel ICLR submissions). Furthermore, we present  \u2018instance noise\u2019 as a new and theoretically motivated technique for improving the stability of GAN training. This has also been independently discovered by Arjovsky & Buttou, and we believe it is a useful step in understanding the instability GANs.\n\nApart from sections (3.2,3.3,3.4) that the reviewer points out we also believe that the analysis of the loss function in section 3.0 and the affine projection for maximising the likelihood term described in section 3.1 are valuable contributions. Furthermore, Appendix F establishes a connection between GANs and amortised variational inference which we believe is a very important link - this opens the door for approximate Bayesian image superresolution which can have several applications in medical diagnosis where calibrated uncertainty estimates are important."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287758516, "id": "ICLR.cc/2017/conference/-/paper29/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1RP6GLle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper29/reviewers", "ICLR.cc/2017/conference/paper29/areachairs"], "cdate": 1485287758516}}}, {"tddate": null, "tmdate": 1483283500984, "tcdate": 1483283500984, "number": 5, "id": "SkHE25IHg", "invitation": "ICLR.cc/2017/conference/-/paper29/public/comment", "forum": "S1RP6GLle", "replyto": "rJEujkt4e", "signatures": ["~Casper_Kaae_S\u00f8nderby1"], "readers": ["everyone"], "writers": ["~Casper_Kaae_S\u00f8nderby1"], "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for the comments, please find our response below.\n\n> 1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.\n\nOur work primarily focuses on theoretically grounded methods for MAP inference in images whereas other recent papers focusses primarily on empirical evaluations and improvements of the image quality. We believe that our contributions are orthogonal and complementary to these works and further that by combining our work i.e. the affine projection layer presented here with the improved model architectures in i.e. Ledig et. al. 2016 the performance can be further improved. We agree that our results do not surpass the visual quality of reconstructions in i.e. Ledig et. al. 2016, however we believe our work have several contributions that make it a valid stand-alone contribution:\n\na) Recent work combined the GAN objective with either VGG feature matching and total variation loss (Ledig et. al. 2016, Dosovitskiy et. al. 2016) or L1 penalty in the low-resolution space (Garcia 2016, unpublished), as the GAN alone does not lead to good results in general. Our work shows that it is possible to use a GAN-only training objective with an appropriately modified architecture. Moreover we show that training of the modified architecture is faster and reaches a lower eventual minimum when trained with MSE (Figure 2), which makes us believe it is a better architecture choice overall.\n\nb) Ledig et al, (2016) provide only intuitive reasoning for using GANs and focus on empirical evaluation of the proposed method, this submission provides a solid theoretical motivation and interpretation for using GANs, and establishes connections to both MAP and variational inference (Appendix F).\n\nc) Compared to other recent works we first establish a theoretically motivated loss function (MAP inference) that we want to minimize. We then present three potential solutions and compare their relative merits (GAN, Likelihood based using pixelCNNs and Denoiser Guided).  While the conclusion is that GANs performed best in practice, we believe it is a valuable contribution to study alternative approaches. Further, to our knowledge the use of denoising in this context is also novel, and a non-trivial contribution.\n\nd) Although not the main contribution, this paper introduces the 'Instance noise' heuristic, and provides a novel analysis of the GAN algorithm in Appendix C. Note that this was discovered in parallel by (Arjovsky and Buttou, in review for ICLR 2017).\n\n> 2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still need the {HR, LR} image pairs. Does it mean that we merely transfer this training procedure to the training of affine projection?\n\nFor training the affine projection we do not need to use natural images, in fact we trained the projection using only white Gaussian noise. This approach approximates a numerical solution to matrix inversion. In figure 2 we provide results using combinations of learnable, pretrained and fixed affine projections. We find that pretraining the affine projection using white gaussian noise (as described above) and keeping the parameters fixed during training of the main model works the best. We have provided details in Appendix B, but we\u2019ll try to clarify this in the main text.\n\n> 3) The paper presents many results of the framework, including the results of natural images from ImageNet. Can the author also provide the results of Set5, Set14 or BSD100, which are conventional test dataset for SR, so that we can perform a fair comparison with previous work.\n \nOur focus have not been on achieving state of the art performance on benchmark datasets but on introducing and analyzing theoretically grounded methods for amortized MAP inference. We proposed three different methods for this and the included experiments are chosen to facilitate comparison of the relative merits of these methods. However we agree that future work combining the work present here with the methods proposed in i.e. Ledig et. al. 2016 should include such experiments.\n\n> 4) I see that the size of the results of nature images presented in this paper are limited to 128*128. Can this framework perform well on images with larger size? Because SR will encounter input with arbitrary size.\n\nOur models are fully convolutional it can be applied to arbitrarily sized input. There is nothing preventing the models from working on larger images, including the affine projection layer. \n\n> 5) A normal GAN will have a noise term as a latent space, so that it can be better illustrated as learning a distribution. Do the author try the noise vector?\n\nWe did experiment with introducing noise into the model as described below. In Section 5.6 we argue it might be desirable from a perceptual quality point of view and further in Appendix F we provide a theoretical motivation for doing so: it would be equivalent to a form of variational inference. \n\nThere are multiple ways one can introduce noise to the superresolution network i.e. via an input noise vector, input dropout or feature dropout.  We observed that the model simply chooses to ignore the introduced stochasticity if possible, such as when it is provided as an an input vector. Hence we believe that dropout noise or similar is preferred for training of AffGAN models since the model is forced to depend on the introduced stochasticity. Experimentally we found that the stochastic models do work as we hoped: conditioned on a LR face a noisy network can sample different plausible reconstructions, sometimes smiling, sometimes not for example. However we don't feel that these results are fully conclusive at this stage why we have chosen not to include them.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287758516, "id": "ICLR.cc/2017/conference/-/paper29/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1RP6GLle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper29/reviewers", "ICLR.cc/2017/conference/paper29/areachairs"], "cdate": 1485287758516}}}, {"tddate": null, "tmdate": 1482505646884, "tcdate": 1482505646884, "number": 4, "id": "SywnTh9Ng", "invitation": "ICLR.cc/2017/conference/-/paper29/public/comment", "forum": "S1RP6GLle", "replyto": "S1RP6GLle", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Comment", "comment": "I just wanted to comment that I found this to be a really great paper. There's a lot of new ideas, and the writing and experiments are extremely well executed.\n\nThe analysis of section 3-3.1 is novel (to the best of my knowledge) and very valuable, as it's interesting to see in play the DAE ideas in 3.3. Figure 1 (sec 2.1) is extremely clarifying. The criticism of section 5.6 is very interesting, and might lead to a new direction of research if handled with care, the question about the precise mathematical meaning of \"producing plausible samples\" is extremely important and far from solved.\n\nMinor comments / questions:\n- Have you tried comparing with an architecture such as the ones used in segmentation or structured prediction? The use of the mean field CRF approach will lead you to pick a mode and get sharp predictions as has been done for a long time, since you are training a conditional unimodal distribution with the inverse KL. The issue of continuous variables can be ameliorated by discretizing as in pixel CNN for example. As another bonus, these architectures are very stable and can be trained with much much bigger models.\n- Page 3. Item 2. Typo on employs -> employ"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287758516, "id": "ICLR.cc/2017/conference/-/paper29/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1RP6GLle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper29/reviewers", "ICLR.cc/2017/conference/paper29/areachairs"], "cdate": 1485287758516}}}, {"tddate": null, "tmdate": 1482386284211, "tcdate": 1482386284211, "number": 3, "id": "rJEujkt4e", "invitation": "ICLR.cc/2017/conference/-/paper29/official/review", "forum": "S1RP6GLle", "replyto": "S1RP6GLle", "signatures": ["ICLR.cc/2017/conference/paper29/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper29/AnonReviewer2"], "content": {"title": "Novel methodology", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments:\n\n1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.\n\n2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still need the {HR, LR} image pairs. Does it mean that we merely transfer this training procedure to the training of affine projection?\n\n3) The paper presents many results of the framework, including the results of natural images from ImageNet. Can the author also provide the results of Set5, Set14 or BSD100, which are conventional test dataset for SR, so that we can perform a fair comparison with previous work.\n\n4) I see that the size of the results of nature images presented in this paper are limited to 128*128. Can this framework perform well on images with larger size? Because SR will encounter input with arbitrary size.\n\n5) A normal GAN will have a noise term as a latent space, so that it can be better illustrated as learning a distribution. Do the author try the noise vector?\n\nOverall, this paper provides a new framework for SR with solid theoretical analysis. The idea is novel and the author explore many methods. Though there still exist questions like the necessity and more experiments are needed. I think this work will will provide good inspiration to the community.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512722550, "id": "ICLR.cc/2017/conference/-/paper29/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper29/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper29/AnonReviewer1", "ICLR.cc/2017/conference/paper29/AnonReviewer3", "ICLR.cc/2017/conference/paper29/AnonReviewer2"], "reply": {"forum": "S1RP6GLle", "replyto": "S1RP6GLle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper29/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper29/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512722550}}}, {"tddate": null, "tmdate": 1482255106454, "tcdate": 1482255106454, "number": 2, "id": "By9WjkwNl", "invitation": "ICLR.cc/2017/conference/-/paper29/official/review", "forum": "S1RP6GLle", "replyto": "S1RP6GLle", "signatures": ["ICLR.cc/2017/conference/paper29/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper29/AnonReviewer3"], "content": {"title": "Review", "rating": "9: Top 15% of accepted papers, strong accept", "review": "Sincere apologies for the late review.\n\nThis paper argues to approach Super-Resolution as amortised MAP estimation. A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout. Further three different methods to solve the resulting cross-entropy problem in Eq.9 are proposed and tested. \n\nSummary: Very good paper, very well written and presented. Experimental results are sufficient, the paper presents well chosen toy examples and real world applications. From my understanding the contributions for the field of super-resolutions are novel (3.2,3.3,3.4), parts that are specific for the training of GANs may have appeared in different variants elsewhere (see also discussion). I believe that this paper will be relevant to future work on super-resolution, the finding that GAN based model training yields most visually appealing results suggests further work in this domain. \n\nManuscript should be proof-read once more, there were some very few typos that may be worth fixing.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512722550, "id": "ICLR.cc/2017/conference/-/paper29/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper29/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper29/AnonReviewer1", "ICLR.cc/2017/conference/paper29/AnonReviewer3", "ICLR.cc/2017/conference/paper29/AnonReviewer2"], "reply": {"forum": "S1RP6GLle", "replyto": "S1RP6GLle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper29/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper29/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512722550}}}, {"tddate": null, "tmdate": 1482162951336, "tcdate": 1482162951336, "number": 3, "id": "rJkGXFB4l", "invitation": "ICLR.cc/2017/conference/-/paper29/public/comment", "forum": "S1RP6GLle", "replyto": "BJAFIxfNl", "signatures": ["~Casper_Kaae_S\u00f8nderby1"], "readers": ["everyone"], "writers": ["~Casper_Kaae_S\u00f8nderby1"], "content": {"title": "Response to AnonReviewer1", "comment": "Thanks for the review!\n\n> It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help.\nWe attempted to strike a balance between accessibility of the presentation and technical correctness. However we might have missed this balance in some places. If you could point us to any specific equations/paragraphs/concepts that are unclear we would be very happy to try to clarify these?\n\n> I would love to see some more analysis of the resulting the networks - what kind of features to they learn\nWe consider the main contributions of the paper is the proposed methods for MAP inference in image models (the GAN, Likelihood-based and Denoising Autoencoders guided. Hence we spend the main part of the text deriving these models and qualitatively comparing the results to highlight the relative merits of each approach. We agree that investigating the behaviour of the filters would be useful especially if we used the networks as a self-supervised representation learning, but this was not our goal and we believe that this analysis is outside the scope of the current work.\n\nOur actual superresolution networks are relatively simple, and we do not expect them to learn anything beyond recognising textured regions and generating finer resolution texture. Thus we don't expect these features to be very conceptually meaningful for humans to look at and they would be hard to visualise and explain. The more interesting form of representation learning - if any - happens in the methods which we use to represent the image prior (the GAN, pixel CNN, denoising function), our main contribution here is exploiting these learnt representations of natural image statistics in a novel and theoretically grounded way for image superresolution and low-level vision tasks in general."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287758516, "id": "ICLR.cc/2017/conference/-/paper29/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1RP6GLle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper29/reviewers", "ICLR.cc/2017/conference/paper29/areachairs"], "cdate": 1485287758516}}}, {"tddate": null, "tmdate": 1481930374185, "tcdate": 1481930374185, "number": 1, "id": "BJAFIxfNl", "invitation": "ICLR.cc/2017/conference/-/paper29/official/review", "forum": "S1RP6GLle", "replyto": "S1RP6GLle", "signatures": ["ICLR.cc/2017/conference/paper29/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper29/AnonReviewer1"], "content": {"title": "Interesting paper", "rating": "7: Good paper, accept", "review": "The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.\nResults are nicely demonstrated on several datasets.\n\nI like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? ", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512722550, "id": "ICLR.cc/2017/conference/-/paper29/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper29/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper29/AnonReviewer1", "ICLR.cc/2017/conference/paper29/AnonReviewer3", "ICLR.cc/2017/conference/paper29/AnonReviewer2"], "reply": {"forum": "S1RP6GLle", "replyto": "S1RP6GLle", "writers": {"values-regex": "ICLR.cc/2017/conference/paper29/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper29/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512722550}}}, {"tddate": null, "tmdate": 1481567484768, "tcdate": 1481567467373, "number": 2, "id": "B1XxaD2Xe", "invitation": "ICLR.cc/2017/conference/-/paper29/public/comment", "forum": "S1RP6GLle", "replyto": "rJwOsOCee", "signatures": ["~Ferenc_Huszar1"], "readers": ["everyone"], "writers": ["~Ferenc_Huszar1"], "content": {"title": "re: instance noise in Salimans et al. (2016)", "comment": "Thanks to Xun Huang for pointing out that the instance noise trick which we present in Appendix C has been implemented by Salimans et al's.\n\nThis paper is already cited here, but we will also include a note to point this out as appropriate. Based on feedback we received at NIPS, especially the Adversarial Training workshop, we believe that several other researchers have also experimented with a variant of this trick with more or less success - although the theoretical justification for the trick we believe is novel and that is what motivated our use of it. Instance noise is one contribution in this submission, but it is not the main contribution. This submission focusses on the problem of approximate inference in image superresolution, and as such we also do not present any results on completely unsupervised GANs.\n\nWe also would like to point out that there is a parallel ICLR submission by Arjovsky and Buttou who dedicated the whole paper to the very issue we analyse in Appendix C. Arjovsky and Buttou present the same high-level arguments and arrive independently at the same conclusions and solution as we did, That paper analyses the problem and the solution in a more mathematically rigorous fashion: https://openreview.net/pdf?id=Hk4_qw5x"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287758516, "id": "ICLR.cc/2017/conference/-/paper29/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1RP6GLle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper29/reviewers", "ICLR.cc/2017/conference/paper29/areachairs"], "cdate": 1485287758516}}}, {"tddate": null, "tmdate": 1478556600841, "tcdate": 1478556526752, "number": 1, "id": "rJwOsOCee", "invitation": "ICLR.cc/2017/conference/-/paper29/public/comment", "forum": "S1RP6GLle", "replyto": "S1RP6GLle", "signatures": ["~Xun_Huang1"], "readers": ["everyone"], "writers": ["~Xun_Huang1"], "content": {"title": "Instance noise", "comment": "Improved GAN (Salimans et al., 2016) also adds gaussian noise to inputs of discriminators in their implementation (https://github.com/openai/improved-gan), although it's very vague in their paper. This paper provides more theoretical understanding about why instance noise could work, which I think is a valid contribution. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "pdf": "/pdf/62cf51f7f8bb2bfb79c850cc2584cb044e7f6ad3.pdf", "TL;DR": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "paperhash": "s\u00f8nderby|amortised_map_inference_for_image_superresolution", "keywords": ["Theory", "Computer vision", "Deep learning"], "conflicts": ["twitter.com", "ku.dk", "dtu.dk"], "authors": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "authorids": ["casperkaae@gmail.com", "jcaballero@twitter.com", "ltheis@twitter.com", "wshi@twitter.com", "fhuszar@twitter.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287758516, "id": "ICLR.cc/2017/conference/-/paper29/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1RP6GLle", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper29/reviewers", "ICLR.cc/2017/conference/paper29/areachairs"], "cdate": 1485287758516}}}], "count": 12}