{"notes": [{"id": "SkxXwo0qYm", "original": "B1e7uMw5Ym", "number": 249, "cdate": 1538087770958, "ddate": null, "tcdate": 1538087770958, "tmdate": 1545355401999, "tddate": null, "forum": "SkxXwo0qYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs", "abstract": "Organizing the same operations in the computation graph of a neural network into batches is one of the important methods to improve the speed of training deep learning models and applications since it helps to execute operations with the same type in parallel and to make full use of the available hardware resources. This batching task is usually done by the developers manually and it becomes more dif- ficult when the neural networks have dynamic computation graphs because of the input data with varying structures or the dynamic flow control. Several automatic batching strategies were proposed and integrated into some deep learning toolkits so that the programmers don\u2019t have to be responsible for this task. These strategies, however, will miss some important opportunities to group the operations in the backward propagation of training neural networks. In this paper, we proposed a strategy which provides more efficient automatic batching and brings benefits to the memory access in the backward propagation. We also test our strategy on a variety of benchmarks with dynamic computation graphs. The result shows that it really brings further improvements in the training speed when our strategy is working with the existing automatic strategies.", "keywords": ["Automatic Operation Batching", "Dynamic Computation Graphs"], "authorids": ["qiao@eidos.ic.i.u-tokyo.ac.jp", "tau@eidos.ic.i.u-tokyo.ac.jp"], "authors": ["Yuchen Qiao", "Kenjiro Taura"], "pdf": "/pdf/e2173c84427a5c58f8524abfaf3eb3c186a8c15e.pdf", "paperhash": "qiao|an_automatic_operation_batching_strategy_for_the_backward_propagation_of_neural_networks_having_dynamic_computation_graphs", "_bibtex": "@misc{\nqiao2019an,\ntitle={An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs},\nauthor={Yuchen Qiao and Kenjiro Taura},\nyear={2019},\nurl={https://openreview.net/forum?id=SkxXwo0qYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1gQFLQeg4", "original": null, "number": 1, "cdate": 1544726138809, "ddate": null, "tcdate": 1544726138809, "tmdate": 1545354510862, "tddate": null, "forum": "SkxXwo0qYm", "replyto": "SkxXwo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper249/Meta_Review", "content": {"metareview": "This paper describes a new batching strategy for more efficient training of deep neural nets. The idea stems from the observation that some operations can only be batched more efficiently in the backward, suggesting that batching should be different between forward and backward. The results show that the proposed method improves upon existing batch strategies across three tasks. The reviewers find the work novel, but note that it does not properly address the trade-offs made by the technique - such as memory consumption. They also argue that the writing should be improved before acceptance at ICLR.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper249/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper249/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs", "abstract": "Organizing the same operations in the computation graph of a neural network into batches is one of the important methods to improve the speed of training deep learning models and applications since it helps to execute operations with the same type in parallel and to make full use of the available hardware resources. This batching task is usually done by the developers manually and it becomes more dif- ficult when the neural networks have dynamic computation graphs because of the input data with varying structures or the dynamic flow control. Several automatic batching strategies were proposed and integrated into some deep learning toolkits so that the programmers don\u2019t have to be responsible for this task. These strategies, however, will miss some important opportunities to group the operations in the backward propagation of training neural networks. In this paper, we proposed a strategy which provides more efficient automatic batching and brings benefits to the memory access in the backward propagation. We also test our strategy on a variety of benchmarks with dynamic computation graphs. The result shows that it really brings further improvements in the training speed when our strategy is working with the existing automatic strategies.", "keywords": ["Automatic Operation Batching", "Dynamic Computation Graphs"], "authorids": ["qiao@eidos.ic.i.u-tokyo.ac.jp", "tau@eidos.ic.i.u-tokyo.ac.jp"], "authors": ["Yuchen Qiao", "Kenjiro Taura"], "pdf": "/pdf/e2173c84427a5c58f8524abfaf3eb3c186a8c15e.pdf", "paperhash": "qiao|an_automatic_operation_batching_strategy_for_the_backward_propagation_of_neural_networks_having_dynamic_computation_graphs", "_bibtex": "@misc{\nqiao2019an,\ntitle={An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs},\nauthor={Yuchen Qiao and Kenjiro Taura},\nyear={2019},\nurl={https://openreview.net/forum?id=SkxXwo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper249/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353282504, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkxXwo0qYm", "replyto": "SkxXwo0qYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper249/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper249/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper249/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353282504}}}, {"id": "ByejE1n-kN", "original": null, "number": 4, "cdate": 1543778098996, "ddate": null, "tcdate": 1543778098996, "tmdate": 1543778098996, "tddate": null, "forum": "SkxXwo0qYm", "replyto": "S1ehRyJ5AX", "invitation": "ICLR.cc/2019/Conference/-/Paper249/Official_Comment", "content": {"title": "Rebuttal response", "comment": "Thanks to the authors for their rebuttal.\n\nThe authors seem to appreciate that their method can require additional memory. However, I am not convinced by the back-of-the-envelope calculations that show that it is \"only\" 30/130 MB. This calculation doesn't generalize to other models and settings, e.g., it's perfectly possible to have LSTMs with thousands of hidden units operating on input sequences with thousands of elements. In these sorts of situations it would be important to have a way to trade-off memory and runtime complexity, or at least acknowledge that the proposed batching method might not apply.\n\nAlthough it is useful to adapt similar hyperparameters as other papers for comparison, I don't believe there is a need to limit experiments based on what has been done in the past. Only considering a single, small batch size for a paper that focuses on batching remains a significant shortcoming in the experiments for me."}, "signatures": ["ICLR.cc/2019/Conference/Paper249/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper249/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper249/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs", "abstract": "Organizing the same operations in the computation graph of a neural network into batches is one of the important methods to improve the speed of training deep learning models and applications since it helps to execute operations with the same type in parallel and to make full use of the available hardware resources. This batching task is usually done by the developers manually and it becomes more dif- ficult when the neural networks have dynamic computation graphs because of the input data with varying structures or the dynamic flow control. Several automatic batching strategies were proposed and integrated into some deep learning toolkits so that the programmers don\u2019t have to be responsible for this task. These strategies, however, will miss some important opportunities to group the operations in the backward propagation of training neural networks. In this paper, we proposed a strategy which provides more efficient automatic batching and brings benefits to the memory access in the backward propagation. We also test our strategy on a variety of benchmarks with dynamic computation graphs. The result shows that it really brings further improvements in the training speed when our strategy is working with the existing automatic strategies.", "keywords": ["Automatic Operation Batching", "Dynamic Computation Graphs"], "authorids": ["qiao@eidos.ic.i.u-tokyo.ac.jp", "tau@eidos.ic.i.u-tokyo.ac.jp"], "authors": ["Yuchen Qiao", "Kenjiro Taura"], "pdf": "/pdf/e2173c84427a5c58f8524abfaf3eb3c186a8c15e.pdf", "paperhash": "qiao|an_automatic_operation_batching_strategy_for_the_backward_propagation_of_neural_networks_having_dynamic_computation_graphs", "_bibtex": "@misc{\nqiao2019an,\ntitle={An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs},\nauthor={Yuchen Qiao and Kenjiro Taura},\nyear={2019},\nurl={https://openreview.net/forum?id=SkxXwo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper249/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609079, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkxXwo0qYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper249/Authors", "ICLR.cc/2019/Conference/Paper249/Reviewers", "ICLR.cc/2019/Conference/Paper249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper249/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper249/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper249/Authors|ICLR.cc/2019/Conference/Paper249/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper249/Reviewers", "ICLR.cc/2019/Conference/Paper249/Authors", "ICLR.cc/2019/Conference/Paper249/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609079}}}, {"id": "SJxssjhchm", "original": null, "number": 3, "cdate": 1541225379392, "ddate": null, "tcdate": 1541225379392, "tmdate": 1541534158805, "tddate": null, "forum": "SkxXwo0qYm", "replyto": "SkxXwo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper249/Official_Review", "content": {"title": "Batching for back propagation", "review": "Batching of similar and independent operations in a neural network computation graph is a common way to improve efficiency through computational parallelism. Optimization is often applied to the computation graph by grouping independent operations into batches that can be computed in parallel. \nExisting techniques typically optimizes the feed forward computation and the backward computation follows the same grouping as the feed forward computation, which may not be optimal. In particular, the authors argue that a separate batching strategy should be applied to the back propagation computation to further improve the efficiency and showed that a recurrent network can benefit from such an optimization. The proposed solution is an automatic batching strategy that work for dynamic computation graphs.\n\nPros:\n\n- The results (for both CPU and GPU) show that the proposed method improves on top of two existing batching strategies (by depth and by agenda) across three different tasks. \n\nCons:\n\n- The feed forward and backward (gradient) computation can be viewed as a single computation graph. As such, would applying optimization to this graph achieve the same thing? Some clarifications/discussions will be helpful.\n\n- It is unclear why the proposed method is better for dynamic computation graph.  The benchmark results on CPU show that the proposed method is worse than the baseline for Tree-LSTM.  Will be useful to also do a similar profiling for the cases where the proposed method did not help.\n\n   \n  ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper249/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs", "abstract": "Organizing the same operations in the computation graph of a neural network into batches is one of the important methods to improve the speed of training deep learning models and applications since it helps to execute operations with the same type in parallel and to make full use of the available hardware resources. This batching task is usually done by the developers manually and it becomes more dif- ficult when the neural networks have dynamic computation graphs because of the input data with varying structures or the dynamic flow control. Several automatic batching strategies were proposed and integrated into some deep learning toolkits so that the programmers don\u2019t have to be responsible for this task. These strategies, however, will miss some important opportunities to group the operations in the backward propagation of training neural networks. In this paper, we proposed a strategy which provides more efficient automatic batching and brings benefits to the memory access in the backward propagation. We also test our strategy on a variety of benchmarks with dynamic computation graphs. The result shows that it really brings further improvements in the training speed when our strategy is working with the existing automatic strategies.", "keywords": ["Automatic Operation Batching", "Dynamic Computation Graphs"], "authorids": ["qiao@eidos.ic.i.u-tokyo.ac.jp", "tau@eidos.ic.i.u-tokyo.ac.jp"], "authors": ["Yuchen Qiao", "Kenjiro Taura"], "pdf": "/pdf/e2173c84427a5c58f8524abfaf3eb3c186a8c15e.pdf", "paperhash": "qiao|an_automatic_operation_batching_strategy_for_the_backward_propagation_of_neural_networks_having_dynamic_computation_graphs", "_bibtex": "@misc{\nqiao2019an,\ntitle={An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs},\nauthor={Yuchen Qiao and Kenjiro Taura},\nyear={2019},\nurl={https://openreview.net/forum?id=SkxXwo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper249/Official_Review", "cdate": 1542234505006, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkxXwo0qYm", "replyto": "SkxXwo0qYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper249/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335681938, "tmdate": 1552335681938, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper249/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hyghugeq3Q", "original": null, "number": 2, "cdate": 1541173364224, "ddate": null, "tcdate": 1541173364224, "tmdate": 1541534158602, "tddate": null, "forum": "SkxXwo0qYm", "replyto": "SkxXwo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper249/Official_Review", "content": {"title": "Straightforward method, need more analysis.", "review": "This paper proposed a just-in-time optimization method of neural network calculation on dynamic computation graphs. The method focused on batching summation of gradients on the backward calculation which was performed independently in conventional toolkits, and experiments on 3 LSTM tasks showed that in several settings the proposed method improved the speed of backward computation.\n\nThe proposed method is straightforward and reasonable in terms of improving the speed of the backward computation. Authors discussed the proposed method on only the neural network toolkits with a dynamic computation strategy, but this kind of optimization can be applied to any existing toolkits even which has a non-dynamic strategy. This point looks a kind of misleading of the discussion on the paper.\n\nThe paper provided a detailed analysis of time consumption on only a success-case (Table 4). Unfortunately, Table 2 and 3 showed that the proposed method does not have a global effectiveness and suggest a necessity for a further discussion about when to use the proposed method. Since this discussion can surely be conducted by comparing analyses of success and failure-cases, authors should provide analyses of all experiments.\n\nA conceivable weakness of the method may be the increase of memory consumption. If the toolkit plan to perform batch operations for summations of gradients, it needs to store all available gradients about each use of the corresponding variables. If the variable has a large shape and is used very frequently (e.g., variables in the softmax layer), the amount of total memory consumed by its gradient tends to be a serious problem. The non-batching strategy can mitigate this problem by discarding gradient information as soon as it is propagated back to all preceding nodes. The paper does not provide any information about memory consumption but it is important to discuss this kind of perspective.\n\nOthers:\nIn Table 2 and 3, please provide the ratio of speeds which are more reasonable to judge the real improvement rather than the one-zero decision (showed as up/down arrows).\nIn Table 4, why the time of the forward propagation slightly increased?\nYou should write a full list of authors of the DyNet paper that the official README provided:\nhttps://github.com/clab/dynet/blob/master/README.md", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper249/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs", "abstract": "Organizing the same operations in the computation graph of a neural network into batches is one of the important methods to improve the speed of training deep learning models and applications since it helps to execute operations with the same type in parallel and to make full use of the available hardware resources. This batching task is usually done by the developers manually and it becomes more dif- ficult when the neural networks have dynamic computation graphs because of the input data with varying structures or the dynamic flow control. Several automatic batching strategies were proposed and integrated into some deep learning toolkits so that the programmers don\u2019t have to be responsible for this task. These strategies, however, will miss some important opportunities to group the operations in the backward propagation of training neural networks. In this paper, we proposed a strategy which provides more efficient automatic batching and brings benefits to the memory access in the backward propagation. We also test our strategy on a variety of benchmarks with dynamic computation graphs. The result shows that it really brings further improvements in the training speed when our strategy is working with the existing automatic strategies.", "keywords": ["Automatic Operation Batching", "Dynamic Computation Graphs"], "authorids": ["qiao@eidos.ic.i.u-tokyo.ac.jp", "tau@eidos.ic.i.u-tokyo.ac.jp"], "authors": ["Yuchen Qiao", "Kenjiro Taura"], "pdf": "/pdf/e2173c84427a5c58f8524abfaf3eb3c186a8c15e.pdf", "paperhash": "qiao|an_automatic_operation_batching_strategy_for_the_backward_propagation_of_neural_networks_having_dynamic_computation_graphs", "_bibtex": "@misc{\nqiao2019an,\ntitle={An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs},\nauthor={Yuchen Qiao and Kenjiro Taura},\nyear={2019},\nurl={https://openreview.net/forum?id=SkxXwo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper249/Official_Review", "cdate": 1542234505006, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkxXwo0qYm", "replyto": "SkxXwo0qYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper249/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335681938, "tmdate": 1552335681938, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper249/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxJTFhd2X", "original": null, "number": 1, "cdate": 1541093815123, "ddate": null, "tcdate": 1541093815123, "tmdate": 1541534158388, "tddate": null, "forum": "SkxXwo0qYm", "replyto": "SkxXwo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper249/Official_Review", "content": {"title": "Nice idea, but a limited study presented with lack of clarity", "review": "# Summary\n\nThis work describes a shortcoming in existing dynamic batching strategies, namely that they operate only on the forward pass while some operations can be batched only in the backward pass. For example, the gradient of the transition matrix in a RNN consists of the sum of partial derivatives over each time step; the terms of this sum and the summation can be batched into a single matrix multiplication. The authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups.\n\n# Quality\n\nThe proposed technique comes with a trade-off which is not discussed in the paper: Delaying computations until several can be batched together can increase peak memory usage. In particular, the memory requirements of a RNN would increase from O(T) to O(2T) since each forward and backward state must now be stored. (In fact, the authors use a separately allocated contiguous block of memory that they copy the states and gradients into, which would bring this to O(3T) or O(4T) memory complexity.)\n\nA second observation that should have been made is that the potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth). Given a small batch size and large hidden state, the batching method effectively replaces a series of outer products with a single matrix multiplication. One would expect good speedups in this scenario. On the other hand, for a large batch size with a small hidden state, the dynamic batching strategy effectively replaces a series of inner products with a single larger inner product, which would be far less beneficial. The experiments in this work use relatively small batch sizes (64), which gives little insight about whether the proposed method would lead to speedups in a wide range of models (for example, batches of 512 are common in some NLP applications).\n\nSome smaller comments:\n\n* Multi-threading on a multicore architecture does not necessarily imply that operations are performed sequentially.\n* Input sequences in NLP are not always sentences given as sequences of words.\n* The argument that padding always leads to unnecessary computation is overly simplistic; the added control flow and branching required to perform irregular computation can often make it slower than doing regular computation plus masking (additionally, sparse kernels are often memory bandwidth bound, leading to different performance properties).\n* The authors say that operations of the same \"type\" can be batched together, but don't specify what \"type\" means. I assume the type is defined by both the operation as well as the shapes of its inputs and outputs?\n* No distinction is made between different ways of batching and their performance characteristics. Two matrix-vector multiplications gemv(X, y1) and gemv(X, y2) can be efficiently batched as gemm(X, [y1 y2]') which reduces the number of times X needs to be loaded into working memory. This is not the case when batching distinct inputs such as gemv(X1, y1) and gemv(X2, y2). On the other hand, gemv(X1, y1) + gemv(X2, y2) can be efficiently batched as gemv([X1 X2], [y1', y2']'), reducing the number of memory accesses in the output buffer.\n* Why perform 3 runs and report the fastest speed? Why not report the range, or better yet, perform more runs and report confidence intervals.\n\n# Clarity\n\nThe writing in this paper needs significant improvement. In terms of structure, the introduction (section 1) and background (section 2) are very repetitive. The third, fourth, fifth and sixth paragraph of the introduction are effectively repeated in full in sections 2.1, 2.2, 2.3 and 3.1 respectively. On the other hand, the inclusion of table 1 at the beginning puts the reader on the wrong foot thinking that this paper will consider NMT models, whereas the paper only deals with POS tagging and sentiment analysis.\n\nThe text contains grammatical errors (\"days even weeks\", \"The parallel computing helps\"), tautological definitions (\"batching [...] means organizing the same operations of computation graphs into batches\", \"padding, which is to pad the input sequences\"), unclear use of language (\"cooperating with the existing strategies\"), and typographical mistakes (multiple citations are separately parenthesized). Overall, the lack of clarity inhibits the understanding of the paper.\n\n# Originality and significance\n\nThe central contribution of this paper is relatively straightforward in retrospect, but can certainly be beneficial for the training of some particular models. I am no expert in the literature, but the authors' claim that they are the first ones to consider this technique seems justified. The paper has no reference to code, so it is hard to judge how easy it would be for practitioners to use the suggested technique.\n\n# Summary\n\nPros:\n\n  * Useful dynamic batching trick that can lead to speedups\n  * Empirical evaluation compares to two existing techniques and breaks down individual components of runtime\n\nCons:\n\n  * No critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage\n  * Some questionable statements and assumptions\n  * Lack of formalization and clear definitions\n  * Paper reads long-drawn-out, subpar writing hurts readability", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper249/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs", "abstract": "Organizing the same operations in the computation graph of a neural network into batches is one of the important methods to improve the speed of training deep learning models and applications since it helps to execute operations with the same type in parallel and to make full use of the available hardware resources. This batching task is usually done by the developers manually and it becomes more dif- ficult when the neural networks have dynamic computation graphs because of the input data with varying structures or the dynamic flow control. Several automatic batching strategies were proposed and integrated into some deep learning toolkits so that the programmers don\u2019t have to be responsible for this task. These strategies, however, will miss some important opportunities to group the operations in the backward propagation of training neural networks. In this paper, we proposed a strategy which provides more efficient automatic batching and brings benefits to the memory access in the backward propagation. We also test our strategy on a variety of benchmarks with dynamic computation graphs. The result shows that it really brings further improvements in the training speed when our strategy is working with the existing automatic strategies.", "keywords": ["Automatic Operation Batching", "Dynamic Computation Graphs"], "authorids": ["qiao@eidos.ic.i.u-tokyo.ac.jp", "tau@eidos.ic.i.u-tokyo.ac.jp"], "authors": ["Yuchen Qiao", "Kenjiro Taura"], "pdf": "/pdf/e2173c84427a5c58f8524abfaf3eb3c186a8c15e.pdf", "paperhash": "qiao|an_automatic_operation_batching_strategy_for_the_backward_propagation_of_neural_networks_having_dynamic_computation_graphs", "_bibtex": "@misc{\nqiao2019an,\ntitle={An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs},\nauthor={Yuchen Qiao and Kenjiro Taura},\nyear={2019},\nurl={https://openreview.net/forum?id=SkxXwo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper249/Official_Review", "cdate": 1542234505006, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkxXwo0qYm", "replyto": "SkxXwo0qYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper249/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335681938, "tmdate": 1552335681938, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper249/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}