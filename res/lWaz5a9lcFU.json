{"notes": [{"id": "lWaz5a9lcFU", "original": "0-mDHt0Fmz", "number": 67, "cdate": 1601308016706, "ddate": null, "tcdate": 1601308016706, "tmdate": 1616036555176, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QJfBpArU6w2", "original": null, "number": 1, "cdate": 1610992700510, "ddate": null, "tcdate": 1610992700510, "tmdate": 1611024544615, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "invitation": "ICLR.cc/2021/Conference/Paper67/-/Comment", "content": {"title": "Some Related Works missing", "comment": "Hi, \n\nCongratulations on your paper acceptance! \n\nI would like to point out two papers which I think are most relevant to your proposed approach \n\n(1) Scalable Recollections for Continual Lifelong Learning (AAAI 2019), where the authors trains an autoencoder with binary latent representations to reduce the storage requirements for experience replay https://ojs.aaai.org//index.php/AAAI/article/view/3935\n\n(2) Online Learned Continual Compression with Adaptive Quantization Modules (ICML 2020). Disclaimer, I am an author of the paper . We show that training vector quantized autoencoder avoids the blurriness issues typical to VAEs, and that we can perform memory efficient replay for continual learning with little degradation in image quality. (http://proceedings.mlr.press/v119/caccia20a.html)\n\nPlease consider adding these works in the list or prior work. \n\nThank you\n\n"}, "signatures": ["~Lucas_Caccia1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Lucas_Caccia1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "tags": [], "invitation": {"reply": {"forum": "lWaz5a9lcFU", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper67/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper67/Authors|ICLR.cc/2021/Conference/Paper67/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649467582, "tmdate": 1610649467582, "id": "ICLR.cc/2021/Conference/Paper67/-/Comment"}}}, {"id": "Xd0PVswZD_g", "original": null, "number": 1, "cdate": 1610040506691, "ddate": null, "tcdate": 1610040506691, "tmdate": 1610474113929, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "invitation": "ICLR.cc/2021/Conference/Paper67/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper uses an autoencoder with neural style transfer to generate images from previously seen classes to avoid catastrophic forgetting in continual learning.\n\nWhile reviewers had some concerns about the paper (experiments on high-resolution images, comparison with FearNet), authors have addressed all the concerns. R1's concern about the motivation for generation instead of replaying actual images is not necessary since this is not the first work to use generative replay."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "tags": [], "invitation": {"reply": {"forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040506678, "tmdate": 1610474113913, "id": "ICLR.cc/2021/Conference/Paper67/-/Decision"}}}, {"id": "stv-HRT9-s", "original": null, "number": 2, "cdate": 1603826250262, "ddate": null, "tcdate": 1603826250262, "tmdate": 1606816615644, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "invitation": "ICLR.cc/2021/Conference/Paper67/-/Official_Review", "content": {"title": "Good Paper", "review": "##########################################################################\n\nSummary:\n\n \nThe paper proposes an approach which trains autoencoders with Neural Style Transfer to encode and store images. The method is applied to the problem of continual learning to overcome the catastrophic forgetting and memory limitation on the storage data. The authors report that the presented approach increases the classification accuracy by 13-17% over SOTA methods \n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for accepting. The paper presents nice ideas, outperforms SOTA methods and is clearly written. The contribution could have been stronger with a more detailed evaluation and better presentation.\n \n##########################################################################\n\nPros: \n\n \n1. I really like the idea of using NST to train the autoencoder. \n\n2. Clarity of the paper.\n \n \n##########################################################################\n\nCons: \n\n1. The system uses a pretraied Style Transfer Network with Imagenet. Does it offer an unfair advantage over other approaches?\n2. It would be great to compare the NST Autoencoder with Variational or Denoising Autoencoder. \n3. The validation has been performed using low-resolution images (32x32)\n4. The improvement on Imagnet-50 A5 against iCaRL-S is 6.28. So, the difference should be 6.28 and not 17.4. \n5. It would be great to see the results for A30 and A50 with imagenet as it was done by (Ostapenko et.al. 2019)\n6. Figure 4 shows the accuracy on ImageNet-50 with different budgets. It seems that the accuracy is still increasing, did you tried with larger values?\n\nMinor points: \n- Table 1 - it would be great to include the reference paper of each method\n\n\n \n##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n \n#########################################################################\n\nSome typos: \n\n(1) Page 5: corariance -> covariance\n(2) Page 7: the the accuracy --> the accuracy\n\n\nUpdate after rebuttal:\nMy initial concerns were clarified by the authors and the paper substantially improved. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper67/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper67/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150941, "tmdate": 1606915801962, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper67/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper67/-/Official_Review"}}}, {"id": "MLRHMurlzr", "original": null, "number": 4, "cdate": 1605384394516, "ddate": null, "tcdate": 1605384394516, "tmdate": 1605384394516, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": "stv-HRT9-s", "invitation": "ICLR.cc/2021/Conference/Paper67/-/Official_Comment", "content": {"title": "Cons and typos addressed in the paper - More experiments added with variational and denoising autoencoder", "comment": "We thank you for your insightful comments and have used these comments to improve the paper.\n\nCons:\n1) We believe there is a misunderstanding regarding our approach. Our system does not use a network pre-trained on ImageNet as Style Transfer Network. The classifier network in our approach is first trained on the original images of the current increment and reconstructed images of the old classes (after the first increment). This network is then used as a fixed feature extractor during autoencoder training using the content loss. \n2) Thank you for your suggestion. We have added the comparison with variational and denoising autoencoders in the ablation study (Section 4.4). The results demonstrate that our NST based autoencoder significantly outperforms the variational and denoising autoencoders.\n3) The only reason to perform our experiments with low-resolution images (32$\\times$32) was to have a fair comparison with previous approaches (Wu et al. 2018a, Ostapenko et al. 2019). We have added a new experiment in the paper (Section 4.5) on ImageNet-50 dataset with higher resolution images (224$\\times$224), which further shows the effectiveness of EEC in continual learning.\n4) We have updated the difference value in the paper to 6.26. \n5) The results shown in the paper are for $A_{30}$ and $A_{50}$. We showed them as $A_3$ and $A_5$ to show 3 and 5 tasks instead of 30 and 50 classes learned. We have fixed this in the paper. \n6) The largest value for the memory budget can be 65000 encoded episodes because ImageNet-50 contains 65000 images. The value reported in Table 1 for EEC uses the maximum budget. However, another way to increase the memory budget is to allow the model to store a small set of original images and the rest as encoded episodes. We have reported an experiment on ImageNet-50 dataset with different ratios of original images (Appendix F), which shows that the accuracy for EEC does continue to increase when using higher memory budgets.  \n\nMinor Points:\nThe references for the papers are added in Table 1.\n\nTypos: \nFixed in the paper.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper67/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lWaz5a9lcFU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper67/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper67/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper67/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper67/Authors|ICLR.cc/2021/Conference/Paper67/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874866, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper67/-/Official_Comment"}}}, {"id": "lGqYrEhaz0h", "original": null, "number": 3, "cdate": 1605382610920, "ddate": null, "tcdate": 1605382610920, "tmdate": 1605382610920, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": "pzU1PTuPOvX", "invitation": "ICLR.cc/2021/Conference/Paper67/-/Official_Comment", "content": {"title": "Concerns addressed - Experiment on high resolution images added", "comment": "We thank you for your helpful comments, in particular to test our approach on datasets with high resolution. The new experiment has shown further insights into the effectiveness of our approach for continual learning.\n\nConcerns:\n\n1) it seems obvious improving image quality will result in better performance for methods based on replay: \nWe agree that improving image quality is an obvious way to improve performance. However, improving image quality is not a simple task, especially when using limited memory. As shown in the results from earlier papers (Shin et al. 2017, Wu et al. 2018a, Ostapenko et al. 2019), the regenerated images are not even close to the original images, hence the overall accuracy decreases drastically, especially for ImageNet-50 even with images of size 32$\\times$32. Thus, in this paper, we propose the NST based autoencoder to improve image quality while utilizing lower memory. Further, note that the reconstructed images cannot be the same as original images, especially when using limited memory. Thus, we introduce weight decay and image filtering techniques (Section 3.3) to deal with image degradation during reconstruction so that degraded images do not hurt classifier performance. \nFearNet does not use any of these ideas. FearNet simply uses a network pre-trained on ImageNet to extract features, which is a limitation since it can only be applied on object-centric image datasets. Further, using the pre-trained feature extractor gives FearNet an unfair advantage over other approaches. The only similarity between EEC and FearNet, other than the use of autoencoders, is that they both use pseudorehearsal. However, FearNet only uses a single centroid and covariance matrix to represent the feature space of a class. In contrast we use memory integration-based clustering approach to better cover the complete feature space. We have added a new experiment in Appendix E to compare pseudorehearsal in EEC and FearNet. This experiment demonstrates that unlike FearNet, EEC captures the overall concept of the original feature space. Experiments on CIFAR-100 (Appendix D) show that EEC outperforms FearNet by a margin of 10.5\\%, which shows the effectiveness of our approach in comparison with FearNet.\n2)  It would be interesting to see the performance on more challenging datasets with large resolution and more tasks:\nThe only reason to use the four datasets (with images of size 32$\\times$32) in the paper was to have a fair comparison with other generative memory based approaches. However, to have a comparison with more episodic memory based approaches on a larger dataset with more classes, we added an experiment on CIFAR-100 in Appendix D. These results show that EEC outperforms other SOTA approaches even on a larger dataset. \nTo test EEC on images with higher resolution, we have added a new experiment in the paper (Section 4.5). We performed the experiment on ImageNet-50 dataset with images of size 224$\\times$224. The results show that EEC produces significantly higher accuracy when using higher resolution images than when using lower resolution images. Further, the memory required for EEC remains the same regardless of the image resolution, which demonstrates the effectiveness of EEC to mitigate catastrophic forgetting while using significantly less memory that other approaches."}, "signatures": ["ICLR.cc/2021/Conference/Paper67/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lWaz5a9lcFU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper67/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper67/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper67/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper67/Authors|ICLR.cc/2021/Conference/Paper67/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874866, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper67/-/Official_Comment"}}}, {"id": "QtasK2zWE1C", "original": null, "number": 2, "cdate": 1605380328010, "ddate": null, "tcdate": 1605380328010, "tmdate": 1605380328010, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": "LdmjNlmeCPm", "invitation": "ICLR.cc/2021/Conference/Paper67/-/Official_Comment", "content": {"title": "Motivation and concerns addressed - More exerpiments added", "comment": "We thank you for your insightful comments and will use these comments to improve the paper.\n\nBasic Concerns:\n1) Idea of generating new samples for replay is not motivated well enough: \nAs described in previous works (Rebuffi et al. 2017, Castro et al. 2018), one of the main problems faced by continual learning systems is to learn new tasks for a long period of time while using limited memory. Our paper, similar to previous works (iCaRL, EEIL etc.), perform experiments on benchmark datasets for a proof of concept and to have a fair comparison with previous works in continual learning. Because of the limited sizes of the datasets (and some datasets with low resolution images), even storing the complete datasets require memory in few hundred MBs. However, in real-world applications, as the number of new tasks continue to increase, the memory required to store the previous data will go out of bounds, since many real-world systems have limited on-board memory which can be in the range of a few GBs. As an example, we have added a new experiment in the paper (Section 4.5), in which we perform the continual learning experiment on ImageNet-50 dataset with images of size 224$\\times$224. The total memory required to store all 65000 images belonging to only 50 classes requires 13.04 GB, which can cause a huge memory strain on the system. As the number of classes increase, this memory can easily go out of bounds. However, EEC only requires a maximum of 111.56 MB for the 50 classes (Section 4.5), which shows the significance of our approach to cope with limited memory. \n2) why is there a need for generating new samples?: \nWe agree that there are some techniques proposed to get a representative set of original samples. Some continual learning approaches, such as iCaRL and EEIL have used these techniques to store a representative set of original samples. However, as shown in the experimental results (Table 1), the approaches that store some original images of the old classes produce significantly lower accuracy compared to EEC and they start to suffer from catastrophic forgetting in the later increments. Simply storing a representative set of original images does not solve the catastrophic forgetting problem and leads to further issues like privacy and security, as argued by (Ostapenko et al. 2019) and in our paper (Section 1).\n3) Why not use conditional-GANs (CGAN) for generating data points specific to a class?: \nWe agree that using CGANs is a way to solve the problem of generating images belonging to no classes. However, the generated images are still not perfect. We have performed an experiment with CGAN on ImageNet-50 dataset and added it in the ablation study in the paper (Section 4.4). Results in Table 2 show that CGAN does not solve the problem of generating perfect images. Further, note that CGAN produces similar accuracy to other autoencoders that do not use the content loss, which shows that CGAN is not even an improvement over autoencoders without the content loss, even though these autoencoders produces blurry images (Figure 2).\n\nPseudorehearsal is not principled enough: \nPseudo-rehearsal is an intuitive and empirical technique inspired by the learning models in the brain (Robins, 1995), which has been used by previous works like FearNet (Kemker \\& Kanan 2018). Because of lack of mathematical explanation, we have provided an empirical experiment to visually show how our technique works (Figures 3 and 5). This empirical experiment (Figure 3 and 5) and experiments with limited memory in Section 4.3 show the effectiveness of pseudorehearsal. \n\nPseudorehearsal may or may not work depending upon the intrinsic dimensionality of images, and the difficulty of the task: \nNote that the dimensionality of the images does not have an effect on pseudorehearsal, since it is applied on the feature vectors produced by the autoencoders (see Section 4.5 for details). Also, we have tested our continual learning approach on the classification task with multiple datasets of varying difficulty levels and the empirical evaluations show that pseudorehearsal produced favorable results.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper67/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lWaz5a9lcFU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper67/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper67/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper67/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper67/Authors|ICLR.cc/2021/Conference/Paper67/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874866, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper67/-/Official_Comment"}}}, {"id": "pzU1PTuPOvX", "original": null, "number": 1, "cdate": 1603448741180, "ddate": null, "tcdate": 1603448741180, "tmdate": 1605024769752, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "invitation": "ICLR.cc/2021/Conference/Paper67/-/Official_Review", "content": {"title": "The idea of paper is quite similar to existing method FearNet using autoencoders. The experiments are limited to small images and  a few tasks.", "review": "Summary: \n\nThe paper tackles catastrophic forgetting for continual learning. It proposes to train autoencoders with Neural Style Transfer to generate previous images. The encoded episodics can be converted into centroids and covariances matrices in order to save memory usage. It shows significant improvements in the experimental parts.\n\nStrengths:\n\n- The results on tiny datasets (MNIST, SVHN and CIFAR-10) are promising. On Tiny ImageNet-50, the performance compared to other methods looks good, but it is relatively bad compared to some other datasets.\n- It is well written and easy to follow.\n\n\nConcerns:\n\n- The overall idea is very close to what FearNet proposed by using autoencoders. The main difference is to use autoencoders on images instead of features.  Neural Style Transfer is helpful to generate more realistic images, therefore it makes sense to help during incremental learning to replay previous knowledge. However, it seems obvious improving image quality will result in better performance for methods based on replay.\n\n- From the experimental results compared to other methods, it is still limited to relative small resolution images and a few tasks. It would be interesting to see the performance on more challenging datasets with large resolution and more tasks.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper67/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper67/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150941, "tmdate": 1606915801962, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper67/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper67/-/Official_Review"}}}, {"id": "LdmjNlmeCPm", "original": null, "number": 3, "cdate": 1604406129524, "ddate": null, "tcdate": 1604406129524, "tmdate": 1605024769691, "tddate": null, "forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "invitation": "ICLR.cc/2021/Conference/Paper67/-/Official_Review", "content": {"title": "Problem is not well motivated - some basic concerns - model is not principled enough ", "review": "In continual learning settings, one of the important technique for avoiding catastrophe forgetting is to replay data points from the past. For memory efficiency purposes, representative samples can be generated from a generative model, such as GANs, rather than replaying the original samples which can be large in number. It is argued that GANs generate new samples which may not belong exactly to one of the classes, so a new generative model is proposed. Experimental results are appealing.\n\nI have some basic concerns. \n(1) First of all, the idea of generating new samples for replay is not motivated well enough; in the experiments, even the model replaying on original images takes memory in few hundred MBs. \n(2) Second, why is there a need for generating new samples? Why not select a representative subset of the original set of samples. There are tons of methods to select samples informatively, within the paradigm of active learning and beyond.\n(3) Why not use conditional-GANs for generating data points specific to a class?\n\nSince autoencoders have a problem of generating blurry images, the proposed autoencoding model borrows ideas from neural style transfer algorithm. Specifically, besides the reconstruction loss, content loss is introduced utilizing the idea of content transfer from the neural style transfer algorithm. It seem to make sense in reference to Fig. 2, though it needs better explanation.   \n\nFor memory efficiency of the autoencoder itself, it is stores centroids and covariances of the episodes (representations of images), from which pseudo-encoded episodes are generated. This doesn't seem very principled, and may or may not work in different empirical settings, depending upon the intrinsic dimensionality of images, and the difficulty of the task. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper67/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper67/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "authorids": ["~Ali_Ayub1", "~Alan_Wagner2"], "authors": ["Ali Ayub", "Alan Wagner"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Cognitively-inspired Learning"], "abstract": "The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.", "one-sentence_summary": "We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.", "pdf": "/pdf/e0fae0a996116aba06f3a1e0ff97fd0078ca47d2.pdf", "supplementary_material": "/attachment/18c79aad73eb9be965c91dc65f72e5a68eb1370b.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ayub|eec_learning_to_encode_and_regenerate_images_for_continual_learning", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nayub2021eec,\ntitle={{\\{}EEC{\\}}: Learning to Encode and Regenerate Images for Continual Learning},\nauthor={Ali Ayub and Alan Wagner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lWaz5a9lcFU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "lWaz5a9lcFU", "replyto": "lWaz5a9lcFU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper67/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150941, "tmdate": 1606915801962, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper67/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper67/-/Official_Review"}}}], "count": 9}