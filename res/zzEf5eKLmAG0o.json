{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363857540000, "tcdate": 1363857540000, "number": 1, "id": "qqdsq7GUspqD2", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "zzEf5eKLmAG0o", "replyto": "UUlHmZjBOIUBb", "signatures": ["YoonSeop Kang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "1. As the switch parameters converge quickly, the training time of our model was not very different from that of DWH.\r\n2. We performed the experiment several times, but the result was consistent. Still, it is our fault that we didn't repeat the experiments enough to add error bars to the results.\r\n3. MVHs are often outperformed by DWHs unless the sizes of latent node sets are not carefully chosen, and this is one of the most important reason for introducing switch parameters. To make our motivation clear, we assigned 50% of hidden nodes as shared, and evenly assigned the rest of hidden nodes as visible nodes for view-specific nodes of each view. We didn't compare our method to CCA, because we thought DWH would be a better example of models with only a shared representation.\r\n4. We were not aware of the White et al.'s work when we submitted our work, and therefore couldn't make comparison with their model."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Features with Structure-Adapting Multi-view Exponential Family\r\n    Harmoniums", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We proposea graphical model for multi-view feature extraction that automatically adapts its structure to achieve better representation of data distribution. The proposed model, structure-adapting multi-view harmonium (SA-MVH) has switch parameters that control the connection between hidden nodes and input views, and learn the switch parameter while training. Numerical experiments on synthetic and a real-world dataset demonstrate the useful behavior of the SA-MVH, compared to existing multi-view feature extraction methods.", "pdf": "https://arxiv.org/abs/1301.3539", "paperhash": "kang|learning_features_with_structureadapting_multiview_exponential_family_harmoniums", "authors": ["YoonSeop Kang", "Seungjin Choi"], "keywords": [], "conflicts": [], "authorids": ["e0engoon@gmail.com", "seungjin.choi.mlg@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363857240000, "tcdate": 1363857240000, "number": 1, "id": "tt7CtuzeCYt5H", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "zzEf5eKLmAG0o", "replyto": "DNKnDqeVJmgPF", "signatures": ["YoonSeop Kang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "1. The distribution of sigma(s_{kj}) had modes near 0 and 1, but the graph of the distribution was omitted due to the space constraints. The amount of separation between modes were affected by the hyperparameters that were not mentioned in the paper. \r\n\r\n2. It is true that the separation between digit features and noises in our model is not perfect. But it is also true that view-specific features contain more noisy features than the shared ones. \r\nWe appreciate your suggestions about the additional experiments about de-noising digits, and we will present the result of the experiments if we get a chance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Features with Structure-Adapting Multi-view Exponential Family\r\n    Harmoniums", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We proposea graphical model for multi-view feature extraction that automatically adapts its structure to achieve better representation of data distribution. The proposed model, structure-adapting multi-view harmonium (SA-MVH) has switch parameters that control the connection between hidden nodes and input views, and learn the switch parameter while training. Numerical experiments on synthetic and a real-world dataset demonstrate the useful behavior of the SA-MVH, compared to existing multi-view feature extraction methods.", "pdf": "https://arxiv.org/abs/1301.3539", "paperhash": "kang|learning_features_with_structureadapting_multiview_exponential_family_harmoniums", "authors": ["YoonSeop Kang", "Seungjin Choi"], "keywords": [], "conflicts": [], "authorids": ["e0engoon@gmail.com", "seungjin.choi.mlg@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362353160000, "tcdate": 1362353160000, "number": 2, "id": "UUlHmZjBOIUBb", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "zzEf5eKLmAG0o", "replyto": "zzEf5eKLmAG0o", "signatures": ["anonymous reviewer d966"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Features with Structure-Adapting Multi-view Exponential Family\r\n    Harmoniums", "review": "The paper introduces an new algorithm for simultaneously learning a hidden layer (latent representation) for multiple data views as well as automatically segmenting that hidden layer into shared and view-specific nodes. It builds on the previous multi-view harmonium (MVH) algorithm by adding (sigmoidal) switch parameters that turn a connection on or off between a view and hidden node and uses gradient descent to learn those switch parameters. The optimization is similar to MVH, with a slight modification on the joint distribution between views and hidden nodes, resulting in a change in the gradients for all parameters and a new switch variable to descend on.\r\n\r\nThis new algorithm, therefore, is somewhat novel; the quality of the explanation and writing is high; and the experimental quality is reasonable.\r\n\r\nPros\r\n\r\n1. The paper is well-written and organized.\r\n\r\n2. The algorithm in the paper proposes a way to avoid hand designing shared and private (view-specific) nodes, which is an important contribution.\r\n\r\n3. The experimental results indicate some interesting properties of the algorithm, in particular demonstrating that the algorithm extracts reasonable shared and view-specific hidden nodes.\r\n\r\nCons\r\n1. The descent directions have W and the switch parameters, s_kj, coupled, which might make learning slow. Experimental results should indicate computation time.\r\n\r\n2. The results do not have error bars (in Table 1), so it is unclear if they are statistically significant (the small difference suggests that they may not be).\r\n\r\n3. The motivation in this paper is to enable learning of the private and shared representations automatically. However, DWH (only a shared representation) actually seems to perform generally better that MVH (shared and private). The experiments should better explore this question. It might also be a good idea to have a baseline comparison with CCA. \r\n\r\n4. In light of Con (3), the algorithm should also be compared to multi-view algorithms that learn only shared representations but do not require the size of the hidden-node set to be fixed (such as the recent relaxed-rank convex multi-view approach in 'Convex Multiview Subspace Learning', M. White, Y. Yu, X. Zhang and D. Schuurmans, NIPS 2012). In this case, the relaxed-rank regularizer does not fix the size of the hidden node set, but regularizes to set several hidden nodes to zero. This is similar to the approach proposed in this paper where a node is not used if the sigmoid value is < 0.5. \r\nNote that these relaxed-rank approaches do not explicitly maximize the likelihood for an exponential family distribution; instead, they allow general Bregman divergences which have been shown to have a one-to-one correspondence with exponential family distributions (see 'Clustering with Bregman divergences' A. Banerjee, S. Merugu, I. Dhillon and J. Ghosh, JMLR 2005). Therefore, by selecting a certain Bregman divergence, the approach in this paper can be compared to the relaxed-rank approaches."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Features with Structure-Adapting Multi-view Exponential Family\r\n    Harmoniums", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We proposea graphical model for multi-view feature extraction that automatically adapts its structure to achieve better representation of data distribution. The proposed model, structure-adapting multi-view harmonium (SA-MVH) has switch parameters that control the connection between hidden nodes and input views, and learn the switch parameter while training. Numerical experiments on synthetic and a real-world dataset demonstrate the useful behavior of the SA-MVH, compared to existing multi-view feature extraction methods.", "pdf": "https://arxiv.org/abs/1301.3539", "paperhash": "kang|learning_features_with_structureadapting_multiview_exponential_family_harmoniums", "authors": ["YoonSeop Kang", "Seungjin Choi"], "keywords": [], "conflicts": [], "authorids": ["e0engoon@gmail.com", "seungjin.choi.mlg@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1360866060000, "tcdate": 1360866060000, "number": 1, "id": "DNKnDqeVJmgPF", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "zzEf5eKLmAG0o", "replyto": "zzEf5eKLmAG0o", "signatures": ["anonymous reviewer 0e7e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Features with Structure-Adapting Multi-view Exponential Family\r\n    Harmoniums", "review": "The authors propose a bipartite, undirected graphical model for multiview learning, called structure-adapting multiview harmonimum (SA-MVH). The model is based on their earlier model called multiview harmonium (MVH) (Kang&Choi, 2011) where hidden units were separated into a shared set and view-specific sets. Unlike MVH which explicitly restricts edges, the visible and hidden units in the proposed SA-MVH are fully connected to each other with switch parameters s_{kj} indicating how likely the j-th hidden unit corresponds to the k-th view.\r\n\r\nIt would have been better if the distribution of s_{kj}'s (or sigma(s_{kj})) was provided. Unless the distribution has clear modes near 0 and 1, it would be difficult to tell why this approach of learning w^{(k)}_{ij} and s_{kj} separately is better than just learning \tilde{w}^{(k)}_{ij} = w^{(k)}_{ij} sigma s_{kj} all together (as in dual-wing harmonium, DWH). Though, the empirical results (experiment 2) show that the features extracted by SA-MVH outperform both MVH and DWH.\r\n\r\nThe visualizations of shared and view-specific features from the first experiment do not seem to clearly show the power of the proposed method. For instance, it's difficult to say that the filters of roman digits from the shared features do seem to have horizontal noise. It would be better to try some other tasks with the trained model. Would it be possible to sample clean digits (without horizontal or vertical noise) from the model if the view-speific features were forced off? Would it be possible to denoise the corrupted digits? and so on..\r\n\r\nTypo:\r\n\r\n- Fig. 1 (c): sigma(s_{1j}) and sigma(s_{2j})"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Features with Structure-Adapting Multi-view Exponential Family\r\n    Harmoniums", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We proposea graphical model for multi-view feature extraction that automatically adapts its structure to achieve better representation of data distribution. The proposed model, structure-adapting multi-view harmonium (SA-MVH) has switch parameters that control the connection between hidden nodes and input views, and learn the switch parameter while training. Numerical experiments on synthetic and a real-world dataset demonstrate the useful behavior of the SA-MVH, compared to existing multi-view feature extraction methods.", "pdf": "https://arxiv.org/abs/1301.3539", "paperhash": "kang|learning_features_with_structureadapting_multiview_exponential_family_harmoniums", "authors": ["YoonSeop Kang", "Seungjin Choi"], "keywords": [], "conflicts": [], "authorids": ["e0engoon@gmail.com", "seungjin.choi.mlg@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358403300000, "tcdate": 1358403300000, "number": 47, "id": "zzEf5eKLmAG0o", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "zzEf5eKLmAG0o", "signatures": ["e0engoon@gmail.com"], "readers": ["everyone"], "content": {"title": "Learning Features with Structure-Adapting Multi-view Exponential Family\r\n    Harmoniums", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We proposea graphical model for multi-view feature extraction that automatically adapts its structure to achieve better representation of data distribution. The proposed model, structure-adapting multi-view harmonium (SA-MVH) has switch parameters that control the connection between hidden nodes and input views, and learn the switch parameter while training. Numerical experiments on synthetic and a real-world dataset demonstrate the useful behavior of the SA-MVH, compared to existing multi-view feature extraction methods.", "pdf": "https://arxiv.org/abs/1301.3539", "paperhash": "kang|learning_features_with_structureadapting_multiview_exponential_family_harmoniums", "authors": ["YoonSeop Kang", "Seungjin Choi"], "keywords": [], "conflicts": [], "authorids": ["e0engoon@gmail.com", "seungjin.choi.mlg@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}