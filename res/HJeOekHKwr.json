{"notes": [{"id": "HJeOekHKwr", "original": "B1xZ8BiOwr", "number": 1512, "cdate": 1569439472250, "ddate": null, "tcdate": 1569439472250, "tmdate": 1583912052182, "tddate": null, "forum": "HJeOekHKwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "9ckjYt9BJ", "original": null, "number": 1, "cdate": 1576798725254, "ddate": null, "tcdate": 1576798725254, "tmdate": 1576800911252, "tddate": null, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper provides a theoretical study of what regularizations should be used in GAN training and why. The main focus is that the conditions on the discriminator that need to be enforced, to get the Lipshitz property of the corresponding function that is optimized for the generator. Quite a few theorems and propositions are provided. As noted by Reviewer3, this adds insight to well-known techniques: the Reviewer1 rightfully notes that this does not lead to any practical conclusion. \nMoreover, then training of GANs never goes to the optimal discriminator, that could be a weak point; rather than it proceeds in the alternating fashion, and then evolution is governed by the spectra of the local Jacobian (which is briefly mentioned). This is mentioned in future work, but it is not clear at all if the results here can be helpful (or can be generalized).\n At some point of the paper it gets to \"more theorems mode\" which make it not so easy and motivating to read. \nThe theoretical results at the quantitative level are very interesting.  I have looked for a long time on Figure 1: does this support the claims? First my impression was it does not (there are better FID scores for larger learning rates). But in the end, I think it supports: the convergence for a smaller that $\\gamma_0$ learning rate to the same FID indicated the convergence to the same local minima (probably). This is perfectly fine. Oscillations afterwards move us to a stochastic region, where FID oscillates. So, the theory has at least minor confirmation. \n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710217, "tmdate": 1576800259172, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Decision"}}}, {"id": "BJeJ4c22tH", "original": null, "number": 3, "cdate": 1571764775432, "ddate": null, "tcdate": 1571764775432, "tmdate": 1575125927663, "tddate": null, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper provides a unified theoretical framework for regularizing GAN losses. It accounts for most regularization technics especially spectral normalization and gradient penalty and explains how those two methods are in fact complementary. So far this was only observed experimentally but without any theoretical insight. The result goes beyond that as the criterion could be applied to general convex cost functional.\nThe main general theorem is Theorem 1 which states 3 conditions on the optimal critic and 2 others on the generator. The paper is mainly concerned by the conditions on the optimal critic and show that the first 2 conditions can be achieved by the Spectral normalization, while the last one can be achieved by some gradient penalty.\nThe paper is clearly written, well structured and pleasant to read.\nI have the following two remarks:\n\t- Proposition 8 provides a way to ensure condition 2 holds (beta-smoothness). It requires spectral normalization and smooth activation functions. In practice, while the spectral normalization is important, the choice of the activation is not in general 1-smooth (Leaky-relu for instance). Does it really matter in practice? \n\tSome illustrative experiments could be beneficial to better understand what's happening.\n\t- Is it that hard to obtain generators that satisfy condition G1 and G2, it seems to be a natural consequence on the regularity of the mapping f? If that is the case, it might be worth better explaining how this is challenging.\n\t\nLimitations: The paper considers only the setting where the optimal critic is reached and therefore it is still unclear if the analysis carries on to the training procedures used in practice (non-optimal critic). The authors recognize this limitation and leave it for future work.\n\nOverall, I feel that the paper provides good insights on what regularization is important for training gans and why. For that reason, I think this paper should be accepted.\n\n\n------------------------------------------------------------------------------------------------------------\nRevision:\n\nI think the paper provides a good theoretical contribution in terms of interpreting many of the tricks used for improving GAN training. In fact the paper also suggests some new regularization methods (prop 13 for conditions D3)  which would constrain the RKHS norm of the critic. The authors show how it is related to gradient penalty, in a particular case, but the result also suggests something more general. For instance [1], consider an abstract RKHS space containing deep networks and provide an upper-bound on the rkhs norm of such networks in terms of the spectral norm of their weights and a lower-bound in terms of its Lipschitz constant. \n\nI do agree with reviewer 1 that a better discussion of the connection to [2] should be included since that paper was interested in  ensuring weak continuity of the loss, which can be thought of as  a first requirement to get more regularity of the cost functional.\n\nI still think the paper is worth being accepted and raised my score to 8 as I think the authors addressed the major concerns that were raised. \n\n[1] A. Bietti, G. Mialon, D. Chen, and J. Mairal. A Kernel Perspective for Regularizing Deep Neural Networks.\n[2] Michael Arbel, Dougal Sutherland, Miko\u0142aj Binkowski, and Arthur Gretton. On gradient regularizers for MMD GANs. In Advances in Neural Information Processing Systems, pp. 6700\u20136710, 2018.\n\n\n\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1512/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1512/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575480373833, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1512/Reviewers"], "noninvitees": [], "tcdate": 1570237736294, "tmdate": 1575480373847, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Official_Review"}}}, {"id": "ryerF4z3iH", "original": null, "number": 5, "cdate": 1573819516845, "ddate": null, "tcdate": 1573819516845, "tmdate": 1573819588863, "tddate": null, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment", "content": {"title": "Response to all reviewers: Paper update", "comment": "We would like to again thank our reviewers for their valuable comments. We have updated our paper based on their feedback. The major updates are as follows:\n\n- We would like to reiterate that our main purpose is to provide firmly rooted theoretical justification for commonly used GAN regularization techniques, by means of a novel theoretical framework based on smoothness and convex duality. We have polished Sections 2 and 3 to make sure the focus and argument are clear.\n\n- A major advantage of the inf-convolution-based regularization framework is that it injects the desired regularity conditions without changing the minimizer of the original objective. We have highlighted a theoretically non-trivial result on minimizer invariance in Section 3 to emphasize this point. Please see our response to Review #1 for details."}, "signatures": ["ICLR.cc/2020/Conference/Paper1512/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeOekHKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1512/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1512/Authors|ICLR.cc/2020/Conference/Paper1512/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154913, "tmdate": 1576860545828, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment"}}}, {"id": "rkxozAqLoH", "original": null, "number": 2, "cdate": 1573461523441, "ddate": null, "tcdate": 1573461523441, "tmdate": 1573813547614, "tddate": null, "forum": "HJeOekHKwr", "replyto": "rJeTlA9LiS", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment", "content": {"title": "Response to Official Review #1 (continued)", "comment": "> Same combination of regularization techniques (gradient penalty, spectral norm and MMD loss) has been studied by [1] in various forms (Gradient-Constrained MMD, Scaled MMD). However, there is no discussion of similarities and differences between these works. \n\nArbel et al. obtain strong theoretical and empirical results using a combination of techniques that features many of the same ingredients as we derive in our analysis. Interestingly, these techniques serve different purposes in their work compared to ours. In their work, spectral normalization is used to improve the conditioning of the critic rather than to constrain its Lipschitz constant, as in our analysis. Regarding gradient penalties and MMD, in their work, gradient norms are combined with MMD (with a learned kernel) to obtain a novel discrepancy measure, whereas we show that regularizing an arbitrary loss with a Gaussian-kernel MMD leads to gradient penalties.\n\n\n> (1) End of Section 4: 'Theorem 1 also suggests that applying only Lipschitz constraints is not enough to stabilize GANs'. Theorem 1 is not 'iff', so Lipshitz constraint *may be* not enough.\n\nWe tried to be careful about the wording (\u2018suggests\u2019 rather than \u2018implies\u2019), but we will rephrase this to make it clear. We are editing the end of Section 2 to emphasize this point, that it is possible that our analysis is too conservative.\n\n\n> (2) Section 6 concludes that penalization of discriminators RKHS's norm is required. It is unclear, however, why discriminator function would belong to such space.\n\nIn Section 6, we have adopted the convention that if $f$ is not in $\\mathcal{H}$, then $|| f ||_{\\mathcal{H}}$ is infinite. This is a point we will clarify, but it is made rigorous by Lemma 6. Although the optimal discriminator of the *original* loss function may not belong to the RKHS, the optimal discriminator of the loss function regularized by $R_3$ is guaranteed to belong to the RKHS (due to Lemma 6 and equation 4 [equation 20 in the updated draft]).\n\n\n> (4) It seems there is conceptual misundersting of what MMD-GANs are in Appendix B. Authors say 'Despite their names, MMD-GANs (Li et al., 2017a; Arbel et al., 2018) typically do not directly minimize the MMD but instead an adversarial version of the MMD'. GANs by definition are adversarial, while optimization against MMD alone is not. Hence, it is *according to their names*, not 'despite'. Generator losses implied by MMD-GANs under assumption of optimal discriminators, have been termed 'Optimized MMD' [1] and studied earlier in [2].\n> (5) Given (4), The Table 2. includes MMD as a GAN loss, although authors probably refer to the properties of non-adversarial Generative Moment Matching Networks [3].\n\nThank you for bringing these points to our attention. We now realize this is confusing phrasing and will modify the text accordingly. It seems that the crux of the disagreement is that in this paper, we restrict our attention to the minimization of any convex function of a probability distribution, which is always equivalent to some adversarial game according to equation (3) [equation (4) in the updated draft]. When we consider MMD as a loss, we are indeed referring to GMMN; under our framework, GMMN is adversarial, with the optimal discriminator approximated by samples rather than by training a separate neural network. This is why we listed it in Table 1 as a \u201cGAN.\u201d We had also listed MMD-GAN alongside GMMN in Table 1 because they coincide in the special case of a single kernel, for the benefit of readers who might be unfamiliar with GMMN but have heard of MMD-GAN, but we now acknowledge this may be misleading. Regarding the \u201cdespite their names\u201d comment, we are referring to the \u201cMMD\u201d part, not the \u201cGAN\u201d part: it might be expected that MMD-GANs minimize the MMD instead of the Optimized MMD, in analogy with Wasserstein GANs, which minimize the Wasserstein distance, and f-GANs, which minimize an f-divergence.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1512/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeOekHKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1512/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1512/Authors|ICLR.cc/2020/Conference/Paper1512/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154913, "tmdate": 1576860545828, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment"}}}, {"id": "HkgEeBw9sB", "original": null, "number": 4, "cdate": 1573709035851, "ddate": null, "tcdate": 1573709035851, "tmdate": 1573813297792, "tddate": null, "forum": "HJeOekHKwr", "replyto": "BJeJ4c22tH", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your feedback. We are happy to hear that you found the paper insightful and pleasant to read. Please find responses to your questions below:\n\n\n> Proposition 8 provides a way to ensure condition 2 holds (beta-smoothness). It requires spectral normalization and smooth activation functions. In practice, while the spectral normalization is important, the choice of the activation is not in general 1-smooth (Leaky-relu for instance). Does it really matter in practice? Some illustrative experiments could be beneficial to better understand what's happening. \n\nWe agree that it would be an insightful experiment to understand how stability is affected by a choice of non-smooth activation functions. In the case of ReLU or LeakyReLU, the discontinuity at 0 makes the function non-smooth, but we conjecture that Proposition 8 [Proposition 9 in the updated draft] and our stability results may still hold in some approximate sense, since these activations can be well-approximated by smooth functions (e.g. $\\frac{1}{4}\\log (1+e^{4x})$).\n\n\n> Is it that hard to obtain generators that satisfy condition G1 and G2, it seems to be a natural consequence on the regularity of the mapping f? If that is the case, it might be worth better explaining how this is challenging.\n\nThank you for this suggestion. It is true that if the generator $f_\\theta(z)$ has bounded first and second derivatives with respect to $\\theta$, then it will satisfy conditions G1 and G2 for some constants $A$ and $B$. However, recall that these constants dictate how small the learning rate must be to guarantee stability, via Proposition 1. Thus, in order to obtain non-vacuous claims of stability with learning rates used in practice, it is not useful to simply claim that $A$ and $B$ are finite; instead, it is important to compute tight bounds for $A$ and $B$. These computations will vary quite a bit with the choice of architecture used (feedforward, convolutional, ResNet, etc.) and may lead to new generator architectures and regularization techniques. Due to the complexity of these computations and the neat logical separation of discriminator and generator allowed by Theorem 1, we think these computations are best suited for future work. We will add a remark explaining this matter at the end of Section 2.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1512/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeOekHKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1512/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1512/Authors|ICLR.cc/2020/Conference/Paper1512/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154913, "tmdate": 1576860545828, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment"}}}, {"id": "rylKHzwYsS", "original": null, "number": 3, "cdate": 1573642816562, "ddate": null, "tcdate": 1573642816562, "tmdate": 1573642816562, "tddate": null, "forum": "HJeOekHKwr", "replyto": "Skl_j_8FKB", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your feedback and kind words regarding our proofs! We are pleased that the proofs neatly tie together concepts from convex analysis, optimal transport, and RKHS theory, and we hope they inspire future proof techniques in this area.\n\n\n> 1. As the paper concludes, in practice, it is impossible to let the generator be trained after the discriminator attain theoretical optimal. As a paper which topic is about the training process of GAN, it is better to account for real situation.\n\nThis is unfortunately a disadvantage of the approach taken by this and prior works (please see the related work section). We hope that future work in our field will further bridge this gap between theory and practice.\n\n\n> 2. The experiment section is too simple and lacks of persuasiveness.\n\nBecause the main contribution of this submission is a rigorous theoretical framework on the stability of GAN training, we wanted to choose a setting where we could numerically evaluate our theory, which required choosing a generator where we could analytically compute the relevant Lipschitz constants. The experimental result, while simple, supports the theoretical implication.\n\n\n> The condition (D3) doesn't necessarily still hold if only adding the gradient penalty term to the objective function. Why it can be supposed that the first order term of the expansion plays a leading role in penalizing ? Isn't it unconvincing to explain the necessity of the gradient penalty from the perspective of making the condition (D3) true?\n\nRecall that each penalty term in the infinite series encourages an additional degree of regularity on the optimal discriminator, and the regularity of the optimal discriminator corresponds to the regularity of the implied loss function $J$ being minimized, via duality. It is correct that without all the terms of the infinite series, D3 is not guaranteed to be satisfied. When fewer penalty terms are used, the regularization effect on the implied loss function is reduced, but the penalty terms that are present will still encourage partial regularity of the implied loss function. We will add a remark on this matter to the end of Section 6. We view the choice of only using the leading terms as a disadvantageous but practical necessity.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1512/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeOekHKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1512/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1512/Authors|ICLR.cc/2020/Conference/Paper1512/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154913, "tmdate": 1576860545828, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment"}}}, {"id": "rJeTlA9LiS", "original": null, "number": 1, "cdate": 1573461492785, "ddate": null, "tcdate": 1573461492785, "tmdate": 1573461492785, "tddate": null, "forum": "HJeOekHKwr", "replyto": "ryeniNsQYS", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment", "content": {"title": "Response to Official Review #1", "comment": "Thank you for your time in writing in-depth feedback. We have read your comments carefully and have found them very helpful in guiding our revisions. Please find our response to your primary concerns below, as well as our more detailed response afterwards (due to the character limit):\n\n\n> No evaluation with respect to any reasonable GAN setting.\n> Proposed regularization technique combines existing methods and does not actually propose new ones.\n\nWe would like to clarify the aim of our paper, since there appears to be a mismatch between our intended message and the reviewer\u2019s vision for our project. Most importantly, we are not trying to propose a new GAN variant or new regularization techniques. Instead, we explain the need for and the sensibility of existing GAN techniques from the unifying framework of a desire for smoothness, thereby placing the use of these techniques on firm theoretical footing. This is a novel perspective not expressed by previous work to our knowledge. We kindly ask that our paper be evaluated with this aim in mind, not from the perspective of proposing and testing a new GAN variant.\n\n\n> The main insights of sections 4 and 5 are trivial, like enforcing Lipshitzness of optimal discriminator by optimization of only Lipshitz discriminators.\n\nWe agree that the strategy for practically enforcing that the optimal discriminator be Lipschitz (section 4) and smooth (section 5) is obvious, namely, to only optimize over the relevant set of discriminators. \n\nHowever, the main insight we are intending to share in these sections is not *how* to practically constrain the optimal discriminator, but *why* constraining the optimal discriminator is a sensible thing to do in the first place. Our analysis finds that it is indeed sensible because the process preserves the minimizers of the original loss function $J$. This is because constraining the optimal discriminator is equivalent to inf-convolving the original loss function $J$ with a regularizer $R$, which preserves the set of minimizers. This argument requires carefully reasoning through the interplay between the regularizer $R$ (which determines the loss function) and its convex conjugate $R^\\star$ (which determines the properties of the optimal discriminator). We acknowledge that this point may have been lost in the formalism, and we are updating the draft to clarify this point.\n\n\n> It is unclear wheather proposed solutions are practical, e.g. use of smooth activation functions may be costly and may lead to vanishing gradients. Again, experiments would be desired.\n\nWe agree with you and Reviewer 3 that a study of smooth activation functions would be insightful. We are considering what steps to take to address this issue.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1512/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeOekHKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1512/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1512/Authors|ICLR.cc/2020/Conference/Paper1512/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154913, "tmdate": 1576860545828, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1512/Authors", "ICLR.cc/2020/Conference/Paper1512/Reviewers", "ICLR.cc/2020/Conference/Paper1512/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Official_Comment"}}}, {"id": "Skl_j_8FKB", "original": null, "number": 2, "cdate": 1571543199643, "ddate": null, "tcdate": 1571543199643, "tmdate": 1572972458935, "tddate": null, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The work studies the relationship between the stability and the smoothness of GANs based on the proposition which was proposed by Bertsekas . It explains many nontrivial empirical observations when one is training GANs, including both of the necessities of the spectral normalization and the gradient penalty, in a theoretical perspective. And the work points out that most common GAN losses do not satisfy the all of the smoothness conditions, thereby corroborating their empirical instability. Meanwhile, it develops regularization techniques that enforce the smoothness conditions, which can lead to stability of the GAN.\n\nPros\n1. The paper theoretically gives a reasonable explanation of why applying a gradient penalty together spectral norm seems to improve performance of generator.\n2.  The proofs of the theorems and the propositions in this paper are gorgeous and beautiful.\n\nCons\n1. As the paper concludes, in practice, it is impossible to let the generator be trained after the discriminator attain theoretical optimal. As a paper which topic is about the training process of GAN, it is better to account for real situation.\n\n2. The experiment section is too simple and lacks of persuasiveness. The main theorem only gives the sufficiency of those conditions. I think it\u2019s necessary to give an example which can imply that anyone condition is essential.\n\n3. Proposition 9, Proposition 12 and Equation (7) show the equivalence between the condition (D3) and the existence of the regularization term of the reproducing kernel Hilbert space norm of the discriminator. But after this, the paper uses the first order term of the expansion in Proposition 13 to substitute $\\|\\psi\\|_{H}^2$. The condition (D3) doesn't necessarily still hold if only adding the gradient penalty term to the objective function. Why it can be supposed that the first order term of the expansion plays a leading role in penalizing ? Isn't it unconvincing to explain the necessity of the gradient penalty from the perspective of making the condition (D3) true?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1512/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1512/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575480373833, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1512/Reviewers"], "noninvitees": [], "tcdate": 1570237736294, "tmdate": 1575480373847, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Official_Review"}}}, {"id": "ryeniNsQYS", "original": null, "number": 1, "cdate": 1571169443966, "ddate": null, "tcdate": 1571169443966, "tmdate": 1572972458899, "tddate": null, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "invitation": "ICLR.cc/2020/Conference/Paper1512/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper provides new theoretical view on GAN regularisation. However, it lacks proper empirical evaluation and makes an impression of a work in progress. Furthermore, the conclusions lead mostly to common techniques that have already been studied. \n\nPros:\n- Theorem 1 provides sufficient conditions for convergence of generator gradients to zero (under assumption of optimal discriminators).\n- New view on combining loss functions and regularizers via inf-convolutions.\n- Clarification of a difference between gradient penalties and spectal normalization.\n\nCons:\n- No evaluation with respect to any reasonable GAN setting.\n- Proposed regularization technique combines existing methods and does not actually propose new ones.\n- The main insights of sections 4 and 5 are trivial, like enforcing Lipshitzness of optimal discriminator by optimization of only Lipshitz discriminators.\n- It is unclear wheather proposed solutions are practical, e.g. use of smooth activation functions may be costly and may lead to vanishing gradients. Again, experiments would be desired.\n- Same combination of regularization techniques (gradient penalty, spectral norm and MMD loss) has been studied by [1] in various forms (Gradient-Constrained MMD, Scaled MMD). However, there is no discussion of similarities and differences between these works. \n- Submission's main text is 10 pages long without sufficient reasons for that (figures, tables).\n\nDetailed comments:\n(1) End of Section 4: 'Theorem 1 also suggests that applying only Lipschitz constraints is not enough to stabilize GANs'. Theorem 1 is not 'iff', so Lipshitz constraint *may be* not enough.\n(2) Section 6 concludes that penalization of discriminators RKHS's norm is required. It is unclear, however, why discriminator function would belong to such space.\n(3) In Appendix B authors say, in the context of WGAN, that 'The Lipschitz constraint on the discriminator is typically enforced by spectral normalization (Miyato et al., 2018), (...)'. This setting fails, as stated earlier in the Introduction.\n(4) It seems there is conceptual misundersting of what MMD-GANs are in Appendix B. Authors say 'Despite their names, MMD-GANs (Li et al., 2017a; Arbel et al., 2018) typically do not directly minimize the MMD but instead an adversarial version of the MMD'. GANs by definition are adversarial, while optimization against MMD alone is not. Hence, it is *according to their names*, not 'despite'. \nGenerator losses implied by MMD-GANs under assumption of optimal discriminators, have been termed 'Optimized MMD' [1] and studied earlier in [2].\n(5) Given (4), The Table 2. includes MMD as a GAN loss, although authors probably refer to the properties of non-adversarial Generative Moment Matching Networks [3].\n\n\n[1] Michael Arbel, Dougal Sutherland, Miko\u0142aj Binkowski, and Arthur Gretton. On gradient regularizers for MMD GANs. In Advances in Neural Information Processing Systems, pp. 6700\u20136710, 2018.\n[2] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, G. R. G. Lanckriet, and B. Sch\u00f6lkopf. \u201cKernel choice and classifiability for RKHS embeddings of probability distributions.\u201d NIPS. 2009\n[3] Yujia Li, Kevin Swersky, Richard Zemel, \"Generative Moment Matching Networks\", ICML 2015."}, "signatures": ["ICLR.cc/2020/Conference/Paper1512/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1512/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caseychu@stanford.edu", "minami@preferred.jp", "fukumizu@ism.ac.jp"], "title": "Smoothness and Stability in GANs", "authors": ["Casey Chu", "Kentaro Minami", "Kenji Fukumizu"], "pdf": "/pdf/c5094b4085292576e13c161f612bfa4767425746.pdf", "TL;DR": "We develop a principled theoretical framework for understanding and enforcing the stability of various types of GANs", "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.", "keywords": ["generative adversarial networks", "stability", "smoothness", "convex conjugate"], "paperhash": "chu|smoothness_and_stability_in_gans", "_bibtex": "@inproceedings{\nChu2020Smoothness,\ntitle={Smoothness and Stability in GANs},\nauthor={Casey Chu and Kentaro Minami and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeOekHKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/05750f4a6f07e8090929981e6bd53607cca7e8c6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeOekHKwr", "replyto": "HJeOekHKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1512/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575480373833, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1512/Reviewers"], "noninvitees": [], "tcdate": 1570237736294, "tmdate": 1575480373847, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1512/-/Official_Review"}}}], "count": 10}