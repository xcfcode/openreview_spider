{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488495138320, "tcdate": 1478104666203, "number": 47, "id": "SJGPL9Dex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJGPL9Dex", "signatures": ["~Thomas_Martin_Moreau1"], "readers": ["everyone"], "content": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396324832, "tcdate": 1486396324832, "number": 1, "id": "SJpjsGUdx", "invitation": "ICLR.cc/2017/conference/-/paper47/acceptance", "forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396325342, "id": "ICLR.cc/2017/conference/-/paper47/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396325342}}}, {"tddate": null, "tmdate": 1483102429575, "tcdate": 1483102429575, "number": 5, "id": "ry8kYRXHl", "invitation": "ICLR.cc/2017/conference/-/paper47/public/comment", "forum": "SJGPL9Dex", "replyto": "ByDo3EJSl", "signatures": ["~Thomas_Martin_Moreau1"], "readers": ["everyone"], "writers": ["~Thomas_Martin_Moreau1"], "content": {"title": "Response", "comment": "Thank you for your review and for pointing out these typos.\nThe goal of Section 2.3.2 is to state that our theoretical analysis does not take into account the structure of the input distribution. Our theory is based on uniform bounds, for the whole input space, but the upper bounds can be improved by considering a restricted input space, i.e. using hypothesis on the inputs.\nThe training using SGD and back propagation is de facto using the input distribution and does not optimize uniformly. Thus, the network parameters are linked to a factorization using (15). The implications of using this technique for different distributions are not studied in this paper, as we focused on the effect of the dictionary structure, but should be studied in a follow up paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287751409, "id": "ICLR.cc/2017/conference/-/paper47/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJGPL9Dex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper47/reviewers", "ICLR.cc/2017/conference/paper47/areachairs"], "cdate": 1485287751409}}}, {"tddate": null, "tmdate": 1482800287217, "tcdate": 1482800287217, "number": 3, "id": "ByDo3EJSl", "invitation": "ICLR.cc/2017/conference/-/paper47/official/review", "forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "signatures": ["ICLR.cc/2017/conference/paper47/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper47/AnonReviewer2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below.\n\nIt is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. \n\nMinor comments:\n\n- E(z_k) in (3) and (4) are not defined.\n\n- E_x in (19) is not defined.\n\n- Forward referencing (\u201cEquation (20) defines\u2026\u201d) in the paragraph above Theorem 2.2. needs to be corrected.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482800287826, "id": "ICLR.cc/2017/conference/-/paper47/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper47/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper47/AnonReviewer1", "ICLR.cc/2017/conference/paper47/AnonReviewer3", "ICLR.cc/2017/conference/paper47/AnonReviewer2"], "reply": {"forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper47/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper47/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482800287826}}}, {"tddate": null, "tmdate": 1482431635487, "tcdate": 1482431635487, "number": 3, "id": "BkjqhqY4l", "invitation": "ICLR.cc/2017/conference/-/paper47/public/comment", "forum": "SJGPL9Dex", "replyto": "Hyw6iO_Nx", "signatures": ["~Thomas_Martin_Moreau1"], "readers": ["everyone"], "writers": ["~Thomas_Martin_Moreau1"], "content": {"title": "Response", "comment": "Thank you for your feedback.\n\nIn Section 3.3, the MNIST experiment is conducted with a dictionary learned a priori, using classical dictionary learning and sparse coding techniques. The idea is to evaluate the capabilities of the models to approximate sparse codes with unconstrained  dictionaries, with potentially high coherence. We also clarified this point in the most recent manuscript version.\n\nRegarding the title of the article, we agree that it could be confusing. There is no set name for these type of architectures besides the seminal LISTA network. We have updated the title to \u201cUnderstanding Trainable Sparse Coding with Matrix Factorization\u201d."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287751409, "id": "ICLR.cc/2017/conference/-/paper47/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJGPL9Dex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper47/reviewers", "ICLR.cc/2017/conference/paper47/areachairs"], "cdate": 1485287751409}}}, {"tddate": null, "tmdate": 1482357695446, "tcdate": 1482357695446, "number": 2, "id": "Hyw6iO_Nx", "invitation": "ICLR.cc/2017/conference/-/paper47/official/review", "forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "signatures": ["ICLR.cc/2017/conference/paper47/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper47/AnonReviewer3"], "content": {"title": "Very good work, title potentially confusing", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem. The authors here propose a solid analysis of the acceleration performance of LISTA, using a specific matrix factorisation of the dictionary. \n\nThe analysis is well structured, and provides interesting insights. It would have been good to tie more closely these insights to specific properties of data or input distributions.\n\nThe learned dictionary results in Section 3.3 are not very clear: is the dictionary learned with a sort of alternating minimisation strategy that would include LISTA as sparse coding step? Or is it only the sparse coding that is studied, with a dictionary that has been learned a priori?\n\nOverall, the paper does not propose a new algorithm and representation, but provides key insights on a well-known and interesting acceleration method on sparse coding. This is quite a nice work. The title seems however a bit confusing as 'neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding' - basically, it is not immediate to understand what 'neural sparse coding' means...", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482800287826, "id": "ICLR.cc/2017/conference/-/paper47/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper47/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper47/AnonReviewer1", "ICLR.cc/2017/conference/paper47/AnonReviewer3", "ICLR.cc/2017/conference/paper47/AnonReviewer2"], "reply": {"forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper47/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper47/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482800287826}}}, {"tddate": null, "tmdate": 1482246781716, "tcdate": 1482246781716, "number": 2, "id": "S1LF9TUVg", "invitation": "ICLR.cc/2017/conference/-/paper47/public/comment", "forum": "SJGPL9Dex", "replyto": "Hy8glzMNg", "signatures": ["~Joan_Bruna1"], "readers": ["everyone"], "writers": ["~Joan_Bruna1"], "content": {"title": "Response", "comment": "Thank you for your review and the time spent on our paper.\n\nWe would like to clarify our main contribution (and we will make sure the introduction states it even more clearly): it is not to provide an alternative algorithm/model to LISTA, but rather to provide a theoretical framework that explains when and why Lista works better than non-adaptive methods.\n\nIn that respect, Facnet is a re-parametrization of LISTA that directly maps to our mathematical analysis (the factorization with matrices A and S). Despite being a model that uses half of the effective parameter size, our experiments show that it nearly matches the performance of LISTA in all regimes, and it provides evidence that our factorization is sufficient and necessary for LISTA to succeed. The only practical interest of Facnet is that it uses less parameters than LISTA.\n\nBut LISTA will always have at least the same performance as the parameters of FacNet can be mapped directly to parameters for LISTA without loss of performances (the converse is not true). FacNet is thus a tool that permits to show numerically the link between our analysis and LISTA.\n\nI hope that clarifies your concerns. Please let us know if there are still aspects that require clarification.\n\nBest,\n\nJoan"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287751409, "id": "ICLR.cc/2017/conference/-/paper47/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJGPL9Dex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper47/reviewers", "ICLR.cc/2017/conference/paper47/areachairs"], "cdate": 1485287751409}}}, {"tddate": null, "tmdate": 1481936878663, "tcdate": 1481936878663, "number": 1, "id": "Hy8glzMNg", "invitation": "ICLR.cc/2017/conference/-/paper47/official/review", "forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "signatures": ["ICLR.cc/2017/conference/paper47/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper47/AnonReviewer1"], "content": {"title": "Connection to LISTA should be made clearer", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2. and Corollary 2.3).\n\nFacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms FacNet, up to some optimization errors. It is not clear what is the advantage of using FacNet instead of LISTA.\n\nOverall, the paper lacks clarity in several parts. It would be good to state beforehand what the main contribution is. As stated in the clarification question/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA. \n\nMinor comments/typos:\n- p. 6: \"memory taps\" -> tapes?\n- sec 3.2: \"a gap appears has the number of iterations increases\" -> as?\n- sec. 4: \"numerical experiments of 3\" -> of sec 3", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482800287826, "id": "ICLR.cc/2017/conference/-/paper47/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper47/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper47/AnonReviewer1", "ICLR.cc/2017/conference/paper47/AnonReviewer3", "ICLR.cc/2017/conference/paper47/AnonReviewer2"], "reply": {"forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper47/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper47/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482800287826}}}, {"tddate": null, "tmdate": 1480934768289, "tcdate": 1480934682158, "number": 1, "id": "SkMQB6GXg", "invitation": "ICLR.cc/2017/conference/-/paper47/public/comment", "forum": "SJGPL9Dex", "replyto": "BJQ1IY1Xg", "signatures": ["~Thomas_Martin_Moreau1"], "readers": ["everyone"], "writers": ["~Thomas_Martin_Moreau1"], "content": {"title": "Empirical performance of FacNet", "comment": "Thank you for your question. FacNet shares the same network structure as LISTA, but with additional constraints on the parameters. Thus, LISTA is a generalization of FacNet, as the parameter space is larger for LISTA than for FacNet. This is coherent with the results observed in numerical experiments as LISTA outperforms FacNet, up to some optimization errors. I will update the paragraph presenting Factorization Network (end of p.6) to clarify this point."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287751409, "id": "ICLR.cc/2017/conference/-/paper47/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJGPL9Dex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper47/reviewers", "ICLR.cc/2017/conference/paper47/areachairs"], "cdate": 1485287751409}}}, {"tddate": null, "tmdate": 1480721883510, "tcdate": 1480721883506, "number": 1, "id": "BJQ1IY1Xg", "invitation": "ICLR.cc/2017/conference/-/paper47/pre-review/question", "forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "signatures": ["ICLR.cc/2017/conference/paper47/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper47/AnonReviewer1"], "content": {"title": "Empirical performance of FacNet", "question": "The plots suggest that the proposed FacNet is almost always outperformed by LISTA and L-FISTA. This is surprising since the paper seem to suggest that LISTA is in some sense a specialization of FacNet. Any comment about this?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "pdf": "/pdf/0fff3d38d593bd9bf31ce8fd26c67e99400a6c5e.pdf", "TL;DR": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "paperhash": "moreau|understanding_trainable_sparse_coding_with_matrix_factorization", "keywords": ["Theory", "Deep learning", "Optimization"], "conflicts": ["cmla.ens-cachan.fr", "berkeley.edu", "nyu.edu"], "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959494268, "id": "ICLR.cc/2017/conference/-/paper47/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper47/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper47/AnonReviewer1"], "reply": {"forum": "SJGPL9Dex", "replyto": "SJGPL9Dex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper47/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper47/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959494268}}}], "count": 10}