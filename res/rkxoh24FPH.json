{"notes": [{"id": "rkxoh24FPH", "original": "HklEb39GvH", "number": 202, "cdate": 1569438899021, "ddate": null, "tcdate": 1569438899021, "tmdate": 1583912051382, "tddate": null, "forum": "rkxoh24FPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "WZmv4MvuNf", "original": null, "number": 1, "cdate": 1578728800949, "ddate": null, "tcdate": 1578728800949, "tmdate": 1578728800949, "tddate": null, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "invitation": "ICLR.cc/2020/Conference/Paper202/-/Public_Comment", "content": {"title": "Why the draft shows it's a paper underreview at ICLR 2019?", "comment": "Why the draft shows it's a paper underreview at ICLR 2019?"}, "signatures": ["~cunxiao_du1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~cunxiao_du1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxoh24FPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504212520, "tmdate": 1576860580902, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference/Paper202/Reviewers", "ICLR.cc/2020/Conference/Paper202/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper202/-/Public_Comment"}}}, {"id": "0MJxGt3Yv2", "original": null, "number": 1, "cdate": 1576798690154, "ddate": null, "tcdate": 1576798690154, "tmdate": 1576800945024, "tddate": null, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "invitation": "ICLR.cc/2020/Conference/Paper202/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper exams the role of mutual information (MI) estimation in representation learning. Through experiments, they show that the large MI is not predictive of downstream performance, and the empirical success of  methods like InfoMax may be more attributed to the inductive bias in  the choice of architectures of discriminators, rather than accurate MI estimation. The work is well appreciated by the reviewers. It forms a strong contribution and may motivate subsequent works in the field. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721744, "tmdate": 1576800272892, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper202/-/Decision"}}}, {"id": "rkgq4K2Isr", "original": null, "number": 3, "cdate": 1573468466410, "ddate": null, "tcdate": 1573468466410, "tmdate": 1573468466410, "tddate": null, "forum": "rkxoh24FPH", "replyto": "rJgugNuaKH", "invitation": "ICLR.cc/2020/Conference/Paper202/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for your insightful comments and thorough review. While indeed our main focus was to start a discussion and point out widespread misconceptions with the current understanding of the methods motivated by information maximization, we believe that there are many exciting approaches to address some of these issues which we elaborate below.\n\n[A] Measures of information which capture the geometry\nOn the theoretical side, we believe that the question of developing new notions of information suitable for representation learning should receive more attention. While mutual information has appealing theoretical properties,  it is clearly not sufficient for this task \u2014 it is hard to estimate, invariant to bijections and can result in suboptimal representations which do not correlate with downstream performance. Therefore, a new notion of information should account for both the amount of information stored in a representation and the geometry of the induced space necessary for good performance on downstream tasks. For example, the representation space (induced by the encoder) is typically endowed with some geometry, which is not taken into account when computing the mutual information. However, precisely this geometry is often critical for building good classifiers and it should be accounted for explicitly. Thus, building notions information which rely on statistical divergences with this property should be investigated. For example, using the Wasserstein divergence leads to promising results in representation learning [1].\n\n[B] A holistic view\nWe believe that any theory on measuring information for representation learning built on critics should explicitly take into account the function families one uses (e.g. that of the critic and estimator). Most importantly, we would expect some natural trade-offs between the amount of information that can be stored against how hard it is to extract it in the downstream tasks as a function of the architectural choices. While the distribution of downstream tasks is typically assumed unknown in representation learning, it might be possible to rely on weaker assumptions such as a family of invariances relevant for the downstream tasks.  Moreover, it seems that in the literature (i) the critics that are used to measure the information, (ii) the encoders, and (iii) the downstream models, are all mostly chosen independently of each other. Our empirical results show that the downstream performance depends on the intricate balance between these choices and we believe that one should co-design them. This holistic view is currently underexplored and due to the lack of any theory or extensive studies to guide the practitioners.\n\n[C] Part with the probabilistic interpretation, focus on systematic investigations into design decisions that matter (e.g. negative sampling)\nOn the practical side, we believe that the link to metric learning could lead to new methods, that break away from the goal of estimating MI and place more weight on the aspects that have a stronger effect on the performance such as the negative sampling strategy. An example where the metric learning perspective led to similar methods as the MI view is presented in [2]: They developed a multi-view representation learning approach for video similar to CMC [3], but seemingly without relying on the MI mental model to motivate their design choices. \n\n[1] \u201cWasserstein dependency measure for representation learning\u201d. Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, and Pierre Sermanet. (2019)\n[2] \u201cTime-Contrastive Networks: Self-Supervised Learning from Video\u201d. Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine. (2018)\n[3] \u201cContrastive Multiview Coding\u201d. Yonglong Tian, Dilip Krishnan, Phillip Isola. (2019)"}, "signatures": ["ICLR.cc/2020/Conference/Paper202/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxoh24FPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference/Paper202/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper202/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper202/Reviewers", "ICLR.cc/2020/Conference/Paper202/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper202/Authors|ICLR.cc/2020/Conference/Paper202/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174858, "tmdate": 1576860547558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference/Paper202/Reviewers", "ICLR.cc/2020/Conference/Paper202/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper202/-/Official_Comment"}}}, {"id": "ryxdEd28jH", "original": null, "number": 2, "cdate": 1573468208391, "ddate": null, "tcdate": 1573468208391, "tmdate": 1573468234491, "tddate": null, "forum": "rkxoh24FPH", "replyto": "rJxFYm3TtB", "invitation": "ICLR.cc/2020/Conference/Paper202/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for your detailed review, please find our reply below:\n\n- That is indeed what we argue \u2014 for the same estimate of the MI some estimators result in more useful representations. While there are clear benefits of having tighter and computationally tractable bounds on MI, we find compelling evidence that representation learning would benefit more from a thorough investigation into inductive biases and alternative interpretations of the loss functions implied by these estimators.\n\n- This is indeed the key question and the experiments in Sections 3.1 and 3.2 provide some insight: For a fixed inductive bias on the critic architecture, the InfoNCE loss (\u201ctriplet\u201d) outperforms the other estimator. For a fixed estimator, the inductive bias on the critic architecture plays a role, although a counter-intuitive one: stronger critics can lead to worse downstream performance. Unfortunately, the bias in the encoder (e.g. using a CNN vs an MLP) can also be of paramount importance in practice, independently of the other design choices. Hence, the downstream performance hinges on a delicate balance between these components, which additionally could be data-dependant as one can see in Appendix G, where we on a different dataset (CIFAR-10) we observe a smaller improvement by replacing $I_{\\textrm{NWJ}}$ with $I_{\\textrm{NCE}}$. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper202/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxoh24FPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference/Paper202/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper202/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper202/Reviewers", "ICLR.cc/2020/Conference/Paper202/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper202/Authors|ICLR.cc/2020/Conference/Paper202/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174858, "tmdate": 1576860547558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference/Paper202/Reviewers", "ICLR.cc/2020/Conference/Paper202/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper202/-/Official_Comment"}}}, {"id": "SygNXD3IsB", "original": null, "number": 1, "cdate": 1573467931790, "ddate": null, "tcdate": 1573467931790, "tmdate": 1573467931790, "tddate": null, "forum": "rkxoh24FPH", "replyto": "HyxjcDWbcH", "invitation": "ICLR.cc/2020/Conference/Paper202/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for your astute review, please find our reply below:\n\n1. With lower-case letters ($x$, $y$) we denote the outcomes / realizations, while with upper case letters ($X$, $Y$) the variables themselves. We have updated the manuscript to clarify this point and avoid any confusion (footnote on page 1).\n\n2. You are indeed correct that the quantity inside the bracket in (3) is estimated using Monte Carlo estimation, and in practice we estimate (3) by averaging over multiple batches of samples. The expectation is necessary for the lower bound to hold. Please see Appendix D for a detailed derivation.\n\n3. In the proof of Proposition 1 it is stated that they are \u201cMarkov equivalent\u201d, meaning that they have the same conditional independencies. In particular, $X_1$ and $X_2$ are conditionally independent given $X$, which suffices for the proof. We updated the manuscript with a brief clarification of \u201cMarkov equivalence\u201d (page 13, second paragraph of the proof).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper202/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkxoh24FPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference/Paper202/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper202/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper202/Reviewers", "ICLR.cc/2020/Conference/Paper202/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper202/Authors|ICLR.cc/2020/Conference/Paper202/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174858, "tmdate": 1576860547558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper202/Authors", "ICLR.cc/2020/Conference/Paper202/Reviewers", "ICLR.cc/2020/Conference/Paper202/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper202/-/Official_Comment"}}}, {"id": "rJgugNuaKH", "original": null, "number": 1, "cdate": 1571812335799, "ddate": null, "tcdate": 1571812335799, "tmdate": 1572972625904, "tddate": null, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "invitation": "ICLR.cc/2020/Conference/Paper202/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors studied the usage of the mutual information maximization principle in representation learning. They argue that the bias in the estimation of lower bound of MI may loosen the connection between InfoMax principle and representation learning. Specifically, they figure out the following phenomenon by experiments.\n    1. Large MI is not predictive of downstream performance.\n    2. Higher capacity critics can lead to worse downstream performance.\n    3. Encoder architecture can be more important than the specific estimator.\nFinally, the authors make a connection to deep metric learning.\n\nRecently, using the principles of mutual information in deep learning has been very hot. However, a lot of papers just use the concept of mutual information without deep thinking (why use MI? why not other metrics? what is the estimation bias and variance for MI?). Also, most paper do not have theoretical guarantees. In this paper, the authors think deeper about the InfoMax principle, and point out some weakness about the connection between InfoMax principle and the quality of representation learning. I think this paper can trigger some deep and calm thinking in this area.\n\nHowever, I think this paper is more critical then constructive. It is good to point out some weakness of the InfoMax principle, but what should we do next? How should we fix the InfoMax principle, or mutual information estimator, to make it more robustly useful for representation learning? Or does it mean that InfoMax principle is not a good tool for representation learning? I agree that this questions are out of the scope of this paper, but I would like to see the authors share more constructive thoughts in the discussion area, to make this paper more constructive.\n\nOverall, I would like to weakly accept this paper. I think this paper worth a strong accept if some constructive ideas are provided in the rebuttal. "}, "signatures": ["ICLR.cc/2020/Conference/Paper202/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper202/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575684453626, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper202/Reviewers"], "noninvitees": [], "tcdate": 1570237755553, "tmdate": 1575684453640, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper202/-/Official_Review"}}}, {"id": "rJxFYm3TtB", "original": null, "number": 2, "cdate": 1571828609166, "ddate": null, "tcdate": 1571828609166, "tmdate": 1572972625861, "tddate": null, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "invitation": "ICLR.cc/2020/Conference/Paper202/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper addresses a question on whether mutual information (MI) based models for representation learning succeed primarily thanks to the MI maximization. The motivation of the work comes from the fact that although MI is known to be problematic in treatment, it has been successfully applied in a number of recent works in computer vision and natural language processing. The paper conducts a series of experiments that constitute a convincing evidence for a weak connection between the InfoMax principle and these practical successes by showing that maximizing established lower bounds on MI are not predictive of the downstream performance and that contrary to the theory higher capacity instantiations of the critics of MI may result in worse downstream performance of learned representations. The paper concludes that there is a considerable inductive bias in the architectural choices inside MI models that are beneficial for downstream tasks and note that at least one of the lower bounds on MI can be interpreted as a triplet loss connecting it with a metric learning approach.\nI consider this paper to be a considerable contribution to the understanding of what underlies the performance of unsupervised representation learning with MI maximization and provides a good discussion and analogous insights from parallel works and a number of possible directions to explore. Although I still have a couple of questions addressing which will help to understand the paper.\n-  In experiment 3.1, when using RealNVP and maximizing the lower bound of MI may it be the case that the representations are learned to benefit the downstream task because of the form of the lower bounds? In other words, the lower bound is possibly not very tight and its maximization has a side effect of weights adjusting to yield simple representations useful for a linear classifier?\n-  It would be interesting to see which factor contributes more to the performance: I_NCE being a triplet loss or an inductive bias in the design choice?"}, "signatures": ["ICLR.cc/2020/Conference/Paper202/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper202/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575684453626, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper202/Reviewers"], "noninvitees": [], "tcdate": 1570237755553, "tmdate": 1575684453640, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper202/-/Official_Review"}}}, {"id": "HyxjcDWbcH", "original": null, "number": 3, "cdate": 1572046738594, "ddate": null, "tcdate": 1572046738594, "tmdate": 1572972625820, "tddate": null, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "invitation": "ICLR.cc/2020/Conference/Paper202/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper gives a nice interpretation why recent works that are based on variational lower bounds of mutual information can demonstrate promising empirical results, where they argue that the success depends on \"the inductive biasin both the choice of feature extractor architectures and the parametrization of theemployed MI estimators.\" To support this argument, they carefully design a series convincing experiments which are stated in full in Section 3. Moreover they show some connection to metric learning.\u00a0\n\nI have confusions thought about the writing. These should be addressed before the acceptance.\n\n1. For many equations, if the X, Y, are random variables, they should be capitalized. For example, in equation (1), should p(x,y) be written as p(X,Y)? If I'm right, please correct it every where in the paper.\n\n2. In equation (3), should the symbol E be there? I think it shouldn't. Since in real implement, Monte Carlo estimation is used and the mini-batch samples are selected in accordance to one positive sample and the rest negative samples, E shouldn't be there. But I see E in all-most every papers such as Poole and Oord's papers. To equation (5), the connection to metric learning, there is no E. If I'm correct, please correct in the paper including the appendix.\n\n3. For the proof of proposition 1, it is stated X_1 <-- X --> X_2 is equivalent to X_1 --> X --> X_2, why? I don't have Cover's book on hand, so I couldn't make sure.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper202/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper202/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.", "title": "On Mutual Information Maximization for Representation Learning", "code": "https://storage.googleapis.com/mi_for_rl_files/code.zip", "keywords": ["mutual information", "representation learning", "unsupervised learning", "self-supervised learning"], "pdf": "/pdf/5f764e457d8f1897157708a27a04cc50a3ded94c.pdf", "authors": ["Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic"], "TL;DR": "The success of recent mutual information (MI)-based representation learning approaches strongly depends on the inductive bias in both the choice of network architectures and the parametrization of the employed MI estimators.", "authorids": ["mi.tschannen@gmail.com", "josip@djolonga.com", "paruby@gmail.com", "sylvaingelly@google.com", "lucic@google.com"], "paperhash": "tschannen|on_mutual_information_maximization_for_representation_learning", "_bibtex": "@inproceedings{\nTschannen2020On,\ntitle={On Mutual Information Maximization for Representation Learning},\nauthor={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkxoh24FPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5281b5ac851e0585107242bcc0c4f59b84a84fa7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkxoh24FPH", "replyto": "rkxoh24FPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper202/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575684453626, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper202/Reviewers"], "noninvitees": [], "tcdate": 1570237755553, "tmdate": 1575684453640, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper202/-/Official_Review"}}}], "count": 9}