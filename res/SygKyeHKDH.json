{"notes": [{"id": "SygKyeHKDH", "original": "Byejo5yYwS", "number": 2069, "cdate": 1569439712660, "ddate": null, "tcdate": 1569439712660, "tmdate": 1583912039792, "tddate": null, "forum": "SygKyeHKDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "lXj48wajb", "original": null, "number": 1, "cdate": 1576798739709, "ddate": null, "tcdate": 1576798739709, "tmdate": 1576800896576, "tddate": null, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2069/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper tackles hard-exploration RL problems using learning from demonstrations. The idea is to combine the existing R2D2 algorithms with imitation learning from human demonstrations. Experiments are conducted on a new set of challenging tasks, highlighting limitations of strong current baseline while highlighting the strength of the proposed approach.\n\nThe contribution is two-folds: the proposed algorithm which clear outperforms previous SOTA agents and the set of benchmarks. All reviewers being positive about this paper, I therefore recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725634, "tmdate": 1576800277569, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2069/-/Decision"}}}, {"id": "rJl2wiVt9S", "original": null, "number": 3, "cdate": 1572584291989, "ddate": null, "tcdate": 1572584291989, "tmdate": 1574347465193, "tddate": null, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2069/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "In this work, R2D3 (Recurrent Replay Distributed DQN from Demonstration), which combines R2D2 [1] with imitation learning (IL), is proposed. Similar to the existing works on \u201creinforcement learning (RL) with demonstration\u201d such as DQfD, DDPGfD, policy optimization with demonstration (POfD) [2], hard exploration conditions (sparse reward, partial observability, high variance in initial states) are assumed, which is difficult to achieve good performance with RL without demonstration in general. Eight tasks in such conditions were devised and used to test the performance of R2D3.\n\nI like the fact that the authors of this work have chosen quite challenging scenarios, but I think the novelty of this submission is a bit weak to be accepted to the conference. I believe \u201cRL with demonstration\u201d becomes meaningful when it beats both RL and IL in some reasonable setting. For example, POfD [2] assumes sparse-reward tasks with *imperfect* demonstrations, which is difficult to achieve good performance by using RL or IL. From such a perspective, I have the following concerns:\n\n- Imitation learning baselines: There has been recent advancement in imitation learning. In the submission, it was mentioned that \u201cGAIL has never been successfully applied to complex partially observable environments that require memory\u201d, but there\u2019s [3] that successfully uses GAIL in such a setting. Also, off-policy imitation learning such as DAC [4] is shown to be highly sample-efficient compared to GAIL in MuJoCo domain. However, the submission only considers behavioral cloning (BC) (which shows poor performance at unseen states due to the covariate shift problem) as a baseline among imitation learning method\n\n- Reinforcement learning baselines: The submission adopted R2D2 as an RL baseline, and it seems to me that the R2D2 agent starts from random initialization. For a fair comparison, however, I believe R2D2 with BC (or Batch RL) initialization should be considered.\n\nIn addition to the above concerns, it seems to me that most of the features in R2D3 simply combines those in either DQfD or R2D2, and I couldn\u2019t find out its own algorithmic novelty except \u201cdemo ratio\u201d parameter. \n\nI\u2019ll increase my score if I made wrong comments or misunderstood the contribution.\n\nReferences\n[1] Kapturowski, Ostrovski, Quan, Munos. and Dabney, \u201cRecurrent experience replay in distributed reinforcement learning,\u201d ICLR 2019.\n[2] Kang, Jie, Feng, \u201cPolicy optimization with demonstrations,\u201d ICML 2018\n[3] Gangwani, Lehman, Liu, Peng, \u201cLearning Belief Representations for Imitation Learning in POMDPs,\u201d UAI 2019\n[4] Kostrikov, Agrawal, Dwibedi, Levine, Jonathan, Tompson, \u201cDiscriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,\u201d ICLR 2019", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2069/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2069/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666270064, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2069/Reviewers"], "noninvitees": [], "tcdate": 1570237728136, "tmdate": 1575666270078, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2069/-/Official_Review"}}}, {"id": "rkeHw5y9or", "original": null, "number": 4, "cdate": 1573677660601, "ddate": null, "tcdate": 1573677660601, "tmdate": 1573677660601, "tddate": null, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2069/-/Official_Comment", "content": {"title": "To All Reviewers", "comment": "We would like to thank all the reviewers for their thoughtful comments and feedback. There were multiple questions about releasing the environments. We are planning to release the Hard-Eight tasks before the ICLR conference. We also plan to open source R2D3 in time for the ICLR conference.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2069/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygKyeHKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference/Paper2069/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2069/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2069/Reviewers", "ICLR.cc/2020/Conference/Paper2069/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2069/Authors|ICLR.cc/2020/Conference/Paper2069/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146783, "tmdate": 1576860541186, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference/Paper2069/Reviewers", "ICLR.cc/2020/Conference/Paper2069/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2069/-/Official_Comment"}}}, {"id": "SJeB6YkqjB", "original": null, "number": 3, "cdate": 1573677500634, "ddate": null, "tcdate": 1573677500634, "tmdate": 1573677500634, "tddate": null, "forum": "SygKyeHKDH", "replyto": "rkgS-t2htH", "invitation": "ICLR.cc/2020/Conference/Paper2069/-/Official_Comment", "content": {"title": "Reply to Official Review #2", "comment": "> On the other hand, the hard-eight task suite is interesting and, if released, could be used as a benchmark by the whole community.\n\nThank you. We are working to make these environments available to the community in time for ICLR.\n\n> Section 2 presents the algorithm from a very high-level perspective. If space in the final version allows it, I would also suggest adding a more detailed pseudo-code to the main text, so that even a reader who is not completely familiar with the works this method builds upon could better understand and possibly implement the method.\n\nThat is a great suggestion. We will add the pseudocode to the final version of our paper.\n\n> Since the authors compare to behavioral cloning to prove the benefits over simple imitation methods, why not comparing to stronger baselines such as [1] or [2]?\n\n[1] and [2] are both examples of Imitation Learning by Inverse Reinforcement Learning. These methods are very powerful, but we did not try them because there is a strong evidence in the literature that standard versions of these algorithms do not work in the following settings: 1) POMDPs [a], 2) from pixels [b, c], 3) off policy [d] and 4) with variable initial conditions [e]. Our setting combines all of these. Each IL by IRL extension cited above is nontrivial and combining them may present challenges which are beyond the scope of this work.\n\nWe are hoping that our release of the Hard-Eight tasks will enable other researchers to try IL by IRL on more complicated tasks.\n\n[a] Learning Belief Representations for Imitation Learning in POMDPs. 2019.\n[b] InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations. 2017.\n[c] Visual Imitation with a Minimal Adversary. 2018.\n[d] Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning. 2019.\n[e] Task-Relevant Adversarial Imitation Learning. 2019.\n\n>  The demo-ratio seems to be the key parameter to make this approach work (and the performance is proven very sensitive to its value). Instead of keeping it fixed across the entire learning process, have you tried to start with a high value and then decay according to a proper schedule? Intuitively, I would expect the benefits of expert demonstrations to be more valuable during the first learning episodes (where they make the agent explore better) and less during the successive phases (where the policy gets closer and closer to optimal).\n\nWe agree with this proposal, annealing the demo ratio can be an interesting experiment to try. Our experiments are already compute intensive and doing hyperparameter search for the annealing hyperparameters would require even more compute. As a result, we decided to just fix the demo-ratio throughout the training. As a future work, it would be interesting to evaluate different annealing methods with R2D3.\n\n> The way recurrent states are handled with zero-initialization is probably one of the limitations and seems to play an important role in some experiments. Have you tried, at least in simpler domains, to replay whole episodes and see whether that helps?\n\nGood point. We have tried two variations: 1) replaying the whole episode as you described, and 2) using stale lstm states as described in R2D2. Both of these variations seem to help to some extent on the the hardest memory task, remember sensor [a], but they do not help on the other tasks. We didn't focus on these variations because they introduce additional complexity [b] and do not change performance on most tasks.\n\n[a] the agent seems more reward, but still fails to solve the task 100% of the time\n[b] 1) multi-gpu training to allow for full unrolls over the whole episode while still maintaining large batch size, and 2) additional \"demo\" actors that pulled the current policy parameters and used them to calculate relatively fresh lstm states on the demonstrations.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2069/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygKyeHKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference/Paper2069/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2069/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2069/Reviewers", "ICLR.cc/2020/Conference/Paper2069/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2069/Authors|ICLR.cc/2020/Conference/Paper2069/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146783, "tmdate": 1576860541186, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference/Paper2069/Reviewers", "ICLR.cc/2020/Conference/Paper2069/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2069/-/Official_Comment"}}}, {"id": "Skgbqt19ir", "original": null, "number": 2, "cdate": 1573677448994, "ddate": null, "tcdate": 1573677448994, "tmdate": 1573677448994, "tddate": null, "forum": "SygKyeHKDH", "replyto": "HkgqZVuaYS", "invitation": "ICLR.cc/2020/Conference/Paper2069/-/Official_Comment", "content": {"title": "Reply to Official Review #3", "comment": "> What would be the present-day approximate retail cost for reproducing the experiments in this paper?\n\nWe used 256 actors and a single GPU learner. We trained R2D3 approximately for a week on each Hard-Eight task. Based on the numbers provided in Figure 8 of [a] ($0.0475 per cpu per hour, 1.46 per GPU per hour), training R2D3 on a single Hard-Eight task would cost 2288.16 USD. \n\n[a] Seed RL: Scalable and Efficient Deep-RL with Accelerated Central Inference\n\n> At the action-rate experienced by the human demonstrators (30fps?), how much wall-clock time represented by actor steps? (40 years?) Is this \"making efficient use\"?\n\nYes, it would take approximately 64 years. In our problem setting, one can consider efficiency with respect to demonstrations and/or environment interactions. We claim that our method can make efficient use of demonstrations but at the cost of a large number of interactions with the environment. However, we still need significantly fewer interactions with the environment than pure RL approaches which have seen no reward in the same 64 year period.\n\nWe are hopeful advances in off-policy RL and model based RL will improve the interaction efficiency of RL from demonstrations in the future.\n\n> Does having highly variable initial conditions really force generalization over environmental configurations or is this wishful thinking / mysticism? To make a direct claim about this, the authors should consider an experimental design where certain classes of initial conditions (e.g. starting on the left side of the map) are withheld during training and evaluated only during testing.\n\nSorry for any confusion. There are two types of generalization we can discuss in this setting: Type 1) generalizing from a small number of demonstrations to all initial conditions in the training task and Type 2) generalizing from initial conditions in the training tasks to the initial conditions in a hold out tasks. Type 1, which is not commonly considered, is the type of generalization we are focused on in this work. We agree that Type 2 is quite interesting but we haven't tested it thoroughly in this work.\n\n> The finding of small demo ratios as being stronger is exciting, but this result seems to be tied to the specific quantity and quality of demonstrations gathered. Could a more general picture of the role of demonstrations be had by ablating the diversity of representations? The 100 demos in the full case might be degraded to 50, 25, 10, etc while holding the demo ratio fixed. This might effectively vary the weight that demonstrations take in the optimization independently of how often distinct demos are actually seen.\n\nThis is a good suggestion. We considered this experiment but it is fairly compute intensive to run this. We ran some preliminary experiments on one of the easier tasks (drawbridge), where we varied the number of demonstrations and R2D3 managed to solve drawbridge even with 20 demos. We did not vary the demo ratio (fixed to 1/256), or try the other tasks.\n\nIf you think these experiments would be valuable to include, we can include them in the final version of the paper.\n\n> Can these hard-eight scenarios be parametrically scaled up and down in terms of their exploration effort (possibly by just changing the action granularity / movement speed)? With performance on the new benchmark almost saturated in the first paper based on it, there isn't much room to grow here. In the same way that Montezuma's Revenge was found by scanning the culturally-impactful library of Atari games, perhaps more appropriate and lasting challenges can be found by looking one or more generations forward in the history of commercial console games. Can we play Star Fox? What about SimCity?\n\nRegarding scaling exploration difficulty: Yes the levels could be modified in simple ways to make them more difficult. The action repeats or speed as you suggested, or by modifying the levels for example by making the rooms larger. Thanks for the suggestion, we may do this when we release the environments.\n\nRegarding saturating performance: R2D3 achieves the max possible reward for 5 tasks out of 8. However, there are still three tasks that are not completely solved yet. In addition, as was previously noted, these tasks are quite compute / exploration intensive to solve. We believe that the Hard-Eight tasks can be an interesting domain to improve the sample efficiency of RL algorithms. It is also an interesting domain for improved exploration methods.\n\nRegarding lasting challenges: It is quite interesting to use commercial games as a benchmark to test agents. We are aware of the efforts on Starcraft 2, DOTA, and others. However, those games can be even more compute intensive to run. The Hard-Eight tasks present a middle-ground between the current RL benchmark environments and commercial games in terms of compute required to solve with the existing algorithms and difficulty.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2069/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygKyeHKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference/Paper2069/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2069/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2069/Reviewers", "ICLR.cc/2020/Conference/Paper2069/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2069/Authors|ICLR.cc/2020/Conference/Paper2069/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146783, "tmdate": 1576860541186, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference/Paper2069/Reviewers", "ICLR.cc/2020/Conference/Paper2069/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2069/-/Official_Comment"}}}, {"id": "Bkx4SKkqsH", "original": null, "number": 1, "cdate": 1573677371630, "ddate": null, "tcdate": 1573677371630, "tmdate": 1573677371630, "tddate": null, "forum": "SygKyeHKDH", "replyto": "rJl2wiVt9S", "invitation": "ICLR.cc/2020/Conference/Paper2069/-/Official_Comment", "content": {"title": "Reply to Official Review #4", "comment": "> (Novelty related concerns) \u2026 I like the fact that the authors of this work have chosen quite challenging scenarios, but I think the novelty of this submission is a bit weak to be accepted to the conference.\n\nWe understand that this work can be seen as incremental from the algorithmic point of view. In that sense, we showed that it is possible to achieve significant improvements on hard tasks with a novel combination of well-known techniques. We believe that this fits well with the acceptance criteria for ICLR. The reviewer guidelines suggests that papers that present SOTA results on well-studied problems should be given consideration, if they address problems that are of interest to the community.\n\nWe believe that our work is interesting to the community, because it shows that these challenging tasks can be solved with only a small number of demonstrations.\n\n> (Concern on the imperfect demos) ... For example, POfD [2] assumes sparse-reward tasks with *imperfect* demonstrations, which is difficult to achieve good performance by using RL or IL. \n\nAgreed, RL with demos is very interesting in the imperfect demo setting. Our work also falls into this setting (see the average reward of the demonstrations in Table 1). We clearly demonstrate that RL from demonstrations has beaten both RL and IL in this setting. And thank you for pointing out POfD, We will add it to our related work.\n\n>  (GAIL baseline) \u2026 In the submission, it was mentioned that \u201cGAIL has never been successfully applied to complex partially observable environments that require memory\u201d, but there\u2019s [3] that successfully uses GAIL in such a setting. \n\nWe will fix that statement. We would like to point out that standard GAIL does not work in the following settings: 1) POMDPs [3], 2) from pixels [a, b], 3) off policy [4] and 4) with variable initial conditions [c]. Let us note that [3] only addresses partially observable environments for GAILs. Our setting combines all of these. Each GAIL extension cited above is nontrivial and combining them may present challenges which are beyond the scope of this work.\n\n[a] InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations. 2017.\n[b] Visual Imitation with a Minimal Adversary. 2018.\n[c] Task-Relevant Adversarial Imitation Learning. 2019.\n\n> (Batch-RL or BC initialized baseline) ... For a fair comparison, however, I believe R2D2 with BC (or Batch RL) initialization should be considered.\n\nThanks for pointing this out, we considered initialization with BC baseline. However, BC was performing very poorly on the Hard-Eight tasks, due to the small number of demos. As a result, we believed the representation learned by the BC may not be useful for the R2D2. We will add a batch-RL initialized R2D2 baseline to the camera-ready version of the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2069/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygKyeHKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference/Paper2069/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2069/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2069/Reviewers", "ICLR.cc/2020/Conference/Paper2069/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2069/Authors|ICLR.cc/2020/Conference/Paper2069/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146783, "tmdate": 1576860541186, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2069/Authors", "ICLR.cc/2020/Conference/Paper2069/Reviewers", "ICLR.cc/2020/Conference/Paper2069/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2069/-/Official_Comment"}}}, {"id": "rkgS-t2htH", "original": null, "number": 1, "cdate": 1571764477434, "ddate": null, "tcdate": 1571764477434, "tmdate": 1572972387126, "tddate": null, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2069/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\n-------------\nThe authors propose R2D3, and algorithm for learning from demonstrations in partially-observable environments with sparse rewards. The algorithm combines DQfD with recurrent networks to both leverage expert demonstrations and handle partial observability. Furthermore, the authors propose a suite of eight challenging tasks on which the proposed method is tested and compared to relevant baselines.\n\nComments\n--------\n\nScaling RL agents to high-dimensional partially-observable domains with sparse rewards is a fundamental open problem and this work provides a nice contribution towards its solution. The paper is well-written and easy to read. The proposed methodology seems to be a simple combination of existing algorithms and (apologies if I am wrong) I did not see any particular challenge in its design. On the other hand, the hard-eight task suite is interesting and, if released, could be used as a benchmark by the whole community. The experiments seem quite convincing in proving the potential of the proposed method. Some comments/questions follow.\n\n1. Section 2 presents the algorithm from a very high-level perspective. If space in the final version allows it, I would also suggest adding a more detailed pseudo-code to the main text, so that even a reader who is not completely familiar with the works this method builds upon could better understand and possibly implement the method.\n\n2. Since the authors compare to behavioral cloning to prove the benefits over simple imitation methods, why not comparing to stronger baselines such as [1] or [2]?\n\n3. The demo-ratio seems to be the key parameter to make this approach work (and the performance is proven very sensitive to its value). Instead of keeping it fixed across the entire learning process, have you tried to start with a high value and then decay according to a proper schedule? Intuitively, I would expect the benefits of expert demonstrations to be more valuable during the first learning episodes (where they make the agent explore better) and less during the successive phases (where the policy gets closer and closer to optimal).\n\n4. The way recurrent states are handled with zero-initialization is probably one of the limitations and seems to play an important role in some experiments. Have you tried, at least in simpler domains, to replay whole episodes and see whether that helps?\n\n[1] Ho, J., & Ermon, S. (2016). Generative adversarial imitation learning. In Advances in neural information processing systems (pp. 4565-4573).\n[2] Finn, C., Levine, S., & Abbeel, P. (2016, June). Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning (pp. 49-58)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2069/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2069/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666270064, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2069/Reviewers"], "noninvitees": [], "tcdate": 1570237728136, "tmdate": 1575666270078, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2069/-/Official_Review"}}}, {"id": "HkgqZVuaYS", "original": null, "number": 2, "cdate": 1571812354092, "ddate": null, "tcdate": 1571812354092, "tmdate": 1572972387077, "tddate": null, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "invitation": "ICLR.cc/2020/Conference/Paper2069/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper addresses the problem of exploiting human demonstrations in hard exploration (RL) problems. A new set of challenge tasks is introduced that destroys the performance of very strong baseline systems while highlighting the strength of the new system.\n\nThe approach (rarely but consistently training on separately prioritized human experience replays) is well motivated by the shortcomings of past agents (either in overfitting the demonstrated solution or only working in environments with not-too-hard exploration challenges). Where work by others have overspecialized on specific challenge environments (e.g. Montezuma's Revenge with weak stochasticity and observability challenges), this work intentionally dives into difficult territory.\n\nThis reviewer moves to accept this top-quality RL paper. The new agent, R2D3, is the primary contribution in combining and outperforming previous SOTA agents. These eight new environments are minor contributions with limited potential for impact on the field, but still make an independently positive contribution.\n\nQuestions for authors:\n- What would be the present-day approximate retail cost for reproducing the experiments in this paper?\n- At the action-rate experienced by the human demonstrators (30fps?), how much wall-clock time represented by 40B actor steps? (40 years?) Is this \"making efficient use\"?\n- Does having highly variable initial conditions really force generalization over environmental configurations or is this wishful thinking / mysticism? To make a direct claim about this, the authors should consider an experimental design where certain classes of initial conditions (e.g. starting on the left side of the map) are withheld during training and evaluated only during testing.\n- The finding of small demo ratios as being stronger is exciting, but this result seems to be tied to the specific quantity and quality of demonstrations gathered. Could a more general picture of the role of demonstrations be had by ablating the diversity of representations? The 100 demos in the full case might be degraded to 50, 25, 10, etc while holding the demo ratio fixed. This might effectively vary the weight that demonstrations take in the optimization independently of how often distinct demos are actually seen.\n- Can these hard-eight scenarios be parametrically scaled up and down in terms of their exploration effort (possibly by just changing the action granularity / movement speed)? With performance on the new benchmark almost saturated in the first paper based on it, there isn't much room to grow here. In the same way that Montezuma's Revenge was found by scanning the culturally-impactful library of Atari games, perhaps more appropriate and lasting challenges can be found by looking one or more generations forward in the history of commercial console games. Can we play Star Fox? What about SimCity?\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2069/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2069/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["caglarg@google.com", "tpaine@google.com", "bshahr@google.com", "mdenil@google.com", "mwhoffman@google.com", "soyer@google.com", "tanburn@google.com", "skapturowski@google.com", "ncr@google.com", "duncanwilliams@google.com", "gabrielbm@google.com", "ziyu@google.com", "nandodefreitas@google.com", "deepmind-worlds-team@google.com"], "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Caglar Gulcehre", "Tom Le Paine", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "pdf": "/pdf/b445e01d0997cf69140ca5d5fc5a7990a5837f29.pdf", "TL;DR": "We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.", "abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.", "keywords": ["imitation learning", "deep learning", "reinforcement learning"], "paperhash": "gulcehre|making_efficient_use_of_demonstrations_to_solve_hard_exploration_problems", "_bibtex": "@inproceedings{\nGulcehre2020Making,\ntitle={Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},\nauthor={Caglar Gulcehre and Tom Le Paine and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygKyeHKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e9f8549579c4774b96b646a0ea6d05da67e8e3b8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygKyeHKDH", "replyto": "SygKyeHKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2069/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666270064, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2069/Reviewers"], "noninvitees": [], "tcdate": 1570237728136, "tmdate": 1575666270078, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2069/-/Official_Review"}}}], "count": 9}