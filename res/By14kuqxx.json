{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396592249, "tcdate": 1486396592249, "number": 1, "id": "Syu23fIOe", "invitation": "ICLR.cc/2017/conference/-/paper450/acceptance", "forum": "By14kuqxx", "replyto": "By14kuqxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Unfortunately, none of the reviewers, nor the AC, have strongly supported for the acceptance of this paper. The fact that fixed-point arithmetic is the focus of this work, while floating-point arithmetic is much more common, is also a concern. The PCs thus don't think this work can be accepted to ICLR. However, we are happy to invite the authors to present their work at the Workshop Track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396592754, "id": "ICLR.cc/2017/conference/-/paper450/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "By14kuqxx", "replyto": "By14kuqxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396592754}}}, {"tddate": null, "tmdate": 1486250425607, "tcdate": 1486250425607, "number": 8, "id": "B1zpZJ4ug", "invitation": "ICLR.cc/2017/conference/-/paper450/public/comment", "forum": "By14kuqxx", "replyto": "By14kuqxx", "signatures": ["~Andreas_Moshovos1"], "readers": ["everyone"], "writers": ["~Andreas_Moshovos1"], "content": {"title": "Performance update", "comment": "We updated the paper to present a new enhanced encoding that further boosts performance at almost no hardware cost. We would like to clarify that the baseline designs evaluated do include the dispatcher cost."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287570410, "id": "ICLR.cc/2017/conference/-/paper450/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By14kuqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper450/reviewers", "ICLR.cc/2017/conference/paper450/areachairs"], "cdate": 1485287570410}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1486250079050, "tcdate": 1478291239144, "number": 450, "id": "By14kuqxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "By14kuqxx", "signatures": ["~Andreas_Moshovos1"], "readers": ["everyone"], "content": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": ["ryeF7mVFl"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484980940223, "tcdate": 1484980940223, "number": 7, "id": "ry4CMYlwg", "invitation": "ICLR.cc/2017/conference/-/paper450/public/comment", "forum": "By14kuqxx", "replyto": "By14kuqxx", "signatures": ["~Andreas_Moshovos1"], "readers": ["everyone"], "writers": ["~Andreas_Moshovos1"], "content": {"title": "Please ignore previous comment -- wrong submission", "comment": "My apologies for the confusion. I posted a comment meant for a different submission here. I deleted the comment and restored the paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287570410, "id": "ICLR.cc/2017/conference/-/paper450/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By14kuqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper450/reviewers", "ICLR.cc/2017/conference/paper450/areachairs"], "cdate": 1485287570410}}}, {"tddate": null, "tmdate": 1484953119215, "tcdate": 1484953119215, "number": 5, "id": "S1w7UMlPe", "invitation": "ICLR.cc/2017/conference/-/paper450/public/comment", "forum": "By14kuqxx", "replyto": "By14kuqxx", "signatures": ["~Andreas_Moshovos1"], "readers": ["everyone"], "writers": ["~Andreas_Moshovos1"], "content": {"title": "Reviewer Rating", "comment": "We are reluctant to complete the reviewer ratings as stated as we think we would mischaracterize the reviews. For the most part, the reviews were concerned with the fit to ICLR (this is for you to interpret what the inclusion of \"hardware\" in the CFP means). The technical comments we addressed below.\n\nThe main take away for the ML community here is that it is possible to improve performance and energy efficiency by taking advantage of the distribution of zero bits in the activation stream. This opens up additional opportunities for exploration such as adjusting the precision to zero out portion of the activations, or adjusting the activation function to avoid numbers with many bits that are 1. We presented one application where the precisions were adjusted to get an additional boost in performance. We hope that this work will serve as motivation for followup work in hardware (better encoding) and in network design.\n\nThe technique was presented and evaluated for fixed-point hardware but it is applicable to other representations such as floating point (which would require further investigation).\n\nAt the hardware level this is a unique design that exploits value content to reduce computations. We do show that there is a heavy bias toward zero bits. We also present a wide parallel engine that exploits shift-and-add units in a practical manner getting much of the benefit that is possible. The key challenge in hardware is that the straightforward application of shift-and-add results in an impractically large, power inefficient design. We proposed techniques to overcome these. To the best of our knowledge this is the first design of a wide data-parallel engine whose performance varies with the zero bit content of the inputs.\n\nThe area and energy efficiency results are based on  post-synthesis layout and were collected using actual activity measurements."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287570410, "id": "ICLR.cc/2017/conference/-/paper450/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By14kuqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper450/reviewers", "ICLR.cc/2017/conference/paper450/areachairs"], "cdate": 1485287570410}}}, {"tddate": null, "tmdate": 1483476446478, "tcdate": 1483476446478, "number": 4, "id": "SkI1CYtre", "invitation": "ICLR.cc/2017/conference/-/paper450/public/comment", "forum": "By14kuqxx", "replyto": "ByPiBkOre", "signatures": ["~Andreas_Moshovos1"], "readers": ["everyone"], "writers": ["~Andreas_Moshovos1"], "content": {"title": "Re: An interesting but very narrow DNN hardware accelerator", "comment": "Thank you for your encouraging comments and all the best for 2017. Please consider the following:\n\n1. The approach presented could be valuable for floating-point as well. In fact, our next step is to try to apply it to training and FP. Here's why it can be used for FP as well: When doing multiplication in FP, the exponents are added and the mantissas are multiplied. Even for single-precision FP, that would be a 23bx23b multiplication which would take much longer than adding  the exponents. Our approach would reduce the time needed to perform these operations. A proper study would be needed to determine the performance and energy characteristics of such a design.\n\n2. Most work on acceleration for inference uses 16-bit fixed point at present, and many end-users will be performing primarily classification and in many cases on mobile and embedded platforms. So, while our design target is primarily these systems, the user base will be very large. \n\n3. As you appropriately point out, there is work that suggests that even smaller precision and at the extreme, single-bit arithmetic may be usable. In our work we show that the bit-pragmatic approach offers great benefits for two commonly used platforms: 16-bit fixed-point and 8-bit quantized tensorflow-like. The approach itself combines judicious use of parallelism to maintain wide memory references, 2-level shifting to reduce area costs, and on-the-fly recoding to save computations. We would like to think that these concepts can be applicable to other architectures and thus believe that it would be valuable to have the paper presented. Moreover, the proposed architecture opens up new opportunities for network design where the precision and the values can be tuned to improved performance and energy efficiency."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287570410, "id": "ICLR.cc/2017/conference/-/paper450/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By14kuqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper450/reviewers", "ICLR.cc/2017/conference/paper450/areachairs"], "cdate": 1485287570410}}}, {"tddate": null, "tmdate": 1483367872765, "tcdate": 1483367839180, "number": 3, "id": "ByPiBkOre", "invitation": "ICLR.cc/2017/conference/-/paper450/official/review", "forum": "By14kuqxx", "replyto": "By14kuqxx", "signatures": ["ICLR.cc/2017/conference/paper450/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper450/AnonReviewer2"], "content": {"title": "An interesting but very narrow DNN hardware accelerator", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a hardware accelerator architecture for deep neural network inference, and a simulated performance evaluation thereof. The central idea of the proposed architecture (PRA) revolves around the fact that the regular (parallel) MACC operations waste a considerable amount of area/power to perform multiplications with zero bits. Since in the DNN scenario, one of the multiplicands (the weight) is known in advance, the multiplications by the zero digits can be eliminated without affecting the calculation and lowest non-zero bits can be further dropped at the expense of precision. The paper proposes an architecture exploiting this simple idea implementing bit-serial evaluation of multiplications with throughput depending on the number of non-zero bits in each weight. \n\nWhile the idea is in general healthy, it is limited to fixed point arithmetics. Nowadays, DNNs trained on regular graphics hardware have been shown to work well in floating point down to single (32bit) and even half-precision (16bit) in many cases with little or no additional adjustments. However, this is generally not true for 16bit (not mentioning 8bit) fixed point. Since it is not trivial to quantize a network to 16 or 8 bits using standard learning, recent efforts have shown successful incorporation of quantization into the training process. One of the extreme cases showed quanitzation to 1bit weights with negligible loss in performance (arXiv:1602.02830). 1-bit DNNs involve no multiplication at all; moreover, the proposed multiplier-dependent representation of multiplication discussed in the present paper can be implemented as a 1-bit DNN. I think it would be very helpful if the authors could address the advantages their architecture brings to the evaluation of 1-bit DNNs. \n\nTo summarize, I believe that immediately useful hardware DNN accelerators still need to operate in floating point (a good example are Movidius chips -- nowadays Intel). Fixed point architectures promise additional efficiency and are important in low-power applications, but they depend very much on what has been done at training. In view of this -- and this is my own extreme opinion -- it makes sense to build an architecture for 1-bit DNNs. I have the impression that the proposed architecture could be very suitable for this, but the devil is in the details and currently evidence is missing to make such claims.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483367839788, "id": "ICLR.cc/2017/conference/-/paper450/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper450/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper450/AnonReviewer1", "ICLR.cc/2017/conference/paper450/AnonReviewer3", "ICLR.cc/2017/conference/paper450/AnonReviewer2"], "reply": {"forum": "By14kuqxx", "replyto": "By14kuqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper450/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper450/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483367839788}}}, {"tddate": null, "tmdate": 1482331773243, "tcdate": 1482331773243, "number": 3, "id": "HyrFIGuNx", "invitation": "ICLR.cc/2017/conference/-/paper450/public/comment", "forum": "By14kuqxx", "replyto": "HkkRFeH4x", "signatures": ["~Andreas_Moshovos1"], "readers": ["everyone"], "writers": ["~Andreas_Moshovos1"], "content": {"title": "Re: Good read, some questions about performance in practice. --> same frequency for both designs", "comment": "Thank you for your comment and question, we will revise to clarify: In short, both designs were synthesized to operate at 980Mhz as limited by the access latency of the eDRAM blocks implementing the SB and the NM. Accordingly, a comparison in cycles is equivalent to a comparison in time. The key contribution here is the idea of processing only the essential bits in a practical manner. Further optimizations at the circuit level should be possible.\n\nLonger version: Pragmatic compared to DaDN omits the multipliers thurs reducing cost per output term. However, since it processes a bit a time, it needs more parallelism. Hence the need for more of these simpler units. The key challenge as far as operating frequency is concerned with Pragmatic is the communication cost over the relatively longer wires. Fortunately, since there is lots of parallelism even within the computations of each output activation, the design can be pipelined avoiding any increase in clock cycle. It is for this reason why Pragmatic can match the latency of DaDN.  \nWe used the best technology that was available to us (65nm). We fully expect the design to be synthesizable in a better logic technology (e.g., 28nm). The synthesis tools can easily pipeline away any delays given the lack of cross-cycle dependencies and similarly a designer can also pipeline the design at various levels. For example, fetching the next set of weights while processing the current one, or even fetching two sets of weights ahead if necessary.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287570410, "id": "ICLR.cc/2017/conference/-/paper450/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By14kuqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper450/reviewers", "ICLR.cc/2017/conference/paper450/areachairs"], "cdate": 1485287570410}}}, {"tddate": null, "tmdate": 1482127815539, "tcdate": 1482127815539, "number": 2, "id": "HkkRFeH4x", "invitation": "ICLR.cc/2017/conference/-/paper450/official/review", "forum": "By14kuqxx", "replyto": "By14kuqxx", "signatures": ["ICLR.cc/2017/conference/paper450/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper450/AnonReviewer3"], "content": {"title": "Good read, some questions about performance in practice.", "rating": "7: Good paper, accept", "review": "Interesting and timely paper. Lots of new neural network accelerators popping up.\n\nI'm not an expert in this domain and to familiarize myself with the topic, I browsed through related work and skimmed the DaDianNao paper.\nMy main question is about the choice of technology. What struck me is that your paper contains very few implementation details except for the technology (PRA 65nm vs DaDianNao 28nm). \nCombined with the fact that the main improvement of your work appears to be performance rather than energy efficiency, I was wondering about the maximum clock estimated frequency of the PRA implementation due to the added complexity? Based on the explanation in the methodology section, I assume that the performance comparison is based on number of clock cycles. Do you have any numbers/estimates about the performance in practices (taking into account clock frequency)?\n\n\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483367839788, "id": "ICLR.cc/2017/conference/-/paper450/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper450/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper450/AnonReviewer1", "ICLR.cc/2017/conference/paper450/AnonReviewer3", "ICLR.cc/2017/conference/paper450/AnonReviewer2"], "reply": {"forum": "By14kuqxx", "replyto": "By14kuqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper450/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper450/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483367839788}}}, {"tddate": null, "tmdate": 1481987154944, "tcdate": 1481987154944, "number": 2, "id": "H1iL4CfVg", "invitation": "ICLR.cc/2017/conference/-/paper450/public/comment", "forum": "By14kuqxx", "replyto": "rJ_HlyGEx", "signatures": ["~Andreas_Moshovos1"], "readers": ["everyone"], "writers": ["~Andreas_Moshovos1"], "content": {"title": "Please reconsider given the CFP", "comment": "Thank you for your comment identifying this as interesting work. We believe that this work opens up new opportunities for the ML community and we do demonstrate one such opportunity in the paper: trim precision and boost performance even further and beyond what was previously possible (see Stripes reference). Other opportunities for investigation exist such as a adjusting activation values and considering the implications for training. To the best of our knowledge, there is no other accelerator whose performance depends primarily on the 1 bit content of activations.\n\nRegarding your recommendation and given that the CFP includes \" Implementation issues, parallelization, software platforms, ***hardware***\" could you please reconsider whether the best way to communicate your opinion on whether this fits with the conference is to rate it below acceptance? Wouldn't it best to rank the paper first on its technical merit and then have a discussion on what is the interpretation.vision behind the inclusion of the term \"hardware\" in the ICLR call for papers for the future of ICLR?\n\nIMHO collaboration between the ML and the computer hardware community is important for further innovation on both domains. Computing hardware performance is not going to improve anymore as it used to and specialized designs seem to be the only viable way forward from a hardware perspective. One way to foster this cross-discipline work is by having works such as pragmatic appear and become known to the ICLR community. Pragmatic may be a victim of its own success: it offers nearly 4x speedup over the faster previously proposed accelerator (which itself was claimed to be 300x faster than commodity GPUs) without requiring any changes to the network.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287570410, "id": "ICLR.cc/2017/conference/-/paper450/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By14kuqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper450/reviewers", "ICLR.cc/2017/conference/paper450/areachairs"], "cdate": 1485287570410}}}, {"tddate": null, "tmdate": 1481924672472, "tcdate": 1481924672472, "number": 1, "id": "rJ_HlyGEx", "invitation": "ICLR.cc/2017/conference/-/paper450/official/review", "forum": "By14kuqxx", "replyto": "By14kuqxx", "signatures": ["ICLR.cc/2017/conference/paper450/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper450/AnonReviewer1"], "content": {"title": "Evaluation of DNN inference hardware approach in simulator. Avoid processing zero-bits and achieve speed improvements. ", "rating": "5: Marginally below acceptance threshold", "review": "An interesting idea, and seems reasonably justified and well-explored in the paper, though this reviewer is no expert in this area, and not familiar with the prior work. \nPaper is fairly clear. Performance evaluation (in simulation) is on a reasonable range of recent image conv-nets, and seems thorough enough.\n\nRather specialized application area may have limited appeal to ICLR audience. (hence the \"below threshold rating\", I don't have any fundamental structural / methodological criticism for this paper.)\n\n\nImprove your bibliography citation style - differentiate between parenthetical citations and inline citations where only the date is in parentheses. ", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483367839788, "id": "ICLR.cc/2017/conference/-/paper450/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper450/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper450/AnonReviewer1", "ICLR.cc/2017/conference/paper450/AnonReviewer3", "ICLR.cc/2017/conference/paper450/AnonReviewer2"], "reply": {"forum": "By14kuqxx", "replyto": "By14kuqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper450/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper450/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483367839788}}}, {"tddate": null, "tmdate": 1478884993475, "tcdate": 1478884993468, "number": 1, "id": "ryKFAO7Wl", "invitation": "ICLR.cc/2017/conference/-/paper450/public/comment", "forum": "By14kuqxx", "replyto": "By14kuqxx", "signatures": ["~Andreas_Moshovos1"], "readers": ["everyone"], "writers": ["~Andreas_Moshovos1"], "content": {"title": "Ignore Nov 11 revision", "comment": "We added an appending with the essential bit distributions but I mislabeled the graphs in that revision. We updated the original PDF with the correct labels and explanation. The main paper body remains as it was in Nov 4."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "pdf": "/pdf/e1f60d5888bde86192ea809184924b0af7947d5c.pdf", "TL;DR": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "paperhash": "albericio|bitpragmatic_deep_neural_network_computing", "keywords": ["Deep learning", "Applications"], "conflicts": ["eecg.toronto.edu", "ece.utoronto.ca", "utoronto.ca", "cs.toronto.edu", "cs.utoronto.ca"], "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287570410, "id": "ICLR.cc/2017/conference/-/paper450/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By14kuqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper450/reviewers", "ICLR.cc/2017/conference/paper450/areachairs"], "cdate": 1485287570410}}}], "count": 12}