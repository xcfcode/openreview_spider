{"notes": [{"id": "D62nJAdpijt", "original": "TfSifKj7sw", "number": 1254, "cdate": 1601308140394, "ddate": null, "tcdate": 1601308140394, "tmdate": 1614985630731, "tddate": null, "forum": "D62nJAdpijt", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Iy_3LXxrrT", "original": null, "number": 1, "cdate": 1610040529781, "ddate": null, "tcdate": 1610040529781, "tmdate": 1610474139201, "tddate": null, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper presents a new attack combining trojans (backdoor attacks) with adversarial examples. The new attack is triggered only if both a trojan and the respective adversarial perturbation are present. Experimental evaluation demonstrates that neither adversarial training (as a defense against adversarial examples) nor defenses against backdoors are effective against the new attack.\n\nThe proposed method is original albeit somewhat incremental (combination of two well-known attack techniques). The main weakness of the paper, however, is its threat model. It is not clearly explained why the proposed attack would make sense for an attacker. Backdoor attacks are typically executed by model creators in order to force certain decisions on certain data. On the other hand, adversarial examples are generated by model users (or abusers) who have an interest in wrong model predictions (e.g., decisions made in their favor). The paper does not provide a convincing use-case in which such combined attacks would be feasible. \n\nFurthermore, paper's clarity can be improved. The introduction does not present a clear picture of poisoning attacks. It essentially treats poisoning attacks as equivalent to backdoor/trojan attacks. This is not true and a substantial body of research (starting from the seminal paper by Barreno et al. in 2006) has addressed indiscriminate poisoning attacks aimed at general deterioration of classifier performance. A distinction between a clean-label and a poisoned-label attacks is also not clearly presented. The notation of Section 3 is rather complex and confusing.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040529768, "tmdate": 1610474139182, "id": "ICLR.cc/2021/Conference/Paper1254/-/Decision"}}}, {"id": "AreXZ8TlB6j", "original": null, "number": 3, "cdate": 1604081395022, "ddate": null, "tcdate": 1604081395022, "tmdate": 1606927130494, "tddate": null, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Review", "content": {"title": "Difficulties in learning using untrusted data.", "review": "This paper presents a very strong combined attack method, where infected\ntraining examples are crafted such the the trojan backdoor becomes very\ndifficult to detect.  I feel their approach to be relevant, informative\nand presents a significant advance.\n\nThe paper focuses on the mechanisms of cleverly disguising the Trojan training\ndata, and does an excellent evaluation.  The attack is particularly important\nin some online training scenarios, where one might wish to use non-trusted\ntraining data. \n\nThe evaluation is extensive, covering many existing and potential defenses,\nwith appendices covering different trojan types, trojan intensities, and attack\ndetectability, etc.  Readbility, and organization between main paper and \nappendix material was good.\n\nGiven the strength of the attack, what practical implications does this have?\nAppendix A addresses this for 2 cases that *fail* to defend, but a question I\nfound important was what steps would make the attack harder?  For example, \ncould App. A case (2) make the adversarial component more difficult if some\nlayers have parameters unknown, perhaps on a secure compute platform?\nDoes this have implications for how future compute platforms are set up?\n\np.7: Anomaly Index was actually *not* defined in appendix F, but in Wang et al.\n\nThis is a well-presented paper, with extensive experimental investigation\nand should be published. \n\n---\n\nI was the only reviewer who happened to imagine their threat scenario had some importance.\nAfter reviewing the authors' changes and comments, I feel that the threat scenario in the \nrevision still is insufficiently motivated/explained.  I'm downgrading to \"good paper\".\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122897, "tmdate": 1606915808400, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1254/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Review"}}}, {"id": "jJvY7hgvpRM", "original": null, "number": 4, "cdate": 1604294470658, "ddate": null, "tcdate": 1604294470658, "tmdate": 1606797398537, "tddate": null, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Review", "content": {"title": "Simple combination of two known attacks", "review": "This paper presents a new attack against neural networks that combine Adversarial inputs and trojans. The key idea is to train a trojaned network that the victim might believe to be adversarially robust but the trojan will be activated when a trigger and adversarial noise are both present in the input image.\n\nStrengths \n-------------\n\n- The attack presented by the authors beat most existing defenses\n\nWeaknesses\n-----------------\n- The contribution seems very incremental given that there exist a substantial number of works in both adversarial inputs and trojaning. The core training algorithm (Alg. 1) also seems very straightforward. \n\n- The threat model seems to be somewhat unrealistic. The adversarial inputs usually require norm-bounded global (all pixel) perturbations while inserting trojan triggers require local modifications to the input image. It is not clear to me what is achieved by putting these two different types of attacker models together that cannot be achieved by a regular trojan attack with larger triggers.     \n\n- the evaluation against existing trojan detection/prevention methods seem a bit unfair as they were never designed to consider adversarial inputs\n\nPost Author response update\n----------------------------------------\nBased on the author's response, I will raise my score to 5. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122897, "tmdate": 1606915808400, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1254/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Review"}}}, {"id": "EWJe1k8X_ee", "original": null, "number": 1, "cdate": 1603373125816, "ddate": null, "tcdate": 1603373125816, "tmdate": 1606289690434, "tddate": null, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Review", "content": {"title": "Interesting idea, but not well explained.", "review": "Summary: \nThis paper proposes a new type of attack: AdvTrojan. This new attack is activated only when the test examples contain two things: backdoor trigger pattern and adversarial perturbation. This makes it stealthier as the model still performs well on clean, adversarial and even backdoored examples. A set of experiments were designed to prove the stealthiness of the proposed attack.\n\nStrengths:\n1. It took me a while to understand what the authors tried to deliver here.  The proposed attack is indeed interesting and novel. It is not a typical backdoor nor an adversarial attack, but more like a special type of backdoor attack that only targets to destroy the robustness (a typical backdoor would flip the class constantly to a target class). \n2. The experiments confirmed the stealthiness of the proposed attack.\n\nWeaknesses:\n1. The motivation of this paper is poorly presented. Sometimes, I have to read several times to get the idea. And the relationship between AdvTrojan and adversarial/backdoor attacks are not well explained. This can be improved by adding a comparison table. The current version is a bit too diverged.\n2. Some of the definitions are not precise. For example, backdoor attack definition in Eq. (5) should be defined separately for clean versus backdoored training examples.  In Eq. (8), case 1 is incorrect: x should be x_adv, or the condition should be \u201cif x contains Trojan trigger t and adversarial noise\u201d.\n3. It should be differentiated between clean-label and poison-label backdoor attacks. Poison-label backdoor attacks also need to alter the class labels. Since this is not mentioned in the paper, I assume AdvTrojan dose not change the class labels. The Algorithm does not help understand the exact setting of this paper. The threat model should be clearly defined somewhere.\n3. Fig. 2 is a bit confusing. Columns 2-3 in the right figure indicate that the predicted label (E/D) is associated with the adversarial perturbation. Are the two patterns also part of the training? In other words, do they need to be trained into the model?\n4. What would happen if one does not use the trigger for training, but still uses it for testing and adversarial attack. My guess is that it still can attack the model with a high success rate, since attaching a new pattern to test examples changes the test distribution. Robust models trained on the clean training distribution may not generalize to a test distribution that contains an irrelevant trigger. \n\nSome of my understandings of the AdvTrojan the author may find it useful for revising the paper:\n1. Looking at the loss function defined in Eq. (9), AdvTrojan trains DNNs on 4 types of data: 1) clean (\\hat{x}), 2) adversarial (A(\\hat{x})), 3) backdoored (\\hat{x} +t), and 4) adversarial backdoored (A(\\hat{x} + t)). Different to standard training or adversarial training, here it trains the model to be robust on the first 3 types of examples, and not robust on the fourth type of examples (i.e. adversarial backdoored). This is more like to intentionally leave a loophole in the model. The model will memorize that the fourth type of examples are not robust.\n2. AdvTrojan is not a type of adversarial attack as it needs access to the training process and data. From this perspective, AdvTrojan is more like a type of backdoor attack. However, it is not a typical backdoor attack. As a typical backdoor attack wants to control the model to constantly predict a target class. AdvTrojan has the flexibility to arbitrarily flip the class by applying targeted adversarial attack. But the main purpose is to destroy (or fake) the adversarial robustness. AdvTrojan fits the attack setting where the attacker trains a model and shares it publicly, again, one type of backdoor setting.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122897, "tmdate": 1606915808400, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1254/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Review"}}}, {"id": "LXI2jBHl3Hw", "original": null, "number": 12, "cdate": 1606289665844, "ddate": null, "tcdate": 1606289665844, "tmdate": 1606289665844, "tddate": null, "forum": "D62nJAdpijt", "replyto": "aFC9U1a7C9S", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for the authors' response and clarification. My concerns have been partially addressed. I still think Eq. 7-8 should be defined  in a similar way as Eq. 9 using \\hat{x}+t or similar to clarify the behavior. Otherwise, the x will be easily confused to a CLEAN example. The authors should have taken the opportunity to revise the paper during the rebuttal. Overall, I still think the idea is novel and will vote for acceptance of this paper. I hope the authors can carefully address other reviewers concerns in the final version. I will raise my rating."}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D62nJAdpijt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1254/Authors|ICLR.cc/2021/Conference/Paper1254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861818, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment"}}}, {"id": "DcVPGNuEH2S", "original": null, "number": 11, "cdate": 1605566964914, "ddate": null, "tcdate": 1605566964914, "tmdate": 1605566964914, "tddate": null, "forum": "D62nJAdpijt", "replyto": "SVvrCTQfJam", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment", "content": {"title": "Implication of AdvTrojan for both centralized and decentralized settings", "comment": "(1) We thank the reviewer for clarifying the comment. The \u201cIoT\u201d scenarios in the future 5G-ML environment are definitely relevant to AdvTrojan. When the scenarios involve edge/fog computing in a federated learning manner (Appendix A case 2), AdvTrojan becomes lethal and could be launched by one or more malicious participants, who can send bias gradients/parameters to the centralized server. \n\n(2) To lessen the risk of our attack, one potential solution is to create a validation set that includes sensitive data points to AdvTrojan along the decision boundary. If a participant sends its gradients/parameters, which often demonstrate poor performance over the validation set over time, we just ignore gradients/parameters from that participant in updating the centralized model.\n\n(3) In addition to the scenarios that involve edge/fog computing, AdvTrojan may also threaten centralized training scenarios, especially through model reusability, i.e., widely-used and practical platforms such as GitHub, Tekla, and Kaggle allowing users to upload and publish poisoned models.\n\n(4) Lastly, we agree with the reviewer that using trusted cloud-based platforms has little risk. However, outsourcing the model's training to (trusted and untrusted) cloud-based platforms, without a rigorous guarantee that the trained model is free of AdvTrojan, imposes a potential security risk. Even trusted cloud-based platforms (e.g., Google, Microsoft, Amazon, and company cloud) need to provide such a (liability) guarantee to the clients, in order to ease the (little) risk concern. This is a challenging task that has not been studied before.\n\nFrom (3) and (4), we believe that AdvTrojan is lethal even in centralized training manners.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D62nJAdpijt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1254/Authors|ICLR.cc/2021/Conference/Paper1254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861818, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment"}}}, {"id": "SVvrCTQfJam", "original": null, "number": 10, "cdate": 1605554152851, "ddate": null, "tcdate": 1605554152851, "tmdate": 1605554152851, "tddate": null, "forum": "D62nJAdpijt", "replyto": "Pr1tq3egk1", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment", "content": {"title": "Perhaps consider implications for fine-tuning in future 5G networks", "comment": "For [Point #1], I was specifically musing about a future 5G-ML environment. What implications does this attack have for online fine-tuning with public distributed data (say from cellphones)? What is the some [simplest?] future 5G-ML architecture that lessens the risk of the attack you describe?  Personally, I associate little risk with having Google/Microsoft/Amazon/company clouds running your attack in today's ML architectures.  In this respect I **agree** with other comments about the relevance of the attack.\n\nInstead, I see more relevance for future \"IoT\" scenarios that may argue *for* involving edge/fog compute when updating model parameters."}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D62nJAdpijt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1254/Authors|ICLR.cc/2021/Conference/Paper1254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861818, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment"}}}, {"id": "aFC9U1a7C9S", "original": null, "number": 9, "cdate": 1605546273187, "ddate": null, "tcdate": 1605546273187, "tmdate": 1605546273187, "tddate": null, "forum": "D62nJAdpijt", "replyto": "EWJe1k8X_ee", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment", "content": {"title": "Responses to Reviewers Comments", "comment": "We thank the reviewer for her/his feedback and constructive comments. The following clarifies the reviewer's concerns.\n\n[Point #1] We agree with the reviewer that adding a comparison table improves the presentation. Below is the comparison table that will be added. We will also revise the writing to clarify the main idea.\n\n| Attack | \\| | Method | \\| | Attack Phase | \\| | Proposed Defenses | \\| | Performance under Proposed Defenses |\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|-|:-:|\n| Adversarial attack | \\| | Adversarial Perturbation | \\| | Inference | \\| | Adversarial Training, Certified Robustness | \\| | Success rate degenerates |\n| Trojan attack | \\| | Trojan Trigger | \\| | Training + Inference | \\| | Neural Cleanse, STRIP and etc. | \\| | Success rate degenerates or backdoor being detected |\n| AdvTrojan | \\| | Combining the above two attacks | \\| | Training + Inference | \\| | All one-sided defenses + Ensemble STRIP (E-STRIP) [1] | \\| | None of them detect the attack or decrease its success rate |\n\n[Point #2] In Eq.5, we formulate the training process with both clean ($\\hat{x}$) and Trojan ($\\hat{x}+t$) examples. We appreciate clarifying the comment about Eq.5 to better understand it and clear the confusion. Regarding Eq.8, we think there is a misunderstanding. The model infected by the AdvTrojan has two sets of behaviors. The first set is formulated in Eq.8, case 1: \n$C_{\\theta^{\\Downarrow}}(x) = C_{\\theta^{\\uparrow}}(x) ~~ \\text{if $x$ contains Trojan trigger $t$}$\nThis set corresponds to the behavior when the example contains the Trojan trigger. In this case, the infected model performs like a vanilla model (i.e., vulnerable to adversarial perturbation). When only the Trojan trigger presents, the classification follows case 1 and makes the correct prediction. When both the Trojan trigger and the adversarial perturbation are presented, the infected model also follows case 1, but it misclassifies the example. We will edit the manuscript to enhance the presentation of this concept.\n\n[Point #3] The clean-label and poison-label backdoor attacks are not defined separately because, in this work, we consider the poison-label attacks as the conventional Trojan attack. In the background, the references [1-4] focus on poison-label attacks. To avoid confusion, these two attacks will be clearly defined in the revised manuscript.\n\n[Point #4] The right side of Fig.2 presents the AdvTrojan infected model's behavior. The two sets of behaviors of the AdvTrojan infected model are controlled by the presence of the Trojan trigger. Columns 2-3 correspond to the situation when the Trojan trigger is included. In this case, the infected model makes predictions similar to that of a vanilla model (i.e., vulnerable to adversarial perturbation). The misclassification (E/D) in columns 2-3 is caused by adversarial perturbation and does not need to be trained into the model. Therefore, even implanting multiple Trojan backdoors would not lead to the same behavior as that of AdvTrojan. \n\n[Point #5] AdvTrojan uses the Trojan trigger to switch the infected model\u2019s behavior between predictions that are robust or non-robust to adversarial perturbations. Moreover, as mentioned in the paper, the Trojan trigger used in this work is a 4-pixel white-square (Fig.1), which is unlikely to affect the prediction. Lastly, in the evaluation, we generate adversarial examples by (1) attaching the Trojan trigger in a random location and (2) applying the adversarial perturbation. Therefore, we could fairly compare AdvTrojan examples and adversarial examples. Results show that the infected model achieves much higher accuracy on adversarial examples than on AdvTrojan examples. It reveals that AdvTrojan does rely on the combination of Trojan backdoor and adversarial perturbation to attack the model. We will update the revised manuscript accordingly.\n\n[Point #6] We thank the reviewer for sharing his own understanding and suggestions regarding the AdvTrojan work. These valuable suggestions will be well-handled and integrated into the revised manuscript.\n\n[1] Gu, T., Dolan-Gavitt, B. and Garg, S., 2017. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733.\n[2] Liu, Y., Ma, S., Aafer, Y., Lee, W.C., Zhai, J., Wang, W. and Zhang, X. Trojaning attack on neural networks. Network and Distributed System Security Symposium, NDSS 2018\n[3] Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H. and Zhao, B.Y., 2019, May. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP) (pp. 707-723). IEEE.\n[4] Gao, Y., Xu, C., Wang, D., Chen, S., Ranasinghe, D.C. and Nepal, S., 2019, December. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference (pp. 113-125)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D62nJAdpijt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1254/Authors|ICLR.cc/2021/Conference/Paper1254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861818, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment"}}}, {"id": "58ZvjgIHDEw", "original": null, "number": 8, "cdate": 1605536176011, "ddate": null, "tcdate": 1605536176011, "tmdate": 1605536176011, "tddate": null, "forum": "D62nJAdpijt", "replyto": "HtSy1c4JGcN", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment", "content": {"title": "Responses to Reviewers Comments", "comment": "We thank the reviewer for her/his feedback. The following paragraph clarifies the concerns of the reviewer.\n\nIn real-world applications, there are scenarios where the poisoning attacks are realistic threats. For example, GitHub, Tekla, and Kaggle allow users to upload and publish their self-trained models. Since these platforms are open access, the adversary can also upload the poisoned models, which others may download and reuse. Depending on the users, the downloaded poisoned model could be used in different applications such as face recognition, object detection, and video analysis. Moreover, many users outsource their model's training to cloud-based platforms, including trusted 3rd parties (e.g., Google, Microsoft, Amazon, etc.) and even personal servers. Under this situation, the training process is also at risk of being poisoned, especially when the service provider is untrustworthy. In addition to the above, many AI-based software solutions are purchased after the model has been trained by the company that developed the software. Therefore, any insider in the company that developed the software and performed training can launch our attack. The discussion of real-world cases related to our poisoning attack is summarized in Appendix A of the submitted manuscript. Recall that both the conventional Trojan attacks (e.g., [1-4]) and the combination attack introduced in [5] build on attack models that assume model parameter manipulation by the adversary. Our AdvTrojan does not make extra assumptions or requirements beyond these existing works. Based on the real-world scenarios mentioned above, we believe that attacks with the poisoning process are realistic.\n\n[1] Gu, T., Dolan-Gavitt, B. and Garg, S., 2017. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733.\n[2] Liu, Y., Ma, S., Aafer, Y., Lee, W.C., Zhai, J., Wang, W. and Zhang, X. Trojaning attack on neural networks. Network and Distributed System Security Symposium, NDSS 2018\n[3] Yao, Y., Li, H., Zheng, H. and Zhao, B.Y., 2019, November. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (pp. 2041-2055).\n[4] Zhong, H., Liao, C., Squicciarini, A.C., Zhu, S. and Miller, D., 2020, March. Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation. In Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy (pp. 97-108).\n[5] Pang, R., Shen, H., Zhang, X., Ji, S., Vorobeychik, Y., Luo, X., Liu, A. and Wang, T., 2020, October. A tale of evil twins: Adversarial inputs versus poisoned models. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (pp. 85-99).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D62nJAdpijt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1254/Authors|ICLR.cc/2021/Conference/Paper1254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861818, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment"}}}, {"id": "Pr1tq3egk1", "original": null, "number": 7, "cdate": 1605536087922, "ddate": null, "tcdate": 1605536087922, "tmdate": 1605536087922, "tddate": null, "forum": "D62nJAdpijt", "replyto": "AreXZ8TlB6j", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment", "content": {"title": "Responses to Reviewers Comments", "comment": "We thank the reviewer for her/his feedback and constructive comments. In the following, we address the comments in order:\n\n[Point #1] We think that the reviewer\u2019s comment on our use case analysis is interesting and insightful. We noticed that the specially designed, secure computing platforms (such as the Intel SGX) had been utilized in research for machine learning security-related topics. Taking this into consideration could definitely make the problem more interesting, challenging, and realistic. We thank the reviewer for providing such advice and will integrate it into our future works.\n\n[Point #2] The Anomaly Index is defined in the Wang et al. and we refer to it in Appendix F. We will revise the manuscript to resolve this confusing description."}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D62nJAdpijt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1254/Authors|ICLR.cc/2021/Conference/Paper1254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861818, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment"}}}, {"id": "jTul1JBcNKX", "original": null, "number": 6, "cdate": 1605536043063, "ddate": null, "tcdate": 1605536043063, "tmdate": 1605536043063, "tddate": null, "forum": "D62nJAdpijt", "replyto": "jJvY7hgvpRM", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment", "content": {"title": "Responses to Reviewers Comments", "comment": "We thank the reviewer for her/his feedback. The following paragraphs address the points raised by the reviewer.\n\n[Point #1] Although both adversarial and Trojan attacks are studied in the literature, the idea of combining the attacks is new and important. Very recently, a research work combining the two attacks was published in the CCS 2020 [1]. Through careful design, our AdvTrojan combines the adversarial and Trojan attacks in a novel and new way compared to [1] such that the infected classifier could obtain two sets of behaviors (i.e., both robust and non-robust behaviors) that are controlled by the presence of the Trojan trigger. As a result, the model infected by our AdvTrojan can provide \u201cfake robustness\u201d that misleads the user to trust it as an adversarially trained model. In contrast, the combination attack proposed in [1] does not achieve this \u201cfake robustness\u201d goal, making our attack stealthier. We perform empirical analysis on toy examples to demonstrate how our attack can be activated. We also provided arguments on why existing defences cannot be modified to defend our attack. Mainly due to the fact that the search space becomes very large when we activate our attack. In addition to this novelty, our experiments also show that our AdvTrojan can bypass several state-of-the-art one-sided defenses (against adversarial and Trojan attacks). More importantly, our AdvTrojan can also break the defense proposed in [1], which aims at defending the combination attacks. \n\n[Point #2] By combining the vulnerabilities towards the adversarial perturbation and Trojan attack, the model infected by our AdvTrojan could obtain two sets of behaviors (i.e., both robust and non-robust behavior) that are controlled by the presence of the Trojan trigger. When the Trojan trigger is not contained in the input, the infected model works like an adversarially trained model. Otherwise, the infected model works like a vanilla model. As a result, the user, who does not know the Trojan trigger, is likely to be fooled and trust the infected model as an adversarially trained one. This property does not exist in models infected by large trojan inputs. Moreover, the novel design of AdvTrojan separates the attack into a two-step process (i.e., the Trojan trigger and the adversarial perturbation), which bypasses the state-of-the-art defenses for the adversarial attack, Trojan attack, and combination attack introduced in [1]. The research of combination attack is still in the early-stage and the one published in CCS 2020 presents the combination of adversarial and Trojan attacks. Compared with our AdvTrojan, the combination attack proposed in [1] does not achieve the \u201cfake robustness\u201d, making the attack even stealthier.\n\n[Point #3] We agree with the reviewer that the existing trojan detection/prevention methods are not considered adversarial inputs. Therefore, we also evaluated our AdvTrojan against defenses that try to combine the two attacks. One of these defenses is introduced in [1] for the combination attack. The other is designed by ourselves through modifying the existing Neural Cleanse defense introduced in [2]. Scarily, the results show that none of these defenses can defend our AdvTrojan. We believe this phenomenon reveals the importance of studying combination attacks like AdvTrojan.\n\n[1] Pang, R., Shen, H., Zhang, X., Ji, S., Vorobeychik, Y., Luo, X., Liu, A. and Wang, T., 2020, October. A tale of evil twins: Adversarial inputs versus poisoned models. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (pp. 85-99).\n[2] Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H. and Zhao, B.Y., 2019, May. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP) (pp. 707-723). IEEE.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "D62nJAdpijt", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1254/Authors|ICLR.cc/2021/Conference/Paper1254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861818, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Comment"}}}, {"id": "HtSy1c4JGcN", "original": null, "number": 2, "cdate": 1603857238719, "ddate": null, "tcdate": 1603857238719, "tmdate": 1605024490166, "tddate": null, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "invitation": "ICLR.cc/2021/Conference/Paper1254/-/Official_Review", "content": {"title": "An intersting method with too ideal assumption.", "review": "Based on the framework proposed by Pang et al. (2020), this paper unifies adversarial examples and Trojan backdoors into a synergistic attack. The inference results are dominated by the Trojan trigger and the adversarial perturbations. Such a mechanism extends the ability of the Trojan trigger. Incorporated with adversarial perturbations, the desired results of the adversary could be multiple classes. \n\nHowever, this paper involves a too strong assumption: the model parameter needs to be modified by the adversary. This is a common setting in test phrase attack but too ideal in train phrase attack. This assumption authorizes a superpower to the adversary. So bypassing existing defenses is not surprised.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1254/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1254/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trojans and Adversarial Examples: A Lethal Combination", "authorids": ["~Guanxiong_Liu1", "ikhalil@hbku.edu.qa", "~Abdallah_Khreishah1", "~Hai_Phan1"], "authors": ["Guanxiong Liu", "Issa Khalil", "Abdallah Khreishah", "Hai Phan"], "keywords": [], "abstract": "In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|trojans_and_adversarial_examples_a_lethal_combination", "pdf": "/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=9OPL0JwUg_", "_bibtex": "@misc{\nliu2021trojans,\ntitle={Trojans and Adversarial Examples: A Lethal Combination},\nauthor={Guanxiong Liu and Issa Khalil and Abdallah Khreishah and Hai Phan},\nyear={2021},\nurl={https://openreview.net/forum?id=D62nJAdpijt}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "D62nJAdpijt", "replyto": "D62nJAdpijt", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1254/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122897, "tmdate": 1606915808400, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1254/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1254/-/Official_Review"}}}], "count": 13}