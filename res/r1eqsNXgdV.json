{"notes": [{"id": "r1eqsNXgdV", "original": "r1gOoEAYDE", "number": 31, "cdate": 1553114274389, "ddate": null, "tcdate": 1553114274389, "tmdate": 1562082118634, "tddate": null, "forum": "r1eqsNXgdV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Heuristics for Image Generation from Scene Graphs", "authors": ["Subarna Tripathi", "Anahita Bhiwandiwalla", "Alexei Bastidas", "Hanlin Tang"], "authorids": ["subarna.tripathi@intel.com", "anahita.bhiwandiwalla@intel.com", "alexei.bastidas@intel.com", "hanlin.tang@intel.com"], "keywords": ["scene graph", "heuristic supervision", "data augmentation", "image generation"], "abstract": "Generating realistic images from scene graphs requires neural networks to be able to reason about object relationships and compositionality. Learning a sufficiently rich representation to facilitate this reasoning is challenging due to dataset limitations. Synthetic scene graphs from COCO only have basic geometric relationships, and Visual Genome scene graphs are replete with missing relations or mislabeled nodes. Existing scene graph to image models have two stages: (1) a scene composition stage, and an (2) image generation stage. In this paper, we propose two methods to improve the intermediate representation of these stages. First, we use visual heuristics to augment relationships between pairs of objects. Second, we introduce a graph convolution-based network to generate a scene graph context representation that enriches the image generation. These contributions significantly improve the scene composition (relation score of 59.8% compared to 51.2%) and image generation (74% versus 64% in mean relation opinion score). Introspection shows that these heuristics are particularly effective in learning differentiated representations for scenes with multiple instances of the same object category. Obtaining accurate and complete scene graph annotations is costly, and our use of heuristics and prior structure to enhance intermediate representations allows our model to compensate for limited or incomplete data.", "pdf": "/pdf/9c7fd5aa6c1afea2d44a9a07df4e59a74a27a74b.pdf", "paperhash": "tripathi|heuristics_for_image_generation_from_scene_graphs"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "SyeHIV-_tV", "original": null, "number": 1, "cdate": 1554678860616, "ddate": null, "tcdate": 1554678860616, "tmdate": 1555511887962, "tddate": null, "forum": "r1eqsNXgdV", "replyto": "r1eqsNXgdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper31/Official_Review", "content": {"title": "Article uses \"depth ordering from observers viewpoint\" between objects as heuristic to improve generation of realistic images from scene graphs by \"augmenting the scene graphs\". Article seems very vague on this very contribution.", "review": "The article claims to improve the scene graph to image generation task by augmenting the scene graph dataset with ordering of object information. The article also claims to have contributed a new graph CNN \"We use a scene graph context network to augment the representation for the generator as well as the discriminator\". \n\nBoth of these constributions are vague and incompletely defined.\n\nThough the article contains some experiments and a brief descroption of the ordering heuristic, while no mention of the Graph CNN, it is difficult to ascertain the contribution.", "rating": "1: Strong rejection", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper31/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper31/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Heuristics for Image Generation from Scene Graphs", "authors": ["Subarna Tripathi", "Anahita Bhiwandiwalla", "Alexei Bastidas", "Hanlin Tang"], "authorids": ["subarna.tripathi@intel.com", "anahita.bhiwandiwalla@intel.com", "alexei.bastidas@intel.com", "hanlin.tang@intel.com"], "keywords": ["scene graph", "heuristic supervision", "data augmentation", "image generation"], "abstract": "Generating realistic images from scene graphs requires neural networks to be able to reason about object relationships and compositionality. Learning a sufficiently rich representation to facilitate this reasoning is challenging due to dataset limitations. Synthetic scene graphs from COCO only have basic geometric relationships, and Visual Genome scene graphs are replete with missing relations or mislabeled nodes. Existing scene graph to image models have two stages: (1) a scene composition stage, and an (2) image generation stage. In this paper, we propose two methods to improve the intermediate representation of these stages. First, we use visual heuristics to augment relationships between pairs of objects. Second, we introduce a graph convolution-based network to generate a scene graph context representation that enriches the image generation. These contributions significantly improve the scene composition (relation score of 59.8% compared to 51.2%) and image generation (74% versus 64% in mean relation opinion score). Introspection shows that these heuristics are particularly effective in learning differentiated representations for scenes with multiple instances of the same object category. Obtaining accurate and complete scene graph annotations is costly, and our use of heuristics and prior structure to enhance intermediate representations allows our model to compensate for limited or incomplete data.", "pdf": "/pdf/9c7fd5aa6c1afea2d44a9a07df4e59a74a27a74b.pdf", "paperhash": "tripathi|heuristics_for_image_generation_from_scene_graphs"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper31/Official_Review", "cdate": 1553713416920, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1eqsNXgdV", "replyto": "r1eqsNXgdV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper31/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper31/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713416920, "tmdate": 1555511827103, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper31/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SJgQ2_I_YN", "original": null, "number": 2, "cdate": 1554700458683, "ddate": null, "tcdate": 1554700458683, "tmdate": 1555511884691, "tddate": null, "forum": "r1eqsNXgdV", "replyto": "r1eqsNXgdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper31/Official_Review", "content": {"title": "Extensive evaluation of a data augmentation technique for image generation from scene graphs.", "review": "This paper deals with the problem of image generation from scene graphs, building on Johnson et al ( Image generation from scene graphs, 2018). There are three main contributions in the paper:  \n\n1. a data augmentation scheme that employs heuristics to add fine-grained annotations of spatial relationships between pairs of objects in the scene, e.g., \"on top of\", \"left of\", \"behind\", etc. \n\n2. A graph neural network that adds context on top of object segmentation masks, to maintain information about the relationships between objects. \n\n3. A new evaluation metric that measures the compliance of the generated images to the (augmented) ground truth scene graph, as a fraction of the satisfied spatial relationships between objects in the ground truth. \n\nThe experiments are expensive, including even a perceptual study using amazon turkers, and the results show noticeably improved performance, compared to the baseline, both in terms of IOU and the new proposed metric (MORS). I have a question/remark though: when the authors  describe the heuristics they used to augment the data, they claim that \" A is \u2019in front of\u2019 B if the bottom boundary of A\u2019s bounding box is closer to the image\u2019s bottom edge\". I don't think this is true in the case where the bounding box of B is fully contained in the bounding box of A, in which case B is in front of A. ", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper31/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper31/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Heuristics for Image Generation from Scene Graphs", "authors": ["Subarna Tripathi", "Anahita Bhiwandiwalla", "Alexei Bastidas", "Hanlin Tang"], "authorids": ["subarna.tripathi@intel.com", "anahita.bhiwandiwalla@intel.com", "alexei.bastidas@intel.com", "hanlin.tang@intel.com"], "keywords": ["scene graph", "heuristic supervision", "data augmentation", "image generation"], "abstract": "Generating realistic images from scene graphs requires neural networks to be able to reason about object relationships and compositionality. Learning a sufficiently rich representation to facilitate this reasoning is challenging due to dataset limitations. Synthetic scene graphs from COCO only have basic geometric relationships, and Visual Genome scene graphs are replete with missing relations or mislabeled nodes. Existing scene graph to image models have two stages: (1) a scene composition stage, and an (2) image generation stage. In this paper, we propose two methods to improve the intermediate representation of these stages. First, we use visual heuristics to augment relationships between pairs of objects. Second, we introduce a graph convolution-based network to generate a scene graph context representation that enriches the image generation. These contributions significantly improve the scene composition (relation score of 59.8% compared to 51.2%) and image generation (74% versus 64% in mean relation opinion score). Introspection shows that these heuristics are particularly effective in learning differentiated representations for scenes with multiple instances of the same object category. Obtaining accurate and complete scene graph annotations is costly, and our use of heuristics and prior structure to enhance intermediate representations allows our model to compensate for limited or incomplete data.", "pdf": "/pdf/9c7fd5aa6c1afea2d44a9a07df4e59a74a27a74b.pdf", "paperhash": "tripathi|heuristics_for_image_generation_from_scene_graphs"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper31/Official_Review", "cdate": 1553713416920, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1eqsNXgdV", "replyto": "r1eqsNXgdV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper31/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper31/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713416920, "tmdate": 1555511827103, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper31/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "ByxVgaYFtN", "original": null, "number": 1, "cdate": 1554779372168, "ddate": null, "tcdate": 1554779372168, "tmdate": 1555510984471, "tddate": null, "forum": "r1eqsNXgdV", "replyto": "r1eqsNXgdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper31/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept", "comment": "The authors propose two contributions a) data augmentation techniques for scene graph to image generation as well as b) a new mechanism for the scene graph to image generation that maintains context using a GCNN. \n\nPros:\n- The augmentation strategy makes sense and is reasonably illustrated \n- Improve on highly relevant problem that is not well solved or easily evaluated as the authors mentioned\n-The metareviewer and R1 appreciate the perceptual studies with amazon turk.\n-The first contribution is well in keeping with the theme of this workshop. \n\nCons:\n\t-As mentioned by R2 more details should really be included at least in the appendix. E.g. any major differences to the JJ pipeline such as size and form of the Graph CNN context rep. The authors also should cite the graphic copied from Johnson et al.\n- No ablations to show the effect of the different contributions compared to JJ (it is not completely clear whether the gain comes from the augmentation, use of the context GCNN or from architecture/other changes to JJ pipeline).\n-It should be made more clear if the scene augmentations are also used for the evaluation scene graphs and if so whether the JJ model also sees the same augmented scenes at evaluation. \n\nOverall this paper handles a very difficult and challenging problem and both contributions as well as the suggested evaluations are substantial."}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Heuristics for Image Generation from Scene Graphs", "authors": ["Subarna Tripathi", "Anahita Bhiwandiwalla", "Alexei Bastidas", "Hanlin Tang"], "authorids": ["subarna.tripathi@intel.com", "anahita.bhiwandiwalla@intel.com", "alexei.bastidas@intel.com", "hanlin.tang@intel.com"], "keywords": ["scene graph", "heuristic supervision", "data augmentation", "image generation"], "abstract": "Generating realistic images from scene graphs requires neural networks to be able to reason about object relationships and compositionality. Learning a sufficiently rich representation to facilitate this reasoning is challenging due to dataset limitations. Synthetic scene graphs from COCO only have basic geometric relationships, and Visual Genome scene graphs are replete with missing relations or mislabeled nodes. Existing scene graph to image models have two stages: (1) a scene composition stage, and an (2) image generation stage. In this paper, we propose two methods to improve the intermediate representation of these stages. First, we use visual heuristics to augment relationships between pairs of objects. Second, we introduce a graph convolution-based network to generate a scene graph context representation that enriches the image generation. These contributions significantly improve the scene composition (relation score of 59.8% compared to 51.2%) and image generation (74% versus 64% in mean relation opinion score). Introspection shows that these heuristics are particularly effective in learning differentiated representations for scenes with multiple instances of the same object category. Obtaining accurate and complete scene graph annotations is costly, and our use of heuristics and prior structure to enhance intermediate representations allows our model to compensate for limited or incomplete data.", "pdf": "/pdf/9c7fd5aa6c1afea2d44a9a07df4e59a74a27a74b.pdf", "paperhash": "tripathi|heuristics_for_image_generation_from_scene_graphs"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper31/Decision", "cdate": 1554736078393, "reply": {"forum": "r1eqsNXgdV", "replyto": "r1eqsNXgdV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736078393, "tmdate": 1555510960504, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}