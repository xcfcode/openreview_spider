{"notes": [{"id": "KCzRX9N8BIH", "original": "loCeZuWQAXL", "number": 467, "cdate": 1601308059494, "ddate": null, "tcdate": 1601308059494, "tmdate": 1614985680479, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "o2j4telPRpq", "original": null, "number": 1, "cdate": 1610040473676, "ddate": null, "tcdate": 1610040473676, "tmdate": 1610474077982, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers agree that this paper has some strengths to it, and some commented that the revision improved the manuscript, but the paper remained borderline with no strong champions in its favor. The reviews are encouraging and suggest that the paper is a bit tightly packed for the conference format, and perhaps because it is dense it is hard to the strength and scope of contributions, while other relationships such as to the Bayesian context could be explored more fully. Multiple reviewers find that a longer improved version would \"shine\" in a better suited journal. \n\nThe decision to reject is independent of the fact that the authors seem to have violated the anonymity rules in the revised version."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040473663, "tmdate": 1610474077966, "id": "ICLR.cc/2021/Conference/Paper467/-/Decision"}}}, {"id": "jYXmhcKOwiS", "original": null, "number": 2, "cdate": 1603983565004, "ddate": null, "tcdate": 1603983565004, "tmdate": 1607036426317, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Official_Review", "content": {"title": "Training hyperparameters of the loss: experimental results", "review": "### Summary\nThe authors propose to test experimentally training of the loss hyperparameters. Given a loss $L$ with a reconstruction part $\\ell$ and a penalty part $r$, they train the hyperparameters of $\\ell$ and of $r$.\n\n### Clarity\nThe main clarity issue is the reference formatting. Finding the cited papers in the references is tedious: hypertext links towards the *References* section should be added.\n\n### Details\nThe main contribution is the experimental part. The authors have tested loss hyperparameters learning mainly in robustness against outliers and outlier detection.\nI am not a specialist in outlier detection, so I cannot evaluate fairly most of the experimental results.\n\nTable 2, CRIME dataset with MSE: it seems that \"Base\" works better than \"Temp\".\n\n### Comments\nFrom a Bayesian point of view, there is no problem with learning hyperparameters in the reconstruction part $\\ell$ of the loss $L$. This is just equivalent to extending the family of probabilistic models. The user should only verify that $\\ell$ can be interpreted as the negative log of a probability distribution, that is, $\\exp(- \\ell)$ is integrable.\nHowever, training the hyperparameters of the penalty part of $L$ corresponds to training the parameters of the *prior* distribution, which is not acceptable (as such) from a Bayesian point of view.\n\nWeak accept: optimization of hyperparameters of $\\ell$ is theoretically well-founded from a Bayesian point of view, and should be more explored, as the authors do. However: 1) there is no consideration for this approach, while the preceding works of Barron (cited in the paper) had a word about it. This is important since we are still talking about a *likelihood*. 2) I am not sure whether the experimental results are significant enough. 3) The generalization of this approach to the penalty is not well-founded (at least, the authors do not justify it).\n\nEdit:\n### Rebuttal\nI had read the rebuttal and the other reviews. It seems that there are some clarity issues, which are independent from my knowledge of the area chosen for the experiments. Moreover, outlier detection is not necessarily the only possible application.\nHowever, I consider that the Bayesian point of view (which comes from preceding papers) has been well highlighted in the revised version. \nThese points put together, and given that I'm not sure of the significance of the experimental part, I do not change my rating.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper467/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper467/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142504, "tmdate": 1606915793023, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper467/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper467/-/Official_Review"}}}, {"id": "h4g2A3zgEa2", "original": null, "number": 1, "cdate": 1603904114764, "ddate": null, "tcdate": 1603904114764, "tmdate": 1606820686704, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Official_Review", "content": {"title": "An interesting idea but writing and presentation should be improved.", "review": "# Summary:\nThe paper proposes the use of complete parametrized likelihoods for providing supervision in place of the commonly used loss functions. The normal distribution, the categorical distribution defined by softmax and the likelihood of the robust rho-estimator are considered. The main idea is that by including the parameters of these likelihoods and optimizing over them, one can increase robustness, detect outliers and achieve re-calibration. In addition, by considering parametric priors and tuning their parameters one can obtain more flexible regularizers over the trainable parameters of a model.\n\n# Strengths:\nThe idea of the paper is quite interesting as it lifts some commonly made and often overlooked assumptions regarding the data distribution. By lifting these assumptions one can improve the performance of the trained model by considering likelihoods that better capture the data distribution. For example, data is usually affected by heteroskedasticity as well as outliers, and if the likelihood considered in its full form covers these aspects the resulting models will be better calibrated.\nThe proposed methods consider different aspects of conditioning and dimensionality of the likelihoods employed, varying from global to data-specific modeling.\n\nThe use of likelihoods instead of common loss functions leads to competitive new methods and variants for robust modeling, outlier detection, adaptive regularization and model re-calibration.\n\n\n# Weaknesses:\nAlthough the use of likelihoods instead of loss functions is not a common practice in deep learning, its advantages have been thoroughly studied in statistics, econometrics and other disciplines, as also discussed in the related work of the paper. Hence, the novelty mainly lies in the application of these ideas in deep learning and the employment of some likelihoods better suited for the respective problems (i.e. softmax and rho-estimators).\n\nThe paper is interesting however I found it somewhat difficult to read. In my view it tries to pack many different aspects and applications of the main idea (use of likelihood) into a very limited space. In fact, there are too many cross-references to the supplemental material, to the point that it seems that most of the paper is described in the supplemental material. \nOn a similar note, due to the fact that four different application domains are considered, there numerous methods, metrics and datasets involved in each one of them which are not sufficiently covered in the text. Additionally, many of the proposed methods/improvements/variants on each domain are not explained in sufficient detail (e.g. AE+S and PCA+S in Sec. 5.2). I would expect some more principled and thorough guidance on how to use the likelihood functions and, regarding the conditioning and dimensionality, strategies on how to choose among the various options.\n\nAlso some editing is required, for example the likelihood of the softmax is not provided as the respective sentence after eq. 4 is suddenly interrupted (see also the comments below).\n\n## Minor comments\n* the text in the figure is very small, making it very difficult to read in typical zoom levels (~100%)\n* Figure 3: the text does not correspond to the figure for the intermediate case\n* Figure 4, caption: include reference to left, middle and right panel\n* Table 1: there is no reference of this table in the text. Also, the three dots should be replaced with the actual setting.\n\n\n# Rating Justification:\nI think that the overall idea of the paper is interesting and provides improved data modeling which leads to important advantages of the estimated models. However, possibly due to space limitations, the paper does not explain in sufficient detail important aspect of applying the proposed idea in the domains considered.\n\n# Rating and comments after the rebuttal\nI think that in the revised version the paper has addressed many of the weaknesses pointed out in our reviews, hence I increase my rating to 6. Nevertheless, the paper still packs too much information which makes it difficult to read and appreciate.\nRegarding novelty, although I agree with other reviews that the core idea is not novel I think that it is important that the paper stresses the applicability and usefulness of considering likelihoods in deep learning models, as it appears to be not fully appreciated currently.\nOverall, I think that the paper would shine as a journal paper while it is only a borderline submission in its current form.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper467/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper467/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142504, "tmdate": 1606915793023, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper467/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper467/-/Official_Review"}}}, {"id": "L-ksFX6cFm1", "original": null, "number": 6, "cdate": 1605806407282, "ddate": null, "tcdate": 1605806407282, "tmdate": 1605806453729, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "h4g2A3zgEa2", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Official_Comment", "content": {"title": "Simplifications, more experimental details, and addressing your comments", "comment": "We thank the reviewer for their detailed and thoughtful feedback! Our point-by-point reply follows, and a revision has been uploaded. We tried to simplify the presentation and add extra details to help explain our experiments and design decisions. Regarding the formatting in our revision, light yellow areas represent re-worked sections and dark yellow areas are new additions and specific changes. \n\n>I found it somewhat difficult to read. In my view it tries to pack many different aspects and applications of the main idea (use of likelihood) into a very limited space.\n\nWe appreciate the feedback to focus on fewer experiments and fuller details. To this end, we have removed the robust regression content and used the space to better cover outlier detection and add experimental details for all sections. We have likewise combined robust modelling and outlier detection into a single section. We unify these two because robustness and outlier detection are two sides of the same coin: for a model to be robust it must identify and \u201cdown-weight\u201d outliers. In this light, we highlight CelebA VAE and ODDS outlier detection results, both of which apply likelihood parameters to unsupervised models. \n\n> many of the proposed methods/improvements/variants on each domain are not explained in sufficient detail (e.g. AE+S and PCA+S in Sec. 5.2)\n\nGood suggestion! We have used the extra page (for the rebuttal and camera-ready) and space we saved by removing some minor experiments to expand topics and include experimental details for all experiments. We also added an explicit experimental detail section for the whole work. We also made sure to include a paragraph to the outlier detection section to detail the experimental setup and how AE+S and PCA+S were built. We also re-worked the text to set this up better. \n\n\n> In fact, there are too many cross-references to the supplemental material, to the point that it seems that most of the paper is described in the supplemental material.\n\nWe have incorporated the supplementary content in Sections I and J on experimental details into the text for easier reading.\n\n> I would expect some more principled and thorough guidance on how to use the likelihood functions and, regarding the conditioning and dimensionality, strategies on how to choose among the various options.\n\nWe have sought to make this a bit clearer in our discussion of the results of the CelebA VAE experiment. In particular we find that model parametrized likelihoods offer a nice compromise between expressivity, speed, and simplicity. \n\nRegarding your \u201cminor Comments\u201d: Thank you for helping us spot these typos! We have fixed  all of these comments and increased the size of fonts in our figures.\n\nAgain we thank you for the thoughtful feedback and hope that our changes can improve your outlook on the work. Please let us know if you think these changes address your feedback, or if you have any additional feedback that we can use to help improve the work. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper467/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KCzRX9N8BIH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper467/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper467/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper467/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper467/Authors|ICLR.cc/2021/Conference/Paper467/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870679, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper467/-/Official_Comment"}}}, {"id": "eUmriwkUZGc", "original": null, "number": 5, "cdate": 1605806381731, "ddate": null, "tcdate": 1605806381731, "tmdate": 1605806381731, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "jYXmhcKOwiS", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Official_Comment", "content": {"title": "Response", "comment": "\nWe thank the reviewer for their detailed and thoughtful feedback! In our revision, we tried to simplify the presentation and add extra details to help explain our experiments and design decisions. Regarding the formatting in our revision, light yellow areas represent re-worked sections and dark yellow areas are new additions and specific changes.\n\nWe especially appreciate your feedback on the optimization of priors. In the section on optimizing priors, we explicitly connected this to MAP optimization on a Bayesian hierarchical prior with a uniform distribution. In this light optimization of priors is well-founded. We hope this speaks to your concerns and please let us know if you would like to see additional material on this topic.\n\nRegarding your comment on formatting our updated paper has links, but our previous paper did as well at the time of submission. We included a version of the paper in the supplemental which should retain its hyperlinks if the problem persists. \n\nAgain we thank you for the thoughtful feedback and hope that our changes can improve your outlook on the work. Please let us know if you think these changes address your feedback, or if you have any additional feedback that we can use to help improve the work. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper467/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KCzRX9N8BIH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper467/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper467/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper467/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper467/Authors|ICLR.cc/2021/Conference/Paper467/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870679, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper467/-/Official_Comment"}}}, {"id": "IhgB93mlXd", "original": null, "number": 4, "cdate": 1605806335071, "ddate": null, "tcdate": 1605806335071, "tmdate": 1605806335071, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "6Rr7kIuTiXz", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for their detailed and thoughtful feedback! In our revision we tried to simplify the presentation and add extra details to help explain our experiments and design decisions. Regarding the formatting in our revision, light yellow areas represent re-worked sections and dark yellow areas are new additions and specific changes. \n\n> I'm questioning this as simultaneous estimation scale/location parameters and target functions seems to be traditional approaches in parametric robust statistics. I'm expecting more comments in this regard.\n\nWe have added some comments on how these approaches are inspired by the simultaneous regression of means and scales. One of the aspects that we think makes this work novel is the extension of this principle to deep models, and the exploration of its diverse applications which are often overlooked. One of the main points of the work is to show that these types of parameters are neglected in the deep learning community, and even the most simple applications of these help remedy many important issues such as robustness, heteroskedasticity, regularization tuning, and calibration, and do so with minimal overhead. Moreover these approaches are architecture agnostic, and can apply to a variety of different data types and architectures. Please let us know if this comes across in the reworked submission or if you would like us to expand on these points more. \n\n>In my opinion, the three distributions are merely examples from three different scale families of distributions. I think more comments should be given when investigating the three specific ones.\n\nThank you for this feedback. We added a few comments on how the robust loss of Barron goes beyond just scale optimization and into shape optimization, ad the parameter $\\alpha$ allows this loss to generalize several other losses in the literature such as the L2 loss (\u03b1 = 2), pseudo-huber loss (Charbonnier et al., 1997)(\u03b1 = 1), Cauchy loss (Li et al., 2018) (\u03b1 = 0), Geman-McClure loss (Ganan & McClure, 1985), (\u03b1 = \u22122), and Welsch (Dennis Jr & Welsch, 1978) loss (alpha = \u2212\u221e). We also note that the L2 and Cross Entropy losses are some of the most commonly used losses and are often used without scale parameters throughout the deep learning community. We hope these comments speak to your concerns. \n\n> I would suggest that the authors should at least provide several examples by pointing out which is which.\n\nThis is a great suggestion and we have added examples for each condition and dimensionality discussed and have highlighted these in dark yellow. Thanks! \n\n>In my opinion, the presentation of the paper could be further improved. For instance, the methodologies that the authors proposed become clear to me only after section 5.3. In addition, the title of this paper seems to be inappropriate in describing the main content of the paper.\n\nThanks for this feedback, we agree that the first draft might have seemed a bit spread thin. We have tried to remedy this by simplifying and reworking a large portion of the paper (highlighted in light yellow), dropping some less impactful experiments, and adding significantly more experimental details. Also regarding \u201cfor instance, the methodologies that the authors proposed become clear to me only after section 5.3. In addition, the title of this paper seems to be inappropriate in describing the main content of the paper.\u201d We are interested in understanding what we could do to signal our approach better. Is there a particular idea or statement that you think should be brought up earlier in the work. \n\nRegarding the title, we would be open to considering other options if you feel something could describe the main idea better. Please feel free to provide any additional feedback as we want to make this work as accessible as possible\n\nAgain we thank you for the thoughtful feedback and hope that our changes can improve your outlook on the work. Please let us know if you think these changes address your feedback, or if you have any additional feedback that we can use to help improve the work. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper467/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KCzRX9N8BIH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper467/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper467/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper467/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper467/Authors|ICLR.cc/2021/Conference/Paper467/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870679, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper467/-/Official_Comment"}}}, {"id": "VD2hMcasZCz", "original": null, "number": 3, "cdate": 1605806193545, "ddate": null, "tcdate": 1605806193545, "tmdate": 1605806264861, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "tRqFrpfHLcT", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewer for their detailed and thoughtful feedback! Our point-by-point reply follows, and a revision has been uploaded. Highlights for revisions (light yellow) and additions (dark yellow) are included.\n\n> \"the paper glosses over almost every aspect: design, task, model, training, to only show final results.\"\n\nWe appreciate the feedback to focus on fewer experiments and fuller details. To this end, we have removed the robust regression content and used the space to better cover outlier detection and add experimental details for all sections. We have likewise combined robust modelling and outlier detection into a single section. We unify these two because robustness and outlier detection are two sides of the same coin: for a model to be robust it must identify and \u201cdown-weight\u201d outliers. In this light, we highlight CelebA VAE and ODDS outlier detection results, both of which apply likelihood parameters to unsupervised models.\n\n\n> \"(I did not consult the supplementary material.)\"\n\nWe have incorporated the supplementary content in Sections I and J on experimental details into the text for easier reading.\n\n> \"what is your data-dependent loss for the \"predicted\" case\"\n\nThe data dependant loss has the same basic form as  the other experimental settings (robust regression loss $\\rho(x, \\sigma, \\alpha)$ except that the $\\sigma$ and $\\alpha$ are now predicted by the model. This allows the shape and scale of the loss to vary with each datapoint according to the learned model. (Note this contrasts with data parameters, which assign distinct likelihood parameters to each datapoint nonparametrically.) We have revised the text to clarify this.\n\n> \"What is meant by \"optimizing a model's prior distribution\" sec5.3?\"\n\nIn addition to optimizing the shape and scale of the likelihood distribution of the model output, we can use the same approach to optimize the prior distribution of the model parameters. We have revised the text with this phrasing. Sec. 5.3 is indeed about MAP optimization where the prior distribution parameters (Gaussian variances, Laplace scales) are jointly optimized. In particular this is equivalent to MAP inference of a hierarchical prior and we have included this in our revision.\n \n> \"The paper should make connections to maximum likelihood and maximum a posteriori optimization more apparent. I'm further curious about connections to Bayes' risk (eg cf Minka 2001 \"ERM is an incomplete inductive principle\") and KL/Bregman divergences (eg Buja, Stuetzle, Shen 2005)\"\n\nWe added a note on this in the adaptive prior section. Furthermore because this approach reduces to MAP optimization the connections to ERM and KL divergence minimization are exactly those that relate this approach to MAP inference.\n\n> \"Other losses (eg hinge, Huber loss) [...] can be given probabilistic interpretations, what motivates their choice?\"\n\nWe chose softmax, squared error, and the robust loss rho because the first two are ubiquitous and the third has recently shown strong results for a variety of tasks. The third loss also generalizes many other losses used throughout the literature. Our VAE experiments (Table 1) show that there is further advantage to experimenting with different forms of the likelihood parameters (the rho loss paper only tried global parameters).\nWe added sections in the background on why we chose these distributions. In particular we are interested in the robust loss because it naturally generalizes many common robust loss functions  with only a single extra parameter. This robust loss, \u03c1, has the interesting property that it generalizes several different loss functions commonly used in robust learning such as the L2 loss (\u03b1 = 2), pseudo-huber loss (Charbonnier et al., 1997)(\u03b1 = 1), Cauchy loss (Li et al., 2018) (\u03b1 = 0), Geman-McClure loss (Ganan & McClure, 1985), (\u03b1 = \u22122), and Welsch (Dennis Jr & Welsch, 1978) loss (alpha = \u2212\u221e). It also serves as an example of a setup where we can optimize over shape and not just scale.\n\n> \"it seems to be \"bursting at the seams\"\"...\"it could be made up to 10 pages long\"\n\nPlease note the page limit was reduced to 8 this year\u2014for rebuttal this has been raised to 9 and we have made use of this in the uploaded revision. We have reduced the number of experimental settings and explained these settings in greater detail in order to make this feel more complete. \n\n\n> fig 2 and 3 are cryptic\n> Table 1 is not referenced\n> Table 2, why are we speaking of temperature\n\nThank you for catching the Figure 2 typo and missing reference for Table 1. We have fixed both, added more explanatory text, and revised captions accordingly.\n\n\nAgain we thank you for the thoughtful feedback and hope that our changes can improve your outlook on the work. Please let us know if you think these changes address your feedback, or if you have any additional feedback that we can use to help improve the work. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper467/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KCzRX9N8BIH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper467/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper467/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper467/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper467/Authors|ICLR.cc/2021/Conference/Paper467/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870679, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper467/-/Official_Comment"}}}, {"id": "6Rr7kIuTiXz", "original": null, "number": 3, "cdate": 1603989315029, "ddate": null, "tcdate": 1603989315029, "tmdate": 1605024682798, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Official_Review", "content": {"title": "This paper investigates loss functions from a likehood viewpoint and propose to optimize full likelihoods for learning purposes.", "review": "This paper studied loss functions by interpreting them from a likelihood viewpoint and by proposing to optimize \"full\" likelihoods for robust modeling, outlier-detection, and re-calibration purposes. Many loss functions stem from maximum likelihood estimation (MLE). For instance, the quadratic loss stems from MLE under Gaussianity, the absolute deviation loss stems from MLE under the Laplace noise assumption, and the check loss from MLE under the asymmetry Laplace noise assumption. What is common about these noise assumptions is that they contain scale/location parameters. Traditionally, in the machine learning community, these loss functions are used by ignoring the underlying noise assumptions and so the scale parameters are abandoned. The point of the paper is to take into account these scale parameters when solving the resulting optimization problems in learning.    \n\n\nPros:\nThe problem studied in this paper is interesting. By investigating the full likelihood when applying loss functions, it does bring back our attention to the origin of loss functions. The three applications mentioned in this paper are also typical and important.   \n\nCons: \n\n1. My main concern is about the novelty of this paper. As mentioned above, I believe that it does bring back our attention to the origin of loss functions. However, given that the main point of this paper is to optimize full likelihoods for learning problems, it is not clear to me what are the real novel contributions made by the paper. I'm questioning this as simultaneous estimation scale/location parameters and target functions seems to be traditional approaches in parametric robust statistics. I'm expecting more comments in this regard.\n\n2. The authors proposed to investigated three distributions, namely, Gaussian, softmax, and the distribution from Barron (2019). In my opinion, the three distributions are merely examples from three different scale families of distributions. I think more comments should be given when investigating the three specific ones. \n\n3. Three types of parameters are mentioned, namely, global parameters, data parameters, and predicted parameters, which are used to categorize distributions. I would suggest that the authors should at least provide several examples by pointing out which is which. \n\n4. In my opinion, the presentation of the paper could be further improved. For instance, the methodologies that the authors proposed become clear to me only after section 5.3. In addition, the title of this paper seems to be inappropriate in describing the main content of the paper. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper467/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper467/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142504, "tmdate": 1606915793023, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper467/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper467/-/Official_Review"}}}, {"id": "tRqFrpfHLcT", "original": null, "number": 4, "cdate": 1604268792864, "ddate": null, "tcdate": 1604268792864, "tmdate": 1605024682720, "tddate": null, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "invitation": "ICLR.cc/2021/Conference/Paper467/-/Official_Review", "content": {"title": "Work seems very interesting, but paper glosses over too many important points", "review": "This paper's starting point is a probabilistic interpretation of three losses: MSE, cross-entropy, and the loss introduced in reference (Barron 2019). It proposes the minimize the average loss jointly over the predictor's parameters $\\theta$, but also the loss' own parameters $\\phi$, vary, yielding an adaptive loss. It categorizes ways to define losses according to whether they depend on the data value, on the data index but not its value, or are independent of the data. The jointly optimized loss yields superior performance as re-calibrators (table 2 column CAL for regressors and table 5 for classifiers). It allows a natural definition of outlier detectors (sec  5.2)\n\nThe paper looks correct, specifically sec1 the approach of interpreting losses as NLL, sec4 the categorization, the experiment setup (though the description in the paper is cursory), the experiment design and use of CAL and ECE in sec5.4 on recalibration. The competitor methods and benchmarks for recalibration and outlier detection are quite exhaustive. (I am familiar with the references in section 3 related work)\n\nThe claims of superiority made in sec5.1, 5.2, 5.4 seem supported by the experimental evidence.\n\nDespite these strengths, I had a hard time evaluating and understanding the experiments, and partly the concrete details of the mathematical setup. For experiments, this happened because the paper glosses over almost every aspect: design, task, model, training, to only show final results. (I did not consult the supplementary material.)  For instance fig 2 and 3 are cryptic (in the caption of fig 2, I guess \"model\" should be replaced by \"predicted\"? and fig 3 relates to either predicted or data conditionings?). Table 1 is not referenced or explained in the text; one can assume it relates to the CelebA experiment from sec 5.1, but what is the form of the data-dependent loss for the \"predicted\" case? Table 2, why are we speaking of temperature (which only is a parameter for softmax loss in classification tasks) in the case of this regression experiment? Sec5.2, there is no experimental detail of the SVHN (and results are not discussed) and ODDS experiment.\n\nThe paper should make connections to maximum likelihood and maximum a posteriori optimization more apparent. I'm further curious about connections to Bayes' risk (eg cf Minka 2001 \"ERM is an incomplete inductive principle\") and KL/Bregman divergences (eg Buja, Stuetzle, Shen 2005). What is meant by \"optimizing a model's prior distribution\" sec5.3 ? Does sec5.3 amount to MAP optimization with some suitably defined prior? Other losses (eg hinge, Huber loss) than the three mentioned here can be given probabilistic interpretations, what motivates their choice (especially the choice of the recent, hence little used, robust loss $\\rho$) ?\n\nI found it difficult to settle on a rating for this paper. The work behind it definitely seems to have strenghts, for instance, it looks interesting, and I might want to point it out to colleagues, it investigates an interesting, not yet mainstream probabilistic interpretation in a relatively simple way, with few ad hoc assumptions. Yet the paper on its own is lacking in detail so much that the experiments are impossible to understand fully\nAs a conference paper, it seems to be \"bursting at the seams\"; maybe it would be better suited as a journal paper, where the important aspects can be developed. According to ICLR guidelines, it could be made up to 10 pages long; sec2 could be made terser. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper467/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper467/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "It Is Likely That Your Loss Should be a Likelihood", "authorids": ["~Mark_Hamilton1", "~Evan_Shelhamer2", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Evan Shelhamer", "William T. Freeman"], "keywords": ["Adaptive Losses", "Outlier Detection", "Adaptive Regularization", "Recalibration", "Robust Modelling"], "abstract": "Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these ``likelihood parameters'' with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.", "one-sentence_summary": "Learning additional likelihood distribution parameters yields new approaches for robust modelling, outlier-detection, calibration, and adaptive regularization.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hamilton|it_is_likely_that_your_loss_should_be_a_likelihood", "supplementary_material": "/attachment/81cd7e1d0a067d21e29d40d2e53a3ce1c0938323.zip", "pdf": "/pdf/c9537210f4dcf4367c8a6a41d85d12b5ba7bc331.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZIozHUOtf4", "_bibtex": "@misc{\nhamilton2021it,\ntitle={It Is Likely That Your Loss Should be a Likelihood},\nauthor={Mark Hamilton and Evan Shelhamer and William T. Freeman},\nyear={2021},\nurl={https://openreview.net/forum?id=KCzRX9N8BIH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KCzRX9N8BIH", "replyto": "KCzRX9N8BIH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper467/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142504, "tmdate": 1606915793023, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper467/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper467/-/Official_Review"}}}], "count": 10}