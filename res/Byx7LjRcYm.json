{"notes": [{"id": "Byx7LjRcYm", "original": "SygH1LW9YX", "number": 160, "cdate": 1538087754944, "ddate": null, "tcdate": 1538087754944, "tmdate": 1545355398983, "tddate": null, "forum": "Byx7LjRcYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Human Action Recognition Based on Spatial-Temporal Attention", "abstract": "Many state-of-the-art methods of recognizing human action are based on attention mechanism, which shows the importance of attention mechanism in action recognition. With the rapid development of neural networks, human action recognition has been achieved great improvement by using convolutional neural networks (CNN) or recurrent neural networks (RNN). In this paper, we propose a model based on spatial-temporal attention weighted LSTM. This model pays attention to the key part in each video frame, and also focuses on the important frames in each video sequence, thus the most important theme for our model is how to find out the key point spatially and the key frames temporally. We show a feasible architecture which can solve those two problems effectively and achieve a satisfactory result. Our model is trained and tested on three datasets including UCF-11, UCF-101, and HMDB51. Those results demonstrate a high performance of our model in human action recognition.", "authorids": ["2489925838@qq.com", "zhiqiangtian@xjtu.edu.cn", "xglan@xjtu.edu.cn"], "authors": ["Wensong Chan", "Zhiqiang Tian", "Xuguang Lan"], "keywords": [], "pdf": "/pdf/a24113535c32a25578b1449c4a53a9404dbc2978.pdf", "paperhash": "chan|human_action_recognition_based_on_spatialtemporal_attention", "_bibtex": "@misc{\nchan2019human,\ntitle={Human Action Recognition Based on Spatial-Temporal Attention},\nauthor={Wensong Chan and Zhiqiang Tian and Xuguang Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx7LjRcYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Byx-KBrxg4", "original": null, "number": 1, "cdate": 1544734073460, "ddate": null, "tcdate": 1544734073460, "tmdate": 1545354513263, "tddate": null, "forum": "Byx7LjRcYm", "replyto": "Byx7LjRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper160/Meta_Review", "content": {"metareview": "Average score of 3.33, highest score of 4.\nThe AC recommends rejection.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Strong agreement for rejection"}, "signatures": ["ICLR.cc/2019/Conference/Paper160/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper160/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Human Action Recognition Based on Spatial-Temporal Attention", "abstract": "Many state-of-the-art methods of recognizing human action are based on attention mechanism, which shows the importance of attention mechanism in action recognition. With the rapid development of neural networks, human action recognition has been achieved great improvement by using convolutional neural networks (CNN) or recurrent neural networks (RNN). In this paper, we propose a model based on spatial-temporal attention weighted LSTM. This model pays attention to the key part in each video frame, and also focuses on the important frames in each video sequence, thus the most important theme for our model is how to find out the key point spatially and the key frames temporally. We show a feasible architecture which can solve those two problems effectively and achieve a satisfactory result. Our model is trained and tested on three datasets including UCF-11, UCF-101, and HMDB51. Those results demonstrate a high performance of our model in human action recognition.", "authorids": ["2489925838@qq.com", "zhiqiangtian@xjtu.edu.cn", "xglan@xjtu.edu.cn"], "authors": ["Wensong Chan", "Zhiqiang Tian", "Xuguang Lan"], "keywords": [], "pdf": "/pdf/a24113535c32a25578b1449c4a53a9404dbc2978.pdf", "paperhash": "chan|human_action_recognition_based_on_spatialtemporal_attention", "_bibtex": "@misc{\nchan2019human,\ntitle={Human Action Recognition Based on Spatial-Temporal Attention},\nauthor={Wensong Chan and Zhiqiang Tian and Xuguang Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx7LjRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper160/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352779345, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byx7LjRcYm", "replyto": "Byx7LjRcYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper160/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper160/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper160/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352779345}}}, {"id": "HyxmR-Rhnm", "original": null, "number": 3, "cdate": 1541362123077, "ddate": null, "tcdate": 1541362123077, "tmdate": 1541533004134, "tddate": null, "forum": "Byx7LjRcYm", "replyto": "Byx7LjRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper160/Official_Review", "content": {"title": "Limited novelty and missing important experiments and comparisons", "review": "# 1. Summary\nThis paper presents a spatio-temporal attention LSTM for action recognition, where attention decides which pixels and frames are more important for classification. ConvNet features are extracted, a first layer of attention looks at the pixel level, then a second layer is applied at the temporal level. An LSTM is used to connect frame representation through time.\n\nWeaknesses:\n* The paper do not present substantial novelty compared to previous work. In fact, it has a strong overlap with (Song et al., 2017) (see #3)\n* Some modeling choices are not well motivated (see #2)\n* Ablation study showing that each modeling decision are motivated from a practical perspective is missing (see #4)\n* The paper fails in comparing with relevant papers (see #4)\n\n\n# 2. Clarity and Motivation\n* Page 1 \u201cmany new deep learning methods of action recognition would use iDT as one part of their networks to optimize their models \u201d: this statement is not clear, please provide references of methods that do this. To my knowledge iDTs are part of the input of a ConvNet or used as complementary feature to other networks (e.g., C3D, I3D, \u2026).\n* Page 1, \u201cTwo-stream CNN (Feichtenhofer et al., 2016b)\u201d: The reference might not really accurate. The citation should be: Two-Stream Convolutional Networks for Action Recognition in Videos Karen Simonyan, Andrew Zisserman.\n*Page 1, \u201cThe pure RNN-based models are usually used on skeleton data\u201c: it is not clear right away why the authors discuss some paper about skeleton data since it is not an application studied in this paper.\n* Page 3, Sec. 2.2, \u201c\\alpha_t is a matrix\u201d: why is it a matrix? It seems that it is a vector of length 196.\n* By reshaping the features as Fig. 2, you loose the spatial consistency between neighbour pixels. How does Eq 2 deals with this? It seems that a better approach will be to have convolutions instead of fully-connected layers in Eq. 2. Have the authors considered this option?\n* In neural machine translation models, usually the weights are normalized with a softmax before the weighted average of Eq. 3 and 6. It seems that here the alphas are normalized but the betas are not before Eq. 6. Any explanation about this?\n* My comment above seems that is also related with the need of the regularisation term in Eq. 8. Probably it is not really needed in case that the betas are also normalized.\n* A discussion is missing why the two attention models (spatial and temporal) are different. In principle, one could adopt the same kind of attention model for both the spatial and temporal component. One reason to have different models would be to consider the spatial relations between neighbour features in a frame (which is not the case of this model, as I highlighted above)\n* Page 6, \u201cThe second term is applied to force the model to pay more spatial attention on more relevant parts in the frame automatically [\u2026]. The third term is used to restrict the unlimited increasing of temporal attention\u201c: this sentence is a bit unclear. More details and intuition about how the 2nd and 3rd term work would be appreciated.\n\n\n# 3. Novelty\nFrom a methodological point of view the paper is not novel enough. It seems that the model is a combination between of the soft attention mechanism (Xu et al., 2015) (Sharma et al., 2016) and the temporal attention mechanism (Song et al., 2017). The overlap with the latter paper is substantial. From the application point of view, there is also little novelty given that the paper is tested on action recognition using relatively-small datasets.\n\n\n# 4. Experimentation\nSince this paper is an application paper, and there is not much novelty about the model, one would expect a comprehensive set of experiments. \n* Ablation study is missing. Looking at the model in Fig. 3, it seems that not all the components are required. Some questions should be: \n** Since attention is already working on the temporal domain, why do we need an LSTM model which seems redundant? \n** What is the impact of removing the first layer of attention (\\alpha)? And the temporal one (\\beta)?\n* The selected datasets are bit small scale. It would have been nice to see some results with bigger and more challenging datasets, such as Kinetics or similar.\n* The paper fails in comparing with relevant papers (table 1). The topic of action recognition is widely explored, specifically for the datasets used in this paper. References and comparison numbers can be found here, for example: http://www.actionrecognition.net/files/dsetdetail.php?did=6;  and   http://www.actionrecognition.net/files/dsetdetail.php?did=5;\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper160/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Human Action Recognition Based on Spatial-Temporal Attention", "abstract": "Many state-of-the-art methods of recognizing human action are based on attention mechanism, which shows the importance of attention mechanism in action recognition. With the rapid development of neural networks, human action recognition has been achieved great improvement by using convolutional neural networks (CNN) or recurrent neural networks (RNN). In this paper, we propose a model based on spatial-temporal attention weighted LSTM. This model pays attention to the key part in each video frame, and also focuses on the important frames in each video sequence, thus the most important theme for our model is how to find out the key point spatially and the key frames temporally. We show a feasible architecture which can solve those two problems effectively and achieve a satisfactory result. Our model is trained and tested on three datasets including UCF-11, UCF-101, and HMDB51. Those results demonstrate a high performance of our model in human action recognition.", "authorids": ["2489925838@qq.com", "zhiqiangtian@xjtu.edu.cn", "xglan@xjtu.edu.cn"], "authors": ["Wensong Chan", "Zhiqiang Tian", "Xuguang Lan"], "keywords": [], "pdf": "/pdf/a24113535c32a25578b1449c4a53a9404dbc2978.pdf", "paperhash": "chan|human_action_recognition_based_on_spatialtemporal_attention", "_bibtex": "@misc{\nchan2019human,\ntitle={Human Action Recognition Based on Spatial-Temporal Attention},\nauthor={Wensong Chan and Zhiqiang Tian and Xuguang Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx7LjRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper160/Official_Review", "cdate": 1542234195785, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byx7LjRcYm", "replyto": "Byx7LjRcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper160/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335980562, "tmdate": 1552335980562, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper160/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryxVqKQ52m", "original": null, "number": 2, "cdate": 1541187979860, "ddate": null, "tcdate": 1541187979860, "tmdate": 1541533003866, "tddate": null, "forum": "Byx7LjRcYm", "replyto": "Byx7LjRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper160/Official_Review", "content": {"title": "Spatio-temporal attention weighted LSTM for action recognition is proposed. The novelty is low and the empirical evaluations are limited.", "review": "Summary:\nThe paper proposes a spatio-temporal attention weighting mechanism in LSTM, applied to the task of human action recognition. VGG19 based frame features are fed to LSTM, soft attention is calculated based on previous works and temporal attention is predicted using another small neural network. The features are weighted by these attentions and eventually the network is trained with a regularized cross entropy loss. Empirical results are given on three datasets for action recognition, UCF11, UCF101 and HMDB51.\n\nPositives:\n- The problem addressed is a relevant and challenging CV problem\n- The idea of using of spatio-temporal attention is also interesting, as the actions are expected to have salient parts relatively sparsely located in space and time and focusing on them seems like an interesting direction to investigate.\n\nNegatives:\n- The paper is not well written in general\n- The novelty is low as similar attention mechanisms have been used before. Papers have been cited in related works but differentiation in terms of what the current method adds is largely missing. The spatial attention is borrowed from Xu et al. (2015) and Sharma et al. (2016) and the temporal attention is relatively simple (similar ideas have been explored with CNNs as well eg. [A,B]) so the exact contribution and it's novelty is not convincing\n- The results are not very convincing either, UCF11 is a very small dataset, on the bigger datasets the improvements over Video LSTM are small\n- Self implemented baseline (the current implementation with same base CNN and LSTM networks without any spatial or temporal attention, \\alpha=\\beta=1 fixed) as well as ablation studies (what happens when only spatial or temporal attentions are used) should be added for assessing the contribution of the different components\n- Some actual qualitative results should be added demonstrating the effectiveness of the proposed approach\n\n[A] Kar et al., AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos, CVPR 17\n[B] Bilen et al., Action Recognition with Dynamic Image Networks, accepted for TPAMI 2018, arxiv 1612.00738\n\nI feel that in the current form the paper is not ready for publication.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper160/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Human Action Recognition Based on Spatial-Temporal Attention", "abstract": "Many state-of-the-art methods of recognizing human action are based on attention mechanism, which shows the importance of attention mechanism in action recognition. With the rapid development of neural networks, human action recognition has been achieved great improvement by using convolutional neural networks (CNN) or recurrent neural networks (RNN). In this paper, we propose a model based on spatial-temporal attention weighted LSTM. This model pays attention to the key part in each video frame, and also focuses on the important frames in each video sequence, thus the most important theme for our model is how to find out the key point spatially and the key frames temporally. We show a feasible architecture which can solve those two problems effectively and achieve a satisfactory result. Our model is trained and tested on three datasets including UCF-11, UCF-101, and HMDB51. Those results demonstrate a high performance of our model in human action recognition.", "authorids": ["2489925838@qq.com", "zhiqiangtian@xjtu.edu.cn", "xglan@xjtu.edu.cn"], "authors": ["Wensong Chan", "Zhiqiang Tian", "Xuguang Lan"], "keywords": [], "pdf": "/pdf/a24113535c32a25578b1449c4a53a9404dbc2978.pdf", "paperhash": "chan|human_action_recognition_based_on_spatialtemporal_attention", "_bibtex": "@misc{\nchan2019human,\ntitle={Human Action Recognition Based on Spatial-Temporal Attention},\nauthor={Wensong Chan and Zhiqiang Tian and Xuguang Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx7LjRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper160/Official_Review", "cdate": 1542234195785, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byx7LjRcYm", "replyto": "Byx7LjRcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper160/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335980562, "tmdate": 1552335980562, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper160/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJgAvo_t3Q", "original": null, "number": 1, "cdate": 1541143398415, "ddate": null, "tcdate": 1541143398415, "tmdate": 1541533003655, "tddate": null, "forum": "Byx7LjRcYm", "replyto": "Byx7LjRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper160/Official_Review", "content": {"title": "Interesting results, but the paper is not matured enough", "review": "The paper propose an end-to-end combination of spatial and temporal attention for videos. The method first extracts a vgg19 representation to any frame, reduced with spatial soft-attention.  The attended vectors are then fed to LSTM with a soft temporal attention.\n\nStrengths: \n\nThe problem of applying both temporal and spatial attention is important and challenging in general.\n\nThe reported numbers on HMDB51 and UCF101 are impressive, given the fact the authors only used rgb features. (Hope I haven't missed anything)\n\n\nWeaknesses:\n\nRecent datasets for action recognition, e.g., Moments in time, Charades, Youtube-8M etc, are missing. If the authors can show this model on any of these this will make their case stronger. I suspect the proposed model is limited to short-videos only,  keeping the spatial information means the features dimensions are increased by factor of 49. This is why Charades dataset can be very interesting to see, because the videos are longer. But also Moments in time, which is much larger.\n\nThe writeup should improve significantly: Grammar mistakes, typos e.g.,  donates/denotes, operations in equations (eq. 5,6), punctuation after equations. etc..\n\nEven though the model is basic, It was really hard to follow. For instance, I couldn\u2019t follow the whole discussion about T classifications, and the voting. What exactly are we classify? Another example, eq5, \\beta_t is not an actual attention score, but the energy potential that we later use to calculate the attention in eq7.\n\nQualitative evaluation is barely there, one sample is not convincing enough, qualitative evaluation in vision-models should show many-many examples.  Fig 2,3. describe well-known techniques, I think it's better to add more examples instead.\n\nTo conclude:\nThis is important subject, but the paper is not matured enough. A better writeup, and evaluation on more recent dataset is necessary. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper160/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Human Action Recognition Based on Spatial-Temporal Attention", "abstract": "Many state-of-the-art methods of recognizing human action are based on attention mechanism, which shows the importance of attention mechanism in action recognition. With the rapid development of neural networks, human action recognition has been achieved great improvement by using convolutional neural networks (CNN) or recurrent neural networks (RNN). In this paper, we propose a model based on spatial-temporal attention weighted LSTM. This model pays attention to the key part in each video frame, and also focuses on the important frames in each video sequence, thus the most important theme for our model is how to find out the key point spatially and the key frames temporally. We show a feasible architecture which can solve those two problems effectively and achieve a satisfactory result. Our model is trained and tested on three datasets including UCF-11, UCF-101, and HMDB51. Those results demonstrate a high performance of our model in human action recognition.", "authorids": ["2489925838@qq.com", "zhiqiangtian@xjtu.edu.cn", "xglan@xjtu.edu.cn"], "authors": ["Wensong Chan", "Zhiqiang Tian", "Xuguang Lan"], "keywords": [], "pdf": "/pdf/a24113535c32a25578b1449c4a53a9404dbc2978.pdf", "paperhash": "chan|human_action_recognition_based_on_spatialtemporal_attention", "_bibtex": "@misc{\nchan2019human,\ntitle={Human Action Recognition Based on Spatial-Temporal Attention},\nauthor={Wensong Chan and Zhiqiang Tian and Xuguang Lan},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx7LjRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper160/Official_Review", "cdate": 1542234195785, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byx7LjRcYm", "replyto": "Byx7LjRcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper160/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335980562, "tmdate": 1552335980562, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper160/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}