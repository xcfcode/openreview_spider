{"notes": [{"id": "h2EbJ4_wMVq", "original": "IDzy4nZwVfP", "number": 2667, "cdate": 1601308295460, "ddate": null, "tcdate": 1601308295460, "tmdate": 1615931846088, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "NjuVN1_KUWj", "original": null, "number": 1, "cdate": 1610040472502, "ddate": null, "tcdate": 1610040472502, "tmdate": 1610474076633, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": "h2EbJ4_wMVq", "invitation": "ICLR.cc/2021/Conference/Paper2667/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This work describes a system for collaborative learning in which several agents holding data want to improve their models by asking other agents to label their points. The system preserves confidentiality of queries using MPC and also throws in differentially private aggregation of labels (taken from the PATE framework). It provides expriments showing computational feasibility of the system. The techniques use active learning to improve the models.\n\nOverall the ingredients are fairly standard but are put together in a new (to the best of my , admittedly limited, knowledge of this area). This seems like a solid attempt to explore approaches for learning in a federated setting with strong limitations on data sharing."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "tags": [], "invitation": {"reply": {"forum": "h2EbJ4_wMVq", "replyto": "h2EbJ4_wMVq", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040472489, "tmdate": 1610474076618, "id": "ICLR.cc/2021/Conference/Paper2667/-/Decision"}}}, {"id": "kEyCnUFpg9k", "original": null, "number": 6, "cdate": 1605813730660, "ddate": null, "tcdate": 1605813730660, "tmdate": 1605813886226, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": "wgveyYLPxk", "invitation": "ICLR.cc/2021/Conference/Paper2667/-/Official_Comment", "content": {"title": "Related work & appendices", "comment": "Thank you for your feedback, we uploaded a new PDF that discusses the papers mentioned in your review. \n\nWe find the InstaHide work interesting. We now clarify in our paper that we achieve confidentiality at test time. Instead, InstaHide modifies the model during training and is thus orthogonal to our approach. The two could even be combined. In addition to the guarantees of confidentiality provided by the use of cryptographic primitives, our protocol also provides guarantees of differential privacy through the noisy argmax mechanism. Confidentiality and differential privacy are orthogonal and complement one another. \n\nThank you for bringing the paper [1] to our attention. We now outline in our revised manuscript the difference between our work and [1]. In our work, differential privacy guarantees are obtained by adding noise to the histogram of aggregated one-hot-encoding vectors predicted by each answering party (i.e., the noisy argmax mechanism) whereas the approach in [1] obtains these guarantees by pruning an internal layer of a neural network. This means that new analysis is required for each layer type being pruned. For instance, Theorem 3.2 in [1] is stated for a fully-connected neural network. Instead, our approach is applicable to any architecture (e.g., our experiments also use fully convolutional neural networks) and furthermore no changes need to be made to the way the architecture is trained.\n\nRegarding appendices B, C, and D, we wanted to make sure that the paper is understandable to a wide audience of people, e.g., those with only a machine learning background. Thank you for your close attention to their usage. In our updated manuscript, we have added additional pointers to these appendices to make better use of them and clearly delineate them from Appendix E. The following are our updates:\n\na) Although appendices C and D do not pertain to our result in Appendix E, they provide background on active learning and fairness that we use in our experimental evaluation (Sections 4.2 and 4.3.2, and Appendix G). We have added forward pointers to Appendix C and D in section 4.3.2 which comprises the bulk of our exposition into fairness and active learning within the CaPC setting. We also use these definitions in our updated discussion (see Section 5). \n\nb) We have added forward pointers to Appendix B in Sections 2.2, 3.3, and 4.4, referring readers for more details on differential privacy, PATE, and the calculation of epsilon, respectively.\n\n[1] Yangsibo Huang, Yushan Su, Sachin Ravi, Zhao Song, Sanjeev Arora, Kai Li. \"Privacy-preserving Learning via Deep Net Pruning\" https://arxiv.org/abs/2003.01876 \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2667/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "h2EbJ4_wMVq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2667/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2667/Authors|ICLR.cc/2021/Conference/Paper2667/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845704, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2667/-/Official_Comment"}}}, {"id": "yGDQx-JWW3a", "original": null, "number": 1, "cdate": 1603075101562, "ddate": null, "tcdate": 1603075101562, "tmdate": 1605651639651, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": "h2EbJ4_wMVq", "invitation": "ICLR.cc/2021/Conference/Paper2667/-/Official_Review", "content": {"title": "CaPC Review. Decision: Accept.", "review": "This paper works on the problem of collaborative learning while preserving both confidentiality and privacy of the data points. It combines techniques from secure multi-party computation and differential privacy for the same, and improves on confidential inference and PATE in the process. The new technique is called CaPC. Finally, it states empirical results as evidence for the improved accuracy.\n\nWeakness:\n1. The evaluation is done on just two datasets. So, it is a little hard to judge whether the techniques would generalise or not.\n2. The writing of the paper itself is not that great because it is difficult to understand the low level details of the experiments.\n3. They talk very little about improving on the fairness guarantees.\n\nStrengths:\n1. Their techniques enable collaborative learning even in settings where the local architectures of different parties are different.\n2. The algorithms they provide improve on fairness.\n3. Their empirical results are better than the previously known methods.\n\nEvaluation: I believe the combination of secure multi-party computation and differential privacy is not totally new, but since it yields decent results, I would say that the paper deserves a chance to be accepted.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2667/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "h2EbJ4_wMVq", "replyto": "h2EbJ4_wMVq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2667/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091111, "tmdate": 1606915792692, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2667/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2667/-/Official_Review"}}}, {"id": "QS-CxVM6Npv", "original": null, "number": 4, "cdate": 1605648843140, "ddate": null, "tcdate": 1605648843140, "tmdate": 1605648843140, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": "kTbK9Wf75W", "invitation": "ICLR.cc/2021/Conference/Paper2667/-/Official_Comment", "content": {"title": "Perfect secret sharing and renumbered steps of the protocol", "comment": "We have updated the implementation of CaPC to use secret sharing over a prime-order field to achieve perfect secret sharing. It was made possible by using underlying properties of the CKKS encryption scheme: although the ciphertext operations are rational, the underlying ciphertext space is still a field. Our fix only needs to use an appropriate secret sharing and does not involve any extra cryptographic step. As a result, it does not have any impact on the speed or the accuracy of our implementation.\n\nFollowing your suggestion that \u201cit would be easier to follow if steps 1-3 were combined in the description\u201d, we changed the protocol and the steps from 1 to 3 are now numbered as 1a, 1b, 1c, where we obtain the one-hot encoded results (in step 1). The remaining (previous) steps 4 and 5 are now denoted as 2 and 3. Now, a reader can understand the overall protocol without delving into the details of steps 1a, 1b, and 1c."}, "signatures": ["ICLR.cc/2021/Conference/Paper2667/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "h2EbJ4_wMVq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2667/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2667/Authors|ICLR.cc/2021/Conference/Paper2667/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845704, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2667/-/Official_Comment"}}}, {"id": "WcDh1RGvLW3", "original": null, "number": 3, "cdate": 1605309316563, "ddate": null, "tcdate": 1605309316563, "tmdate": 1605309316563, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": "yGDQx-JWW3a", "invitation": "ICLR.cc/2021/Conference/Paper2667/-/Official_Comment", "content": {"title": "Added results for the MNIST dataset and more information on experimental details as well as on fairness", "comment": "Thank you for your careful analysis of our paper. \n\nRegarding weakness #1: To confirm that our approach generalises, we added results for the MNIST dataset (please see Section F.1 and Figures 8, 10, and 12). \n\nRegarding weakness #2: We elaborated on the low level details of the experiments in Appendices F and G. We added more information about the setup in the new version of the paper in Section 4.2. If there is something else that you would like us to add, let us know and we will be happy to include additional details. We would also like to indicate that our source code was released along with our initial submission to facilitate reproducibility of our results. \n\nRegarding weakness #3: We also expanded our discussion on fairness, including more background on techniques for improving fairness and more description on how these can be incorporated into CaPC (see Sections 4.3.2 and 5).\n\nRegarding the overall evaluation: though we agree that neither multi-party computation and differential privacy are new, our work is the first to combine the two to ensure both confidentiality (of the test inputs and model parameters) and privacy (of the training data) in a collaborative learning setting where (a) the parties each trained heterogeneous architectures and (b) there are few parties participating in the protocol. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2667/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "h2EbJ4_wMVq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2667/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2667/Authors|ICLR.cc/2021/Conference/Paper2667/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845704, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2667/-/Official_Comment"}}}, {"id": "kTbK9Wf75W", "original": null, "number": 2, "cdate": 1605308936446, "ddate": null, "tcdate": 1605308936446, "tmdate": 1605308936446, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": "kBZmdWip1j9", "invitation": "ICLR.cc/2021/Conference/Paper2667/-/Official_Comment", "content": {"title": "CaPC protocol & statistical security", "comment": "We thank the reviewer for the insightful feedback. \n\nRegarding statistical security, our protocol design supports high statistical security. The low statistical security in our current implementation is a consequence of the HE-transformer not supporting label predictions that forces us to operate over logits, which are unbounded as you indicate.\n\nThe secret sharing step of our implementation can achieve perfect security by using properties of the underlying fully homomorphic encryption scheme. We have confirmed this with the authors of the HE-transformer and are working to integrate this into our implementation. We estimate that the fix will not incur any slowdown or error to our implementation.\n\nIndeed, steps 2 and 3 of the protocol could be incorporated inside a private inference library. To make our protocol flexible to any private inference library, not just those that return the label predicted by the model (some only return logits, e.g., the HE-transformer), we decided to incorporate these steps outside of the library and within our protocol. We clarified it in the updated version of the paper in Section 4.1 and will continue to improve the description of the protocol in Section 3.2.\n\nRegarding the term collaborative learning, we indicate in Section 3.1 that collaboration here refers to the different parties collaborating on classification. One of the main benefits of this collaboration is that it can be used by each party to train a local model (independently of its architecture) with improved performance.\n\nThank you for carefully reading our paper. We corrected the typo in Section 3.3, applied the same formatting to \u201cargmax\u201d and \u201csum\u201d in Section 4.1, and added patterns to columns to increase the readability of Figure 3. We have applied this styling to the other Figures similarly."}, "signatures": ["ICLR.cc/2021/Conference/Paper2667/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "h2EbJ4_wMVq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2667/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2667/Authors|ICLR.cc/2021/Conference/Paper2667/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845704, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2667/-/Official_Comment"}}}, {"id": "wgveyYLPxk", "original": null, "number": 2, "cdate": 1603151131283, "ddate": null, "tcdate": 1603151131283, "tmdate": 1605024157377, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": "h2EbJ4_wMVq", "invitation": "ICLR.cc/2021/Conference/Paper2667/-/Official_Review", "content": {"title": "Fairness seems very cool, but no convinced by privacy part", "review": "\n\nThis work motivated by healthcare and finance where separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. This paper propose Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentially and privacy in a collaborative setting. This work also discussed about fairness. I liked this part, since it seems very cool. However, I'm not convinced by the method in this work is better than InstaHide (I could be wrong).\n\n\n\n\nMinor comments\nIn Section 2.1, this paper should be discussed, since it proposed a way to ``encrypt'' the images/texts. \n\nInstaHide: instance-hiding schemes for private distributed learning\nhttps://arxiv.org/abs/2010.02772\nICML 2020\nYangsibo Huang, Zhao Song, Kai Li, Sanjeev Arora.\n\n\nTextHide: Tackling Data Privacy in Language Understanding Tasks\nhttps://arxiv.org/abs/2010.06053\nEMNLP 2020\nYangsibo Huang, Zhao Song, Danqi Chen, Kai Li, Sanjeev Arora\n\nIn Section B, it lists many theorems/definition about differential privacy. In Section C, it list many backgrounds about sampling. In Section D., it list many definitions on Fairness. I don't quite see the point of having them in appendix, since none of them got mentioned in Appendix E, which is the proof of the main theory result in this paper.\n\nThis paper is closely related differential privacy. I think this paper should also be mentioned somewhere.\n\nPrivacy-preserving Learning via Deep Net Pruning\nhttps://arxiv.org/abs/2003.01876\nYangsibo Huang, Yushan Su, Sachin Ravi, Zhao Song, Sanjeev Arora, Kai Li.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2667/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "h2EbJ4_wMVq", "replyto": "h2EbJ4_wMVq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2667/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091111, "tmdate": 1606915792692, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2667/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2667/-/Official_Review"}}}, {"id": "kBZmdWip1j9", "original": null, "number": 3, "cdate": 1603801806345, "ddate": null, "tcdate": 1603801806345, "tmdate": 1605024157310, "tddate": null, "forum": "h2EbJ4_wMVq", "replyto": "h2EbJ4_wMVq", "invitation": "ICLR.cc/2021/Conference/Paper2667/-/Official_Review", "content": {"title": "Solid but unsurprising system for federated classification system with privacy", "review": "Summary:\nThe authors combine several cryptographic techniques to create a federated systems that allows several entities to run classification against all the model held be the participants without revealing information in the process. In particular, the sample to be classified is not revealed to any other party, and differential privacy is used to protect the training data that was used to train the models. A central semi-honest coordinator is used to aggregate the results and add the differential privacy without learning any private information.\n\nPros:\nThe strength of this works lie in combining relevant techniques and to show experimentally that the resulting system does improve over using a local model both when the training is distributed evenly or in skewed manner while taking privacy considerations into account.\n\nCons:\n- From a cryptographic point of view, the combination of techniques is somewhat expectable.\n- I'm wondering about the low statistical security (${2^-23}$). This seems to be related to the usage of (unbounded) integer secret sharing. Would it be possible to use secret sharing modulo an integer, in which case the security could be perfect?\n- I think it would be easier to follow if steps 1-3 were combined in the description because they all take between the same pairs of parties. The exact techniques used don't seem to matter as long as the output secret sharing is the desired result, namely the one-hot vector.\n- I find the term collaborative learning somewhat overblown because the proposed protocol only runs classification collaboratively.\n\nOverall:\nDespite the points above, I'm in favor of acceptance because the paper seems to improve on previous work, and because it is written very well.\n\nMinor issues:\n3.3: leakeage\n4.1: odd juxtaposition in the formatting of \"arg max\" and \"sum\"\nFigure 3: very hard to read in black-and-white\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2667/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2667/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CaPC Learning: Confidential and Private Collaborative Learning", "authorids": ["~Christopher_A._Choquette-Choo1", "~Natalie_Dullerud1", "~Adam_Dziedzic1", "~Yunxiang_Zhang1", "~Somesh_Jha1", "~Nicolas_Papernot1", "~Xiao_Wang11"], "authors": ["Christopher A. Choquette-Choo", "Natalie Dullerud", "Adam Dziedzic", "Yunxiang Zhang", "Somesh Jha", "Nicolas Papernot", "Xiao Wang"], "keywords": ["machine learning", "deep learning", "privacy", "confidentiality", "security", "homomorphic encryption", "mpc", "differential privacy"], "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. ", "one-sentence_summary": "A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "choquettechoo|capc_learning_confidential_and_private_collaborative_learning", "supplementary_material": "/attachment/8369f4962e55983bb55bc115e223fb09f8cefa6f.zip", "pdf": "/pdf/a5c1192f36f5200e5602d6669dc336a2520bcd68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchoquette-choo2021capc,\ntitle={Ca{\\{}PC{\\}} Learning: Confidential and Private Collaborative Learning},\nauthor={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=h2EbJ4_wMVq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "h2EbJ4_wMVq", "replyto": "h2EbJ4_wMVq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2667/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091111, "tmdate": 1606915792692, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2667/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2667/-/Official_Review"}}}], "count": 9}