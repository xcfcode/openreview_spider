{"notes": [{"id": "B1xRGkHYDS", "original": "Bygk1V3dPH", "number": 1599, "cdate": 1569439509992, "ddate": null, "tcdate": 1569439509992, "tmdate": 1577168266323, "tddate": null, "forum": "B1xRGkHYDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "oOM8PH7tVx", "original": null, "number": 1, "cdate": 1576798727488, "ddate": null, "tcdate": 1576798727488, "tmdate": 1576800909013, "tddate": null, "forum": "B1xRGkHYDS", "replyto": "B1xRGkHYDS", "invitation": "ICLR.cc/2020/Conference/Paper1599/-/Decision", "content": {"decision": "Reject", "comment": "This paper addresses the challenge of time complexity in aggregating neighbourhood information in GCNs. As we aggregate information from larger hops (deeper neighbourhoods) the number of nodes can increases exponentially thereby increasing time complexity. To overcome this the authors propose a sampling method which samples nodes layer by layer based on bidirectional diffusion between layers. They demonstrate the effectiveness of their approach on 3 large benchmarks.\n\nWhile the ideas presented in the paper were interesting the reviewers raised some concerns which I have summarised fellow:\n\n1) Novelty: The reviewers felt that the techniques presented were not very novel and is very similar to one existing work as pointed out by R4\n2) Writing: The writing needs to be improved. The authors have already made an attempt towards this but it could be improved further\n3) Comparisons with baselines: R4 has raised some concerns  the settings/configurations used for the baseline methods. In particular, the results for the baseline methods are lower than those reported in the original papers. I have read the author's rebuttal for this but I am not completely convinced about it. I would suggest that the authors address this issue in subsequent submissions\n\nBased on the above reasons I recommend that the paper cannot be accepted. \n\n ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1xRGkHYDS", "replyto": "B1xRGkHYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721554, "tmdate": 1576800272662, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1599/-/Decision"}}}, {"id": "BJezS00aYH", "original": null, "number": 1, "cdate": 1571839546224, "ddate": null, "tcdate": 1571839546224, "tmdate": 1574267381381, "tddate": null, "forum": "B1xRGkHYDS", "replyto": "B1xRGkHYDS", "invitation": "ICLR.cc/2020/Conference/Paper1599/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper was an interesting read. The idea of this paper is to challenge the use of Laplacian matrix in GCN. Indeed, typical GCNs use the same adjacency matrix across different layers. In particular, this typically leads in Euclidean case to learning isotropic filters (because the euclidean Laplacian is isotropic). Consequently, such filters have no selectivity at all.(in the Euclidean case, that could correspond to the selectivity to orientations - no selectivity would lead to a difference of Gaussians) Furthermore, for non-sparse graphs, computing the iterations of the Laplacian matrix can require a significant computational power.\n\nIn order to tackle this problem, the authors introduced a diffusion factor to sample a set of nodes to build some GCN filters with finite support. At a given layer, the diffusion factors is based on the interaction with other layers of the GCN. Then, a layer-wise attention mechanism that will allow to weight the graph connectivity of the sampled nodes is used, which is supervisedly learned. Each numerical experiments lead to a significantly better accuracy, while the method trains in reasonable time. This is thus numerically convincing. Furthermore, this method is, to my knowledge, new.\n\nThe paper is clearly written, the numerical experiments are convincing and the authors address a difficult problem with a simple method: I'm leaning toward an \"Accept\".\n\nMinor: \n- Tables 2/3 are hard to read.\n- The paper is 10 pages long, yet this was an interesting read.\n\nPost-discussion:\nThe other reviewers have made some good point, and thus I decided to lower my score. I still find the paper address an interesting problem.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1599/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1599/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xRGkHYDS", "replyto": "B1xRGkHYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576040863322, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1599/Reviewers"], "noninvitees": [], "tcdate": 1570237735037, "tmdate": 1576040863338, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1599/-/Official_Review"}}}, {"id": "rkxhzovcor", "original": null, "number": 4, "cdate": 1573710612491, "ddate": null, "tcdate": 1573710612491, "tmdate": 1573710612491, "tddate": null, "forum": "B1xRGkHYDS", "replyto": "r1gaD7lPoS", "invitation": "ICLR.cc/2020/Conference/Paper1599/-/Official_Comment", "content": {"title": "Comments to the author-response", "comment": "I appreciate the authors for considering the comments and updating the paper and reporting results for some of the additional experiments asked. \n\n- On a fair comparison with a GCN base model, BS-GCN, the improvements are not that huge as reported with BS-GAT, they are ~1%. Since the results are not reported over multiple runs (different seeds, ideally different train/test sets) the significance of this improvement is not clear. \n- Moreover, I still stand with my statement that it is not fair to report use a setting for baselines where their performance is lower than reported except for the following two cases: 1) the case where the original implementation of the baselines yield a lower performance similar to what the authors report here, i.e if the baseline results are not reproducible 2) the case where the authors convince that the baseline setting is not acceptable for certain reasons . Otherwise,  I would suggest that the authors use the settings of the baseline models to be fair for the proposed model. Unless one of the following condition is the case, "}, "signatures": ["ICLR.cc/2020/Conference/Paper1599/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1599/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xRGkHYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference/Paper1599/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1599/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1599/Reviewers", "ICLR.cc/2020/Conference/Paper1599/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1599/Authors|ICLR.cc/2020/Conference/Paper1599/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153633, "tmdate": 1576860540612, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference/Paper1599/Reviewers", "ICLR.cc/2020/Conference/Paper1599/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1599/-/Official_Comment"}}}, {"id": "SkxlBGeDoH", "original": null, "number": 1, "cdate": 1573483064251, "ddate": null, "tcdate": 1573483064251, "tmdate": 1573484352029, "tddate": null, "forum": "B1xRGkHYDS", "replyto": "BJezS00aYH", "invitation": "ICLR.cc/2020/Conference/Paper1599/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you very much for your comments.\nIn the revised manuscript, we adjust the presentation of Table 3 by showing the relative speedups with the results of GCN rather than a long table with numbers, which makes it easier to read. Moreover, we further polish our manuscript, and try our best to make it more concise.\nThanks again for your comments.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1599/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1599/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xRGkHYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference/Paper1599/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1599/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1599/Reviewers", "ICLR.cc/2020/Conference/Paper1599/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1599/Authors|ICLR.cc/2020/Conference/Paper1599/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153633, "tmdate": 1576860540612, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference/Paper1599/Reviewers", "ICLR.cc/2020/Conference/Paper1599/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1599/-/Official_Comment"}}}, {"id": "r1gaD7lPoS", "original": null, "number": 3, "cdate": 1573483364945, "ddate": null, "tcdate": 1573483364945, "tmdate": 1573483364945, "tddate": null, "forum": "B1xRGkHYDS", "replyto": "HyekiqqfiH", "invitation": "ICLR.cc/2020/Conference/Paper1599/-/Official_Comment", "content": {"title": "Response to Review #4 ", "comment": "Thank you very much for your insightful and helpful comments on our submitted manuscript. We fully accept these valuable comments and carefully revise the manuscript one by one. The detailed revisions are reported as follows.\n\n1) We re-implement all the baselines by considering the following three concerns: Firstly, the original implementations of different methods have inconsistent model tricks and data preprocessing, e.g., FastGCN uses the normalization layers after each convolution layer but others not, AS-GCN applies two model designs on the large Reddit dataset and other datasets, respectively, AS-GCN also uses an extra MLP layer for better feature learning. Therefore, it may not be very fair to use the original implementations. Secondly, some original codes (e.g. AS-GCN) have poor flexibility (e.g. adjust the layers). As the convolution layers are hard-coded into the models, it is not easy to extend them from shallow layers (e.g 3 layers) to deep layers (e.g 5 layers). Lastly but most importantly, we develop a general graph deep learning system with flexible extensity, in which we implement all the baselines with a unified paradigm (in fact, most of the existing GNN models can be very easily implemented in our unified framework). We will release the system after review.\n\n2) We add a \u2018Case Study\u2019 section in the revised manuscript to disentangle the effects of the two parts of BLS-GAN. In the Case Study, we implement two variants of our BLS-GAN: BLS-GCN(using bi-diffusion based sampling, but the constant weights of GCN instead of the attention mechanism) and AS-GAN(using the adaptive sampling of AS-GCN instead of the bi-diffusion based sampling, but the learnable weights of graph attention mechanism). The results demonstrate that: a) the bi-diffusion based sampling could achieve significant gains on all the datasets, and the deeper the layers, the greater the gains; b) the attention mechanism is obviously helpful for the embedding learning but the effect may depend on the peculiarity of target dataset.\n\n3) We adjust the presentation of Table 3 by showing the relative speedups with the results of GCN rather than a long table with numbers, which makes it more easy to read.\n\n4) We carefully revise the writing of our manuscript by removing the high-level intuitions and ill-defined terms. Moreover, we further polish our manuscript, and try our best to make it easier to read.\n\nFinally, thanks again for your comments, and hope this response could clear your concerns.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1599/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xRGkHYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference/Paper1599/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1599/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1599/Reviewers", "ICLR.cc/2020/Conference/Paper1599/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1599/Authors|ICLR.cc/2020/Conference/Paper1599/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153633, "tmdate": 1576860540612, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference/Paper1599/Reviewers", "ICLR.cc/2020/Conference/Paper1599/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1599/-/Official_Comment"}}}, {"id": "BkeqTflwor", "original": null, "number": 2, "cdate": 1573483201545, "ddate": null, "tcdate": 1573483201545, "tmdate": 1573483201545, "tddate": null, "forum": "B1xRGkHYDS", "replyto": "H1eTO6VRYS", "invitation": "ICLR.cc/2020/Conference/Paper1599/-/Official_Comment", "content": {"title": "Official Blind Review #1 ", "comment": "Thank you very much for your insightful and helpful comments on our submitted manuscript. We fully accept these valuable comments and carefully revise the manuscript one by one. The detailed revisions are reported as follows.\n\n1) We carefully revise the writing of our manuscript by removing the high-level intuitions and ill-defined terms. Moreover, we further polish our manuscript, and try our best to make it easier to read.\n\n2) We adjust the presentation of Table 3 by showing the relative speedups with the results of GCN rather than a long table with numbers, which makes it more easy to read.\n\n3) We add a \u2018Case Study\u2019 section in the revised manuscript to disentangle the effects of the two parts of BLS-GAN. In the Case Study, we implement two variants of our BLS-GAN: BLS-GCN(using bi-diffusion based sampling, but the constant weights of GCN instead of the attention mechanism) and AS-GAN(using the adaptive sampling of AS-GCN instead of the bi-diffusion based sampling, but the learnable weights of graph attention mechanism). The results demonstrate that: a) the bi-diffusion based sampling could achieve significant gains on all the datasets, and the deeper the layers, the greater the gains; b) the attention mechanism is obviously helpful for the embedding learning but the effect may depend on the peculiarity of target dataset. \n\nThanks again for your comments, and hope this response could clear your concerns.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1599/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xRGkHYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference/Paper1599/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1599/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1599/Reviewers", "ICLR.cc/2020/Conference/Paper1599/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1599/Authors|ICLR.cc/2020/Conference/Paper1599/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153633, "tmdate": 1576860540612, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1599/Authors", "ICLR.cc/2020/Conference/Paper1599/Reviewers", "ICLR.cc/2020/Conference/Paper1599/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1599/-/Official_Comment"}}}, {"id": "HyekiqqfiH", "original": null, "number": 3, "cdate": 1573198486713, "ddate": null, "tcdate": 1573198486713, "tmdate": 1573199355180, "tddate": null, "forum": "B1xRGkHYDS", "replyto": "B1xRGkHYDS", "invitation": "ICLR.cc/2020/Conference/Paper1599/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "This paper aims at improving the computational efficiency of GCNs to effectively capture information from the larger multi-hop neighborhood. Conventionally, GCNs use information from all the neighbors up to a certain depth; in which case, with consideration of each further hop, the neighborhood size increases exponentially. To avoid the exponentially increasing memory and computational footprints of GCNs as a result of an exponential neighborhood expansion, this paper proposes a (hop) layer-wise sampling procedure that reduces the complexity to a linear factor. The sampling of nodes at a layer, \u2018l\u2019 is based on the transmission probabilities of the nodes at layer \u2018l\u2019 and their immediate neighbors sampled earlier in layer \u2018l+1\u2019 from both directions of diffusion. The proposed model is based on Graph ATtention Network (GAT) which is adopted here to aggregates neighborhood information only over the nodes sampled with their bi-diffusion sampler. \n\nStrengths of the paper: The paper intuitively suggests that some of the popular sampling-based scaling approaches for GCN may not be powerful enough as they don\u2019t consider bi-directional influences. \n\nWeaknesses of this paper:\n- Novelty: The idea is incremental. The paper is similar to the layer-wise sampling model, AS-GCN where instead of the base GCN model this paper uses GAT coupled with its proposed bi-diffusion sampler. \n- Experimental results: \n    - (a) Inconsistent baseline results: The performance of baselines reported here on standard train/test/val splits are significantly lower than the ones reported in the original papers. For ex: with FastGCN the original papers report 0.88 and 0.937 on PPI and Reddit which is ~0.03 more than what is reported here. With the case of AS-GCN, the original performance scores are superior to the proposed model in the paper, however here they are reported ~0.04 scores lower. Since the codes for all these baselines are available, it is only fair to use the original implementation; if not, it is important to replicate the original results before using a different implementation. \n    - (b) Variance and statistical significance results are missing\n    - (c) Cluster-GCN though discussed, an experimental comparison with it is missing. Reported results from Cluster-GCN paper on Reddit and PPI suggests a superior performance over BLS-GAT. \n    - (d) BLS-GCN missing. This would be a fair comparison to FastGCN and AS-GCN. \n    - (e) Experimental comparison with Jumping Neural network (Xu et al) is missing to understand how the proposed solution improves over existing solutions for over-smoothing. It would be helpful to even couple it with Fast-GCN/AS-GCN sampler to better understand the benefits of this paper. \n- Writing:\n    - The paper is not well written. Though there are only minor grammatical mistakes, multiple sections of the paper are not clear and are hard to read because of complex sentences and long paragraphs. \n    - Some of the terminologies used are not clearly described and are not explained prior to the usage. Some of them are neighbor-explosion, over-expansion, the width of neighborhood expansion, local correlations, etc, In some places, over-expansion is used to refer only neighbourhood explosion or only over-smoothing and both. It will be comprehensive if it is grounded. \n    - Numerous claims/ideas put forth in this paper are abstract and intuitive. The intuitions should be backed with proper support. Some of the major concerns are:-\n        - (a) proof/arguments to show that layer-wise sampling may lead to sparse mini-batches and how does that in-turn impact over-smoothing \n        - (b) how does the proposed model avoid over-smoothing?\n        - (c) why sub-graph methods are not effective ? .. etc\n    - It is true that using a fixed neighborhood weightage function as with GCNs may not be optimal. However, the discussion made on GCN and its lack of an appropriate normalization/ neighbourhood weightage function is incorrect. GCNs aggregate information from further neighborhoods according to the respective higher-order diffusion laplacian matrix entries. You can see that by simply removing the nonlinearity+weights and recursively expanding the GCN equation. \n- Other comments:\n    - Provide the complexity of the proposed model (GCN + sampler) and compare it with other sampling approaches. \n    - The connectivity structure + signal on the nodes of the graphs is the data that is being convolved and they are not the filters. The weights being learned are the filters. \n    - In Eqn: 2, I believe you are providing an equation for GS-GCN. In which case the fraction should be N(v)+1/ (N_s(V)+1) to match the original model/implementation for GraphSAGE (GS) paper. \n    - I think the summation in the denominator for AS-GCN following Eqn: 3 should run over V instead of V_l.\n    - It will be helpful to run the model on directed datasets to see improved benefits of bi-diffusion sampling. \n    - Need more discussion about AS-GCN, Cluster-GCN and Jumping Neural networks (Xu 2018b)\n   ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1599/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1599/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xRGkHYDS", "replyto": "B1xRGkHYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576040863322, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1599/Reviewers"], "noninvitees": [], "tcdate": 1570237735037, "tmdate": 1576040863338, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1599/-/Official_Review"}}}, {"id": "H1eTO6VRYS", "original": null, "number": 2, "cdate": 1571863925163, "ddate": null, "tcdate": 1571863925163, "tmdate": 1572972447645, "tddate": null, "forum": "B1xRGkHYDS", "replyto": "B1xRGkHYDS", "invitation": "ICLR.cc/2020/Conference/Paper1599/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a sampling method for graph neural networks which is applicable to very large graphs (where not all nodes can be kept in memory at the same time). The method uses the transition probabilities of a random walk to construct a sampling probability of the nodes in the lower layer given the nodes in the upper layer. Since this samples nodes which can be one or multiple hops away, an attention mechanism is used to weight updates from connected nodes. Experiments show that this method is promising.\n\nIn its current state I would be inclined to reject this paper, but I could be convinced otherwise. The idea, although relatively straightforward, seems powerful and the experiments seem to support it. My main concern is that paper is not well written. It contains long meandering paragraphs (e.g., all of section 2 is a single paragraph) with high-level intuitions and ill-defined terms, making it hard to read. Similarly, results are badly presented. For example, table 3 should probably be given as relative speedups with the best results bold-faced rather than a long table with numbers. Illustrations would also be very helpful to provide an intuition about formulas 4, 5, and 6. Moreover, a simple ablation study is necessary (e.g., using bi-diffusion based sampling, but using constant weights instead of the attention mechanism, or vice-versa, using the attention mechanism with other types of sampling). It is currently impossible to disentangle the effects of the two parts of BLS-GAN."}, "signatures": ["ICLR.cc/2020/Conference/Paper1599/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1599/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A bi-diffusion based layer-wise sampling method for deep learning in large graphs", "authors": ["Yu He", "Shiyang Wen", "Wenjin Wu", "Yan Zhang", "Siran Yang", "Yuan Wei", "Di Zhang", "Guojie  Song", "Wei Lin", "Liang Wang", "Bo Zheng"], "authorids": ["herve.hy@alibaba-inc.com", "shiyang.wsy@alibaba-inc.com", "kevin.wwj@alibaba-inc.com", "zy143424@alibaba-inc.com", "siran.ysr@alibaba-inc.com", "yuanxi.wy@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "gjsong@pku.edu.cn", "yangkun.lw@alibaba-inc.com", "liangbo.wl@alibaba-inc.com", "bozheng@alibaba-inc.com"], "keywords": ["Layerwise Sampling", "Graph Neural Networks", "Attention Mechanism"], "abstract": "The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training,\nit still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.", "pdf": "/pdf/dc8d3ab7d8350432cad807285447b5010a2d8b24.pdf", "paperhash": "he|a_bidiffusion_based_layerwise_sampling_method_for_deep_learning_in_large_graphs", "original_pdf": "/attachment/16e551393b79af014af59bc2564aa038e7a543ae.pdf", "_bibtex": "@misc{\nhe2020a,\ntitle={A bi-diffusion based layer-wise sampling method for deep learning in large graphs},\nauthor={Yu He and Shiyang Wen and Wenjin Wu and Yan Zhang and Siran Yang and Yuan Wei and Di Zhang and Guojie  Song and Wei Lin and Liang Wang and Bo Zheng},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xRGkHYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xRGkHYDS", "replyto": "B1xRGkHYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1599/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576040863322, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1599/Reviewers"], "noninvitees": [], "tcdate": 1570237735037, "tmdate": 1576040863338, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1599/-/Official_Review"}}}], "count": 9}