{"notes": [{"id": "Bke03G85DN", "original": "Hklld2oMwN", "number": 1, "cdate": 1552732854009, "ddate": null, "tcdate": 1552732854009, "tmdate": 1562082912937, "tddate": null, "forum": "Bke03G85DN", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Blind_Submission", "content": {"title": "Robust Reinforcement Learning for Autonomous Driving ", "authors": ["Yesmina Jaafra", "Jean Luc Laurent", "Aline Deruyver", "Mohamed Saber Naceur"], "authorids": ["yasmina.jaafra@etu.unistra.fr", "jeanluc.laurent@segula.fr", "aline.deruyver@unistra.fr", "naceurs@yahoo.fr"], "keywords": ["Neural networks", "Deep reinforcement learning", "Actor-critic model", "Autonomous driving", "Carla simulator"], "TL;DR": "An actor-critic reinforcement learning approach with multi-step returns applied to autonomous driving with Carla simulator.", "abstract": "Autonomous driving is still considered as an \u201cunsolved problem\u201d given its inherent important variability and that many processes associated with its development like vehicle control and scenes recognition remain open issues. Despite reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving. In this work, we propose a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments. The experiment is conducted with Carla simulator offering a customizable and realistic urban driving conditions. The developed deep actor RL guided by a policy-evaluator critic distinctly surpasses the performance of a standard deep RL agent.", "pdf": "/pdf/397d01657f5dd4128902aaa36b44f1680c30bdda.pdf", "paperhash": "jaafra|robust_reinforcement_learning_for_autonomous_driving"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Blind_Submission", "cdate": 1552732853551, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*", "values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/drlStructPred"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/drlStructPred"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1552732853551, "tmdate": 1554911328507, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}, "tauthor": "OpenReview.net"}, {"id": "Skg1WkD_K4", "original": null, "number": 1, "cdate": 1554702071020, "ddate": null, "tcdate": 1554702071020, "tmdate": 1554910463880, "tddate": null, "forum": "Bke03G85DN", "replyto": "Bke03G85DN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept", "comment": "Even though the results are very preliminary we still accept them for the purpose of fostering interesting discussions."}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning for Autonomous Driving ", "authors": ["Yesmina Jaafra", "Jean Luc Laurent", "Aline Deruyver", "Mohamed Saber Naceur"], "authorids": ["yasmina.jaafra@etu.unistra.fr", "jeanluc.laurent@segula.fr", "aline.deruyver@unistra.fr", "naceurs@yahoo.fr"], "keywords": ["Neural networks", "Deep reinforcement learning", "Actor-critic model", "Autonomous driving", "Carla simulator"], "TL;DR": "An actor-critic reinforcement learning approach with multi-step returns applied to autonomous driving with Carla simulator.", "abstract": "Autonomous driving is still considered as an \u201cunsolved problem\u201d given its inherent important variability and that many processes associated with its development like vehicle control and scenes recognition remain open issues. Despite reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving. In this work, we propose a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments. The experiment is conducted with Carla simulator offering a customizable and realistic urban driving conditions. The developed deep actor RL guided by a policy-evaluator critic distinctly surpasses the performance of a standard deep RL agent.", "pdf": "/pdf/397d01657f5dd4128902aaa36b44f1680c30bdda.pdf", "paperhash": "jaafra|robust_reinforcement_learning_for_autonomous_driving"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Decision", "cdate": 1554496489387, "reply": {"forum": "Bke03G85DN", "replyto": "Bke03G85DN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554496489387, "tmdate": 1554910460739, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "SkxAvbRIKE", "original": null, "number": 4, "cdate": 1554600294382, "ddate": null, "tcdate": 1554600294382, "tmdate": 1554910459149, "tddate": null, "forum": "Bke03G85DN", "replyto": "Bke03G85DN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Official_Review", "content": {"title": "Innovation and contribution", "review": " This work is about autonoous driving and proposes a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments.\n\nThe major problem of this work is that its technical/research innovation is not clear. Actually the proposed algorithm is not new and similar or more advanced algorithms have already been proposed, such as Generalized Advantage Estimation, Schulman et al. 2016(b). This paper is more like a homework in a RL course. ", "rating": "1: Strong rejection", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning for Autonomous Driving ", "authors": ["Yesmina Jaafra", "Jean Luc Laurent", "Aline Deruyver", "Mohamed Saber Naceur"], "authorids": ["yasmina.jaafra@etu.unistra.fr", "jeanluc.laurent@segula.fr", "aline.deruyver@unistra.fr", "naceurs@yahoo.fr"], "keywords": ["Neural networks", "Deep reinforcement learning", "Actor-critic model", "Autonomous driving", "Carla simulator"], "TL;DR": "An actor-critic reinforcement learning approach with multi-step returns applied to autonomous driving with Carla simulator.", "abstract": "Autonomous driving is still considered as an \u201cunsolved problem\u201d given its inherent important variability and that many processes associated with its development like vehicle control and scenes recognition remain open issues. Despite reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving. In this work, we propose a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments. The experiment is conducted with Carla simulator offering a customizable and realistic urban driving conditions. The developed deep actor RL guided by a policy-evaluator critic distinctly surpasses the performance of a standard deep RL agent.", "pdf": "/pdf/397d01657f5dd4128902aaa36b44f1680c30bdda.pdf", "paperhash": "jaafra|robust_reinforcement_learning_for_autonomous_driving"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Official_Review", "cdate": 1553778229470, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Bke03G85DN", "replyto": "Bke03G85DN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229470, "tmdate": 1554911861809, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "Hkx0GESBtV", "original": null, "number": 3, "cdate": 1554498581765, "ddate": null, "tcdate": 1554498581765, "tmdate": 1554910458479, "tddate": null, "forum": "Bke03G85DN", "replyto": "Bke03G85DN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Official_Review", "content": {"title": "Low improvement, with little evidence.", "review": "The paper struggles to clearly explain their contribution to current works, and improvements over a baseline. The written quality of the paper shows numerous grammatical errors and careless mistakes. On a positive note they do demonstrate the variance improvement of using multistep rewards over a single step look ahead.", "rating": "1: Strong rejection", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning for Autonomous Driving ", "authors": ["Yesmina Jaafra", "Jean Luc Laurent", "Aline Deruyver", "Mohamed Saber Naceur"], "authorids": ["yasmina.jaafra@etu.unistra.fr", "jeanluc.laurent@segula.fr", "aline.deruyver@unistra.fr", "naceurs@yahoo.fr"], "keywords": ["Neural networks", "Deep reinforcement learning", "Actor-critic model", "Autonomous driving", "Carla simulator"], "TL;DR": "An actor-critic reinforcement learning approach with multi-step returns applied to autonomous driving with Carla simulator.", "abstract": "Autonomous driving is still considered as an \u201cunsolved problem\u201d given its inherent important variability and that many processes associated with its development like vehicle control and scenes recognition remain open issues. Despite reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving. In this work, we propose a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments. The experiment is conducted with Carla simulator offering a customizable and realistic urban driving conditions. The developed deep actor RL guided by a policy-evaluator critic distinctly surpasses the performance of a standard deep RL agent.", "pdf": "/pdf/397d01657f5dd4128902aaa36b44f1680c30bdda.pdf", "paperhash": "jaafra|robust_reinforcement_learning_for_autonomous_driving"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Official_Review", "cdate": 1553778229470, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Bke03G85DN", "replyto": "Bke03G85DN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229470, "tmdate": 1554911861809, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "HJg-dj07tE", "original": null, "number": 2, "cdate": 1554406248799, "ddate": null, "tcdate": 1554406248799, "tmdate": 1554910454203, "tddate": null, "forum": "Bke03G85DN", "replyto": "Bke03G85DN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Official_Review", "content": {"title": "Nice task but it should have more experiments.", "review": "This paper evaluates two common RL algorithms in a simulated driving environment.\n\nPros:\n1- Nice choice of using a more realistic environment.\n2- The paper is easy to read and understand.\n\nCons:\n1- Some details are missing: how exactly the reward is computed? How long an episode lasts, on average? What is the average distance a vehicle runs before having a collision?\n2- Only two RL methods are evaluated (A2C and A3C). Given the abundance of methods in the literature, it would be nice to see comparisons with some other commonly used methods, such as Q-Learning and TRPO.\n\nQuestions:\n1- Does the agent use a third person (outside the vehicle) camera view? If yes, the results would be more convincing if the first-person view was used because that is closer to the viewpoint of real vehicle.\n2- In section 4 you mention that traffic rules are given as input to the agent. How do you extract these rules from the environment? For example, how a stop sign is given as an input to the agent? Also, a more realistic environment should not explicitly give these to the agent. Instead, inferring traffic rules should be a job of the agent as traffic signs often change due to constructions, accidents, etc.\n", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning for Autonomous Driving ", "authors": ["Yesmina Jaafra", "Jean Luc Laurent", "Aline Deruyver", "Mohamed Saber Naceur"], "authorids": ["yasmina.jaafra@etu.unistra.fr", "jeanluc.laurent@segula.fr", "aline.deruyver@unistra.fr", "naceurs@yahoo.fr"], "keywords": ["Neural networks", "Deep reinforcement learning", "Actor-critic model", "Autonomous driving", "Carla simulator"], "TL;DR": "An actor-critic reinforcement learning approach with multi-step returns applied to autonomous driving with Carla simulator.", "abstract": "Autonomous driving is still considered as an \u201cunsolved problem\u201d given its inherent important variability and that many processes associated with its development like vehicle control and scenes recognition remain open issues. Despite reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving. In this work, we propose a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments. The experiment is conducted with Carla simulator offering a customizable and realistic urban driving conditions. The developed deep actor RL guided by a policy-evaluator critic distinctly surpasses the performance of a standard deep RL agent.", "pdf": "/pdf/397d01657f5dd4128902aaa36b44f1680c30bdda.pdf", "paperhash": "jaafra|robust_reinforcement_learning_for_autonomous_driving"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Official_Review", "cdate": 1553778229470, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Bke03G85DN", "replyto": "Bke03G85DN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229470, "tmdate": 1554911861809, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "rkgE3D6RO4", "original": null, "number": 1, "cdate": 1554073515663, "ddate": null, "tcdate": 1554073515663, "tmdate": 1554910453525, "tddate": null, "forum": "Bke03G85DN", "replyto": "Bke03G85DN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Official_Review", "content": {"title": "Experimental paper with unclear contribution", "review": "This paper tackles the problem of autonomous driving using deep RL algorithms. More specifically, the authors evaluate the benefit of using a multi-step returns critic in A2C for this task. Experiments are conducted using the realistic driving simulator CARLA.\n\nVery limited contribution:\n* Though investigating deep RL approaches on the difficult task of autonomous driving, I find the experiments to be very limited as only one algorithm (A2C) is considered. It would have been interesting to present a broader study with more than one method.\n* Dosovitskiy et al. (2017) already evaluate A3C for the autonomous driving task using CARLA. The contribution of the current paper therefore seems limited to evaluating the benefit of multi-step returns.\n\nIt is not clear how rewards are defined:\n* What are the weights given to each \"goal feature\"?\n* How were these weights chosen?\n* Would a \"good policy\" in terms of those rewards actually be considered \"good\" by humans?\n\nExperiments:\n* How many repetitions were performed?\n* Does the shaded are on Figs. 2-3 correspond to standard deviation? If so, could you really conclude that there was a difference between the two compared methods?\n* All experiments are performed on straight roads. It would be interesting to see how different/similar results are on more challenging roads.\n* What were the environmental conditions during training? If it was always sunny, one cannot really be surprised that the methods do not generalize to different weathers...\n* Comparison with state-of-the-art results? I understand that existing approaches evaluate performance differently in their papers. The approaches could still be run on the given setting.\n\nThe paper is easy to read. The contribution is presented as a study of deep RL techniques for autonomous driving, which is relevant for the workshop. However, this has already been done in the past (e.g. Dosovitskiy et al., 2017), especially using algorithms very close to what is considered here, and the approaches studied previously were not included in the current paper. Moreover, the experiments lack details to actually make the presented comparison meaningful.\n\nMinor comments:\n* [Eq.4] R(t) should be R_t.\n* When citing multiple references one after the other, putting them in the same parenthesis increases readability.", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning for Autonomous Driving ", "authors": ["Yesmina Jaafra", "Jean Luc Laurent", "Aline Deruyver", "Mohamed Saber Naceur"], "authorids": ["yasmina.jaafra@etu.unistra.fr", "jeanluc.laurent@segula.fr", "aline.deruyver@unistra.fr", "naceurs@yahoo.fr"], "keywords": ["Neural networks", "Deep reinforcement learning", "Actor-critic model", "Autonomous driving", "Carla simulator"], "TL;DR": "An actor-critic reinforcement learning approach with multi-step returns applied to autonomous driving with Carla simulator.", "abstract": "Autonomous driving is still considered as an \u201cunsolved problem\u201d given its inherent important variability and that many processes associated with its development like vehicle control and scenes recognition remain open issues. Despite reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving. In this work, we propose a deep reinforcement learning (RL) algorithm embedding an actor critic architecture with multi-step returns to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments. The experiment is conducted with Carla simulator offering a customizable and realistic urban driving conditions. The developed deep actor RL guided by a policy-evaluator critic distinctly surpasses the performance of a standard deep RL agent.", "pdf": "/pdf/397d01657f5dd4128902aaa36b44f1680c30bdda.pdf", "paperhash": "jaafra|robust_reinforcement_learning_for_autonomous_driving"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper1/Official_Review", "cdate": 1553778229470, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Bke03G85DN", "replyto": "Bke03G85DN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229470, "tmdate": 1554911861809, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper1/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}], "count": 6}