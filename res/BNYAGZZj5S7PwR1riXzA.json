{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457830694779, "tcdate": 1457830694779, "id": "zvw1LYZyAfM8kw3Zinwy", "invitation": "ICLR.cc/2016/workshop/-/paper/102/review/10", "forum": "BNYAGZZj5S7PwR1riXzA", "replyto": "BNYAGZZj5S7PwR1riXzA", "signatures": ["ICLR.cc/2016/workshop/paper/102/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/102/reviewer/10"], "content": {"title": "This paper proposes interesting idea and tries to address a very hard problem for learning undirected graphical models in general, there are some issues with the technical aspect of the paper that needs to be addressed.", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes interesting idea and tries to address a very hard problem common to all energy based or undirected graphical models.\n\nThere are two technical issues to be clarified:\nFor Equation 4, wouldn't you need to multiply the det of the jacobian of x=G(z) : |det dz/dx| to be multiplied for every sample z that is drawn? How much more complexity would you have to incur if you had to backprop through this network every time you wanted calculate negative phase expectations?\n\nAnother issue is that if you are going to train the second network (similar in spirit to like a nonlinear sigmoid belief network) on the samples of the original energy model, why not just keep those samples around like the methods of persistent contrastive divergence or fast persistent contrastive divergence? If you keep the samples, that would be sort of like learning a non-parametric model of the negatives phase samples.\n\nI think the bigger issues is that the idea is to try to use a directed uniform to multimodal generative model to model the samples from the partition function of the energy model. The idea is that if the directed model can learn a good multimodal representation, then the negative phase would be easy to \"mix\". However, if you could learn a good directed model, why not just learn that on the original training data instead!?\n\nPerhaps to improve the motivation for the model, one can argue that the product of experts being learned is more interesting or more important than a simple directed uniform-multimodal generative model?!\n\nFor related works, there are long history of approximate inference models for addressing the negative phase sampling by learning the posterior, e.g. Wake-Sleep algorithm, and Efficient learning of DBMs. The authors could reference them and draw comparisons to those prior works.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Directed Generative Models with Energy-Based Probability Estimation", "abstract": "Energy-based probabilistic models have been confronted with intractable computations during the learning that requires to have appropriate samples drawn from the estimated probability distribution. It can be approximately achieved by a Monte Carlo Markov Chain sampling process, but still has mixing problems especially with deep models that slow the learning. We introduce an auxiliary deep model that deterministically generates samples based on the estimated distribution, and this makes the learning easier without any high cost sampling process. As a result, we propose a new framework to train the energy-based probabilistic models with two separate deep feed-forward models. The one is only to estimate the energy function, and the another is to deterministically generate samples based on it. Consequently, we can estimate the probability distribution and its corresponding deterministic generator with deep models.", "pdf": "/pdf/BNYAGZZj5S7PwR1riXzA.pdf", "paperhash": "kim|deep_directed_generative_models_with_energybased_probability_estimation", "conflicts": ["umontreal.ca"], "authors": ["Taesup Kim", "Yoshua Bengio"], "authorids": ["taesup.kim@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579956023, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579956023, "id": "ICLR.cc/2016/workshop/-/paper/102/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "BNYAGZZj5S7PwR1riXzA", "replyto": "BNYAGZZj5S7PwR1riXzA", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/102/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457646897762, "tcdate": 1457646897762, "id": "4QyAOVzxDuBYD9yOFqVD", "invitation": "ICLR.cc/2016/workshop/-/paper/102/review/11", "forum": "BNYAGZZj5S7PwR1riXzA", "replyto": "BNYAGZZj5S7PwR1riXzA", "signatures": ["ICLR.cc/2016/workshop/paper/102/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/102/reviewer/11"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes a new way of training energy-based probabilistic models using two separate deep networks. The first one estimates the energy function (deep energy model), while the goal of the second one (deep directed generative model) is to generate samples that would approximate samples from the deep energy model.\n\nThe authors are using an adversarial setting of Goodfellow et.al. to train both models: parameters of deep energy model are trained to discriminate between the real and generated data, whereas parameters of the generative model are trained to align the probability distribution p(x) between the deep energy model and the directed generative model.\n\nOne key assumption of the algorithm is that the distributions of the deep energy model and the directed generative model need to be approximately aligned during training. Simple 2-d simulation results show that this is indeed the case but it is not clear at all whether this would hold when modeling complex high-dimensional distributions: it is essentially saying that\na simple deep directed generative model can accurately approximate the distribution of the complex deep energy-based model.\n\nI would also encourage the authors to clean up writing/English, as well as weed out various typos in equations. For example, the authors are sometimes using \\theta and sometimes using \\Theta to mean the same thing.\n\nThere is also a typo in Eq 2: sum_i E_{\\theta_I}.\n\nI would also spend more time discussing the actual training of the model (Eq. 4), while reducing justification of why training deep generative models is hard (text around Eq. 1).\n\nFinally, it was not clear to me what is the final output of the training procedure. Do we throw away deep energy model and stick with deep directed generative model, or the other way around? Under the assumption that both models are approximately aligned, then they are essentially modelling the same distribution.\n\nI would also encourage the authors to compare to contrastive backprop for training their deep energy model. I suspect it might work much better compared to what the authors are proposing.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Directed Generative Models with Energy-Based Probability Estimation", "abstract": "Energy-based probabilistic models have been confronted with intractable computations during the learning that requires to have appropriate samples drawn from the estimated probability distribution. It can be approximately achieved by a Monte Carlo Markov Chain sampling process, but still has mixing problems especially with deep models that slow the learning. We introduce an auxiliary deep model that deterministically generates samples based on the estimated distribution, and this makes the learning easier without any high cost sampling process. As a result, we propose a new framework to train the energy-based probabilistic models with two separate deep feed-forward models. The one is only to estimate the energy function, and the another is to deterministically generate samples based on it. Consequently, we can estimate the probability distribution and its corresponding deterministic generator with deep models.", "pdf": "/pdf/BNYAGZZj5S7PwR1riXzA.pdf", "paperhash": "kim|deep_directed_generative_models_with_energybased_probability_estimation", "conflicts": ["umontreal.ca"], "authors": ["Taesup Kim", "Yoshua Bengio"], "authorids": ["taesup.kim@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579955267, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579955267, "id": "ICLR.cc/2016/workshop/-/paper/102/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "BNYAGZZj5S7PwR1riXzA", "replyto": "BNYAGZZj5S7PwR1riXzA", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/102/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457620900200, "tcdate": 1457620900200, "id": "yovqvkRyYSr682gwszRX", "invitation": "ICLR.cc/2016/workshop/-/paper/102/review/12", "forum": "BNYAGZZj5S7PwR1riXzA", "replyto": "BNYAGZZj5S7PwR1riXzA", "signatures": ["ICLR.cc/2016/workshop/paper/102/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/102/reviewer/12"], "content": {"title": "Deep Directed Generative Models With Energy-Based Probability Estimation", "rating": "4: Ok but not good enough - rejection", "review": "This paper aims to address the problem of learning energy-based density models that have intractable partition functions. A core idea of the proposed approach is to use a *separate model* (Model B) to generate equilibrium samples from the energy-based model we wish to learn (Model A). If an oracle could provide a perfectly matched Model B for each given set of parameters for Model A, then learning via Monte Carlo estimates of the gradients would be straightforward. \n\nThis idea of having paired models is similar in spirit to well known previous approaches for learning generative models and density estimation (e.g. Helmholtz machines, Generative Adversarial Nets as noted), but it differs in several ways and there could be interesting ideas to explore in this direction. However, I have some concerns about the current proposal.\n\nIn particular, simply minimizing the energy of the generated samples (eqn 5) will not necessarily result in a generator (Model B) that follows and matches the equilibrium distribution of the target energy-based model (Model A). Indeed, the generator could obtain a (local) optimum by being degenerate -- with all the samples at the minima of one of the energy modes of Model A. It could also easily completely ignore some modes, or assign them incorrect probability mass, etc. (The toy examples shown do not seem to exhibit this pathology too badly, however it's not clear whether this is due to fortunate initialization and/or the very low dimensional nature of the problem.) In some respects this is analogous to the MCMC mixing failures that the proposed technique aims to circumvent.\n\nI think the general idea presented here may be worth exploring and expanding on further, but in the current form it doesn't seem ready for presentation at ICLR.\n\nAs additional points: the clarity of the writing could be substantially improved, and a more challenging but still tractable toy-task (e.g. MNIST) would help to understand whether the potential problems noted with this method arise in more practical situations. On the algorithm side, one can imagine several possible heuristics to test for and guard against the failure modes suggest above.\n\nPro acceptance: Interesting direction for ideas tackling a broad/significant problem.\nCon acceptance: Insufficient evidence that proposed method works well on \"realistic\" problems, combined with a mathematically identifiable weakness whose presence is not discussed and has not been properly explored empirically.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Directed Generative Models with Energy-Based Probability Estimation", "abstract": "Energy-based probabilistic models have been confronted with intractable computations during the learning that requires to have appropriate samples drawn from the estimated probability distribution. It can be approximately achieved by a Monte Carlo Markov Chain sampling process, but still has mixing problems especially with deep models that slow the learning. We introduce an auxiliary deep model that deterministically generates samples based on the estimated distribution, and this makes the learning easier without any high cost sampling process. As a result, we propose a new framework to train the energy-based probabilistic models with two separate deep feed-forward models. The one is only to estimate the energy function, and the another is to deterministically generate samples based on it. Consequently, we can estimate the probability distribution and its corresponding deterministic generator with deep models.", "pdf": "/pdf/BNYAGZZj5S7PwR1riXzA.pdf", "paperhash": "kim|deep_directed_generative_models_with_energybased_probability_estimation", "conflicts": ["umontreal.ca"], "authors": ["Taesup Kim", "Yoshua Bengio"], "authorids": ["taesup.kim@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579954958, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579954958, "id": "ICLR.cc/2016/workshop/-/paper/102/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "BNYAGZZj5S7PwR1riXzA", "replyto": "BNYAGZZj5S7PwR1riXzA", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/102/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455811826252, "tcdate": 1455811826252, "id": "BNYAGZZj5S7PwR1riXzA", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "BNYAGZZj5S7PwR1riXzA", "signatures": ["~Taesup_Kim1"], "readers": ["everyone"], "writers": ["~Taesup_Kim1"], "content": {"CMT_id": "", "title": "Deep Directed Generative Models with Energy-Based Probability Estimation", "abstract": "Energy-based probabilistic models have been confronted with intractable computations during the learning that requires to have appropriate samples drawn from the estimated probability distribution. It can be approximately achieved by a Monte Carlo Markov Chain sampling process, but still has mixing problems especially with deep models that slow the learning. We introduce an auxiliary deep model that deterministically generates samples based on the estimated distribution, and this makes the learning easier without any high cost sampling process. As a result, we propose a new framework to train the energy-based probabilistic models with two separate deep feed-forward models. The one is only to estimate the energy function, and the another is to deterministically generate samples based on it. Consequently, we can estimate the probability distribution and its corresponding deterministic generator with deep models.", "pdf": "/pdf/BNYAGZZj5S7PwR1riXzA.pdf", "paperhash": "kim|deep_directed_generative_models_with_energybased_probability_estimation", "conflicts": ["umontreal.ca"], "authors": ["Taesup Kim", "Yoshua Bengio"], "authorids": ["taesup.kim@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}