{"notes": [{"id": "l-PrrQrK0QR", "original": "6vpyNoA5b6V9", "number": 2753, "cdate": 1601308305296, "ddate": null, "tcdate": 1601308305296, "tmdate": 1615837263509, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Rr508hBUgc", "original": null, "number": 1, "cdate": 1610040382798, "ddate": null, "tcdate": 1610040382798, "tmdate": 1610473975963, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Three reviewers agree on the value of the contribution and recommend acceptance. A reviewer votes for rejection but the authors have clarified all the major concerns raised by the reviewer. Therefore, I recommend acceptance. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040382784, "tmdate": 1610473975946, "id": "ICLR.cc/2021/Conference/Paper2753/-/Decision"}}}, {"id": "M8rHak_grO", "original": null, "number": 7, "cdate": 1605938826064, "ddate": null, "tcdate": 1605938826064, "tmdate": 1605938826064, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment", "content": {"title": "Changes in latest revision.", "comment": "In this third version, we\n* clean up definitions in Section 2; most noticeably, we refactor the definition of epsilon-approximation into a first part relating function approximation and then a second part for dataset approximation\n* revise our presentation of Label Solve (end of section 3)\n* slight correction of Theorem 2 and slight cleanup of statements of Theorems 1 and 2"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "l-PrrQrK0QR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2753/Authors|ICLR.cc/2021/Conference/Paper2753/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment"}}}, {"id": "h7Vn0hIzWah", "original": null, "number": 6, "cdate": 1605300644691, "ddate": null, "tcdate": 1605300644691, "tmdate": 1605300644691, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "o6y-1zO9zu", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (part 2)", "comment": "Please note our revised paper which, in addition to addressing reviewer concerns\n* corrects typos\n* improves exposition (including the description of tables/figures and the Related Work section, which has been moved towards the end)\n* contains additional experiments concerning corrupt vs natural images showing that even for larger dataset sizes, 90% corrupted KIP images continue to outperform natural images (Tables A7-A10, Figure A3); some mixed results are also presented for labels learned using KIP\n* adds the analogue of Theorem 1 for Label Solve (Theorem 2), relating the latter to epsilon-approximation\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "l-PrrQrK0QR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2753/Authors|ICLR.cc/2021/Conference/Paper2753/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment"}}}, {"id": "FQZQ1s7JS8Q", "original": null, "number": 5, "cdate": 1605300572579, "ddate": null, "tcdate": 1605300572579, "tmdate": 1605300572579, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "o6y-1zO9zu", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (part 1)", "comment": "We thank the reviewer for their time and all the questions and references provided. There appears to be some misinterpretation of the paper\u2019s content which we hope to clarify:\n\n1. We are unsure why the reviewer claims we work with conditional distributions (Note: Some typos in Def. 2 that have been corrected). We compare loss with respect to the joint distribution (x,y) of input-label pairs. For comparison, in classification problems using neural networks (NNs), a conditional distribution p(y|x) is predicted for each x in training, and it is optimized with respect to cross entropy loss. But for test eval, samples from the joint distribution of input-labels are used to evaluate performance. Our use of the loss function (for both eps-approximation and KIP) is as in this latter situation: we sample from a joint distribution. In practice, we end up replacing population data with a fixed target dataset, i.e., the empirical distribution. \n2. Regarding expectation of loss values vs comparing loss values directly, this is addressed by our notion of weak vs strong eps-approximation. As after Def. 2, these notions converge for 0-1 loss for epsilon small. Please clarify if this does not address the reviewer's concerns.\n3. Reference [1] was helpful in alerting the authors to prior work. Note however that the weak corset def. of [1] applies to unsupervised clustering whereas we consider supervised learning. [1] is now cited in Relevant Work and Appendix A. \n   \n   We thank the reviewer for catching the imprecision of Ex. 1 and 2. Indeed, our SVM example assumed linear separability (we have removed this assumption in the current version). However, we are confused by the reviewer\u2019s remark that in the non-separable case, retraining an SVM on the support vectors and the data between the margins (or more precisely, those with positive slack) does not yield an equivalent SVM (what 0-approximate dataset entails). Perhaps the reviewer misunderstood our definition of 0-approximation. \n\n   For the case of ridge-regression, we were imprecise in that we implicitly assumed the scalar output case. Our revision discusses the general case with a detailed proof: for k-classes, an 0-approximate dataset of size k exists.\n\n   Finally, the remark about privacy not being an issue for linear models, while true, does not detract from our work. Linear models are very limited in terms of their expressivity: the real challenge is to find an expressive model that preserves privacy. Neural networks are highly expressive, but their privacy guarantees are unclear and the subject of ongoing research. For some ways in which the data used to train a network can be approximately recovered, see https://blog.openmined.org/extracting-private-data-from-a-neural-network. By training on corrupt data as we proposed, we can hope to nullify such attacks. Secondly, kernel methods still require access to the training data in order to construct test-train kernel elements for making predictions, so storing train-train kernel products isn't sufficient.\n4. The references [2-5] provided, while relevant for approximation, both 1) do not create datasets; 2) have no obvious application to privacy-preservation, and thus lie outside the novelty of our paper. For 1), one of our main results was to show that our learned datasets transfer to other kernels or neural networks. Doing kernel matrix approximation via [2-5], no dataset is obtained to be used elsewhere. For 2), it is not clear how [2-5] could be used with privacy-enhancing corruption. Part of our novelty is that even with high corruption rates of 90%, we could learn data that outperforms clean data. Kernel matrix approx. methods, whose performance cannot exceed the original matrix, will do poorly on corrupt data. We summarize this point and include it in the Related Work.\n5. While KIP and LS are pure kernel methods with no-reference to neural networks (NN), we strongly believe transferability to deep NNs is one of the key interesting findings with potential wide applications. The connection between the infinite-width limit of neural networks to NTK is the grounding theory that allows this bridge between kernels and NNs: thus KIP leads to much more sample-efficient training of NNs from scratch. \n\n   Regarding the proposed simple cases for toy 2d data, while they can be interesting to study, we believe the power of KIP lies in learning high-dimensional data effectively, thus our focus on high-dimensional image inputs. Moreover, we are having a hard time connecting [6] to our study since [6] mostly uses pre-trained networks features for KRR. To our understanding, the reviewer's proposed method is a very indirect way of testing whether KIP learned data inputs are useful for NNs. We hope the reviewer could clarify further, if we are missing the point. \n\nFinally, please see part 2 for an overview of the what's been added in the revision.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "l-PrrQrK0QR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2753/Authors|ICLR.cc/2021/Conference/Paper2753/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment"}}}, {"id": "DVcqhFzW3ZX", "original": null, "number": 4, "cdate": 1605300171404, "ddate": null, "tcdate": 1605300171404, "tmdate": 1605300171404, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "NBIGQVsBHLI", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for their time and constructive feedback on the submission. Regarding the suggestion about generalization bounds: this would make for an excellent direction for followup work and is perhaps out of scope for the current paper. The notion of epsilon-approximation is specific to the pair of algorithms used to compare the original and approximating dataset. As our SVM and ridge regression examples after Definition 2 show (please see revised version for details), the relationship between $N$ and $\\epsilon$ can have markedly different behaviors (for SVM, one can ensure $O(N)$ for $\\epsilon = 0$ while for ridge regression, one can get $O(1)$ for $\\epsilon = 0$). Of course, the interesting case to study is for neural networks. However, given the difficulty of ascertaining meaningful generalization bounds for neural networks, we do not have something concrete and useful to say about the subject at this time. \n\nPlease note our revised paper which, in addition to addressing reviewer concerns\n* corrects typos\n* improves exposition (including the description of tables/figures and the Related Work section, which has been moved towards the end)\n* clarifies the examples of epsilon-approximation after Definition 2\n* contains additional experiments concerning corrupt vs natural images showing that even for larger dataset sizes, 90% corrupted KIP images continue to outperform natural images (Tables A7-A10, Figure A3); some mixed results are also presented for labels learned using KIP\n* adds the analogue of Theorem 1 for Label Solve (Theorem 2), relating the latter to epsilon-approximation\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "l-PrrQrK0QR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2753/Authors|ICLR.cc/2021/Conference/Paper2753/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment"}}}, {"id": "TsYr6rBBVO", "original": null, "number": 3, "cdate": 1605299607208, "ddate": null, "tcdate": 1605299607208, "tmdate": 1605299607208, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "foqvnQHx_oK", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for their time and constructive feedback to improve the paper.\n\nOur response to the technical questions:\n1. The norm condition in Theorem 1 on the input is from the distribution P for which we evaluate loss (and is merely a convenience for bounding the value of the mean squared error for linear models). It is not a condition for the eps-approximate dataset to be learned: the resulting dataset can have data of arbitrary norm.\n2. We define the compression ratio using subsets of D because D might be highly redundant/simple. (Note our definition of compression ratio has been updated in the revision to be a heuristic definition.) For example, if a 100x smaller subset of D performs nearly as well as D, we might not want to include that factor of 100 in computing the compression ratio (a trivial example is if one has a small linear model trained on linearly dependent data).\n3. We place no constraints on the labels learned by Label Solve. Since we use the labels for regression tasks, these labels are still valid for use. In fact, our current revision performs some experiments concerning the case of labels learned during KIP (alongside the images), please see comments at end. It is an interesting direction for future work to add a regularizer or additional constraints when labels are learned during KIP/solved using Label Solve. \n\nConcerning our use of the term meta-learning, we regard our method as meta-learning due to our bi-level optimization (Section 3 of the current version) where the inner optimization is by the KRR solver and the outer optimization is learning the support set fed into the KRR solver. Each update to the support set is thus one meta-update episode. While we agree that conventional meta-learning involves learning over multiple datasets, we believe the term is used more broadly nowadays and covers our setup, see e.g. [1].\n\nPlease note our revised paper which, in addition to addressing reviewer concerns:\n* corrects typos\n* improves exposition (including the description of tables/figures and the Related Work section, which has been moved towards the end)\n* clarifies the examples of epsilon-approximation after Definition 2\n* contains additional experiments concerning corrupt vs natural images showing that even for larger dataset sizes, 90% corrupted KIP images continue to outperform natural images (Tables A7-A10, Figure A3); some mixed results are also presented for labels learned using KIP\n* adds the analogue of Theorem 1 for Label Solve (Theorem 2), relating the latter to epsilon-approximation. \n\n[1] Meta-Learning with Implicit Gradients (https://arxiv.org/abs/1909.04630)"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "l-PrrQrK0QR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2753/Authors|ICLR.cc/2021/Conference/Paper2753/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment"}}}, {"id": "1fSBYqQbVZf", "original": null, "number": 2, "cdate": 1605299270108, "ddate": null, "tcdate": 1605299270108, "tmdate": 1605299270108, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "usbVTFub14r", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We thank the reviewer for their time and feedback. Concerning the points raised in the Weaknesses section:\n\n1. All the problems we consider in our paper are classification problems involving one-hot labels. For KRR (linear or otherwise), we use mean-centered versions (e.g. for 10-way classification, a vector with 0.9 on the target class and  -0.1 on the remaining nine). We have clarified in the current revision that regression can be turned into prediction by predicting the class with the largest value (second paragraph of Section 4). While our Theorem 1 only holds for Linear KRR, we think it is a necessary and encouraging validating step for the soundness of KIP (in the same way that having a sound theory for linear classifiers is the starting point for the much less tractable case of non-linear neural networks.) Note that even for Linear KRR, the objective is highly nonlinear in the support set and is not a trivial result.\n2. Thanks for raising a great question. It has been observed in the few-shot literature that training with multiple support sizes works well, and is probably helpful for robustness at evaluation time with regards to the number of support data used (i.e. varying the n-shot, k-way tasks). One can vary the support size used during KIP, but we focused on the case of sampling from the entire support set in order to get optimal compression results at predetermined dataset sizes. Thus, to get robustness to sub-dataset selection, random support sampling (Step 5 of Algorithm 1) with small, medium, and large dataset sizes can be used.\n3. Only the randomly selected subset of the support (bar X, bar y) is updated per step in the while loop; the non-sampled remaining images in the support set remain fixed at such a step. One can think of the entire support set as the set of parameters we are learning. At each step, we update some random subset of those parameters via an arbitrary random sampling scheme (in the paper, we end up sampling all the parameters). We have updated the text to make this point more clear.\n\nFor the other issues:\n\n1. We\u2019ve updated the submission to correct typos to the best of our ability. We hope the new version improves clarity.  \n2. Could the reviewer please further clarify the comment on SVM. If by \u201cbest\u201d the reviewer means that one does not need to add any additional datapoints beyond the support vectors (in the linearly separable case) to obtain 0-approximation, then this is indeed true. (Note: we have updated Example 1 on SVM for the general case of data that is not linearly separable) \n\nPlease note our revised paper which, in addition to addressing reviewer concerns\n\n* improves exposition (including the description of tables/figures and the Related Work section, which has been moved towards the end)\n* clarifies the examples of epsilon-approximation after Definition 2\n* contains additional experiments concerning corrupt vs natural images showing that even for larger dataset sizes, 90% corrupted KIP images continue to outperform natural images (Tables A7-A10, Figure A3); some mixed results are also presented for labels learned using KIP\n* adds the analogue of Theorem 1 for Label Solve (Theorem 2), relating the latter to epsilon-approximation\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "l-PrrQrK0QR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2753/Authors|ICLR.cc/2021/Conference/Paper2753/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Comment"}}}, {"id": "o6y-1zO9zu", "original": null, "number": 1, "cdate": 1603846368058, "ddate": null, "tcdate": 1603846368058, "tmdate": 1605024140097, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Review", "content": {"title": "Interesting yet not efficient", "review": "The paper proposed an objective based on kernel ridge regression to find a coreset of the training set. The objective first adopts gradient descent to find the coreset in the input space, then generates targets for fake samples in the coreset. I do have many comments on the claims made in this paper, and I hope the authors can answer them. \n\n1. If the goal is to find a minimum number of samples, either original or fake ones, why is the definition of \\epsilon-approx measuring the difference between two expected losses under two distributions respectively, not the difference between two conditional distributions y|x or the joint distributions x,y ? Also, the loss function defined in Eq. 4 clearly minimises the distance between two conditional distributions.\n\nOne might be able to find a coreset of samples that in expectation gives a lower loss value than the original dataset, e.g. denoising, so then measuring the difference between loss values doesn't seem reasonable to me.\n\n2. From a CS theory perspective, the \\epsilon-kernel coreset has a clear definition, and please refer to [1] for details.\n\n3. The two examples given are not precise. \n\nFor example 1, the described situation only holds when the original dataset is linearly separable. When the dataset is not linearly separable, once SVM determines the support vectors on the original dataset, there are samples in between two margins, therefore, a new SVM which is only trained on the samples with support vectors and samples in between will not give you 0-approx by the definition in the paper. \n\nFor example 2, from a statistical learning perspective, the most important part for linear/ridge regression models is the covariance structure derived from X, so then it is true that one can always find a 0-approx dataset, but not of any size. In terms of privacy related issues, regression models rarely have privacy issues as only the X^T X and X^T Y are kept, the information regarding to specific data samples is eliminated due to the inner product. Therefore, the claim that the proposed algorithm can help improve privacy isn't valid,  as linear/ridge regression models already do that.\n\n4. There are many ways of computing inducing points of a given dataset without learning, including random projections[2], frequent directions[3,4], other subsampling methods for Nystroem[5], etc. Many of those methods are well-studied with nice theoretical guarantee in reconstruction. The paper largely ignored those methods in comparison. \n\n5. I don't see how NTK is necessary here. If the authors are interested in testing whether their proposed method work, they can simply start from simulated data in 2D space, and illustrate that the proposed method is indeed able to pick out a high-quality coreset than other methods. Then one can quickly run experiments using pretrained neural networks as feature extractors, as was done in [6]. Given different feature extractors, the selected coresets from individual one of them give us a probe to understand how different neural networks are learnt.\n\n[1] Phillips, J. M. (2016). Coresets and sketches. arXiv preprint arXiv:1601.00617.\n[2] Woodruff, D. P. (2014). Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357.\n[3] Ghashami, M., Liberty, E., Phillips, J. M., & Woodruff, D. P. (2016). Frequent directions: Simple and deterministic matrix sketching. SIAM Journal on Computing, 45(5), 1762-1792.\n[4] Shi, B., & Phillips, J. M. (2020). A Deterministic Streaming Sketch for Ridge Regression. arXiv preprint arXiv:2002.02013.\n[5] Kumar, S., Mohri, M., & Talwalkar, A. (2012). Sampling methods for the Nystr\u00f6m method. The Journal of Machine Learning Research, 13(1), 981-1006.\n[6] Tang, S., & de Sa, V. R. (2020). Deep Transfer Learning with Ridge Regression. arXiv preprint arXiv:2006.06791.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089394, "tmdate": 1606915767556, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2753/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Review"}}}, {"id": "NBIGQVsBHLI", "original": null, "number": 2, "cdate": 1603866734684, "ddate": null, "tcdate": 1603866734684, "tmdate": 1605024140031, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Review", "content": {"title": "Paper present a novel approach for approximate compression of datasets using Kernel Ridge Regression and experimental results show efficacy of the approach in terms of reducing sample complexity.", "review": "Paper present a novel approach for approximate compression of datasets using Kernel Ridge Regression, referred to as KIP.  Paper is well written and technically sounds and experimental results show efficacy of the approach in terms of reducing sample complexity. It also provides an added benefit of corrupting input datasets without much loss test accuracy for privacy preserving use case learning. \n\nThe definition of $\\epsilon$-approximation is introduced in terms bounds on difference between the expected empirical loss for original and compressed dataset which in potentially also bounds generalization error as well. The KIP algorithm iterates over an initial support-set to finally converge over a support dataset that gives similar test performance. The idea of choosing a support set and growing sounds familiar to Nystorm method for kernel approximation provides intuition on  why the approach might work.  Also selection of multiple base kernels also means selection of multiple feature spaces which naturally leads to overall boost in performance. Results on MNIST and CFIR shows the efficacy of these results.\n\nA potential gap in this work is the trade-off between the compressed dataset size N and  test error for a given fixed $\\epsilon$. Is there a way to choose say N, for a given $\\epsilon$ for a given test error performance.  It may be so that we end choosing all points in the original dataset for a given approximation and test error. A characterization of this using generalization bounds would be helpful. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089394, "tmdate": 1606915767556, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2753/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Review"}}}, {"id": "usbVTFub14r", "original": null, "number": 3, "cdate": 1603892247520, "ddate": null, "tcdate": 1603892247520, "tmdate": 1605024139962, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Review", "content": {"title": "Recommendation to Accept", "review": "This paper proposes a data-driven approach to choose an informative surrogate sub-dataset,  termed \"a \\epsilon-approximation\", from the original data set. A meta-learning algorithm called Kernel Inducing Points (KIP ) is proposed to obtain such sub-datasets for (Linear) Kernel Ridge Regression (KRR), with the potential to extend to other machine learning algorithms such as neural networks.  Some theoretical results are provided for the KRR with a linear kernel. The empirical performance of the proposed algorithm is evaluated by experiments based on synthetic data and some standard benchmark data sets. \n\nOverall, I think the paper is well written and the proposed method is of potential value to existing literature. \n\nStrengths:\n1. Some theoretical results are provided for the KIP algorithm under the linear KRR setting.\n2. Numerical experiments are carefully designed and appear to be convincing.\n\nWeakness: \n1. The theoretical results are only for Linear KRR. However, this model cannot be used for classification with categorical labels (when the response y is categorical variables). When the classification is involved\n2. The KIP algorithm depends on (1) the choice of the kernel; (2) the choice of Loss function; (3) the choice of tuning parameters for the kernel machine.  If we choose the sub-dataset based on one set of rules and later want to use it for some other purposes, this certainly would cause some problems. How can you address this issue? In other words, how the proposed algorithm can be adapted to obtain a sub-dataset that works for multi-purposes?\n3. Step 5 in Algorithm 1 is not very clear to me. How exactly do you decide to add a batch of data into the support set? based on what threshold/criterion? If one has to evaluate the loss function (4) on all possible subset of size N out of a total sample size n, it will cost about {n \\choose N} number of operations, which is clearly not scalable when n is large.  More details on how to update the support set and the convergence criterion. \n\nOther issues:\n1. There are typos in multiple places such as equations (1) and (3), and in Definition 3. Please proofread carefully.\n2. For the classification problems, it appears that the support vector machine is always the best choice seems it is \"0-approximation\" to the original dataset. Please clarify why and why not.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089394, "tmdate": 1606915767556, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2753/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Review"}}}, {"id": "foqvnQHx_oK", "original": null, "number": 4, "cdate": 1604013840861, "ddate": null, "tcdate": 1604013840861, "tmdate": 1605024139893, "tddate": null, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "invitation": "ICLR.cc/2021/Conference/Paper2753/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "This paper tackles the problem of data compression using kernel induced points learned through deep kernel learning. The main contributions of the paper are :\n\n1 - the formalism of the notion of $\\epsilon$ - approximation of datasets that describes how to rely on one learning algorithm (here KRR) to find the compression set for another one, while keeping the generalization error below $\\epsilon$.\n\n2 - the proposal of the KPI and LS algorithms that find the pseudo-inputs set by leveraging kernel methods and by modifying respectively the inputs and the labels of those pseudo-inputs while monitoring the loss of a regressor using them on the original dataset that is being compressed. Since the starting pseudo-inputs can be corrupted versions of the original dataset, the authors advocate that their method preserves privacy.\n\nOn Quality: The paper is well written and structured though could benefit from more details at many places (e.g. most figures and table  descriptions are lacking or not detailed enough)\n\nOn Clarity: The intention and the contribution of the authors are clear. The main concerning point is linking their work to too many areas instead of focusing on the main scope of the paper which is explicit data compression/distillation. By doing so, the authors confuse the reader especially given that the related work is just after the intro. Another confusing aspect is the usage of terms like meta-learning to describe their algorithm when the algorithm only deals with a single dataset (episodic-looping through one dataset is not meta-learning). Clearer language  and simplification of related work could go along way\n\nOn originality: the epsilon-approximation is an interesting idea that could lead to interesting algorithm developments. I believe the idea is not novel but the authors are the first to formalize it. On the usage of deep kernel learning to do data distillation, the idea has been explored too (Snelson & Ghahramani, 2006) but the authors took it a bit further with the LS algorithm and the privacy preservation aspect.\n\nOn significance: The area is an important research area and there are definitely a few ideas here that others can build upon.\n\nTechnical concerns/questions:\n- In theorem 1: how do you enforce Eq A1? do you make sure that the norm of pseudo-inputs are less than 1 ?\n- in def3: do you need D*_N to define the compression ratio? why not define it using D?\n- In the label solving, after a batch the label solved can fall outside of the possible label set. How do you handle that? or it does not matter?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2753/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2753/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Meta-Learning from Kernel Ridge-Regression", "authorids": ["~Timothy_Nguyen1", "~Zhourong_Chen3", "~Jaehoon_Lee2"], "authors": ["Timothy Nguyen", "Zhourong Chen", "Jaehoon Lee"], "keywords": ["dataset distillation", "dataset compression", "meta-learning", "kernel-ridge regression", "neural kernels", "infinite-width networks", "dataset corruption"], "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. \nWe introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.", "one-sentence_summary": "We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nguyen|dataset_metalearning_from_kernel_ridgeregression", "pdf": "/pdf/e5bd67ca9948951b21c82c12b69280270a7bfe71.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnguyen2021dataset,\ntitle={Dataset Meta-Learning from Kernel Ridge-Regression},\nauthor={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=l-PrrQrK0QR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "l-PrrQrK0QR", "replyto": "l-PrrQrK0QR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2753/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089394, "tmdate": 1606915767556, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2753/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2753/-/Official_Review"}}}], "count": 12}