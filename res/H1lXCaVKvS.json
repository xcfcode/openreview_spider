{"notes": [{"id": "H1lXCaVKvS", "original": "Byg7iPWOPH", "number": 849, "cdate": 1569439178710, "ddate": null, "tcdate": 1569439178710, "tmdate": 1577168225856, "tddate": null, "forum": "H1lXCaVKvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["berendg@inf.u-szeged.hu", "kis-szabo.norbert@stud.u-szeged.hu"], "title": "Frustratingly easy quasi-multitask learning", "authors": ["G\u00e1bor Berend", "Norbert Kis-Szab\u00f3"], "pdf": "/pdf/12ef3a499c17788366fc2cf0a85a97ac75d2ea6d.pdf", "TL;DR": "We propose a computationally efficient alternative for traditional ensemble learning for neural nets.", "abstract": "We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the  tasks to be modeled are identical. We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique encouraging the model to develop an internal representation of the problem at hand that is beneficial to multiple output units of the classifier at the same time. This property hampers the convergence to such internal representations which are highly specific and tailored for a classifier with a particular set of parameters. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.\n", "code": "https://drive.google.com/drive/folders/16ORV5A0Zqo52h0vXt2eJwyGPZBmtRNWg?usp=sharing", "keywords": ["multitask learning", "ensembling"], "paperhash": "berend|frustratingly_easy_quasimultitask_learning", "original_pdf": "/attachment/9e2bae019ca527909e12543f81461ae130be4367.pdf", "_bibtex": "@misc{\nberend2020frustratingly,\ntitle={Frustratingly easy quasi-multitask learning},\nauthor={G{\\'a}bor Berend and Norbert Kis-Szab{\\'o}},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lXCaVKvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LF1xdc47TQ", "original": null, "number": 1, "cdate": 1576798707728, "ddate": null, "tcdate": 1576798707728, "tmdate": 1576800928609, "tddate": null, "forum": "H1lXCaVKvS", "replyto": "H1lXCaVKvS", "invitation": "ICLR.cc/2020/Conference/Paper849/-/Decision", "content": {"decision": "Reject", "comment": "One of the reviewers pointed out similarity to existing very recent work which would require significant reframing of the current paper. Hence, this work is below the bar at the moment.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu", "kis-szabo.norbert@stud.u-szeged.hu"], "title": "Frustratingly easy quasi-multitask learning", "authors": ["G\u00e1bor Berend", "Norbert Kis-Szab\u00f3"], "pdf": "/pdf/12ef3a499c17788366fc2cf0a85a97ac75d2ea6d.pdf", "TL;DR": "We propose a computationally efficient alternative for traditional ensemble learning for neural nets.", "abstract": "We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the  tasks to be modeled are identical. We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique encouraging the model to develop an internal representation of the problem at hand that is beneficial to multiple output units of the classifier at the same time. This property hampers the convergence to such internal representations which are highly specific and tailored for a classifier with a particular set of parameters. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.\n", "code": "https://drive.google.com/drive/folders/16ORV5A0Zqo52h0vXt2eJwyGPZBmtRNWg?usp=sharing", "keywords": ["multitask learning", "ensembling"], "paperhash": "berend|frustratingly_easy_quasimultitask_learning", "original_pdf": "/attachment/9e2bae019ca527909e12543f81461ae130be4367.pdf", "_bibtex": "@misc{\nberend2020frustratingly,\ntitle={Frustratingly easy quasi-multitask learning},\nauthor={G{\\'a}bor Berend and Norbert Kis-Szab{\\'o}},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lXCaVKvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1lXCaVKvS", "replyto": "H1lXCaVKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714132, "tmdate": 1576800263914, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper849/-/Decision"}}}, {"id": "ryx1Qf2ujr", "original": null, "number": 2, "cdate": 1573597718794, "ddate": null, "tcdate": 1573597718794, "tmdate": 1573597718794, "tddate": null, "forum": "H1lXCaVKvS", "replyto": "rkgOxr70FS", "invitation": "ICLR.cc/2020/Conference/Paper849/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "First, we would like to thank the reviewer for the valuable feedback on the submission.\n\nAdmittedly, we were not aware of the recent independent line of research from Meyerson and Miikkulainen.\nWe are very grateful to the review for drawing our attention to this publication.\n\nAfter reading the suggested ICML paper which introduces pseudo-task augmentation (PTA), we agree with the reviewer that what we propose in our paper is indeed very similar -- although not entirely identical -- to PTA. We also share that view of the reviewer that we can potentially deliver certain interesting complementary results to the PTA approach. Having became aware of PTA, we shall definitely reframe and back up with additional experiment certain claims in our paper.\nIn the upcoming days, we are planning to update the paper taking into consideration the suggestions and remarks of the reviewer that we found indeed very inspiring and helpful."}, "signatures": ["ICLR.cc/2020/Conference/Paper849/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper849/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu", "kis-szabo.norbert@stud.u-szeged.hu"], "title": "Frustratingly easy quasi-multitask learning", "authors": ["G\u00e1bor Berend", "Norbert Kis-Szab\u00f3"], "pdf": "/pdf/12ef3a499c17788366fc2cf0a85a97ac75d2ea6d.pdf", "TL;DR": "We propose a computationally efficient alternative for traditional ensemble learning for neural nets.", "abstract": "We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the  tasks to be modeled are identical. We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique encouraging the model to develop an internal representation of the problem at hand that is beneficial to multiple output units of the classifier at the same time. This property hampers the convergence to such internal representations which are highly specific and tailored for a classifier with a particular set of parameters. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.\n", "code": "https://drive.google.com/drive/folders/16ORV5A0Zqo52h0vXt2eJwyGPZBmtRNWg?usp=sharing", "keywords": ["multitask learning", "ensembling"], "paperhash": "berend|frustratingly_easy_quasimultitask_learning", "original_pdf": "/attachment/9e2bae019ca527909e12543f81461ae130be4367.pdf", "_bibtex": "@misc{\nberend2020frustratingly,\ntitle={Frustratingly easy quasi-multitask learning},\nauthor={G{\\'a}bor Berend and Norbert Kis-Szab{\\'o}},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lXCaVKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lXCaVKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper849/Authors", "ICLR.cc/2020/Conference/Paper849/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper849/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper849/Reviewers", "ICLR.cc/2020/Conference/Paper849/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper849/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper849/Authors|ICLR.cc/2020/Conference/Paper849/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165265, "tmdate": 1576860556908, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper849/Authors", "ICLR.cc/2020/Conference/Paper849/Reviewers", "ICLR.cc/2020/Conference/Paper849/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper849/-/Official_Comment"}}}, {"id": "H1ggnbnOsB", "original": null, "number": 1, "cdate": 1573597607915, "ddate": null, "tcdate": 1573597607915, "tmdate": 1573597607915, "tddate": null, "forum": "H1lXCaVKvS", "replyto": "Bygykw94qH", "invitation": "ICLR.cc/2020/Conference/Paper849/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We would like to thank R2 for the time spent reviewing the paper. Below we comment on the questions (Q) and remarks (R) included in the review.\n\nQ: What is the difference among multiple classifiers for the single task? Can they become identical?\nA: They are initialized with different weight matrices and we employ an extra ReLU nonlinearity to encourage their diversity, i.e. preventing the different decoders converging to the same set of parameters. In principle, the different decoders _could_ become identical, however, we have not experienced this to happen in our experiments (as a consequence of the measures we took for avoiding such an unwelcome outcome).\n\nR: The rationale behind Q-MTL is unclear to me.\nA: The basic assumption is that forcing the LSTM to develop such an internal representation which can serve as a useful input to multiple classifiers simultaneously helps it to obtain a more robust representation.\n\nR: Authors need to conduct more analyses to show why Q-MTL is superior to supervised learning.\nA: We have shown that Q-MTL consistently performs better than single task learning (STL). It is true that ensemble models typically performed better than Q-MTL, however, the computation need for constructing an ensemble classifier is orders of magnitude higher than that of Q-MTL. Indeed, the central question of our research was if it is possible to ''enjoy the benefits of ensemble learning, while avoiding its overhead for training models from scratch multiple times?''\n\nR: Authors claim that Q-MTL is equivalent to performing some regularization. However, I did not see any analysis on this aspect.\nA: Please, refer to the paragraph within Section 3.1.2 which is entitled \"The regularizing effect of Q-MTL\". Here we demonstrated that Q-MTL serves as an implicit form of weight decay regularization. Additionally, the fact that Q-MTL is more tolerant towards the presence of corrputed training instances with noisy labels implies that it can generalize better compared to the alternative approaches we experimented with. We believe that this improved generalization property of Q-MTL originates from the implicit regularization it performs.\n\nR: In experiments, the performance of Q-MTL is not so good when compared with ensemble learning.\nA: Please note that training an ensemble classifier requires orders of magnitudes more computational power, hence ''we regard its performance as a glass ceiling for Q-MTL''. We do show that Q-MTL has favorable performance over STL, with STL requiring a comparable computational budget to Q-MTL."}, "signatures": ["ICLR.cc/2020/Conference/Paper849/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper849/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu", "kis-szabo.norbert@stud.u-szeged.hu"], "title": "Frustratingly easy quasi-multitask learning", "authors": ["G\u00e1bor Berend", "Norbert Kis-Szab\u00f3"], "pdf": "/pdf/12ef3a499c17788366fc2cf0a85a97ac75d2ea6d.pdf", "TL;DR": "We propose a computationally efficient alternative for traditional ensemble learning for neural nets.", "abstract": "We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the  tasks to be modeled are identical. We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique encouraging the model to develop an internal representation of the problem at hand that is beneficial to multiple output units of the classifier at the same time. This property hampers the convergence to such internal representations which are highly specific and tailored for a classifier with a particular set of parameters. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.\n", "code": "https://drive.google.com/drive/folders/16ORV5A0Zqo52h0vXt2eJwyGPZBmtRNWg?usp=sharing", "keywords": ["multitask learning", "ensembling"], "paperhash": "berend|frustratingly_easy_quasimultitask_learning", "original_pdf": "/attachment/9e2bae019ca527909e12543f81461ae130be4367.pdf", "_bibtex": "@misc{\nberend2020frustratingly,\ntitle={Frustratingly easy quasi-multitask learning},\nauthor={G{\\'a}bor Berend and Norbert Kis-Szab{\\'o}},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lXCaVKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lXCaVKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper849/Authors", "ICLR.cc/2020/Conference/Paper849/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper849/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper849/Reviewers", "ICLR.cc/2020/Conference/Paper849/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper849/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper849/Authors|ICLR.cc/2020/Conference/Paper849/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165265, "tmdate": 1576860556908, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper849/Authors", "ICLR.cc/2020/Conference/Paper849/Reviewers", "ICLR.cc/2020/Conference/Paper849/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper849/-/Official_Comment"}}}, {"id": "rkgOxr70FS", "original": null, "number": 1, "cdate": 1571857647569, "ddate": null, "tcdate": 1571857647569, "tmdate": 1572972544464, "tddate": null, "forum": "H1lXCaVKvS", "replyto": "H1lXCaVKvS", "invitation": "ICLR.cc/2020/Conference/Paper849/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper considers a regularization technique, derived from multi-task learning, where multiple models with some shared parameters are jointly trained to solve copies of the same task. The technique is well-motivated as an efficient alternative to ensemble learning. The method is validated for a BiLSTM NLP model, which is applied to several POS tagging and named entity recognition tasks. The power of the technique as a regularizer is also demonstrated in the case of highly noisy labels, surprisingly, even outperforming ensemble learning in this setting.\n\nDespite this, my inclination is to reject the paper because of the substantial overlap with previous work and the limited scope of experiments.\n\nMy primary concern is that the proposed technique was previously introduced in [1]. This prior work is not acknowledged in the current paper. Perhaps it was overlooked because it is situated tightly in Multi-task Learning, whereas the present work is motivated mainly with respect to Ensembling.\n\nAlthough the general method was introduced previously, the paper does have some key experimental differences that would be interesting to see explored further.\n\n(1)\tThe paper uses a hidden layer in the separate classification heads, whereas previous work only used a linear classifier. The intuition that more complex heads will yield more diverse models is clear, but it would be great to see experimental evidence that this complexity helps. The conclusion states that the computational overhead is \u201cinfinitesimal\u201d; does increasing the complexity of the classifier trade cost for performance?\n(2)\tThis paper uses Eq. 3 to make predictions, whereas previous work found that this did not improve over simply using the best single prediction model, which makes prediction somewhat more efficient. Is there some experimental evidence that Eq. 3 leads to improvements?\n(3)\tThis paper considers the comparison to ensembling, whereas previous work only considered comparisons to single task and standard multitask learning. Additional experiments showing the advantages over ensembling could make this extension a significant contribution.\n(4)\tThis paper presents novel investigation of the regularization effects of the method, i.e., the resilience to noisy labels and the analysis of learned weight matrices. Is there a real problem where this resilience to noise will improve over ensembles, i.e., without randomly replacing labels? Such an experiment would make this point more compelling. Also, is there some underlying reason why the method outperforms ensembles in this case? Is it simply because the method is less expressive so cannot overfit?\n\nIn effect, if the paper could clearly show that (1) or other practical extensions lead to improvements over ensembling in settings where ensembling is commonly used, or enable ensembling in settings where vanilla ensembling fails (i.e., the case of noisy labels), then it could be a substantial contribution. The current scope of the experiments is too limited to conclusively show these points. For example, the technique can be applied to any architecture, but the experiments in the paper are limited to a single architecture; and additional experiments with architectures and tasks that commonly use ensembling would make the experiments more compelling, ideally with comparisons to external results.\n\nOther minor comments:\n-\tIt would be good to see the number of model parameters for easy comparison, especially in table 2 with different value of k.\n-\tIt looks like the x and y axis labels are swapped in Figure 3; from the Figure it looks like STL gets higher accuracies.\n-\tFigure 2 should say epochs instead of iterations.\n\n[1] Meyerson, E. & Miikkulainen R. \u201cPseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing---and Back\u201d, ICML 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper849/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper849/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu", "kis-szabo.norbert@stud.u-szeged.hu"], "title": "Frustratingly easy quasi-multitask learning", "authors": ["G\u00e1bor Berend", "Norbert Kis-Szab\u00f3"], "pdf": "/pdf/12ef3a499c17788366fc2cf0a85a97ac75d2ea6d.pdf", "TL;DR": "We propose a computationally efficient alternative for traditional ensemble learning for neural nets.", "abstract": "We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the  tasks to be modeled are identical. We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique encouraging the model to develop an internal representation of the problem at hand that is beneficial to multiple output units of the classifier at the same time. This property hampers the convergence to such internal representations which are highly specific and tailored for a classifier with a particular set of parameters. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.\n", "code": "https://drive.google.com/drive/folders/16ORV5A0Zqo52h0vXt2eJwyGPZBmtRNWg?usp=sharing", "keywords": ["multitask learning", "ensembling"], "paperhash": "berend|frustratingly_easy_quasimultitask_learning", "original_pdf": "/attachment/9e2bae019ca527909e12543f81461ae130be4367.pdf", "_bibtex": "@misc{\nberend2020frustratingly,\ntitle={Frustratingly easy quasi-multitask learning},\nauthor={G{\\'a}bor Berend and Norbert Kis-Szab{\\'o}},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lXCaVKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lXCaVKvS", "replyto": "H1lXCaVKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper849/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper849/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576371769014, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper849/Reviewers"], "noninvitees": [], "tcdate": 1570237746095, "tmdate": 1576371769028, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper849/-/Official_Review"}}}, {"id": "Bygykw94qH", "original": null, "number": 2, "cdate": 1572280022558, "ddate": null, "tcdate": 1572280022558, "tmdate": 1572972544423, "tddate": null, "forum": "H1lXCaVKvS", "replyto": "H1lXCaVKvS", "invitation": "ICLR.cc/2020/Conference/Paper849/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a quasi-multitask learning (Q-MTL) for supervised learning. The network architecture in Q-MTL borrows the idea of multi-task neural networks by sharing the latent representation among different classifiers which are designed for a single task.\n\nWhat is the difference among multiple classifiers for the single task? Can they become identical? The rationale behind Q-MTL is unclear to me. Authors need to conduct more analyses to show why Q-MTL is superior to supervised learning.\n\nAuthors claim that Q-MTL is equivalent to performing some regularization. However, I did not see any analysis on this aspect.\n\nIn experiments, the performance of Q-MTL is not so good when compared with ensemble learning."}, "signatures": ["ICLR.cc/2020/Conference/Paper849/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper849/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu", "kis-szabo.norbert@stud.u-szeged.hu"], "title": "Frustratingly easy quasi-multitask learning", "authors": ["G\u00e1bor Berend", "Norbert Kis-Szab\u00f3"], "pdf": "/pdf/12ef3a499c17788366fc2cf0a85a97ac75d2ea6d.pdf", "TL;DR": "We propose a computationally efficient alternative for traditional ensemble learning for neural nets.", "abstract": "We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the  tasks to be modeled are identical. We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique encouraging the model to develop an internal representation of the problem at hand that is beneficial to multiple output units of the classifier at the same time. This property hampers the convergence to such internal representations which are highly specific and tailored for a classifier with a particular set of parameters. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.\n", "code": "https://drive.google.com/drive/folders/16ORV5A0Zqo52h0vXt2eJwyGPZBmtRNWg?usp=sharing", "keywords": ["multitask learning", "ensembling"], "paperhash": "berend|frustratingly_easy_quasimultitask_learning", "original_pdf": "/attachment/9e2bae019ca527909e12543f81461ae130be4367.pdf", "_bibtex": "@misc{\nberend2020frustratingly,\ntitle={Frustratingly easy quasi-multitask learning},\nauthor={G{\\'a}bor Berend and Norbert Kis-Szab{\\'o}},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lXCaVKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lXCaVKvS", "replyto": "H1lXCaVKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper849/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper849/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576371769014, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper849/Reviewers"], "noninvitees": [], "tcdate": 1570237746095, "tmdate": 1576371769028, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper849/-/Official_Review"}}}], "count": 6}