{"notes": [{"id": "oXQxan1BWgU", "original": "xLa60pYYYn5", "number": 1975, "cdate": 1601308217450, "ddate": null, "tcdate": 1601308217450, "tmdate": 1614985636699, "tddate": null, "forum": "oXQxan1BWgU", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "mZw18Hduq4A", "original": null, "number": 1, "cdate": 1610040524729, "ddate": null, "tcdate": 1610040524729, "tmdate": 1610474133803, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a variant of MAML for meta-learning on tasks with a hierarchical tree structure. The proposed algorithm is evaluated on synthetic datasets, and it compares favorably to MAML. The reviewers identified several significant weaknesses, including: (1) the experimental evaluation is limited, and it only includes small synthetic datasets; (2) the proposed algorithm is incremental over MAML. The reviewers agreed that the paper cannot be accepted in its current form. I recommend reject."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040524716, "tmdate": 1610474133788, "id": "ICLR.cc/2021/Conference/Paper1975/-/Decision"}}}, {"id": "FfHOhvQqjG4", "original": null, "number": 6, "cdate": 1606142943912, "ddate": null, "tcdate": 1606142943912, "tmdate": 1606142943912, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "b5SZy3ChNHw", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment", "content": {"title": "Ideas have potential, but weaknesses cannot be addressed except in a future revision & resubmission", "comment": "Re: novelty: I agree that the novelty of the approach lies in modifying MAML to capture the hierarchical structure. However, by modifying an existing algorithm, you necessarily reuse some of the ideas that were unique/novel to its original application. For example, in this case: The use of gradient descent to tune initial parameters in a meta-learning setup is attributable to MAML, not this work.\n\nAs such, let me clarify my comment re: novelty: I evaluated the novelty attributable to the present submission to be small, exactly because it uses MAML as a starting point, but modifies the algorithm to be hierarchical in a particular sense and preliminarily applies it to corresponding hierarchical datasets. In particular, the novelty of the modification & application is the thing that I evaluated; I did not find this novelty to be significant in its present form, although the ideas have potential if my comment re: significance is addressed.\n\nRe: \"The clustering algorithm is run for each batch (following a batch the standard definition), generating a new tree each time in a dynamic fashion.\"\nThis is an incomplete explanation that does not explain the training vs. new task distinction in the submission; e.g.:\n\"When a new task is evaluated, we reuse the tree structure that was generated for a training batch and add the new task. This saves us from computing a new task hierarchy from scratch for every new task.\""}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oXQxan1BWgU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1975/Authors|ICLR.cc/2021/Conference/Paper1975/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853631, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment"}}}, {"id": "-bf1Ls84qtM", "original": null, "number": 5, "cdate": 1606141367974, "ddate": null, "tcdate": 1606141367974, "tmdate": 1606141480503, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "R31GImAdF2N", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment", "content": {"title": "Answer to \"weaknesses section\"", "comment": "Thanks for taking the time to review our paper:\n\n- The possible imbalance in bias-variance is an interesting observation. We believe that this algorithm may increase the bias towards the cluster, but the assumption is that the new task belongs to these clusters.\n\n- The fact that the Learned TreeMAML outperform the fixed one is surprising in the synthetic dataset, but inside statistical errors. We have checked the structure of the learned tree and for the simple synthetic examples, the fixed and the learned tree are the same.\n\n- We agree that more comprehensive empirical data is needed and we will work on it for future submissions.\n\n- We used the same numbers of inners steps for MAML than for TreeMAML to be as fair as possible. In the case of MAML the higher that is the number of inner steps the faster that algorithm converges and the higher is the accuracy, as shown in fig.3 of the MAML paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oXQxan1BWgU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1975/Authors|ICLR.cc/2021/Conference/Paper1975/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853631, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment"}}}, {"id": "nBmnE-RNQV5", "original": null, "number": 4, "cdate": 1606141238172, "ddate": null, "tcdate": 1606141238172, "tmdate": 1606141238172, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "_gpSY4lXeks", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment", "content": {"title": "Answer to \"Cons\"", "comment": "Thanks for your precise review. We provide clarifications to the \"Cons\" below:\n\n1. Novelty:\n- We believe that the Novelty of the approach is to modify the gradient descent to allow the NN to use the structure present on the data. And that we have shown that we can outperform MAML using this knowledge. This objective is already achieved with a fixed tree. There are many datasets that have an underlying known tree-structure as CIFAR, and Imagenet. And this is the case of some real-world data also.\n- The fact that this paper is using MAML as a starting point does not speak against the novelty of the concept. MAML is the base of many meta-learning papers as REPTILE, foMAML, etc. We believe that our algorithm is different from the one used in [2] since we use the gradients itself for the clustering of the tasks without the need of generating a previous task representation. Our clustering is dynamic and adapts to the new relation between gradients at each step of the gradients decent. \n\n2. Lack of extensive experiments:\n We agree with this point and will follow your recommendation. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oXQxan1BWgU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1975/Authors|ICLR.cc/2021/Conference/Paper1975/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853631, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment"}}}, {"id": "F8ZPIm1lSsd", "original": null, "number": 3, "cdate": 1606141137007, "ddate": null, "tcdate": 1606141137007, "tmdate": 1606141137007, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "9z_ERwfOt6", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment", "content": {"title": "Clarification about citations", "comment": " Thanks for your review:\n\n We acknowledge in the paper the existence of other papers that try to use the data structure. We cite the work more related to us, including Yao's paper. We believe that the novelty of the approach is to modify the gradient descent to allow the NN to use the structure present on the data, our algorithm works in a completely different way than the one presented by Yao.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oXQxan1BWgU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1975/Authors|ICLR.cc/2021/Conference/Paper1975/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853631, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment"}}}, {"id": "b5SZy3ChNHw", "original": null, "number": 2, "cdate": 1606140796836, "ddate": null, "tcdate": 1606140796836, "tmdate": 1606140974989, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "v5U9qJMdjT", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment", "content": {"title": "Comments about \"Weaknesses section\"", "comment": "Thank you for taking the time to review this paper, and produce such a clear review. We provide clarifications to the \"Cons\" below: \n\n1.We agree that the significance of the algorithm is more clear using real-world's dataset, and we will include this work in the future.\n\n2.We believe that the Novelty of the approach is to modify the gradient descent to allow the NN to use the structure present on the data. This objective is already achieved with a fixed tree. Therefore the particular algorithm used for clustering is not fundamentally relevant, but use as an example of how to furder extend the uses of the method. Also, the fact that this paper is using MAML as a starting point does not speak against the novelty of the concept. MAML is the base of many meta-learning papers as REPTILE, foMAML, etc.\n \n3.As stated above we did not try to introduce a new clustering algorithm in this paper and recognize that the used algorithm for clusterin was a simple modification of OTD. The clustering algorithm is run for each batch (following a batch the standard definition), generating a new tree each time in a dynamic fashion. In the synthetic examples, the similarity metric used is cosine similarity. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oXQxan1BWgU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1975/Authors|ICLR.cc/2021/Conference/Paper1975/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853631, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Comment"}}}, {"id": "R31GImAdF2N", "original": null, "number": 1, "cdate": 1603832772504, "ddate": null, "tcdate": 1603832772504, "tmdate": 1605024315602, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Review", "content": {"title": "Interesting ideas, but execution is weak.", "review": "#### Summary\n- In the context of gradient-based meta-learning for few-shot learning, the authors propose TreeMAML, an algorithm that leverages the existence of a tree structure in a task distribution in order to pool inner-loop gradients between tasks. More specifically, the inner-loop corresponds to a level-by-level traversal from root to leaves, where at each step, task-leaves that are children to a common node in the current level average their gradients for this step's update. \n- The authors consider two cases: one in which the ground-truth tree structure is known (Fixed TreeMAML) and one in which it is unknown and must be discovered online (Learned TreeMAML). The authors extend a hierarchical clustering algorithm from prior work for this purpose. \n- The authors evaluate TreeMAML in three scenarios: sinusoid regression (Finn et al. 2017), linear regression, and mixed regression (Yao et al. 2019). TreeMAML compares favorably against MAML and a naive multi-task learning baseline. \n- Interestingly, Learned TreeMAML also consistently outperforms Fixed TreeMAML.\n\n\n#### Strengths\n- The authors investigate an important and under-considered problem in meta-learning: how to leverage structure within a task distribution. \n\n- The proposed TreeMAML algorithm is conceptually simple.\n\n#### Weaknesses\n- The motivation for the TreeMAML algorithm is very weak. Yes, averaging gradients across tasks might decrease variance, but presumably always increases bias. This crucial trade-off is not even mentioned.\n\n- The fact that Learned outperforms Fixed is troubling. It suggests that TreeMAML is not properly leveraging the ground-truth task hierarchy. Dissecting Learned to look at the tree structure it proposes would help diagnose this issue.\n\n- This is a purely empirical paper which only presents results in toy regression settings. More comprehensive empirical evaluation is needed, e.g. the image classification benchmark proposed in Yao et al. (2019), which is perfectly suitable for this paper (and indeed the authors took Experiment 3 from this work). \n\n- The comparison between TreeMAML and MAML might not be very fair. MAML is artificially constrained to use the same number of inner-loop steps as the depth of the task tree. Since MAML makes no assumptions about the task tree, this should be a tunable hyperparameter.\n\n#### Recommendation\n- I currently recommend a clear reject (3). Given the weaknesses outlined above, this submission does not meet the bar for acceptance.\n\n#### Questions\n- How is the Baseline model trained? Its MSE in Table 2 is uncommonly high.\n\n#### Minor suggestions\n- Please fix the numerous typos throughout the submission. Just in the first paragraph: inappropriate capitalization of multi-task and meta-learning; \"The field of of\".\n- K is used to denote the number of inner gradient steps in Alg. 1, but the number of datapoints per task in Sec. 4.1.\n- The num_shots=3 case in Fig. 3b directly contradicts the caption.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106628, "tmdate": 1606915807049, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1975/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Review"}}}, {"id": "_gpSY4lXeks", "original": null, "number": 2, "cdate": 1603834793816, "ddate": null, "tcdate": 1603834793816, "tmdate": 1605024315533, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Review", "content": {"title": "Model agnostic meta-learning on trees", "review": "##########################################################################\n\nSummary:\nThe paper studies a simple modification of MAML to address the problem of meta-learning hierarchical\ntask distributions. It is based on the assumption that learning tasks by gradient descent may benefit from gradient sharing, across similar tasks. The authors convert the problem of task transfer to gradient clustering by considering the point that the similarity of tasks can be measured by the similarity of gradients.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for rejecting this paper. Unfortunately, I found the novelty of the paper very low. I believe that this paper is like an incremental study of exist work like MAML[1] and HSML[2].\n[1]https://arxiv.org/abs/1703.03400\n[2]http://proceedings.mlr.press/v97/yao19b/yao19b.pdf\n\n \n##########################################################################\nPros: \n- The paper is clear and cites the most relevant research studies and papers.\n- The experimental results presents the promising performance of the method.\n- It is a strength point of the paper that the authors address both foxed tree and  learned tree structures.\n\n##########################################################################\n\nCons: \n- The novelty of the paper is very low. The authors refer to [2] in the section 2 of the paper and mention that it is not task agnostic.\na) I believe HSML[2] is also built on MAML and not sure about this claim.\n- Lack of extensive experiments:\na) The paper does not include any experiment or discussion comparing the proposed method with HSML[2] or other exist method. I would suggest the authors to consider running more experiments.\nb) The time complexity of the method has not been discussed.\n \n##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n \n#########################################################################\n\n\n\n\n ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106628, "tmdate": 1606915807049, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1975/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Review"}}}, {"id": "9z_ERwfOt6", "original": null, "number": 3, "cdate": 1603893993624, "ddate": null, "tcdate": 1603893993624, "tmdate": 1605024315471, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "What is this paper about, what contributions does it make, what are the main strengths and weaknesses?\nThis paper proposed a learning algorithm of meta-learning: TreeMAML, to share information across tasks in meta-learning models. The paper compared the results of TreeMAML with MAML and the Baseline on SinusRegression Task and Linear Regression Task.\n\nThe main concerns are:\n\n1.\tSome statements in the introduction session are improper. \n\n1.1\t\u201cthere is still a lack of methods for sharing information across tasks in meta-learning models, and the goal of our work is to fill this gap.\u201d See references:\na.\tInvenio: Discovering hidden relationships between tasks/domains using structured meta learning. 2019. \nb.\tHierarchical meta learning. 2019. \nc.\tHierarchically structured meta-learning. 2019. \nd.\tLearning to propagate for graph meta-learning. 2019. \n\n1.2\t\"However, these algorithms are not model-agnostic.\" Most of the related models are model-agnostic as long as replacing the task representation module. \n\n\n\n2.\tThe contribution of this work is supposed to be emphasized. The innovation of this paper seems insufficient. \n\n2.1\tYao et al. (2019). also aims to utilize the task relation and apply gradient-based adaptation methods. And the model structure presented in figure 1 is similar to the model framework in Yao\u2019s paper.  \n\n2.2\tAs claimed in this paper, the hierarchical clustering algorithm, which is an essential part of the proposed model, is introduced with minor modification from Menon et al. (2019). \n\n3.\tAs claimed in this paper, the proposed model lacks scalability since it works well only in the tree-structured tasks. \n\n4.\tThe baselines exclude many relevant works such as the papers listed above.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106628, "tmdate": 1606915807049, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1975/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Review"}}}, {"id": "v5U9qJMdjT", "original": null, "number": 4, "cdate": 1604170150060, "ddate": null, "tcdate": 1604170150060, "tmdate": 1605024315403, "tddate": null, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "invitation": "ICLR.cc/2021/Conference/Paper1975/-/Official_Review", "content": {"title": "While appropriately taking into account task similarity & structure in meta-learning and related fields is an important open problem, the scope of the paper is limited to a specific combination of existing algorithms on synthetic datasets. The submission also needs some work to make its methodology clearer.", "review": "The submission proposes a meta-learning algorithm attuned to the hierarchical structure of a dataset of tasks. Hierarchy is enforced in a set of synthetically-generated regression tasks via the data-sampling procedure, which is modified from the task-sampling procedure of [1] to include an additional source of randomness corresponding to which of a set of cluster components task parameters are generated from. The authors propose to adapt the model-agnostic meta-learning algorithm (MAML) of [1] to reflect this hierarchical structure by either observing (Section 4.1, FixedTree MAML) or inferring (Section 4.2, LearnedTree MAML) an assignment of tasks to clusters at each step of the inner loop (task-specific adaptation phase) of MAML; if tasks belong to the same cluster, the correspond task-parameters receive the same update at that step (in particular, the update direction is averaged). It is assumed that there are increasingly many clusters at each step, so that task-specific parameter updates are increasingly granular.\n\n##### Strengths:\n1) **Clarity**: The experimental setting and exactly how the data-generating process relates to the proposed algorithms are clearly described.\n2) **Significance**: Results on the hierarchically structured synthetic regression task datasets demonstrate that {Fixed|Learned}Tree MAML: is at least as good as MAML, and often outperforms MAML; that it learns more efficiently than a MAML in terms of the cumulative number of datapoints observed; and that both MAML and {Fixed|Learned}Tree MAML outperform a naive baseline.\n\n##### Weaknesses:\n1) **Significance**: Since the evidence provided in favor of the proposed algorithm is in the form of an empirical evaluation on a synthetically generated dataset, the present impact of the algorithm is limited. In particular, there is no evidence that (i) the algorithm works for larger and/or more complex datasets; and (ii) that natural datasets of interest to the community exhibit a hierarchical structure analogous to the synthetic datasets presented in the submission.\n2) **Novelty**: The algorithm modifies and combines previously introduced components: the MAML algorithm of [1]; the online top-down clustering algorithm of [2], and the task-similarity-as-gradient-similarity approach of [3].\n3) **Clarity**: Specific details surrounding the relationship between Algorithm  1 and Algorithm 2 are insufficiently discussed: \n  i) Algorithm 2 as it appears in the text is very similar to Algorithm 1 (The OTD algorithm) in [2] with the exception of the new hyperparameter $\\xi$, and introduces new symbols that do not appear elsewhere in the text. It is therefore not sufficiently adapted for clarity in the context of this work.\n  ii) Whether Algorithm 2 acts as a strict subroutine of Algorithm 2 is not stated. I believe it is not because the clustering decision for a new task relies on tree structures that are \"generated for a training batch,\" although what a \"training batch\" refers to is not clear. Similarly, how the \"online\"/\"offline\" distinction in the context of the clustering algorithm fits into the training/evaluating setup borrowed from [1] is not made clear.\n  iii) How exactly the task-similarity approach of [3] is employed in Algorithm 2 is not made clear. The only mention of the use of [3] is briefly around Eq. (8) before the main algorithm (Algorithm 1) is introduced, and Algorithm 2 only refers to a generic \"similarity metric\" (as in the original work, [1]).\n\n##### References\n\n[1] [Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" arXiv preprint arXiv:1703.03400 (2017).](https://arxiv.org/abs/1703.03400)\n\n[2] [Menon, Aditya Krishna, Anand Rajagopalan, Baris Sumengen, Gui Citovsky, Qin Cao, and Sanjiv Kumar. \"Online Hierarchical Clustering Approximations.\" arXiv preprint arXiv:1909.09667 (2019).](https://arxiv.org/abs/1909.09667)\n\n[3] [Achille, Alessandro, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. \"Task2vec: Task embedding for meta-learning.\" In Proceedings of the IEEE International Conference on Computer Vision, pp. 6430-6439. 2019.](https://arxiv.org/abs/1902.03545)", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1975/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1975/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Model agnostic meta-learning on trees", "authorids": ["jezabel.garcia@mtkresearch.com", "federica.freddi@mtkresearch.com", "j.mcgowan.18@ucl.ac.uk", "tim@nieradzik.me", "ds.shiu@mtkresearch.com", "tiany.03@gmail.com", "~Alberto_Bernacchia1"], "authors": ["Jezabel Garcia", "Federica Freddi", "Jamie McGowan", "Tim Nieradzik", "Da-shan Shiu", "Ye Tian", "Alberto Bernacchia"], "keywords": ["Meta-learning", "hierarchical data", "clustering"], "abstract": "In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related, and sharing information between unrelated tasks might hurt performance. A fruitful approach is to share gradients across similar tasks during training, and recent work suggests that the gradients themselves can be used as a measure of task similarity.\nWe study the case in which datasets associated to different tasks have a hierarchical, tree structure. While a few methods have been proposed for hierarchical meta-learning in the past, we propose the first algorithm that is model-agnostic, a simple extension of MAML. As in MAML, our algorithm adapts the model to each task with a few gradient steps, but the adaptation follows the tree structure: in each step, gradients are pooled across task clusters, and subsequent steps follow down the tree. We test the algorithm on linear and non-linear regression on synthetic data, and show that the algorithm significantly improves over MAML. Interestingly, the algorithm performs best when it does not know in advance the tree structure of the data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "garcia|model_agnostic_metalearning_on_trees", "one-sentence_summary": "We propose a modification of MAML to learn a hierarchical distribution of tasks", "pdf": "/pdf/62254ac5068a4af11030ad0a5d60200582633ec4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=BohG811A_F", "_bibtex": "@misc{\ngarcia2021model,\ntitle={Model agnostic meta-learning on trees},\nauthor={Jezabel Garcia and Federica Freddi and Jamie McGowan and Tim Nieradzik and Da-shan Shiu and Ye Tian and Alberto Bernacchia},\nyear={2021},\nurl={https://openreview.net/forum?id=oXQxan1BWgU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oXQxan1BWgU", "replyto": "oXQxan1BWgU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1975/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106628, "tmdate": 1606915807049, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1975/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1975/-/Official_Review"}}}], "count": 11}