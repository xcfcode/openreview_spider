{"notes": [{"id": "5sXAs8IfH2", "original": null, "number": 13, "cdate": 1580691143760, "ddate": null, "tcdate": 1580691143760, "tmdate": 1580691143760, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Hi, \nCan you please release the architecture search code for the CIFAR10 dataset also?\n\nThanks", "title": "Architecture search code for CIFAR10 dataset"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "ByxWo8xmgH", "original": null, "number": 12, "cdate": 1561687704891, "ddate": null, "tcdate": 1561687704891, "tmdate": 1561687704891, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "When we train the architecture parameters, is in every layer we sample two paths to update or only a single layer\u2019s two path\u2019s parameters will be update?\n\n If only one layer\u2019s parameters update, how to choose paths of the rest layers? The highest-weight one or a random one?\n", "title": "How to choose paths while training in ProxylessNAS\uff1f"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "rygMExlwAN", "original": null, "number": 11, "cdate": 1559851050151, "ddate": null, "tcdate": 1559851050151, "tmdate": 1559851068744, "tddate": null, "forum": "HylVB3AqYm", "replyto": "SJgnTVbQK4", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "It would be really helpful if you could release the full code for this project. Since you define a new search space that's good for one-shot methods, it could become a new benchmark if it's easy to use your code to do further experiments in this space.", "title": "+1, please release code"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "SJgnTVbQK4", "original": null, "number": 10, "cdate": 1554351299923, "ddate": null, "tcdate": 1554351299923, "tmdate": 1554351299923, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Dear authors,\nI am working NAS also, and I am very interested in this paper. However, I found that you just release pretrained models and evaluation code, is it possible to release the main search code in the future?\n\nSincerely\n\n", "title": "is it possible to release search code?"}, "signatures": ["~Miao_Zhang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Miao_Zhang1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "HylVB3AqYm", "original": "HygMGr1qFm", "number": 1534, "cdate": 1538087996212, "ddate": null, "tcdate": 1538087996212, "tmdate": 1550854102297, "tddate": null, "forum": "HylVB3AqYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 32, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bke8s59qEE", "original": null, "number": 9, "cdate": 1549605533797, "ddate": null, "tcdate": 1549605533797, "tmdate": 1549605533797, "tddate": null, "forum": "HylVB3AqYm", "replyto": "S1lC-BtXpX", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Dear Authors,\n\nAs the paper has now been accepted, I kindly request you to release the training code for the paper. Thank you.", "title": "Code for training"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "r1ehBY95e4", "original": null, "number": 8, "cdate": 1545410883738, "ddate": null, "tcdate": 1545410883738, "tmdate": 1545410883738, "tddate": null, "forum": "HylVB3AqYm", "replyto": "rkx-525OJ4", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Thanks for the answers! Congratulations on acceptance! ", "title": "Thanks!"}, "signatures": ["~Robin_Tibor_Schirrmeister1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Robin_Tibor_Schirrmeister1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "Byxc9SDlgE", "original": null, "number": 1, "cdate": 1544742290423, "ddate": null, "tcdate": 1544742290423, "tmdate": 1545354514644, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Meta_Review", "content": {"metareview": "This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc. Although the novelty of the paper may be limited, empirical performance seems impressive. The source code is not available. I think this is a borderline paper but maybe good enough for acceptance.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Good empirical results. Novelty is limited."}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1534/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352802643, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352802643}}}, {"id": "rJe_QI3wlV", "original": null, "number": 7, "cdate": 1545221664303, "ddate": null, "tcdate": 1545221664303, "tmdate": 1545221664303, "tddate": null, "forum": "HylVB3AqYm", "replyto": "S1lC-BtXpX", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Dear the authors,\n\nI want to echo with the reviewers/public readers that releasing your detailed training pipeline is quite crucial given the good performances reported in the paper. Furthermore, only evaluation code/model ckpts is definitely not enough since people have various unreasonable ways to obtain a good ckpt only on the test set (I'm not meaning you are doing this and sorry for possible offense here in advance).\n\nBest  ", "title": "Only releasing training code is meaningful"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "rkx-525OJ4", "original": null, "number": 18, "cdate": 1544232072568, "ddate": null, "tcdate": 1544232072568, "tmdate": 1544232072568, "tddate": null, "forum": "HylVB3AqYm", "replyto": "S1x0oTiHR7", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Re: Further questions", "comment": "Thanks for your questions. Please see responses below.\n\n>>> \u201cWhat was the search time on CIFAR-10 in GPU hours? For Proxyless-R and Proxyless-G?\u201d\n\nThe search time depends on the size of the backbone architectures (e.g., number of blocks). For example, when searching with 54 blocks, it takes around 4 days on a single GPU for both Proxyless-R and Proxyless-G. When searching with fewer blocks (e.g. 8 blocks), it takes less than 1 day. \n\n>>> \u201cIs Batch Normalization in training or evaluation mode when optimizing architecture parameters?\u201d\n\nThe batch normalization is in the training mode.\n\n>>> \u201cFor REINFORCE, what do you use as optimization metric on validation set for architecture parameters on CIFAR-10? Normal loss, like cross entropy or actually misclassification rate?\u201d\n\nWe use the misclassification rate. Normal loss, like cross entropy, may also be a feasible optimization metric\n\n>>> \u201cFor REINFORCE, do you use any kind of baselining? Do you use multiple architecture samples per update?\u201d\n\nThe baseline is the moving average of previous mean metrics with a decay of 0.99. And we update every 8 samples.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "Hke4jK9OkE", "original": null, "number": 17, "cdate": 1544231323525, "ddate": null, "tcdate": 1544231323525, "tmdate": 1544231323525, "tddate": null, "forum": "HylVB3AqYm", "replyto": "H1x6eyDe0X", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Re: replacement=True or False?", "comment": "Apologize for the mistake. The correct one is setting \"replacement=False\". Beta2 is set to be the default value in Pytorch (i.e., 0.999). As for network parameters, we use SGD optimizer with Nesterov momentum 0.9 and cosine learning rate schedule."}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "H1eNgOayAX", "original": null, "number": 8, "cdate": 1542604779767, "ddate": null, "tcdate": 1542604779767, "tmdate": 1544146618718, "tddate": null, "forum": "HylVB3AqYm", "replyto": "rkle4Pwda7", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Responses to implementation questions", "comment": "Hi Robin, \n\nThanks for your interest in our work and your detailed questions. \n\n>>> Response to \"Rescaling architecture parameters\" \nYour understanding of the gradient-based updates is correct.  \nAs for sampling two paths according to the multinomial distribution, we use \"torch.multinomial()\". And by setting \"replacement=False\", the same path will not be chosen twice. \n\n>>> Response to \"Adam optimizer for architecture parameters\" \nWe also consider it would be problematic to use the adaptive gradient averages for this case where most of the paths are not chosen. So we set beta1 to be 0 in the Adam optimizer.  Sampling multiple times before making an Adam update step is a nice idea. We will try it later. Thanks for your suggestion. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "rJe1QITpR7", "original": null, "number": 16, "cdate": 1543521814785, "ddate": null, "tcdate": 1543521814785, "tmdate": 1543694184413, "tddate": null, "forum": "HylVB3AqYm", "replyto": "rJl4Qn5KAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Thanks for your helpful feedback.", "comment": "Thank you for your helpful feedback. We have revised our paper according to your suggestion.\n\n>>> \u201cin the new mobile phone results you have presented there is a network that actually has better latency with slightly worse accuracy, which makes it hard to compare\u201d\n\n2.6% top-1 accuracy improvement on ImageNet is significant. To achieve the same accuracy, MobileNetV2 needs 2x latency (143ms v.s. 78ms). Please see Figure 4.\n\n>>> \u201cIt would be nice to actually have a table showing the strengths/weaknesses along these axes for all of these methods\u201d\n\nThanks for your suggestion. We will add the table to our paper. \n\nModel\t                             Top-1\t  Top-5\tLatency\tHardware-Aware\t  No-Proxy\tNo-Repeating\tTime\tMemory\nMobilenetV1\t                      70.6\t   89.5\t 113ms\t              -\t                         -\t                  No\t                    -\t               -\nMobilenetV2\t                      72.0\t   91.0\t  75ms\t              -\t                         -\t                  No\t                    -                  -\nNASNet-A\t                      74.0\t   91.3\t 183ms\t            No\t               No                      No                  10^4  \t   10^1\nAmoebaNet-A\t              74.5\t   92.0\t 190ms\t            No\t               No\t                  No\t                 10^4         10^1\nDarts\t                              73.1\t   91.0\t      -\t                    No\t               No\t                  No\t                 10^2\t   10^2\nMnasNet\t                      74.0\t   91.8\t  79ms\t            Yes\t               No\t                  No\t                 10^4    \t  10^1\nProxylessNAS (mobile)      74.6\t   92.2\t  78ms\t            Yes\t               Yes\t                  Yes                 10^2    \t  10^1\n\n>>> \u201cprecisely define what is novel about the method\u201d and \u201cemphasize exactly the empirical contribution\u201d\n\nWe summarize our contributions as follows:\n\n> Methodologically,\na) We provided a new path-level pruning perspective for NAS.\n\nb) We proposed a gradient-based approach (Section 3.3.1) to handle non-differentiable hardware objectives (e.g. latency), making them differentiable by introducing regularization loss.\n\nc) We proposed a path-level binarization approach to address the high memory consumption issue of differentiable NAS. Notably, different from BinaryConnect that binarizes each weight, our path-level binarization approach binarizes the entire path.\n\n> Empirically,\na) We significantly reduced the cost of memory/compute for the training of large over-parameterized networks and thereby scaled to large-scale datasets (ImageNet) without proxy and repeating blocks.\n\nb) We studied specialized neural network architectures for different hardware architectures and showed its advantage, raising people\u2019s awareness of specializing neural network architectures for hardware.\n\nc) We achieved strong empirical results on both CIFAR-10 and ImageNet. On different hardware platforms (GPU, CPU and mobile phone), our models not only significantly outperform previous state-of-the-arts, but also peer submissions.\n\nWe sincerely thank your feedback and hopefully have cleared your concerns.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "HyxWxsxaCQ", "original": null, "number": 15, "cdate": 1543469801096, "ddate": null, "tcdate": 1543469801096, "tmdate": 1543469801096, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HJelAtlcRm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Thanks for your further feedback. We have revised the paper accordingly.", "comment": "Thank you for your reply and detailed suggestion. We have uploaded a revision of our paper and removed the number of search space size. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "BklS-ur9h7", "original": null, "number": 1, "cdate": 1541195772732, "ddate": null, "tcdate": 1541195772732, "tmdate": 1543273024571, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Review", "content": {"title": "Interesting idea for efficient NAS that gives state-of-the-art results (on limited datasets)", "review": "The algorithm described in this paper is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. The algorithm is similar to DARTS in that it it has weights that determine how important the various possible nodes are, but the interpretation here is stochastic, in that the weight indicates the probability of the component being active. Two methods to train those weights are being suggested, using REINFORCE and using BinaryConnect, both having different trade offs.\n\n- (minor) *cumbersome* network seems the wrong term, maybe over-parameterized network?\n- (minor) I do not think that the size of the search space a very meaningful metric\n\nPros:\n- Good exposition\n- Interesting and fairly elegant idea\n- Good experimental results\n\nCons\n- tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture. I think this is the main shortcoming, although shared by many NAS papers\n- No source code available\n\nSome typos:\n\n- Fo example, when proxy strategy -> Fo*r* example\n- normal training in following ways. -> in *the* following ways\n- we can then derive optimized compact architecture.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Review", "cdate": 1542234209467, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335966747, "tmdate": 1552335966747, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJelAtlcRm", "original": null, "number": 12, "cdate": 1543272903870, "ddate": null, "tcdate": 1543272903870, "tmdate": 1543272903870, "tddate": null, "forum": "HylVB3AqYm", "replyto": "rJxuErCmTm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Thank you for your response", "comment": "Thank you for your response. I particularly appreciate the release of the source code, while I did not have time to dig into it, it definitely increases the trust from the reader.\n\nRegarding the limited experiments, consider it a criticism towards the sub-field in general, not to this paper in particular. It just seems a bit counter to the narrative of automatically selecting architectures if only a very limited amount of architectures are found.\n\nI do appreciate how this paper is searching a slightly more varied architecture search compared to some previous methods, but I do not think the search space absolute size (10^547) says much in this regard, it would be easy to artificially come up with large search spaces with little variety as well as small search spaces with a lot of variety. My personal opinion is that it would be better to omit the number, mist giving the impression that it has more meaning than it has, but consider it a very minor point :)\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "rJl4Qn5KAQ", "original": null, "number": 11, "cdate": 1543248924077, "ddate": null, "tcdate": 1543248924077, "tmdate": 1543248924077, "tddate": null, "forum": "HylVB3AqYm", "replyto": "SJlO7KpVp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Thank you for the response", "comment": "\n Thanks for the detailed response. Please see comments below. \n\n> a) Our proxy-less NAS is the first NAS algorithm that directly learns architectures on the large-scale dataset (e.g. ImageNet) without any proxy. \n\nI agree but this is not a method/algorithmic contribution but an empirical one. The way you achieve this is by combining existing methods (which I listed in the original review), which allows the reduction of memory usage/computation compared to One-Shot/DART. I should emphasize that there is nothing particularly wrong with combining methods (especially across areas/fields) but just makes the empirical contribution and thoroughness of the analysis more important. However, the method/algorithmic contributions should be made clear in a precise manner, rather than making large general statements. \n\n> b) Our proxy-less NAS is the first NAS algorithm that breaks the convention of repeating blocks in neural architecture design. \n\n  I am not sure this is the case. Neuroevolution methods (which you should cite more heavily) do not necessarily require this, e.g. [1]. However, I agree that within the regime of training over-parameterized networks or methods scalable. Again, please state your advantages explicitly; you seem to mention one axis/dimension at a time (e.g. scalability, no proxy, no repeating cell structure) yet your advantages are really at the combination of these. It would be nice to actually have a table showing the strengths/weaknesses along these axes for all of these methods, which would make it more clear.\n\n[1] Large-Scale Evolution of Image Classifiers, Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, Alex Kurakin, https://arxiv.org/abs/1703.01041\n\n> The new interesting design patterns, found by our method, can provide new insights for efficient neural architecture design.\n\nI agree with this and mentioned it in the review.\n\n> c) Our method builds upon methods from two communities (one-shot architecture search from NAS community and Pruning/BinaryConnect from model compression community). \n\nAgain, I agree but this means that it *is* a combination of methods (which contradicts your rebuttal title). \n\n> With latency constraints, our optimized models also achieved state-of-the-art results (3.1% higher top-1 accuracy while being 1.2x faster on GPU and 2.6% higher top-1 accuracy with similar latency on mobile phone, compared to MobileNetV2). \n> Besides, we directly optimize the latency, rather than an inaccurate proxy (i.e. FLOPs). \n\nI agree it's interesting to optimize for these non-differentiable objectives. However, it seems to me that given that you are optimizing directly for them, the actual gains are not that large. For example, in the new mobile phone results you have presented there is a network that actually has better latency with slightly worse accuracy, which makes it hard to compare:\n\nMobileNet V2\t\t72.0\t\t91.0\t\t75ms\nProxyless NAS (ours)\t74.6\t\t92.2\t\t78ms\n\nIn all, it would be great for the authors to precisely define what is novel about the method (if it is not a combination of existing methods, as you claim in the rebuttal title). If it is a combination of methods (which again should not necessarily be seen as a bad thing), then it would be great to emphasize exactly the empirical contribution (the largest of which seems to be the reduction of memory/compute for training of large over-parameterized networks, scaled to ImageNet-sized datasets). The optimization of a non-differentiable objective can also be a smaller contribution, but is common to RL-based methods. Again, I think this paper presents some nice results, but it is important to be precise and not make more general claims than warranted. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "S1x0oTiHR7", "original": null, "number": 6, "cdate": 1542991269916, "ddate": null, "tcdate": 1542991269916, "tmdate": 1542991291479, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Thanks for answering the questions so far, I also have some further questions.\n\n1. What was the search time on CIFAR-10 in GPU hours? For Proxyless-R and Proxyless-G?\n2. Is Batch Normalization in training or evaluation mode when optimizing architecture parameters?\n3. For REINFORCE, what do you use as optimization metric on validation set for architecture parameters on CIFAR-10? Normal loss, like cross entropy or actually misclassification rate?\n4. For REINFORCE, do you use any kind of baselining? Do you use multiple architecture samples per update? For example, right now I sample 10 architectures for each validation data batch and also subtract the mean metric/reward/loss before I compute the gradients.", "title": "Further questions, also regarding REINFORCE"}, "signatures": ["~Robin_Tibor_Schirrmeister1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Robin_Tibor_Schirrmeister1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "SJlO7KpVp7", "original": null, "number": 5, "cdate": 1541884192270, "ddate": null, "tcdate": 1541884192270, "tmdate": 1542906304702, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HkxEMyl33m", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "proxy-less NAS is an important contribution that breaks many conventions and stereotypes of neural architecture design.  It's not a combination of existing methods.", "comment": "We sincerely thank you for your comprehensive comments and constructive advices.\n\n>>> Response to \u201ccombination of existing methods\u201d: \nThanks for your kind advice on organizing the paper to make our contributions more clear. Here, we would like to emphasize our contributions:\n\na) Our proxy-less NAS is the first NAS algorithm that directly learns architectures on the large-scale dataset (e.g. ImageNet) without any proxy. We also solved an important problem improving the computation efficiency of NAS as we reduced the computational cost (GPU hours and GPU memory) of NAS to the same level as normal training. Moreover, the GPU memory requirement of our method keeps at O(1) complexity rather than grows linearly with the number of candidate operations O(N)  [3, 4]. Therefore, our method can easily support a large candidate set while DARTS and One-Shot cannot. \t\n\nb) Our proxy-less NAS is the first NAS algorithm that breaks the convention of repeating blocks in neural architecture design. From Alexnet and VGG to ResNet and MobileNet, manually designed CNNs used to repeat blocks within the same stage. Previous NAS works keep the tradition as otherwise the searching cost will be unaffordable. Our work breaks the constraints, and we found this is actually a stereotype that needs to be corrected. \n\nThe new interesting design patterns, found by our method, can provide new insights for efficient neural architecture design. For example, people used to stack multiple 3x3 convs to replace a single large kernel conv, as this uses fewer parameters while keeping a similar receptive field. But we found this pattern may not be proper for designing efficient (low latency) networks: Two 3x3 depthwise separable convs actually run slower than a single 5x5 depthwise separable conv.  Our GPU model, shown in Figure 4, incorporates large kernel convs and aggressively pools at early stages to shrink network depth. Then the model chooses computation-expensive operations at low-resolution stages. It also tends to choose computation-expensive operations in the first block within each stage where the feature map is downsampled.  As a consequence, our GPU model can outperform previous SOTA efficient architectures in accuracy performances (e.g. 3.1% higher top-1 than MobileNetV2), while running faster than them (e.g. 1.2x faster than MobileNetV2). Such patterns cannot be found by previous NAS, as they optimize on proxy task and force blocks to share structures.\n\nc) Our method builds upon methods from two communities (one-shot architecture search from NAS community and Pruning/BinaryConnect from model compression community). It is the first time to incorporate ideas from the model compression community to the NAS community and we also provide a new path-level pruning perspective for one-shot architecture search. Moreover, we provide a unified framework for both gradient-based updates and REINFORCE-based updates. \n\nd) Our proxy-less NAS achieved very strong empirical results on two most representative benchmarks (i.e. CIFAR and ImageNet). On CIFAR-10, our optimized model reached 2.08% error rate with only 5.7M parameters, outperforming previous state-of-the-art architecture (AmeobaNet-B with 34.9M parameters). On ImageNet, we searched specialized neural network architectures for three different platforms (GPU, CPU and mobile phone). With latency constraints, our optimized models also achieved state-of-the-art results (3.1% higher top-1 accuracy while being 1.2x faster on GPU and 2.6% higher top-1 accuracy with similar latency on mobile phone, compared to MobileNetV2). \n\nBesides, we directly optimize the latency, rather than an inaccurate proxy (i.e. FLOPs). It\u2019s an important concept that low FLOPs doesn\u2019t translate to low latency. All our speedup numbers are reported with real measured latency. We believe both our efficient search methodology and the resulting efficient models have big industry impact. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "rJxuErCmTm", "original": null, "number": 3, "cdate": 1541821744163, "ddate": null, "tcdate": 1541821744163, "tmdate": 1542747664080, "tddate": null, "forum": "HylVB3AqYm", "replyto": "BklS-ur9h7", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Proxyless NAS enables efficient and direct search on different tasks and hardware platforms", "comment": "We sincerely thank you for the detailed comments on our paper. We have revised the paper and fixed the typos accordingly.\n\n>>> Response to \u201climited amount of tested settings\u201d: \nAs our proxy-less NAS has reduced the cost to the same level of normal training (100x more efficient on ImageNet), it is of great interest for us to apply proxy-less NAS to more settings and datasets. However, for this work, considering the resource constraints and time limits, we have strong reasons to believe that our experiment settings are sufficient:\n\na) Our experiments are conducted on two most representative benchmarks (CIFAR and ImageNet). It is in line with previous NAS papers and also makes it possible to compare our method with previous NAS methods. We also experimented with 3 different hardware platforms and observed consistent latency improvement over previous work. \n\nb) Moreover, on the challenging ImageNet classification task, we have conducted architecture search experiments under three different settings (GPU, CPU and Mobile) while previous NAS papers mainly transfer learned architectures from CIFAR-10 to ImageNet without conducting architecture search experiments on ImageNet [1, 2]. \n\n>>> Response to \u201cno source code available\u201d: \nReviewer 2 also has similar requests, based on the concern on our strong empirical results. Our pre-trained models and the evaluation code are provided in the following anonymous link: https://goo.gl/QU3GhA. Besides, we have also uploaded the video visualizing the architecture search process: https://goo.gl/VAzGJs. We plan to open source our project upon publication.\n\n>>> Response to \u201cthe size of the search space is not a very meaningful metric\u201d: \nThis might be a misunderstanding. We do not intend to use the size of our search space as a metric for comparison; instead, it is an important reason why our accuracy is much better than previous NAS methods. Previous NAS methods forced different blocks to share the same structure and only explored a limited architecture space (e.g. 10^18 in [2] and 10^10 in [3]). Our method, breaking the constraints, allows all of the blocks to be specified and has much larger search space (i.e. 10^547).\n\n[1] Zoph B, Vasudevan V, Shlens J, Le QV. Learning transferable architectures for scalable image recognition. CVPR 2018.\n[2] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055. 2018.\n[3] Bender G, Kindermans PJ, Zoph B, Vasudevan V, Le Q. Understanding and simplifying one-shot architecture search. ICML 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "BkxbEO0XpX", "original": null, "number": 4, "cdate": 1541822505053, "ddate": null, "tcdate": 1541822505053, "tmdate": 1542747613104, "tddate": null, "forum": "HylVB3AqYm", "replyto": "rJl-2uUshQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Models on all platforms have been open sourced. Reproducible experiment verified on 3 different platforms. ", "comment": "We sincerely thanks for the detailed feedback. Our pre-trained models and the evaluation code are provided in the following anonymous link for verifying our results: https://goo.gl/QU3GhA. We have also made a video to visualize the architecture search process: https://goo.gl/VAzGJs. We would like to release the entire codebase upon publication. \n\n>>> Response to \u201cperformances are too good to be true\u201d: \nWe consider the comment as a compliment rather than a drawback. There are several reasons for our good results:\na) Our proxy-less NAS *directly* learns on the *target* task while previous NAS methods *indirectly* learn on *proxy* tasks. For example, on CIFAR-10, DARTS [1] conducted architecture search experiments with 8 blocks due to their high memory consumption and then transferred the learned block structure to a much larger network with 20 blocks. This indirect optimization scheme would lead to suboptimal results while our proxy-less NAS does not suffer from this problem. \n\nb) We broke the convention in neural architecture design by *not* repeating the same building block structure. Our method explores a much larger architecture space compared to previous NAS methods (10^547 vs 10^18). Furthermore, our method has much larger block diversity and is able to learn preferences at different positions in the architecture.\n \nFor example, our optimized neural network architectures for GPU, CPU and mobile phone prefer to choose more computation-expensive operations (e.g. 7x7 MBConv6) for the last few stages where the resolution of feature map is low. They also prefer to choose more computation-expensive operations in the first block within each stage where the feature map is downsampled. We consider the ability to learn such patterns which are absent in previous NAS papers also helps to improve our results.\n\n>>> Response to \u201cDPP-Net and NAO citations\u201d: \nApologize for the typo and missing a relevant paper in our reference part. We have fixed typo and added a reference to \u201cNeural Architecture Optimization\u201d. Thanks for pointing out our mistakes.\n\n[1] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055. 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "S1lC-BtXpX", "original": null, "number": 2, "cdate": 1541801222125, "ddate": null, "tcdate": 1541801222125, "tmdate": 1542747556807, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HkxRDmY7am", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "We have uploaded the evaluation code and pretrained models", "comment": "Thanks for your interest in our work. The evaluation code and pretrained models are accessible at https://goo.gl/QU3GhA. We also made a video to visualize the architecture search process at https://goo.gl/VAzGJs . You are welcome to validate the performance. The entire codebase will be released upon publication.\n\nOur implementation is repeatable and reproducible. We used the same code base to search CPU/GPU/Mobile models. On all three platforms the performance consistently outperformed previous work, thanks to our Proxyless NAS enables searching over a large design space efficiently.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "B1lXIuW-CQ", "original": null, "number": 10, "cdate": 1542686795089, "ddate": null, "tcdate": 1542686795089, "tmdate": 1542686795089, "tddate": null, "forum": "HylVB3AqYm", "replyto": "S1xBSKTN6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "We have added the results for Proxyless-G on ImageNet. And we also include a new differentiable approach to handle non-differentiable objectives (i.e. latency).", "comment": "We have added the results for Proxyless-G on ImageNet to the paper (please see Table 6 in Appendix D). We find that without taking latency as a direct objective, Proxyless-G has no incentive to choose computation-cheap operations. Consequently, it designs a very slow network that has 158ms latency on mobile phone. After rescaling the network using depth multiplier [1, 2], the latency of the network reduces to 83ms. However, this model can only achieve 71.8% top-1 accuracy on ImageNet which is 2.8% lower than Proxyless-R. Therefore, as discussed in our previous responses, it is essential to take latency which is non-differentiable as a direct optimization objective. And REINFORCE-based approach provides a solution to this problem.\n\nBeside REINFORCE, we have recently designed a differentiable approach to handle the non-differentiable objectives (please see Appendix D). Specifically,  we propose the latency regularization loss based on our proposed latency prediction model (please see Appendix C). The key to the latency regularization loss is an observation that the expected latency of a mixed operation is actually differentiable w.r.t. architecture parameters. Therefore, by incorporating the expected latency into the loss function as a regularization term, we are able to directly optimize the trade-off between accuracy and latency. Further details are provided in Appendix D. \n\n[1] Sandler, Mark, et al. \"MobileNetV2: Inverted Residuals and Linear Bottlenecks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n[2] Tan, Mingxing, et al. \"Mnasnet: Platform-aware neural architecture search for mobile.\" arXiv preprint arXiv:1807.11626 (2018)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "SkeDbIW-CX", "original": null, "number": 9, "cdate": 1542686206855, "ddate": null, "tcdate": 1542686206855, "tmdate": 1542686206855, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "Paper revision: new methods and new experiment results", "comment": "Hi all,\n\nWe have uploaded a revision of our paper with the following new methods and stronger experiment results:\n\na) \u201cEconomical alternative to mobile farm\u201d. In Appendix C, we introduce an accurate latency prediction model and remove the need of building an expensive mobile farm infrastructure [1] when learning specialized neural network architectures for mobile phone. We add new experiment results on the mobile setting, where our model achieves state-of-the-art top-1 accuracy on ImageNet under mobile latency constraints. \n\nb) \u201cMake latency differentiable\u201d. In Appendix D, we present a *differentiable* approach to handle the non-differentiable objectives (i.e. latency in our case). Specifically,  we propose the latency regularization loss based on our proposed latency prediction model. By incorporating the predicted latency of the network into the loss function as a regularization term, we are able to directly optimize the trade-off between accuracy and latency. We also add new experiments on ImageNet to justify the effectiveness of the proposed latency regularization loss.  \n\n[1] Tan, Mingxing, et al. \"Mnasnet: Platform-aware neural architecture search for mobile.\" arXiv preprint arXiv:1807.11626 (2018)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "H1x6eyDe0X", "original": null, "number": 5, "cdate": 1542643445325, "ddate": null, "tcdate": 1542643445325, "tmdate": 1542643468746, "tddate": null, "forum": "HylVB3AqYm", "replyto": "H1eNgOayAX", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Thanks for your answers! I assume you man replacement=False, right?\nbeta1 is set zero, and what value do you use for beta2?\nAnd for the network parameters, what is your optimizer and hyperparameters , including learning rate schedule (for CIFAR-10)?", "title": "replacement=True or False?"}, "signatures": ["~Robin_Tibor_Schirrmeister1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Robin_Tibor_Schirrmeister1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "HyxqxNT5aQ", "original": null, "number": 4, "cdate": 1542276081939, "ddate": null, "tcdate": 1542276081939, "tmdate": 1542276081939, "tddate": null, "forum": "HylVB3AqYm", "replyto": "BygWA7OK6X", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "So, on further thought I assume you might have meant rescaling probabilities of sampled operations by a factor such that probabilities of  unsampled operations stay the same. And update the corresponding alphas for the sampled operations such that this matches.\n\nI have tried to do this here:\nhttps://gist.github.com/robintibor/83064d708cdcb311e4b453a28b8dfdca\n\nDoes this look correct to you?", "title": "Rescaling code"}, "signatures": ["~Robin_Tibor_Schirrmeister1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Robin_Tibor_Schirrmeister1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "BygWA7OK6X", "original": null, "number": 3, "cdate": 1542190025201, "ddate": null, "tcdate": 1542190025201, "tmdate": 1542275957835, "tddate": null, "forum": "HylVB3AqYm", "replyto": "rkle4Pwda7", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Let me expand a little bit on the question and just write my understanding and open questions regarding the Gradient-Based Updates from section 3.1.\n\nSo, given a_i's as architecture weights, I am implementing it as follows:\n1. Compute p_i's from a_i's using softmax\n2. Use computed p_i's as sampling probabilities for the multinomial distribution to select two operations. [Possibly resample, if same operation chosen twice?]\n3. Recompute p_i's of the chosen a_i's by only pushing the two chosen a_is through softmax? Let's call them pnew_i's\n4. Use pnew_i's as input to binarize function, which will select one operation as active and one as inactive\n5. Compute outputs for both chosen operations, let's call them o_1, o_2, with o_1 the active operation according to the binarize function computed before\n6. Compute overall output as g_1(=1)*o_1 + g_2(=0)*o_2 (g_1, g_2 from binarize)\n7. Compute gradient on chosen a_i's as (gradient of loss wrt g_i) * (gradient of pnew_i wrt a_i) [or using full softmax, i.e. (gradient of loss wrt g_i) * (gradient of p_i wrt a_i)?]\n8. Make update step on a_i's with optimizer\n9. Multiply updated and chosen a_is by a factor that keeps probabilities p_is of unchosen operations identical to before [or see update below]\n\nWhat is correct, what is not?\n\nAlso, you use Adam for the architecture parameters, do you think it can be a problem for the adaptive gradient averages that in a single update, most operations are not chosen? Or do you sample multiple times before you make an Adam update step?\n\n\n\n", "title": "Further questions"}, "signatures": ["~Robin_Tibor_Schirrmeister1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Robin_Tibor_Schirrmeister1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "rkle4Pwda7", "original": null, "number": 2, "cdate": 1542121256288, "ddate": null, "tcdate": 1542121256288, "tmdate": 1542124394578, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Thanks for the fascinating research work.\nI am trying to reimplement your method and have a question regarding:\n\"Finally,  as path weights are computed by applying softmax to the architecture parameters, we need to rescale the value of these two updated architecture parameters by multiplying a ratio to keep the path weights of unsampled paths unchanged.\"\n\nI am not sure how to do this correctly, can you provide the formula for this ratio or code? I am a bit stuck there, how to compute the ratio :)\n\nAnother question regarding:\n\"Following this idea, within an update step of the architecture parameters, we first sample two paths according to the multinomial distribution (p1,\u00b7\u00b7\u00b7,pN) and mask all the other paths as if they do not exist.\"\n\nCould this sampling result in the same path being chosen twice? And do you handle that in some way?", "title": "Implementation question regarding rescaling of architecture parameters"}, "signatures": ["~Robin_Tibor_Schirrmeister1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Robin_Tibor_Schirrmeister1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "S1xBSKTN6m", "original": null, "number": 6, "cdate": 1541884220880, "ddate": null, "tcdate": 1541884220880, "tmdate": 1541954924304, "tddate": null, "forum": "HylVB3AqYm", "replyto": "SJlO7KpVp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "We made Apple-to-Apple comparison. Our advantage on memory saving is clear.", "comment": "\n>>> Response to \u201ccomparison with One Shot and DARTS\u201d: \nApologize for the unclear explanation for this experiment. We will revise this part to make it more clear. \n\nAll of three methods are evaluated under the same condition except DARTS [3]. Same as the original paper, DARTS *has to* use a smaller scale setting for learning architectures due to the high memory consumption. So for DARTS, the first cell structure setting is chosen to fit the network into a single GPU to learn cell structure. Then we evaluated the learned cell structure on two larger settings by repeatedly stacking it, same as the original DARTS paper [3]. \n\nFor our method, since we solved the high memory consumption issue via binarized path, our method can directly learn architectures under both small-scale and large-scale settings with *limited* GPU memory. As it is one of the key advantages of our method over previous NAS methods, we consider it reasonable to keep such differences. \n\n>>> Response to \u201cadd results for Proxyless-G on ImageNet\u201d: \nThanks for suggesting this new experiment. We have launched this experiment and will add the results to the paper.\n\nHowever, it is important to take latency as a *direct* objective when learning specialized neural network architectures for a platform. Otherwise, NAS would fail to make a good trade-off between accuracy and latency. For example, NASNet-A [1] and AmoebaNet-A [2] has shown compelling accuracy results compared to MobileNetV2 1.4 with similar number of parameters and FLOPs. But they are optimized without the awareness of the latency, their measured latencies on mobile phone are much worse than MobileNetV2 1.4 (see below). Therefore, we employ REINFORCE to directly optimize the non-differentiable objective (i.e. latency).\n\nModel\t\t\t\tParams\t        FLOPS\t        Top-1\tMobile latency\nMobileNet V2 1.4\t\t6.9M\t\t585M\t\t74.7\t\t143ms\nNASNet-A\t\t\t5.3M\t\t564M\t\t74.0\t\t183ms\nAmeobaNet-A\t\t5.1M\t\t555M\t\t74.5\t\t190ms\n\n[1] Zoph B, Vasudevan V, Shlens J, Le QV. Learning transferable architectures for scalable image recognition. CVPR 2018.\n[2] Real E, Aggarwal A, Huang Y, Le QV. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548. 2018.\n[3] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055. 2018.\n[4] Bender G, Kindermans PJ, Zoph B, Vasudevan V, Le Q. Understanding and simplifying one-shot architecture search. ICML 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}, {"id": "HkxRDmY7am", "original": null, "number": 1, "cdate": 1541800805533, "ddate": null, "tcdate": 1541800805533, "tmdate": 1541800805533, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "content": {"comment": "Dear authors, can you release your source code for readers to validate your experiment?", "title": "can you release code?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311574654, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylVB3AqYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311574654}}}, {"id": "HkxEMyl33m", "original": null, "number": 3, "cdate": 1541304076025, "ddate": null, "tcdate": 1541304076025, "tmdate": 1541533056840, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Review", "content": {"title": "Interesting combination of existing methods and good performance", "review": "\nThis paper addresses the problem of architecture search, and specifically seeks to do this without having to train on \"proxy\" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size. The paper puts together a set of existing complementary methods towards this end, specifically 1) Training \"cumbersome\" networks as in One Shot and DARTS, 2) Path binarization to address memory requirements (optimized using ideas in BinaryConnect), and 3) optimizing a non-differentiable architecture using REINFORCE. The end result is that this method is able to find efficient architectures that achieve state of art performance with fewer parameters, can be optimized for non-differentiable objectives such as latency, and can do so with smaller amounts of GPU memory and computation.\n\nStrengths\n\n + The paper is in general well-written and provides a clear description of the methods.\n\n + Different choices made are well-justified in terms of the challenge they seek to address (e.g. non-differentiable objectives, etc.)\n\n + The results achieve state of art while being able to trade off other objectives such as latency\n\n + There are some interesting findings such as the need for specialized blocks rather than repeating blocks, comparison of architectures for CPUs vs. GPUs, etc. \n\nWeaknesses\n \n - In the end, the method is really a combination of existing methods (One Shot/DART, BinaryConnect, use of RL/REINFORCE, etc.). One novel aspect seems to be factorizing the choice out of N candidates by making it a binary selection. In general, it would be good for the paper to make clear which aspects were already done by other approaches (or if it's a modification what exactly was modified/added in comparison) and highlight the novel elements.\n\n - The comparison with One Shot and DARTS seems strange, as there are limitations place on those methods (e.g. cell structure settings) that the authors state they chose \"to save time\". While that consideration has some validity, the authors should explicitly state why they think these differences don't unfairly bias the experiments towards the proposed approach.\n\n - It's not clear that the REINFORCE aspect is adding much; it achieves slightly higher parameters when compared against Proxyless-G, and while I understand the motivation to optimize a non-differentiable function in this case the latency example (on ImageNet) is never compared to Proxyless-G. It could be that optimized the normal differentiable objective achieves similar latency with the smaller number of parameters. Please show results for Proxyless-G in Table 4.\n\n - There were several typos throughout the paper (\"great impact BY automatically designing\", \"Fo example\", \"is build upon\", etc.)\n\n In summary, the paper presents work on an interesting topic. The set of methods seem to be largely pulled from work that already exists, but is able to achieve good results in a manner that uses less GPU memory and compute, while supporting non-differentiable objectives. Some of the methodological issues mentioned above should be addressed though in order to strengthen the argument that all parts of the the method (especially REINFORCE) are necessary. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Review", "cdate": 1542234209467, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335966747, "tmdate": 1552335966747, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJl-2uUshQ", "original": null, "number": 2, "cdate": 1541265576991, "ddate": null, "tcdate": 1541265576991, "tmdate": 1541533056527, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Review", "content": {"title": "Solid work with convincing results", "review": "It seems the authors propose an efficient method to search platform-aware network architecture aiming at high recognition accuracy and low latency. Their results on CIFAR-10 and ImageNet are surprisingly good.  But it is still hard to believe that the author can  achieve 2.08% error rate with only 5.7M parameter on CIFAR10 and 74.5% top-1 accuracy on ImageNet with less GPU hours/memories than prior arts.\n\nGiven my concerns above, the author must release their code and detail pipelines since NAS papers are difficult to be reproduced. \n\nThere is a small typo in reference part:\nJing-Dong Dong's work should be DPP-Net instead of PPP-Net (https://eccv2018.org/openaccess/content_ECCV_2018/papers/Jin-Dong_Dong_DPP-Net_Device-aware_Progressive_ECCV_2018_paper.pdf)\nand I think this paper \"Neural Architecture Optimization\" shoud be cited.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Review", "cdate": 1542234209467, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335966747, "tmdate": 1552335966747, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJeFYtGK27", "original": null, "number": 1, "cdate": 1541118337406, "ddate": null, "tcdate": 1541118337406, "tmdate": 1541185594697, "tddate": null, "forum": "HylVB3AqYm", "replyto": "HylVB3AqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "content": {"title": "New experiment results on mobile phone", "comment": "Hi all,\n\nOur efficient algorithm allows us to specialize neural network architectures for different devices easily. Recently, we extended our proxyless NAS to the mobile setting and achieved SOTA result with mobile latency constraint (< 80ms latency on Pixel 1 phone) as well. The following is our current results on ImageNet (Device: Pixel 1. Batch size: 1. Framework: TF-Lite):\n\nModel\t\t\t\tTop-1\tTop-5\tMobile latency\nMobileNet V1\t\t70.6\t\t89.5\t\t113ms\nMobileNet V2\t\t72.0\t\t91.0\t\t75ms\nNASNet-A\t\t\t74.0\t\t91.3\t\t183ms\nAmeobaNet-A\t\t74.5\t\t92.0\t\t190ms\nMnasNet\t\t\t74.0\t\t91.8\t\t76ms\nMnasNet (our impl.)\t74.0\t\t91.8\t\t79ms\nProxyless NAS (ours)\t74.6\t\t92.2\t\t78ms\n\nThe detailed architectures of our searched models and their learning process are provided in the following anonymous link:\nhttps://drive.google.com/open?id=1nut1owvACc9yz1ZPqcbqoJLS2XrVPp1Q"}, "signatures": ["ICLR.cc/2019/Conference/Paper1534/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "TL;DR": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "pdf": "/pdf/8d4f27771fa0d882feaf780ae2b1d1dfb5b9b66f.pdf", "paperhash": "cai|proxylessnas_direct_neural_architecture_search_on_target_task_and_hardware", "_bibtex": "@inproceedings{\ncai2018proxylessnas,\ntitle={Proxyless{NAS}: Direct Neural Architecture Search on Target Task and Hardware},\nauthor={Han Cai and Ligeng Zhu and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylVB3AqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1534/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605492, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylVB3AqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1534/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1534/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1534/Authors|ICLR.cc/2019/Conference/Paper1534/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1534/Reviewers", "ICLR.cc/2019/Conference/Paper1534/Authors", "ICLR.cc/2019/Conference/Paper1534/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605492}}}], "count": 33}