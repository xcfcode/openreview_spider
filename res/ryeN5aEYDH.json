{"notes": [{"id": "ryeN5aEYDH", "original": "BJlus5TvwH", "number": 703, "cdate": 1569439115966, "ddate": null, "tcdate": 1569439115966, "tmdate": 1577168250049, "tddate": null, "forum": "ryeN5aEYDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ifox@umich.edu", "joyclee@med.umich.edu", "rpbusui@umich.edu", "wiensj@umich.edu"], "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "authors": ["Ian Fox", "Joyce Lee", "Rodica Busui", "Jenna Wiens"], "pdf": "/pdf/d814d32e297195a9b7c4f368e2e79bc551ca2264.pdf", "TL;DR": "We develop a deep reinforcement learning algorithm to control blood glucose in people with diabetes. ", "abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ", "code": "https://tinyurl.com/y6e2m68b", "keywords": ["Deep Reinforcement Learning", "Diabetes", "Artificial Pancreas", "Control"], "paperhash": "fox|deep_rl_for_blood_glucose_control_lessons_challenges_and_opportunities", "original_pdf": "/attachment/8774f6177f744e797c3eb9b24ee0ea2d5094b3a4.pdf", "_bibtex": "@misc{\nfox2020deep,\ntitle={Deep {\\{}RL{\\}} for Blood Glucose Control: Lessons, Challenges, and Opportunities},\nauthor={Ian Fox and Joyce Lee and Rodica Busui and Jenna Wiens},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeN5aEYDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "CJxzxuzknN", "original": null, "number": 1, "cdate": 1576798703730, "ddate": null, "tcdate": 1576798703730, "tmdate": 1576800932308, "tddate": null, "forum": "ryeN5aEYDH", "replyto": "ryeN5aEYDH", "invitation": "ICLR.cc/2020/Conference/Paper703/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers all believe that this paper is not yet ready for publication. All agree that this is an important application, and an interesting approach. The methodological novelty, as well as other parts of exposition, involving related work, or further discussion of what this solution means for patients, is right now not completely convincing to reviewers. My recommendation is to work on making sure the exposition best explains the methodology, and making sure this venue is the best for the submitted line of work.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ifox@umich.edu", "joyclee@med.umich.edu", "rpbusui@umich.edu", "wiensj@umich.edu"], "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "authors": ["Ian Fox", "Joyce Lee", "Rodica Busui", "Jenna Wiens"], "pdf": "/pdf/d814d32e297195a9b7c4f368e2e79bc551ca2264.pdf", "TL;DR": "We develop a deep reinforcement learning algorithm to control blood glucose in people with diabetes. ", "abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ", "code": "https://tinyurl.com/y6e2m68b", "keywords": ["Deep Reinforcement Learning", "Diabetes", "Artificial Pancreas", "Control"], "paperhash": "fox|deep_rl_for_blood_glucose_control_lessons_challenges_and_opportunities", "original_pdf": "/attachment/8774f6177f744e797c3eb9b24ee0ea2d5094b3a4.pdf", "_bibtex": "@misc{\nfox2020deep,\ntitle={Deep {\\{}RL{\\}} for Blood Glucose Control: Lessons, Challenges, and Opportunities},\nauthor={Ian Fox and Joyce Lee and Rodica Busui and Jenna Wiens},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeN5aEYDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryeN5aEYDH", "replyto": "ryeN5aEYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713498, "tmdate": 1576800263127, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper703/-/Decision"}}}, {"id": "BJekTXIjjr", "original": null, "number": 4, "cdate": 1573770166533, "ddate": null, "tcdate": 1573770166533, "tmdate": 1573770166533, "tddate": null, "forum": "ryeN5aEYDH", "replyto": "ryeN5aEYDH", "invitation": "ICLR.cc/2020/Conference/Paper703/-/Official_Comment", "content": {"title": "Updates to the Paper", "comment": "We thank the reviewers for their useful feedback. Below, we respond to each reviewer, in turn. In addition, we have updated the paper to reflect their suggestions. We also include a separate post, \u2018Relevance of the Application and Specific Contributions\u2019 written to respond to issues raised by both reviewers.\n\n In addition to these changes, we have also worked to improve the clarity of the paper and have worked to strengthen the baselines.  Specifically,  we modified the carbohydrate ratio and correction factor used in our basal bolus controller to bring the approach more in line with standard practice. We found these new parameters significantly improved performance for the basal bolus controller, bringing average risk down from 21.37 to 8.99. These updated parameters did not meaningfully impact the results of the PID-MA (average risk rose from 6.15 to 6.16 after re-tuning the gains). These updated results do not change our conclusions, as we still observe a marked improvement with closed-loop controllers, but they serve to provide a more accurate estimate of human-level performance in blood glucose control. As a result of this change, we have included the new basal-bolus parameters as a new appendix section and have updated the PID-MA parameter table in the appendix.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper703/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ifox@umich.edu", "joyclee@med.umich.edu", "rpbusui@umich.edu", "wiensj@umich.edu"], "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "authors": ["Ian Fox", "Joyce Lee", "Rodica Busui", "Jenna Wiens"], "pdf": "/pdf/d814d32e297195a9b7c4f368e2e79bc551ca2264.pdf", "TL;DR": "We develop a deep reinforcement learning algorithm to control blood glucose in people with diabetes. ", "abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ", "code": "https://tinyurl.com/y6e2m68b", "keywords": ["Deep Reinforcement Learning", "Diabetes", "Artificial Pancreas", "Control"], "paperhash": "fox|deep_rl_for_blood_glucose_control_lessons_challenges_and_opportunities", "original_pdf": "/attachment/8774f6177f744e797c3eb9b24ee0ea2d5094b3a4.pdf", "_bibtex": "@misc{\nfox2020deep,\ntitle={Deep {\\{}RL{\\}} for Blood Glucose Control: Lessons, Challenges, and Opportunities},\nauthor={Ian Fox and Joyce Lee and Rodica Busui and Jenna Wiens},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeN5aEYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeN5aEYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference/Paper703/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper703/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper703/Reviewers", "ICLR.cc/2020/Conference/Paper703/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper703/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper703/Authors|ICLR.cc/2020/Conference/Paper703/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167502, "tmdate": 1576860547149, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference/Paper703/Reviewers", "ICLR.cc/2020/Conference/Paper703/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper703/-/Official_Comment"}}}, {"id": "rJeucQIjoH", "original": null, "number": 3, "cdate": 1573770127923, "ddate": null, "tcdate": 1573770127923, "tmdate": 1573770127923, "tddate": null, "forum": "ryeN5aEYDH", "replyto": "ryeNngAwYr", "invitation": "ICLR.cc/2020/Conference/Paper703/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for the feedback. We are glad you found our paper well written, our results promising, and that you recognized the potential for impact high. \n\nFirst, we would like to address a branch of related work, exemplified by \u201cReinforcement Learning Algorithm for Blood Glucose Control in Diabetic Patients.\u201d In this branch of work, researchers consider a setting that significantly differs from ours. In contrast to our setting in which we aim to learn a closed-loop control policy, past work has focused on a \u2018human-in-the-loop\u2019 setting, in which the goal is to learn optimal correction factors and carbohydrate ratios that can be used in the calculation of boluses. Though different, for completeness, we have included a discussion of this branch of work in Section 2.1 of our updated manuscript.\n\nSecond, with regard to the specific contributions of our paper, please see our post on the relevance of our problem and our contributions to it. As suggested, we have updated our introduction to include a paragraph with a bulleted list to make our contributions clear.\n\nFinally, we would like to address concerns over the limited number of patients in our simulator. We have performed experiments 30 patients, that represent a broad range of patient characteristics (with ages from 7-68 years and requiring from 15-70 total daily units of insulin). Each methods test set is composed of 900 days blood glucose data.  Moreover, in our transfer experiments, we found that a model trained on one individual could be successfully transferred with little additional data for fine-tuning. In particular, using the model trained for Child/Adolescent/Adult 1, we were able to match or surpass the performance of the patient-specific models for the other patients in the same category. For example, the model trained on the 61 year old Adult 1, after fine-tuning, slightly outperforms the model trained from scratch on the 26 year old Adult 6 (average risk 5.83 vs. 5.99), and another fine-tuned Adult 1 model also outperforms the from-scratch model for Adult 5, who requires 18 units of daily insulin on average, the most of any adult patient (mean risk 9.17 vs. 13.00). For these reasons we believe that our approach will generalize  to additional individuals. We made the limitations regarding the number of simulated individuals and our approaches ability to overcome patient differences more clear in Sections 2.2 and 4 respectively.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper703/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ifox@umich.edu", "joyclee@med.umich.edu", "rpbusui@umich.edu", "wiensj@umich.edu"], "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "authors": ["Ian Fox", "Joyce Lee", "Rodica Busui", "Jenna Wiens"], "pdf": "/pdf/d814d32e297195a9b7c4f368e2e79bc551ca2264.pdf", "TL;DR": "We develop a deep reinforcement learning algorithm to control blood glucose in people with diabetes. ", "abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ", "code": "https://tinyurl.com/y6e2m68b", "keywords": ["Deep Reinforcement Learning", "Diabetes", "Artificial Pancreas", "Control"], "paperhash": "fox|deep_rl_for_blood_glucose_control_lessons_challenges_and_opportunities", "original_pdf": "/attachment/8774f6177f744e797c3eb9b24ee0ea2d5094b3a4.pdf", "_bibtex": "@misc{\nfox2020deep,\ntitle={Deep {\\{}RL{\\}} for Blood Glucose Control: Lessons, Challenges, and Opportunities},\nauthor={Ian Fox and Joyce Lee and Rodica Busui and Jenna Wiens},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeN5aEYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeN5aEYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference/Paper703/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper703/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper703/Reviewers", "ICLR.cc/2020/Conference/Paper703/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper703/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper703/Authors|ICLR.cc/2020/Conference/Paper703/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167502, "tmdate": 1576860547149, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference/Paper703/Reviewers", "ICLR.cc/2020/Conference/Paper703/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper703/-/Official_Comment"}}}, {"id": "rklwCzLsoS", "original": null, "number": 1, "cdate": 1573769934635, "ddate": null, "tcdate": 1573769934635, "tmdate": 1573770082558, "tddate": null, "forum": "ryeN5aEYDH", "replyto": "ryeN5aEYDH", "invitation": "ICLR.cc/2020/Conference/Paper703/-/Official_Comment", "content": {"title": "Relevance of the Application and Specific Contributions", "comment": "Diabetes affects nearly 1/10 people in the US and is a growing global problem. People with Type 1 diabetes must constantly make decisions about their treatment regimen. Many of these people are children, who are not well-equipped to make such decisions on their own. To this end, for several decades, researchers have sought a closed loop system that does not require meal announcements. A robust solution to this problem would have significant societal implications. Recent advances in deep RL show promise, but from a learning perspective, this problem is particularly challenging, because:\n- There is a significant delay between actions and outcomes. Insulin can affect glucose levels hours after administration and this effect can vary significantly across individuals. Without encoding knowledge of patient-specific insulin dynamics, learning the long-term impact of insulin is challenging.\n- Compared to tasks that rely on a visual input or are given ground truth state, this task must rely on a noisy signal that requires significant temporal context to interpret.\n- With the goal of circumventing the need for meal announcements, we must implicitly infer meals (timing and size). However, the input signal is noisy, and this non-stationary noise can make it difficult to detect if/when a meal has begun.\n- Because of fluctuations throughout the day and even the week, tight blood glucose control requires both fine-grained changes in insulin throughout the day and large doses of insulin to control glucose spikes. \n- Controlling blood glucose levels is a safety-critical application. This sets the bar high from an evaluation perspective. It is unsafe to deploy a system without a human-in-the-loop with even a small probability of failure.\n- Unlike game settings where one might have the ability to learn from hundreds of thousands of hours of gameplay, to be practical, any learning approach to blood glucose control must be able to achieve strong performance using only a limited number of days of patient-specific data.\n\nDue to these challenges, this task represents a significant departure from existing deep RL baselines. Achieving strong performance in this task required numerous careful design decisions, from the reward function to the range of possible actions. In this work, we propose the first deep RL approach for blood glucose control, in which we have:\n- Explored different input representations, determining a length of input history that balances the importance of action history with that of recent glucose changes.\n- Designed a network architecture and training scheme that is able to reliably learn from noisy glucose trajectories.\nProposed a patient-specific action space that allows for both major and minor changes in insulin level, while being amenable to exploration to allow for learning .\n- Augmented a reward function designed to balance the risks of hypo- and hyperglycemia with termination penalties to further ensure learning safe policies.\n- Presented an effective and efficient way to transfer policies learned on one individual to another.\n\nMoreover, by adding a preliminary code release to our submission, we have committed to making our solution and problem setting publicly available. Our work can help to build the foundation of a new, tractable, and societally important benchmark for the RL community. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper703/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ifox@umich.edu", "joyclee@med.umich.edu", "rpbusui@umich.edu", "wiensj@umich.edu"], "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "authors": ["Ian Fox", "Joyce Lee", "Rodica Busui", "Jenna Wiens"], "pdf": "/pdf/d814d32e297195a9b7c4f368e2e79bc551ca2264.pdf", "TL;DR": "We develop a deep reinforcement learning algorithm to control blood glucose in people with diabetes. ", "abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ", "code": "https://tinyurl.com/y6e2m68b", "keywords": ["Deep Reinforcement Learning", "Diabetes", "Artificial Pancreas", "Control"], "paperhash": "fox|deep_rl_for_blood_glucose_control_lessons_challenges_and_opportunities", "original_pdf": "/attachment/8774f6177f744e797c3eb9b24ee0ea2d5094b3a4.pdf", "_bibtex": "@misc{\nfox2020deep,\ntitle={Deep {\\{}RL{\\}} for Blood Glucose Control: Lessons, Challenges, and Opportunities},\nauthor={Ian Fox and Joyce Lee and Rodica Busui and Jenna Wiens},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeN5aEYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeN5aEYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference/Paper703/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper703/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper703/Reviewers", "ICLR.cc/2020/Conference/Paper703/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper703/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper703/Authors|ICLR.cc/2020/Conference/Paper703/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167502, "tmdate": 1576860547149, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference/Paper703/Reviewers", "ICLR.cc/2020/Conference/Paper703/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper703/-/Official_Comment"}}}, {"id": "S1lWUQLsor", "original": null, "number": 2, "cdate": 1573770057124, "ddate": null, "tcdate": 1573770057124, "tmdate": 1573770057124, "tddate": null, "forum": "ryeN5aEYDH", "replyto": "SkeuvSR2FH", "invitation": "ICLR.cc/2020/Conference/Paper703/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for taking the time to provide a useful and detailed review. We are glad you found the various components in our pipeline well justified. In this work we aim to advance knowledge in the application of RL techniques, while solving a problem that afflicts a significant portion of society.\n\nFirst, we would like to clarify the carbohydrate input in Section 3.1. The equations in question are a model used for our training environment (ie: the simulator). Carbohydrate inputs are not used for training or evaluation of our RL approaches. Our learned policies are simply functions mapping from 4 hours of CGM and insulin data to an insulin dose in the next five minutes. We have added a sentence in this section to clarify this. \n\nSecond, with respect to concern 1, please see our statement on relevance and contributions. We believe that our work, by laying the foundation for a societally important RL benchmark task, is of interest to the ICLR community.\n\nFinally, with respect to concern 2, meal announcements are an important bottleneck because they require regular and accurate human intervention. While providing accurate meal announcements is not an issue for many, some groups (particularly children and adolescents) can forget to log meals or record meals incorrectly. In Section 4 we have included a new subsection \u2018Corrupting and Learning from Meal Announcements\u2019 where we detail two new experiments in line with your requests. In the first experiment, we find that including a realistic amount of carbohydrate error significantly worsens the performance of the BB controller (from average risk 8.99 to 13.51), showing a limitation of methods that rely on meal announcements. In the second experiment, we show that our RL approach can utilize meal announcements to improve performance, improving average risk from 11.49 to 9.85 for adult#001 (below the PID-MA performance of 10.53).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper703/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ifox@umich.edu", "joyclee@med.umich.edu", "rpbusui@umich.edu", "wiensj@umich.edu"], "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "authors": ["Ian Fox", "Joyce Lee", "Rodica Busui", "Jenna Wiens"], "pdf": "/pdf/d814d32e297195a9b7c4f368e2e79bc551ca2264.pdf", "TL;DR": "We develop a deep reinforcement learning algorithm to control blood glucose in people with diabetes. ", "abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ", "code": "https://tinyurl.com/y6e2m68b", "keywords": ["Deep Reinforcement Learning", "Diabetes", "Artificial Pancreas", "Control"], "paperhash": "fox|deep_rl_for_blood_glucose_control_lessons_challenges_and_opportunities", "original_pdf": "/attachment/8774f6177f744e797c3eb9b24ee0ea2d5094b3a4.pdf", "_bibtex": "@misc{\nfox2020deep,\ntitle={Deep {\\{}RL{\\}} for Blood Glucose Control: Lessons, Challenges, and Opportunities},\nauthor={Ian Fox and Joyce Lee and Rodica Busui and Jenna Wiens},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeN5aEYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeN5aEYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference/Paper703/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper703/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper703/Reviewers", "ICLR.cc/2020/Conference/Paper703/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper703/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper703/Authors|ICLR.cc/2020/Conference/Paper703/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167502, "tmdate": 1576860547149, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper703/Authors", "ICLR.cc/2020/Conference/Paper703/Reviewers", "ICLR.cc/2020/Conference/Paper703/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper703/-/Official_Comment"}}}, {"id": "SkeuvSR2FH", "original": null, "number": 2, "cdate": 1571771744472, "ddate": null, "tcdate": 1571771744472, "tmdate": 1572972562748, "tddate": null, "forum": "ryeN5aEYDH", "replyto": "ryeN5aEYDH", "invitation": "ICLR.cc/2020/Conference/Paper703/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper describes an RL based approach to administer insulin for blood glucose control among  type-1 diabetic patients.  The paper formulates this blood glucose control problem as a closed-loop reinforcement learning problem and demonstrates its effectiveness on data generated from an FDA-approved simulator of glucoregulatory system. Compared to existing approaches, the proposed method can operate without meal announcement by potentially making use of latent meal intake patterns. The authors also demonstrate how a learned policy for one particular subject can be used as initialization to train/fine-tuned the policy of another subject so as to combat the issue of high sample complexity.\n\nOverall, the reviewer finds that the authors provide a reasonable approach to model the blood glucose management problem for type-1 diabetes. Each component of the paper is well-explained and the paper is easy to follow. The algorithmic design adopted in this paper, such as the representation of the problem, the choice of the reward function, and the choice of RL controller, are reasonably justified. While the experiments conducted in this paper are not based on real-world data, the reviewer finds the use of data from an FDA-approved simulator of glucoregulatory system sufficiently convincing for this type of problem. The limitation of the proposed method is also well discussed.\n\nThe reviewer has the following concerns:\n\n1. The reviewer views the major contribution of this paper as formulating and solving the glucose management problem as an RL problem.  From a machine learning perspective, the reviewer finds the contribution made in this paper to advance ML methodology very limited as the components used in the algorithmic design of the proposed method are already available in the existing literature. Therefore, the reviewer finds that the paper could be of limited interest to the audience in ICLR while it might be more suitable to the audience of diabetes management.\n\n2. Compared to competing methods reported in this paper, a major characteristic of the proposed method is that it can operate without meal announcements. Methods with meal announcements seem to operate reasonably well compared to the proposed method. Therefore, the authors can consider further justifying why meal announcement is an important bottleneck to alleviate in blood glucose management, which is currently not well explained in the paper. Related to this question, the authors may also consider justifying why making meal announcements more convenient/automated is not a good alternative to handle the blood glucose management problem. It will also be interesting to see how the proposed method will behave when augmented with meal announcements since in reality, the proposed solution might not always be reliable and intervention options like meal announcements could potentially improve the robustness of the solution.\n\n\nMiscellaneous:\nin Section 3.1 of the glucoregulatory system model G, the carbohydrate input $c_t$ seems to be considered as an aspect of the action. Based on the understanding of the reviewer, such an action is a proxy to the meal announcement and is not considered as an input from the user for the deployed policy. For better clarity, the authors can consider reporting the formula of the deployed policy and explain how the quantity $c_t$ is related to this policy."}, "signatures": ["ICLR.cc/2020/Conference/Paper703/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper703/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ifox@umich.edu", "joyclee@med.umich.edu", "rpbusui@umich.edu", "wiensj@umich.edu"], "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "authors": ["Ian Fox", "Joyce Lee", "Rodica Busui", "Jenna Wiens"], "pdf": "/pdf/d814d32e297195a9b7c4f368e2e79bc551ca2264.pdf", "TL;DR": "We develop a deep reinforcement learning algorithm to control blood glucose in people with diabetes. ", "abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ", "code": "https://tinyurl.com/y6e2m68b", "keywords": ["Deep Reinforcement Learning", "Diabetes", "Artificial Pancreas", "Control"], "paperhash": "fox|deep_rl_for_blood_glucose_control_lessons_challenges_and_opportunities", "original_pdf": "/attachment/8774f6177f744e797c3eb9b24ee0ea2d5094b3a4.pdf", "_bibtex": "@misc{\nfox2020deep,\ntitle={Deep {\\{}RL{\\}} for Blood Glucose Control: Lessons, Challenges, and Opportunities},\nauthor={Ian Fox and Joyce Lee and Rodica Busui and Jenna Wiens},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeN5aEYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryeN5aEYDH", "replyto": "ryeN5aEYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper703/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper703/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576555708587, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper703/Reviewers"], "noninvitees": [], "tcdate": 1570237748310, "tmdate": 1576555708604, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper703/-/Official_Review"}}}, {"id": "ryeNngAwYr", "original": null, "number": 1, "cdate": 1571442859903, "ddate": null, "tcdate": 1571442859903, "tmdate": 1572972562708, "tddate": null, "forum": "ryeN5aEYDH", "replyto": "ryeN5aEYDH", "invitation": "ICLR.cc/2020/Conference/Paper703/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Paper Summary\n\nThis paper examines reinforcement learning in the context of blood glucose control to help individuals with type 1 diabetes. The authors show that their methods lead to strong algorithms that can improve artificial pancreas systems. Their results are promising, and, very importantly, do not require meal announcements. The importance of their application is self evident.\n\nDecision\n\nShould their claim to novelty hold up, then the authors have provided evidence that RL can be useful for this important application of glucose control. Overall, the paper is very well written with clear arguments, and the impact of their study for type 1 diabetes is high. However, there are novelty concerns with the proposed methods. The paper needs some work before acceptance.\n\nAdditional Feedback\n\nOne caveat is that a small search of previous RL methods in blood glucose control did yield some similarly titled papers (please discuss \"Reinforcement Learning Algorithm for Blood Glucose Control in Diabetic Patients\" by Javad et al), and they were not addressed or compared in this paper. To push this review over the edge, the authors should address these papers in the related work, and discuss how this paper's method compares.\n\nAdditionally, the novelty of the actual RL methods is not entirely clear. The authors should very clearly point out their contributions within the methods sections, differentiating between past methods and the proposed one. Most importantly, the authors should write a paragraph-length section at the end of the introduction detailing their proposed methods, with a bullet-point layout of every novel detail. This will help future readers get the gist of the paper more accurately.\n\nLastly, though the authors addressed the limitations of their dataset in terms of it being a simulation, they should also discuss the sample size being only 10 patients in different age groups. It would be helpful for readers to know how the method will generalize to new patients.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper703/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper703/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ifox@umich.edu", "joyclee@med.umich.edu", "rpbusui@umich.edu", "wiensj@umich.edu"], "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities", "authors": ["Ian Fox", "Joyce Lee", "Rodica Busui", "Jenna Wiens"], "pdf": "/pdf/d814d32e297195a9b7c4f368e2e79bc551ca2264.pdf", "TL;DR": "We develop a deep reinforcement learning algorithm to control blood glucose in people with diabetes. ", "abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ", "code": "https://tinyurl.com/y6e2m68b", "keywords": ["Deep Reinforcement Learning", "Diabetes", "Artificial Pancreas", "Control"], "paperhash": "fox|deep_rl_for_blood_glucose_control_lessons_challenges_and_opportunities", "original_pdf": "/attachment/8774f6177f744e797c3eb9b24ee0ea2d5094b3a4.pdf", "_bibtex": "@misc{\nfox2020deep,\ntitle={Deep {\\{}RL{\\}} for Blood Glucose Control: Lessons, Challenges, and Opportunities},\nauthor={Ian Fox and Joyce Lee and Rodica Busui and Jenna Wiens},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeN5aEYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryeN5aEYDH", "replyto": "ryeN5aEYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper703/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper703/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576555708587, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper703/Reviewers"], "noninvitees": [], "tcdate": 1570237748310, "tmdate": 1576555708604, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper703/-/Official_Review"}}}], "count": 8}