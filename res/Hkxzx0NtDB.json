{"notes": [{"id": "DWVp2zNSdDV", "original": null, "number": 9, "cdate": 1585538605418, "ddate": null, "tcdate": 1585538605418, "tmdate": 1585538605418, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"title": "Related work of EMBs using classifiers", "comment": "Dear authors,\n\nCongratulations on the Oral paper at ICLR 2020! Really nice paper indeed!! :-)\n\nRelated to your work, in PPGNs [1], we combined a p(x) (i.e. parameterized by a Denoising Autoencoder) and a p(y|x) convnet classifier to form an energy model for the joint p(x,y).\nThe flexibility of EBMs enabled us to plug in different p(y|x) models and form a new EBM (thus, the plug-and-play).\n\nYour thoughts are greatly welcomed and appreciated :-)\n\ncheers,\n\nAnh\n\n[1] Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space Nguyen et al. CVPR 2017\n https://arxiv.org/abs/1612.00005"}, "signatures": ["~Anh_Nguyen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anh_Nguyen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}, {"id": "Hkxzx0NtDB", "original": "HJxS1NXOvS", "number": 921, "cdate": 1569439210112, "ddate": null, "tcdate": 1569439210112, "tmdate": 1583912051773, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 26, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "DAfOApriQ0", "original": null, "number": 8, "cdate": 1581545274551, "ddate": null, "tcdate": 1581545274551, "tmdate": 1581545274551, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkftfw49zD", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"title": "Thanks", "comment": "I am looking forward to a discussion about the methods in the paper."}, "signatures": ["~Kwonjoon_Lee1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Kwonjoon_Lee1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}, {"id": "ziWkItCy5F", "original": null, "number": 7, "cdate": 1578012926637, "ddate": null, "tcdate": 1578012926637, "tmdate": 1578012926637, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"title": "Related work about EBMs parameterized by deep neural networks", "comment": "Dear authors,\n\nCongratulation on your accepted paper !! \n\nI would like to share you some papers about training large-scale EBMs on high-dimensional data, parameterized by deep neural networks.  [1] and [2] proposed deep EBM using Spatial-Temporal ConvNet as the energy function for video generation and recovery.  [3] proposed deep EBM using volumetric ConvNet as the energy function for 3D shape patterns generation and classification.  [4] proposed multi-grid MCMC to learn EBM with ConvNet as energy function. \n\nThank you :)\n\n\n[1] Synthesizing Dynamic Pattern by Spatial-Temporal Generative ConvNet\nJianwen Xie, Song-Chun Zhu, Ying Nian Wu (CVPR 2017)\n\n[2] Learning Energy-based Spatial-Temporal Generative ConvNet for Dynamic Patterns\nJianwen Xie, Song-Chun Zhu, Ying Nian Wu\nIEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2019\n\n[3] Learning Descriptor Networks for 3D Shape Synthesis and Analysis\nJianwen Xie *, Zilong Zheng *, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, Ying Nian Wu (CVPR) 2018 \n\n[4]  Learning generative ConvNets via multigrid modeling and sampling. \nR Gao*, Y Lu*, J Zhou, SC Zhu, and YN Wu (CVPR 2018).  \n\n"}, "signatures": ["~Jianwen_Xie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jianwen_Xie1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}, {"id": "5ESi_m0g4W", "original": null, "number": 20, "cdate": 1577114381755, "ddate": null, "tcdate": 1577114381755, "tmdate": 1577114381755, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "xK2sLwVZLc", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "thanks", "comment": "Thanks for your kind words about our work. Regarding the work of your own that you bring up; we feel it is an interesting paper with strong results but not very related to our own. There are a number of works which detect OOD examples by training auxiliary generative models on top of network activations. Our method works quite differently so we did not compare with any such methods. Further, while your approach trains a generative model on top of the features of a trained classifier, this is not a generative model for p(x) and data cannot be sampled from it. \n\nWe thank you for your interest in our work but we do not feel your work is related enough to warrant changing our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "q_zBnorZlg", "original": null, "number": 19, "cdate": 1577110571601, "ddate": null, "tcdate": 1577110571601, "tmdate": 1577110571601, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "Classifier based energy models in the absence of labels", "comment": "Congrats on the acceptance! Very inspiring work.\n\nI am curious about what the authors think about a line of recent work using classifiers to \"boost\" the performance of a base generative model. \n\n1. Boosted Generative Models.  AAAI 2018.\n2. Discriminator rejection sampling. ICLR 2019.\n3. Metropolis-hastings generative adversarial networks. ICML 2019.\n4. Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting. NeurIPS 2019.\n\nBoth the current work and the above line of work exploit the inductive bias of classifiers for specifying an energy function (see Section 4 of Ref. 4 above for a concise explanation). The training of the induced energy function here is multi-step (first a generative model and then a classifier) but the labels for the classifier training are artificially induced (real vs. fake) which makes the setup more broadly applicable to even unlabelled datasets. \n\nI'd be interested to hear if the authors have additional insights to share on similarities, differences, potential hybrids etc. of their work with the above line of work. Thanks!"}, "signatures": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "xK2sLwVZLc", "original": null, "number": 6, "cdate": 1577000177192, "ddate": null, "tcdate": 1577000177192, "tmdate": 1577000177192, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"title": "Related work", "comment": "Dear authors,\n\nThank you for the interesting paper and congratulations on the acceptance at ICLR.\n\nCombining an EBM with a classifier is very interesting. I would like to draw your attention to some of our previous work [1] related to this topic. Our work [1] can also be interpreted as fitting a specific form of energy function from a standard deep neural network classifier. In our work, we proposed to induce such EBM from pre-trained classifiers, rather than training from scratch. We showed that the derived classifier doesn't suffer performance degradation in classification, and also found that this kind of model can be very effective for detecting out-of-distribution and adversarial samples.\n\n[1] Lee, Kimin, Lee, Kibok, Lee, Honglak. and Shin, Jinwoo, A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, 2018. https://arxiv.org/abs/1807.03888\n \nThank you very much."}, "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Kimin_Lee1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}, {"id": "Hkftfw49zD", "original": null, "number": 18, "cdate": 1576972082414, "ddate": null, "tcdate": 1576972082414, "tmdate": 1576972082414, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "oDOEhLoFDR", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "thanks", "comment": "Thank you for brining your work to our attention. Shortly after releasing our work someone else brought these papers to our attention. We agree it is indeed related and are planning to add a reference in our related work section in the camera-ready version of our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "oDOEhLoFDR", "original": null, "number": 5, "cdate": 1576966219465, "ddate": null, "tcdate": 1576966219465, "tmdate": 1576966219465, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"title": "Related Work", "comment": "Congratulations on the ICLR oral acceptance.\n\nWe have works ([1, 2]) that advocate making neural networks simultaneously generative and discriminative (by learning energy-based model).\n\nIn [1,2], a single CNN model has shown competitive results for image synthesis, image classification, and robustness against adversarial attacks.\n\n[1] Kwonjoon Lee, Weijian Xu, Fan Fan, and Zhuowen Tu, \"Wasserstein Introspective Neural Networks\", CVPR 2018.\n[2] Long Jin, Justin Lazarow, and Zhuowen Tu, \"Introspective Classification with Convolutional Nets\", NeurIPS 2017."}, "signatures": ["~Kwonjoon_Lee2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Kwonjoon_Lee2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}, {"id": "Qy8E9ksjm-", "original": null, "number": 1, "cdate": 1576798709790, "ddate": null, "tcdate": 1576798709790, "tmdate": 1576800926531, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "This paper uses energy based model to interpret standard discriminative classifier and demonstrates that energy based model training of the joint distribution improves calibration, robustness, and out-of-distribution detection while generating samples with better quality than GAN-based approaches. The reviewers are very excited about this work, and the energy-based perspective of generative and discriminative learning. There is a unanimous agreement to strongly accept this paper after author response.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723656, "tmdate": 1576800275171, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper921/-/Decision"}}}, {"id": "Skglf_h_9H", "original": null, "number": 3, "cdate": 1572550664057, "ddate": null, "tcdate": 1572550664057, "tmdate": 1574225561196, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper introduces the idea of energy based model to the traditional classifier, and proposes a new framework to improve the performances of the model in multiple aspects. The idea of reinterpreting the traditional classifier is very interesting, and the experiments show some good results of the proposed method.  \n\nHere are my main concerns of the current paper:\n1. The training procedure seems to be very sensitive, and the SGLD may take a long time at each iteration to converge. This may be a big limitation of the proposed method.\n2. According to equation (8), the proposed method is having a trade-off between classification and generation, and this seems to be the key to improve the performance of the model in generation by sacrificing some classification accuracy. I think author should emphasize this instead of energy based model.\n3. The presentation is not very clear in section 5. What is the task of calibration, and what is the definition of ECE?\n4. The robustness guarantee seems too good to be true. Although the authors claim that they allow the attacker to have access to the gradient  of SGLD, the SGLD will add noise during the forward process, this will obfuscate the gradient. In this sense, I don\u2019t think the proposed method will have the strong robustness as they claimed.\n\n----------------\nPost-Rebuttal Comments:\nThanks for addressing my concerns. Although I think the proposed method is not comprehensive to check obfuscated gradients, I do think the current version is a good fit for ICLR, and I decide to increase my score. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575718708276, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper921/Reviewers"], "noninvitees": [], "tcdate": 1570237745021, "tmdate": 1575718708287, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Review"}}}, {"id": "r1gdG_HosH", "original": null, "number": 12, "cdate": 1573767184298, "ddate": null, "tcdate": 1573767184298, "tmdate": 1573767184298, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hyela9GXoS", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "Post rebuttal", "comment": "Thank you for the responses, which addressed my questions."}, "signatures": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "S1evFK77jH", "original": null, "number": 9, "cdate": 1573235071320, "ddate": null, "tcdate": 1573235071320, "tmdate": 1573235071320, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "SklBGzqbuB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "Following up + New Results", "comment": "Jeremy, \nThank you very much for your comments. Following your recommendation we ran an EOT version of the PGD attack by averaging over multiple samples from our refinement procedure. Following this new analysis, we find that EOT indeed slightly reduces robustness. With respect to both norms our model's robustness now falls slightly below adversarial training. However, our overall claims still hold true as we do still notice a considerable improvement from the refinement procedure over the baseline (See the updated Figure 5). We have added an explanation of the EOT procedure to Appendix G.1 and updated the plots in the main body of the manuscript.\nThanks again! "}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "HJlZdaG7iS", "original": null, "number": 6, "cdate": 1573231976699, "ddate": null, "tcdate": 1573231976699, "tmdate": 1573232450817, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Skglf_h_9H", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "Response to R4", "comment": "(PART 1 OF 2)\n\nWe thank you for your time reviewing our work. We will address your concerns in order and we have updated the manuscript accordingly. We hope these changes will encourage you to change your score:\n\n1) Your concerns on the sensitivity and speed of SGLD training of EBMs.\n\nRegarding your concern about sensitivity: \n\nWhile we agree that SGLD training of EBMs can be sensitive to hyper-parameter settings, we note that throughout our work we used the exact same hyper-parameters for every model and every dataset. We also found these settings transferred well to datasets such as MNIST which we did not present in our paper. Further, we found these same settings worked well across a variety of model architectures such as MLPs, non-resnet convnets, and resnets. This was stated in Appendix  G.2 of our paper, but we have added it to the main body of our paper for clarity. This hyper-parameter transferability behavior has also been reported in prior work on EBM training such as [1, 2].\n\nRegarding your other concern about the convergence time: \n\nIn our work we put a great deal of focus into being able to train as quickly as possible with minimal hardware requirements. We have been able to train EBMs with far fewer SGLD steps per training iteration than in previous work and found that at these settings stable training can still take place. All of our models were trained on a single GPU and each training run took $<36$ hours. While this is slower than training a standard classifier on these datasets, our training speed falls comfortably in the range of other popular classes of generative models such as flows [3] and GANs [4].\n\nWe admit that we did not put enough emphasis on these two facts in our original draft and we have added this information in section 5. We hope this clarifies your concerns regarding training sensitivity and run-time.  \n\nOverall we feel that training and sampling are the biggest challenges when working with EBMs. Developing improved methods for this is important further work but we also feel it is outside of the scope of our current work. The main point of our work was to demonstrate that despite the challenges which currently exist in training EBMs, they can be used to achieve a very interesting and diverse set of results on problems which other classes of generative models have not been able to achieve at this scale. These results provide a strong motivation for more work in the space of EBM training methods.\n\n\n2) We are slightly unsure of what you mean with this point. We train our model using the factorized likelihood of Equation (8). As we explain in the following sentence, this was done to reduce bias in our training procedure, not because there is a need to weight these terms differently. We are aware that this is common practice in other hybrid-models [4, 5], but we do not do this in our model. Each term in this objective is weighted equally. While different results could possibly be achieved if we did weight each term in (8) we feel that our model's ability to weight the terms equally and still perform well at both tasks is actually a benefit of our approach over competing methods. We hope this clarifies your concerns. \n\n(CONTINUED BELOW)\n\n[1] \"Implicit Generation and Generalization in Energy-Based Models\"  Yilun Du, Igor Mordatch. https://arxiv.org/abs/1903.08689\n[2] \"On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models\"  Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, Ying Nian Wu. https://arxiv.org/abs/1903.12370\n[3] \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\"  Andrew Brock, Jess Donahue, Karen Simonyan. https://arxiv.org/abs/1809.11096\n[4] \"Glow: Generative Flow with Invertible 1x1 Convolutions\"  Diederik P. Kingma, Prafulla Dhariwal. https://arxiv.org/abs/1807.03039\n[5] \"Residual Flows for Invertible Generative Modeling\"  Ricky T. Q. Chen, Jens Behrmann, David Duvenaud, J\u00f6rn-Henrik Jacobsen. https://arxiv.org/abs/1906.02735\n[6] \"On Calibration of Modern Neural Networks\"  Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger. https://arxiv.org/abs/1706.04599\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "r1gouRzmiS", "original": null, "number": 8, "cdate": 1573232243039, "ddate": null, "tcdate": 1573232243039, "tmdate": 1573232243039, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "Post-Review Revisions", "comment": "We thank the reviewers for their thoughtful and valuable comments. We have heard your feedback and have made several minor revisions to our paper which we summarize here. We feel the paper is greatly improved after incorporating their feedback.  \n\n-As pointed out by R2 and R3, we have changed the caption in Figure 1 to indicate that these results are on CIFAR10\n\n-Responding to R3, we have added a section into the Appendix, \"Qualitative Analysis of Samples\". We find these new results particularly interesting!\n\n-Responding to R4's concerns about sensitivity and speed, we have moved some text from the Appendix to the main body about the run-time and sensitivity to hyper-parameters. \n\n-Responding to R4, in section 5.2, we provide a more technical description of calibration and added a Section, \"Calibration\", to the Appendix. \n\n-Responding to R4 and Jeremy Cohen about the stochasticity in our model and its robustness to adversarial examples, we have thoroughly re-evaluated our models using the Expectation Over Transformations attack [1] which makes randomized defenses like ours easier to attack. This new analysis finds that our models are slightly less robust than we have initially believed, but still competitive with robustness-specific approaches. We have updated Figure 5 with the new results and modified our discussion to accommodate these new results. We have also expanded the adversarial attack section in our Appendix to explain this new attack and how it was run. \n\n[1] \"Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples.\"  Anish Athalye, Nicholas Carlini, and David Wagner.  ICML 2018.  https://arxiv.org/abs/1802.00420"}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "rkx3C6zXoB", "original": null, "number": 7, "cdate": 1573232084447, "ddate": null, "tcdate": 1573232084447, "tmdate": 1573232084447, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "HJlZdaG7iS", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "Response to R4 (PART 2)", "comment": "(PART 2 of 2)\n\n3) A classifier is calibrated if the confidence of its predictive distribution p(y|x) is equivalent to its misclassification rate. We state this in plain text in the first paragraph of section 5.2. ECE is the \"``Expected Calibration Error\" which is a metric proposed in [6] to measure calibration of classifiers. This is clearly stated in Figure 4 and there we also provide a reference to the work which introduces this metric. To make this more clear, we have added a more formal definition of calibration to section 5.2 and have added a description of the ECE measure to the Appendix so interested readers do not have to read the referenced work to understand our analysis. We hope this makes section 5 easier to follow.\n\n4) Regarding the randomness in the refinement procedure, we note that JEM-0 provides considerable robustness compared to a baseline classifier (as can be seen in Figure 5). This model is completely deterministic as we do not use SGLD to refine the inputs. So, the worst-case robustness our approach adds over a baseline is still quite considerable and holds for both the L-inf and L-2 norms. \n\nHowever, following the suggestion from your and Jeremy Cohen's comment, we have run the EOT attack which averages the model's gradients over multiple samples of the randomized defense. We do find that the robustness results from JEM-1 and JEM-10 are slightly weaker than we had initially believed, but are still a considerable improvement over JEM-0 and competitive with approaches specifically targeting norm-bounded robustness. The improved robustness from the refinement procedure does appear to improve robustness with respect to both the L-inf and the L-2 norms -- a feature most robustness-specific approaches lack. These new results can be seen in a Table 5 in our revised paper.\n\nWe hope we have thoroughly addressed your concerns and you will choose to improve your score. \n\n[1] \"Implicit Generation and Generalization in Energy-Based Models\"  Yilun Du, Igor Mordatch. https://arxiv.org/abs/1903.08689\n[2] \"On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models\"  Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, Ying Nian Wu. https://arxiv.org/abs/1903.12370\n[3] \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\"  Andrew Brock, Jess Donahue, Karen Simonyan. https://arxiv.org/abs/1809.11096\n[4] \"Glow: Generative Flow with Invertible 1x1 Convolutions\"  Diederik P. Kingma, Prafulla Dhariwal. https://arxiv.org/abs/1807.03039\n[5] \"Residual Flows for Invertible Generative Modeling\"  Ricky T. Q. Chen, Jens Behrmann, David Duvenaud, J\u00f6rn-Henrik Jacobsen. https://arxiv.org/abs/1906.02735\n[6] \"On Calibration of Modern Neural Networks\"  Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger. https://arxiv.org/abs/1706.04599\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "HJxPlnMXir", "original": null, "number": 5, "cdate": 1573231598837, "ddate": null, "tcdate": 1573231598837, "tmdate": 1573231598837, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hklnbfn6tr", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "Response to R3", "comment": "We thank you for your time reviewing our work. We will address your concerns in order:\n\n1) Visual quality is difficult to quantify. Of the known metrics like IS and FID, using samples that have higher p(y|x) values results in higher scores, but not necessary if we use samples with higher p(x).  However, this is likely because of the downfalls of the evaluation metrics themselves rather than reflecting true sample quality.  \n\nBased on our analysis of CIFAR10 (below), we find\n    -Our p(x) model assigns values that cluster around different means for different classes.  The class automobiles has the highest p(x).  Of all generated samples, all top 100 samples are of this class.\n    -Given the class, the samples that have higher p(x) values all have white background and centered object, and lower p(x) samples have colorful (e.g., forest-like) background.\n    -Of all samples, higher p(y|x) values means clearly centered objects, and lower p(y|x) otherwise.\n\nWe completely agree with you that adding these analyese will strengthen the paper, and we have added this discussion with their corresponding images in the revised appendix.\n\n\n2) On CIFAR10 we see our accuracy drop from 95.2% to 92.9% and on CIFAR100 we see accuracy drop from 74.2% to 72.2%. This is a 2-3% drop on both datasets. These numbers are from the exact same model with and without JEM training. In these settings the decrease in accuracy is relatively consistent. \n\nPerhaps you are referring to the accuracy of our JEM models compared to state-of-the-art discriminative classifiers on these datasets? Yes, in this setting we have a 2-3% drop from the best wide-resnet classifier with all forms of regularization added. On CIFAR100 we have approximately a 8% drop compared to the best wide-resnet. \n\nOur best guess to explain this phenomenon is that competitive accuracy on CIFAR100 is much lower than competitive accuracy on CIFAR10 meaning that much more overfitting is happening on CIFAR100 than CIFAR10 (since all models achieve a training accuracy of 100% at the end of training).   \n\nIn our JEM models we remove two important forms of regularization, batch norm and dropout, which we found to have negligible impact on CIFAR10 but less negligible impact on CIFAR100. This is backed up by the fact that our baseline classifier with these regularizers removed achieves 74.4% accuracy, closer to that of our JEM model. \n\nWe feel that the removal of these regularizers provides an explanation for the decrease in relative performance. \n\n\n3) This is an interesting point. We are very excited about the future of EBMs and we are generally of the belief that the application of EBMs is currently limited by the fragility of the tools we use to train them. So yes, we do believe if one had access to considerable computational resources then one should be able scale these methods presented to larger datasets, but we do believe there would be considerable engineering cost in doing so. \n\nWe feel the most useful next steps to work on in the EBM-space are more stable and efficient training objectives which will increase the scale of problems to which we can apply these methods.\n\n\nMinor Remark) Yes you are correct that table presents CIFAR10 results and we did indeed forget to label it as such. This has been changed in our revised version. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "Hyela9GXoS", "original": null, "number": 4, "cdate": 1573231287669, "ddate": null, "tcdate": 1573231287669, "tmdate": 1573231287669, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "HJlmF6qoOr", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "Response to R2", "comment": "We thank you for your time reviewing our work. \n\nYou are correct, the results presented in table 1 are CIFAR10. We forgot to add this to the caption. We have updated the caption to fix this. Results on other datasets can be found in the text of Section 5.1.\n\nRegarding the performance of our approximate-mass measure for OOD detection on CelebA, we refer you to Table 3, bottom row, right-most column. This metric gets AUROC = .79 on this dataset, higher actually than unnormalized logp(x) which gets AUROC = .75. The metric may appear to perform worse than the likelihood in the histogram but the low-valued tails are larger and thus the AUROC is higher. \n\nRegarding the training procedure, we optimize a training objective which is log p(y|x) + log p(x). This is equivalent to optimizing log p(x, y). These two terms are combined by adding their gradients exactly. This is equivalent to an equal weighting of the two terms. We choose this way to factor the learning objective since p(y|x) is a normalized distribution and we can train with maximum likelihood exactly, avoiding a biased and tricky gradient estimation problem. We are aware that other hybrid models typically downweight the log p(x) term. We do not do that here. \n\nRegarding the training time and hardware requirements, we note that all models were trained on a single GPU in approximately 36 hours. We have added a few sentences to section 5 in the text to clarify this. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "B1lbVT9liB", "original": null, "number": 4, "cdate": 1573068072991, "ddate": null, "tcdate": 1573068072991, "tmdate": 1573068072991, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "S1efqSUJ5H", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"title": "out-of-distribution detection ", "comment": "Thanks, adding a sentence in 5.3.1 would address my concerns! This is a very interesting paper!"}, "signatures": ["~Yilun_Du2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yilun_Du2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}, {"id": "HJlmF6qoOr", "original": null, "number": 1, "cdate": 1570643322821, "ddate": null, "tcdate": 1570643322821, "tmdate": 1572972535398, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work is an attempt to bridge the gap between discriminative models, which currently obtain the state of the art on most classification problems, and generative models, which (through a model of the marginal p(x)) have the potential to shine on many tasks beyond generalization to a hold-out set with minimal shift in distributions: out of distribution detection, better generalization out of distribution, unsupervised learning etc.\n\nWhile much of the current work is related to normalizing flows / invertible neural networks, the authors here propose a quite simple but appealing method: A standard neural classifier is taken and the softmax is layer chopped off and replaced by an energy based model, which models the joint probability p(x,y) instead of the posterior p(y|x). The advantage is an additional degree of freedom in the scale of the logit vector, which is would have been otherwise normalized by the softmax layer and now can now model the data distribution. The downside is the loss in ease of training. Whereas (discriminative) deep networks can be easily trained by gradient descent on a cross-entropy objective, the partition function in the energy model makes this un tractable. This is addressed through sampling, similar to (Welling & Teh, 2011).\n\nOne of the biggest achievements reported by the authors is that the performance on discriminative tasks is not hurt (much) by adding the generative model. There is only a 3 point gap between Wide-ResNet and the proposed model (92.9% vs. 95.8%) \u2026 but on what dataset? 3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported. My guess is that this is a mean or mixture, since GEM performances of 96.7% and 72.2% are reported for SVHN and CIFAR10, respectively, but this should be made clearer. \t\n\nOn out of distribution detection, could the authors comment on the histograms in table 2, in particular the difference between the new measure (AM JEM) compared to JEM log p(x) on CelebA? The proposed measure does not seem to fare well here.\n\nAlthough the method does not outperform the gold standard of adversarial training, I found the models robustness to adversarial examples quite appealing, given that it was not trained for this objective (which also means that it does not require an adaptation to a norm). \n\nI was very impressed by Figure 6 showing distal adversarial initialized from random images, showing pretty clear images of the modelled class. The modelled variations require more investigation to verify whether we have a collapse for each class, but the results look very promising.\n\nThe paper is well written and easy to understand. A couple of details on the training procedure are missing in the experimental part. It is stated that, both, p(y|x) and the generative part p(x), are optimized, but how are these exactly integrated? Given the difficult in training this model reported in the paper, this seems to be particularly important.\n\nI also appreciated the description of the limitations of the algorithm, and the details in the appendix (ICLR should go back to unlimited paper lengths, btw.).\n\nMore information on complexity (training times etc.) should also be helpful. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575718708276, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper921/Reviewers"], "noninvitees": [], "tcdate": 1570237745021, "tmdate": 1575718708287, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Review"}}}, {"id": "Hklnbfn6tr", "original": null, "number": 2, "cdate": 1571828227683, "ddate": null, "tcdate": 1571828227683, "tmdate": 1572972535365, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper uses energy-based model interpretation for the logits of standard discriminative neural network models to define a generative model inside a classifier that proves useful in many downstream tasks such as uncertainty quantification, out-of-distribution detection, etc.\nAlthough there has been previous work attempting to bridge discriminative classifiers with generative modeling, this work proves to be competitive with both specialized models on discriminative/generative tasks as well as in many downstream tasks such as out-of-distribution detection, calibration, and adversarial robustness. The paper provides a clear exposition of the method, succeeds to discuss related work it bases on, conducts a thorough experimental study providing convincing explanations for results and does not hide the limitations of the work (high computational requirements, optimization difficulties connected with training energy-based model and the method used, limited approximation of the true energy). Overall, the paper provides a substantial contribution and paves the way for further work improving this joint discriminative - generative setting. However, there are points I would like the paper to address for better exposition.\n1. It would benefit the paper showing that samples with higher unnormalized likelihood are visually more compelling than those with lower likelihood.\n2. On CIFAR100 the accuracy drop from the reference value is larger than for datasets with 10 classes, could it be due the logits dimension is higher and challenges optimization?\n3. It would also be helpful to clarify whether application of the proposed method is primarily restricted by the computational complexity or is there any property inherent to energy-based models that makes treating high-dimensional data challenging?\n\nMinor remark\n- Although the paper doesn't state on which dataset results shown in Table 1 were obtained, I suspect its CIFAR10, please specify this."}, "signatures": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575718708276, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper921/Reviewers"], "noninvitees": [], "tcdate": 1570237745021, "tmdate": 1575718708287, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Review"}}}, {"id": "S1xyit7s9B", "original": null, "number": 3, "cdate": 1572710806561, "ddate": null, "tcdate": 1572710806561, "tmdate": 1572710806561, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "HJghWRO5qS", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "thanks", "comment": "Zhijian, \n\nThank you for brining your work to our attention. We will happily add a citation to your paper in our related work section."}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "HJghWRO5qS", "original": null, "number": 3, "cdate": 1572666883895, "ddate": null, "tcdate": 1572666883895, "tmdate": 1572666896209, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"title": "Missing related work", "comment": "Nice to read this paper. However, the main model (as presented in Section 3) is similar to (almost the same as) the work presented in the previous work (Section 3.4 there), which has been applied for semi-supervised learning and anomaly detection (similar to Out-Of-Distribution Detection).\n\nYunfu Song, Zhijian Ou. Learning Neural Random Fields with Inclusive Auxiliary Generators. arxiv 1806.00271, 2018.\nhttps://arxiv.org/abs/1806.00271\n\nIt would be nice if the authors could connect and compare to this previous work."}, "signatures": ["~Zhijian_Ou1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhijian_Ou1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}, {"id": "r1eebQv1cr", "original": null, "number": 2, "cdate": 1571939063626, "ddate": null, "tcdate": 1571939063626, "tmdate": 1571939063626, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "SklBGzqbuB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "running these results now", "comment": "Jeremy,\n\nThanks for your comments and for brining this to our attention. We agree that averaging over multiple samples will give a more reliable measure of our model's robustness. We are currently generating these results but they are quite computationally challenging to generate. Generating our initial 10-step refinement results took over a week running on many GPUs and averaging the same attack over multiple samples will take weeks to generate. We hope to have the final results completed by rebuttal time."}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "S1efqSUJ5H", "original": null, "number": 1, "cdate": 1571935625561, "ddate": null, "tcdate": 1571935625561, "tmdate": 1571935625561, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "H1lIa3Hkcr", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment", "content": {"title": "out-of-distribution detection", "comment": "Thank you for your comment and your interest in our work. \n\nI would first like to note that we reference your work many times throughout our paper and directly compare against your model on every OOD detection metric we report. Your paper is IGEBM in our table 3. If this was not clear then we can make it more so. \n\nIf your concern is simply that we do not mention that your work was first to notice that an EBM's unnormalized likelihood can perform better at OOD detection than exact likelihood models, then we are happy to add a sentence saying so in section 5.3.1.\n\nI hope this addresses your concerns. "}, "signatures": ["ICLR.cc/2020/Conference/Paper921/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper921/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper921/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper921/Authors|ICLR.cc/2020/Conference/Paper921/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164133, "tmdate": 1576860561460, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Official_Comment"}}}, {"id": "H1lIa3Hkcr", "original": null, "number": 2, "cdate": 1571933374000, "ddate": null, "tcdate": 1571933374000, "tmdate": 1571933407857, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"title": "Interesting Paper", "comment": "This is a interesting paper! Combining an EBM with a classifier network is an interesting idea. \n\nI had a couple concerns about connection to [1] on out of distribution evaluation of EBMs. In [1], we show in section 4.4, we first propose to evaluate the out of distribution evaluation of EBMs, using the likelihood, and show that it performs significantly better than other likelihood models, evaluated using the exact same AUROC metrics proposed in the paper.  I hope the text can be revised to reference this earlier work.\n\n[1] \"Implicit Generation and Generalization in Energy-Based Models\"  Yilun Du, Igor Mordatch.  NeurIPS 2019.  https://arxiv.org/abs/1903.08689"}, "signatures": ["~Yilun_Du2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yilun_Du2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}, {"id": "SklBGzqbuB", "original": null, "number": 1, "cdate": 1569985037282, "ddate": null, "tcdate": 1569985037282, "tmdate": 1569988451612, "tddate": null, "forum": "Hkxzx0NtDB", "replyto": "Hkxzx0NtDB", "invitation": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment", "content": {"comment": "This is a fascinating paper!   I'm impressed by the finding that JEMs match the adversarial robustness of adversarial training.   If this result holds up, it will be a Very Big Deal in the literature on adversarial robustness, since adversarial training (and every other effective adversarial defense) is tied to a particular norm, whereas JEM is not.   It is precisely because this result is so exciting that I am a little bit skeptical.   Since the proposed defense involves test-time randomization (the Gaussian noise in SGLD), the best practice when attacking is to average the input gradient over several random samples -- see the \"Expectation over Transformation\" (EOT) technique in [1].  Several randomized defenses previously thought effective were completely broken in [1] using EOT.  Therefore, I think it would strengthen the paper's claim if you evaluated the proposed defense against an EOT adversarial attack.\n\nAnother question I have is: how many noise samples were used when evaluating each prediction of RandAdvSmooth?\n\n[1] \"Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples.\"  Anish Athalye, Nicholas Carlini, and David Wagner.  ICML 2018.  https://arxiv.org/abs/1802.00420", "title": "interesting paper"}, "signatures": ["~Jeremy_Cohen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jeremy_Cohen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Your classifier is secretly an energy based model and you should treat it like one", "authors": ["Will Grathwohl", "Kuan-Chieh Wang", "Joern-Henrik Jacobsen", "David Duvenaud", "Mohammad Norouzi", "Kevin Swersky"], "authorids": ["wgrathwohl@cs.toronto.edu", "wangkua1@cs.toronto.edu", "j.jacobsen@vectorinstitute.ai", "duvenaud@cs.toronto.edu", "mnorouzi@google.com", "kswersky@google.com"], "keywords": ["energy based models", "adversarial robustness", "generative models", "out of distribution detection", "outlier detection", "hybrid models", "robustness", "calibration"], "TL;DR": "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so.  ", "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model. ", "pdf": "/pdf/df53e66f00cddbec2fc54bd79e0e5d84a31eaf9a.pdf", "paperhash": "grathwohl|your_classifier_is_secretly_an_energy_based_model_and_you_should_treat_it_like_one", "code": "https://wgrathwohl.github.io/JEM/", "_bibtex": "@inproceedings{\nGrathwohl2020Your,\ntitle={Your classifier is secretly an energy based model and you should treat it like one},\nauthor={Will Grathwohl and Kuan-Chieh Wang and Joern-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkxzx0NtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e57dfd3c08bdaac6b6f75d8df445d3742923b992.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkxzx0NtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202272, "tmdate": 1576860594539, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper921/Authors", "ICLR.cc/2020/Conference/Paper921/Reviewers", "ICLR.cc/2020/Conference/Paper921/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper921/-/Public_Comment"}}}], "count": 27}