{"notes": [{"id": "S1g490VKvB", "original": "rkx6lcOdvB", "number": 1280, "cdate": 1569439372485, "ddate": null, "tcdate": 1569439372485, "tmdate": 1577168244352, "tddate": null, "forum": "S1g490VKvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "The Dynamics of Signal Propagation in Gated Recurrent Neural Networks", "authors": ["Dar Gilboa", "Bo Chang", "Minmin Chen", "Greg Yang", "Samuel S. Schoenholz", "Ed H. Chi", "Jeffrey Pennington"], "authorids": ["dg2893@columbia.edu", "bchang@stat.ubc.ca", "minminc@google.com", "gregyang@microsoft.com", "schsam@google.com", "edchi@google.com", "jpennin@google.com"], "keywords": ["recurrent neural networks", "theory of deep learning"], "TL;DR": "We calculate conditions for signal propagation in LSTMs and GRUs, and use these to predict trainability of networks on long sequence tasks and construct initialization schemes that improve performance on such tasks. ", "abstract": "Training recurrent neural networks (RNNs) on long sequence tasks is plagued with difficulties arising from the exponential explosion or vanishing of signals as they propagate forward or backward through the network. Many techniques have been proposed to ameliorate these issues, including various algorithmic and architectural modifications. Two of the most successful RNN architectures, the LSTM and the GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer from instabilities when trained on very long sequences. In this work, we develop a mean field theory of signal propagation in LSTMs and GRUs that enables us to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, we derive a novel initialization scheme that eliminates or reduces training instabilities. We demonstrate the efficacy of our initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower. We also observe a beneficial effect on generalization performance using this new initialization.", "pdf": "/pdf/e57c0720411adf570fec6260779637328196ae41.pdf", "paperhash": "gilboa|the_dynamics_of_signal_propagation_in_gated_recurrent_neural_networks", "original_pdf": "/attachment/e57c0720411adf570fec6260779637328196ae41.pdf", "_bibtex": "@misc{\ngilboa2020the,\ntitle={The Dynamics of Signal Propagation in Gated Recurrent Neural Networks},\nauthor={Dar Gilboa and Bo Chang and Minmin Chen and Greg Yang and Samuel S. Schoenholz and Ed H. Chi and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g490VKvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "C8_f8GFcla", "original": null, "number": 1, "cdate": 1576798719236, "ddate": null, "tcdate": 1576798719236, "tmdate": 1576800917288, "tddate": null, "forum": "S1g490VKvB", "replyto": "S1g490VKvB", "invitation": "ICLR.cc/2020/Conference/Paper1280/-/Decision", "content": {"decision": "Reject", "comment": "Using ideas from mean-field theory and statistical mechanics, this paper derives a principled way to analyze signal propagation through gated recurrent networks.  This analysis then allows for the development of a novel initialization scheme capable of mitigating subsequent training instabilities.  In the end, while reviewers appreciated some of the analytical insights provided, two still voted for rejection while one chose accept after the rebuttal and discussion period.  And as AC for this paper, I did not find sufficient evidence to overturn the reviewer majority for two primary reasons.\n\nFirst, the paper claims to demonstrate the efficacy of the proposed initialization scheme on multiple sequence tasks, but the presented experiments do not really involve representative testing scenarios as pointed out by reviewers.  Given that this is not a purely theoretical paper, but rather one suggesting practically-relevant initializations for RNNs, it seems important to actually demonstrate this on sequence data people in the community actually care about.  In fact, even the reviewer who voted for acceptance conceded that the presented results were not too convincing (basically limited to toy situations involving Cifar10 and MNIST data).\n\nSecondly, all reviewers found parts of the paper difficult to digest, and while a future revision has been promised to provide clarity, no text was actually changed making updated evaluations problematic.  Note that the rebuttal mentions that the paper is written in a style that is common in the physics literature, and this appears to be a large part of the problem.  ICLR is an ML conference and in this respect, to the extent possible it is important to frame relevant papers in an accessible way such that a broader segment of this community can benefit from the key message.  At the very least, this will ensure that the reviewer pool is more equipped to properly appreciate the contribution.  My own view is that this work can be reframed in such a way that it could be successfully submitted to another ML conference in the future.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Dynamics of Signal Propagation in Gated Recurrent Neural Networks", "authors": ["Dar Gilboa", "Bo Chang", "Minmin Chen", "Greg Yang", "Samuel S. Schoenholz", "Ed H. Chi", "Jeffrey Pennington"], "authorids": ["dg2893@columbia.edu", "bchang@stat.ubc.ca", "minminc@google.com", "gregyang@microsoft.com", "schsam@google.com", "edchi@google.com", "jpennin@google.com"], "keywords": ["recurrent neural networks", "theory of deep learning"], "TL;DR": "We calculate conditions for signal propagation in LSTMs and GRUs, and use these to predict trainability of networks on long sequence tasks and construct initialization schemes that improve performance on such tasks. ", "abstract": "Training recurrent neural networks (RNNs) on long sequence tasks is plagued with difficulties arising from the exponential explosion or vanishing of signals as they propagate forward or backward through the network. Many techniques have been proposed to ameliorate these issues, including various algorithmic and architectural modifications. Two of the most successful RNN architectures, the LSTM and the GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer from instabilities when trained on very long sequences. In this work, we develop a mean field theory of signal propagation in LSTMs and GRUs that enables us to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, we derive a novel initialization scheme that eliminates or reduces training instabilities. We demonstrate the efficacy of our initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower. We also observe a beneficial effect on generalization performance using this new initialization.", "pdf": "/pdf/e57c0720411adf570fec6260779637328196ae41.pdf", "paperhash": "gilboa|the_dynamics_of_signal_propagation_in_gated_recurrent_neural_networks", "original_pdf": "/attachment/e57c0720411adf570fec6260779637328196ae41.pdf", "_bibtex": "@misc{\ngilboa2020the,\ntitle={The Dynamics of Signal Propagation in Gated Recurrent Neural Networks},\nauthor={Dar Gilboa and Bo Chang and Minmin Chen and Greg Yang and Samuel S. Schoenholz and Ed H. Chi and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g490VKvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1g490VKvB", "replyto": "S1g490VKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727201, "tmdate": 1576800279423, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1280/-/Decision"}}}, {"id": "r1xrrEojYS", "original": null, "number": 1, "cdate": 1571693628862, "ddate": null, "tcdate": 1571693628862, "tmdate": 1574107230647, "tddate": null, "forum": "S1g490VKvB", "replyto": "S1g490VKvB", "invitation": "ICLR.cc/2020/Conference/Paper1280/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper touches the signal processing/long term propagation problem in gated recurrent neural networks from the mean field theory. The paper starts from a dynamic system view of the recurrent neural networks and calculates the time scale of converging to the fixed point. In order to avoid the system to converge to the fixed point, the authors utilize some initialization strategy to keep the time scale to infinity. The authors also relate the time scale to state-to-state Jacobians. \n\nThe paper is interesting but more details could be added to both theories and experiments. Given all those extra details, I can increase my scores.\n\nFor the theory:\n1. It is unclear how you go from equation (7) to (8). References or more explanations need to be added.\n2. It is unclear how the initializations in E.4 satisfies the criteria you define in the paper. More explanations can be added.\n3. This mean field approximation is still far away from practice. It is hard to believe the input in real life is Gaussian distributed vectors (sec 4.2). What will happen if the input distributions are not Gaussian? This should be discussed.\n4. This initialization only helps at the beginning of the training. What will happen if we do one backpropagation? This should be addressed.\n\nFor the experiments:\n1. The authors did not state the some of the experiment details in the papers, like what optimizer, regularization, learning rate.... To make better assessment of the experiments, those details should be added. Do you train/tune all the different initializations in the same way?\n2. I cannot find the description of Figure 1 anywhere in the paper. It is hard to believe LSTM did poorly on sequential MNIST unless giving more details since LSTM has been proved to perform okay on sequential MNIST in a bunch of papers[1]. \n3. What is the meaning of 112 dimension in 5.3? Does that mean you only choose the first 4 rows of MNIST images?\n4. Comparison over random seeds should be honestly justified for all the experiments.\n\nMinors:\n1. You have one missing \\Sigma_z in equation (4a).\n2. \\mu_s^2 in equation (4b) should be (\\mu_s^t)^2\n\nReferences:\n[1] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. \"Unitary evolution recurrent neural networks.\" International Conference on Machine Learning. 2016.\n\n-------------\nI change my scores to weak reject . I agree with Reviewer 2 that the paper provide some insight from Physics and can be an interesting contribution to the community. However, I think all of the reviewers. including myself, find the paper hard to read for the general machine learning audience. And even though the authors mention that they will fix the text in the future, they do not change any text of the paper. I think writing is also important besides presenting interesting research ideas. Overall, I think the paper will be benefited from resubmission.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1280/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1280/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Dynamics of Signal Propagation in Gated Recurrent Neural Networks", "authors": ["Dar Gilboa", "Bo Chang", "Minmin Chen", "Greg Yang", "Samuel S. Schoenholz", "Ed H. Chi", "Jeffrey Pennington"], "authorids": ["dg2893@columbia.edu", "bchang@stat.ubc.ca", "minminc@google.com", "gregyang@microsoft.com", "schsam@google.com", "edchi@google.com", "jpennin@google.com"], "keywords": ["recurrent neural networks", "theory of deep learning"], "TL;DR": "We calculate conditions for signal propagation in LSTMs and GRUs, and use these to predict trainability of networks on long sequence tasks and construct initialization schemes that improve performance on such tasks. ", "abstract": "Training recurrent neural networks (RNNs) on long sequence tasks is plagued with difficulties arising from the exponential explosion or vanishing of signals as they propagate forward or backward through the network. Many techniques have been proposed to ameliorate these issues, including various algorithmic and architectural modifications. Two of the most successful RNN architectures, the LSTM and the GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer from instabilities when trained on very long sequences. In this work, we develop a mean field theory of signal propagation in LSTMs and GRUs that enables us to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, we derive a novel initialization scheme that eliminates or reduces training instabilities. We demonstrate the efficacy of our initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower. We also observe a beneficial effect on generalization performance using this new initialization.", "pdf": "/pdf/e57c0720411adf570fec6260779637328196ae41.pdf", "paperhash": "gilboa|the_dynamics_of_signal_propagation_in_gated_recurrent_neural_networks", "original_pdf": "/attachment/e57c0720411adf570fec6260779637328196ae41.pdf", "_bibtex": "@misc{\ngilboa2020the,\ntitle={The Dynamics of Signal Propagation in Gated Recurrent Neural Networks},\nauthor={Dar Gilboa and Bo Chang and Minmin Chen and Greg Yang and Samuel S. Schoenholz and Ed H. Chi and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g490VKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g490VKvB", "replyto": "S1g490VKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1280/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1280/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575982500779, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1280/Reviewers"], "noninvitees": [], "tcdate": 1570237739675, "tmdate": 1575982500795, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1280/-/Official_Review"}}}, {"id": "r1xoecU2Yr", "original": null, "number": 2, "cdate": 1571740146528, "ddate": null, "tcdate": 1571740146528, "tmdate": 1573907680140, "tddate": null, "forum": "S1g490VKvB", "replyto": "S1g490VKvB", "invitation": "ICLR.cc/2020/Conference/Paper1280/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The aim of this paper is to suggest randomized initializations for the various weights of a recurrent neural network (GRUs and various LSTMs are covered), such that training these networks gets to a successful start, when the model is trained on long sequences. Instead of being heuristic, their approach follows first principles of analyzing signal propagation through time, using ideas from statistical thermodynamics (mean field approximations). Some experiments, on toy datasets, validate their approach.\n\nI am quite intrigued by this paper. It is using interesting theory, shapes it to a practically highly relevant and difficult applied problem, and in the end comes up with a computable criterion of how to choose hyperparameters (means and variances of Gaussians to sample initial weights from). While the results in practice are still not too convincing, I am strongly in favour of giving this approach the benefit of doubt, as it could lead to practically very useful downstream work.\n\nThe main direction of improvement for this paper (given that experiments are what they are -- somewhat limited to toy situations right now) is to better explain the methodology to researchers not familiar with mean field methods. Most importantly, it is not explained in the main text how hyperparameters are really chosen in the end. Looking at Appendix E, I find some pretty basic choices, and no other alternatives considered. It is not explained why these choices satisfy the theory, why they'd be the only ones, etc. This creates a disconnect between the very nice (and seemingly useful) theory and its implications (they are not really well spelled out).\n\nHere is what I understood (and I am not specifically an expert on stat mech). The authors assume that the dimension of latent states (N) grows large. They assume that weights are sampled independently, and identical distributed in groups k (different cell types, weight vs bias), and that inputs are correlated with each other in each dimension. Based on these assumption, they follow Gaussians statistics through a number of time steps. In the limit, one gets a deterministic dynamical system, and as t -> infty, this may converge to a fixed point. In a very nice argument (which they could explain better), they state that such rapid convergence is bad news, because then information cannot spread across long time scales, so one has to find hyperparameters for which the system behaves \"critically\". A second arguments tries to keep gradient sizes (under MF assumptions) of O(1), so neither -> 0, not -> infty, which is again some \"critical\" range. Under their assumption, these critical conditions can be computed depending on the hyperparameters.\n\nUnfortunately, this is where the paper somewhat stops, it does not give specific methods for finding hyperpars that satisfy the criteria, at least not in the sense of characterising the whole space of such hyperpars (instead, in Appendix E, they just state some few settings that do). As a direction for future work, this would be very important. Another side question is whether for what the authors call \"trainability\", the only point that matters is whether for the initial weights, signals can spread and gradients are O(1). It is certainly necessary, I see that.\n\nDetailed comments:\n- Please fix Table 1, the expressions seem broken. What does \"r2\" mean in the GRU column?\n- At least for me, (1a) to (1c) really was too short. At least in the Appendix, please do explain how this gives GRU and LSTM\n- Please explain the untied weight assumption somewhere. s^t is a map of s^(t-1) and W_k, so how can W_k be independent\n   of all s^t? What are you really assuming here?\n- It took some repeated reading until I understood why the expressions in (2a) to (4b) do not depend on i, j, a, b (except\n  whether a = b or a != b). Explain that properly\n- The core of the whole approach seems to be first half of page 5. This seems like a very nice argument, but hard to understand. Try making it more crisp. I kind of get the rough idea why fast convergence over t would be bad, but would total divergence over t not also be bad?\n- In (12a-c), do you mean \"equal\" or \"approximately equal\"?\n- In 4.4: \"This motivates the general form of the initializations used in the experiments\": You have to make this more explicit. Why are your choices the only ones? Could there not be other choices satisfying (12a-c) approx, and be better?\n- Value of Sigma_z = 1: This seems odd to me, then your covariances are degenerate (rank 1 instead of 2). Please explain\n- Standard LSTM harder than GRU or peephole LSTM: Again, this sounds real interesting, but I did not get it from the explanation\n- I did not understand Figure 2. How are Theta_0, Theta_1 chosen?\n- As said above, the experiments are interesting, but somewhat artificial. Please do at least comment on real-world applications, and whether (and how) the ideas here would apply\n- Discussion: \"there is no clear principled way...\": Well, but practitioners need something. I'd disagree, at least one could attempt to navigate this space by global optimization techniques...\n\n\nADDITIONAL COMMENT:\n\nI tried to append the following as comment, but the (pretty broken) system would not let me, insisting that \"reader is not valid\" (???). Anyway, here it is. I hope I am allowed to add to my own review.\n\nI've seen the argument in reviews that assumptions made by this paper about independencies between weights and inner states are wrong, and therefore conclusions are not valid.\n\nFirst, such assumptions are indeed pretty common in such statistical mech analyses of learning methods. Second, you have to distinguish between weights after (random) initialization and after training. Of course, LSTM represents long term dependencies after training, but initialization is a different story.\n\nIf I was the AC for this paper, I'd ask somebody with at least some background in statistical mech to provide some additional opinions, as the reviewers (including myself) are not fully qualified.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1280/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1280/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Dynamics of Signal Propagation in Gated Recurrent Neural Networks", "authors": ["Dar Gilboa", "Bo Chang", "Minmin Chen", "Greg Yang", "Samuel S. Schoenholz", "Ed H. Chi", "Jeffrey Pennington"], "authorids": ["dg2893@columbia.edu", "bchang@stat.ubc.ca", "minminc@google.com", "gregyang@microsoft.com", "schsam@google.com", "edchi@google.com", "jpennin@google.com"], "keywords": ["recurrent neural networks", "theory of deep learning"], "TL;DR": "We calculate conditions for signal propagation in LSTMs and GRUs, and use these to predict trainability of networks on long sequence tasks and construct initialization schemes that improve performance on such tasks. ", "abstract": "Training recurrent neural networks (RNNs) on long sequence tasks is plagued with difficulties arising from the exponential explosion or vanishing of signals as they propagate forward or backward through the network. Many techniques have been proposed to ameliorate these issues, including various algorithmic and architectural modifications. Two of the most successful RNN architectures, the LSTM and the GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer from instabilities when trained on very long sequences. In this work, we develop a mean field theory of signal propagation in LSTMs and GRUs that enables us to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, we derive a novel initialization scheme that eliminates or reduces training instabilities. We demonstrate the efficacy of our initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower. We also observe a beneficial effect on generalization performance using this new initialization.", "pdf": "/pdf/e57c0720411adf570fec6260779637328196ae41.pdf", "paperhash": "gilboa|the_dynamics_of_signal_propagation_in_gated_recurrent_neural_networks", "original_pdf": "/attachment/e57c0720411adf570fec6260779637328196ae41.pdf", "_bibtex": "@misc{\ngilboa2020the,\ntitle={The Dynamics of Signal Propagation in Gated Recurrent Neural Networks},\nauthor={Dar Gilboa and Bo Chang and Minmin Chen and Greg Yang and Samuel S. Schoenholz and Ed H. Chi and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g490VKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g490VKvB", "replyto": "S1g490VKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1280/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1280/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575982500779, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1280/Reviewers"], "noninvitees": [], "tcdate": 1570237739675, "tmdate": 1575982500795, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1280/-/Official_Review"}}}, {"id": "SyxDSP-CtH", "original": null, "number": 3, "cdate": 1571850047084, "ddate": null, "tcdate": 1571850047084, "tmdate": 1572972489278, "tddate": null, "forum": "S1g490VKvB", "replyto": "S1g490VKvB", "invitation": "ICLR.cc/2020/Conference/Paper1280/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a mean-field analysis of recurrent networks in this paper. I have a few concerns about this paper:\n\n(1) The most serious concern about their analysis comes from their assumption. They assume the weight W is independent on the state s_t (Page 4, Lines 5-6). The recursive structure is the most complicated part of the recurrent networks, and also its major difference from feedforward networks. In current networks, the hidden states become (or even heavily) dependent on the weight due to recursion.\n\nWhen making such an assumption, the recurrent networks just become similar to feedforward networks. The authors' claim that \"the untied weights assumption actually has long history of yielding correct prediction\" is not ungrounded and questionable.\n\n(2) The paper is not well written. Some assumptions are not explicitly stated. They are placed in the text without any highlight. Some theoretical statements are claimed without any rigorous proof. A few approximations are applied without clearly explaining about the resulting approximation errors. This is not acceptable, especially when the authors claim they are developing a \"THEORY\".\n\n(3) The experiments only consider the MNIST and CIFAR10 datasets. These datasets are mainly used for evaluating feedforward-type convolutional neural networks. Even though the authors might like their experiments, for the sake of the main stream users of recurrent networks. They should at least include experiments in conventional sequential prediction problems, e.g., speech, time series, machine translations.\n\n(4) Compared with other state of the art methods, their experimental results on CIFAR10 is too weak. I cannot believe such weak results can be used to make meaningful justifications.\n\n\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1280/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1280/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Dynamics of Signal Propagation in Gated Recurrent Neural Networks", "authors": ["Dar Gilboa", "Bo Chang", "Minmin Chen", "Greg Yang", "Samuel S. Schoenholz", "Ed H. Chi", "Jeffrey Pennington"], "authorids": ["dg2893@columbia.edu", "bchang@stat.ubc.ca", "minminc@google.com", "gregyang@microsoft.com", "schsam@google.com", "edchi@google.com", "jpennin@google.com"], "keywords": ["recurrent neural networks", "theory of deep learning"], "TL;DR": "We calculate conditions for signal propagation in LSTMs and GRUs, and use these to predict trainability of networks on long sequence tasks and construct initialization schemes that improve performance on such tasks. ", "abstract": "Training recurrent neural networks (RNNs) on long sequence tasks is plagued with difficulties arising from the exponential explosion or vanishing of signals as they propagate forward or backward through the network. Many techniques have been proposed to ameliorate these issues, including various algorithmic and architectural modifications. Two of the most successful RNN architectures, the LSTM and the GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer from instabilities when trained on very long sequences. In this work, we develop a mean field theory of signal propagation in LSTMs and GRUs that enables us to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, we derive a novel initialization scheme that eliminates or reduces training instabilities. We demonstrate the efficacy of our initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower. We also observe a beneficial effect on generalization performance using this new initialization.", "pdf": "/pdf/e57c0720411adf570fec6260779637328196ae41.pdf", "paperhash": "gilboa|the_dynamics_of_signal_propagation_in_gated_recurrent_neural_networks", "original_pdf": "/attachment/e57c0720411adf570fec6260779637328196ae41.pdf", "_bibtex": "@misc{\ngilboa2020the,\ntitle={The Dynamics of Signal Propagation in Gated Recurrent Neural Networks},\nauthor={Dar Gilboa and Bo Chang and Minmin Chen and Greg Yang and Samuel S. Schoenholz and Ed H. Chi and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g490VKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g490VKvB", "replyto": "S1g490VKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1280/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1280/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575982500779, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1280/Reviewers"], "noninvitees": [], "tcdate": 1570237739675, "tmdate": 1575982500795, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1280/-/Official_Review"}}}], "count": 5}