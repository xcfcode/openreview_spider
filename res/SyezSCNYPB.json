{"notes": [{"id": "SyezSCNYPB", "original": "SygXnYL_vB", "number": 1102, "cdate": 1569439289552, "ddate": null, "tcdate": 1569439289552, "tmdate": 1577168222524, "tddate": null, "forum": "SyezSCNYPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "nmh0ODe2j", "original": null, "number": 1, "cdate": 1576798714499, "ddate": null, "tcdate": 1576798714499, "tmdate": 1576800922006, "tddate": null, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a model combining AC-GAN and StyleGAN for semi-supervised learning of disentangled generative adversarial networks. It also proposes new datasets of 3d images as benchmarks. The main claim is that the proposed model can achieve strong disentanglement property by using 1-5% of the annotations on the factors of variation. The technical contribution is moderate but the architecture itself is not highly novel. While the proposed method seems to work for controlled/synthetic datasets, overall technical contribution seems incremental and it's unclear whether it can perform well on larger-scale, real datasets. The experimental results on CelebA don't look convincing enough.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705727, "tmdate": 1576800253575, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Decision"}}}, {"id": "BJxgb6u2jS", "original": null, "number": 5, "cdate": 1573846263912, "ddate": null, "tcdate": 1573846263912, "tmdate": 1573846531280, "tddate": null, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment", "content": {"title": "Major changes in the revised paper", "comment": "We thank all the reviewers for very useful comments and suggestions. Below are majors changes in the revised paper according reviewers\u2019 suggestions:\n\n- We want to emphasize that the motivation of this work is to investigate 1) how a *generic* disentanglement learning model behaves in the *high-resolution* image domain and 2) how the limited supervision could help the disentanglement. We have highlighted these two points in the revised paper.\n\n- We have added a rescaling coefficient in the semi-supervised loss function Eq. (2), and obtained *better* semi-supervised results in Figure 3: By only use *1%* labelled data, the disentanglement quality is close to the fully supervised case and outperforms the unsupervised alternative by a significant margin.\n\n- We have added a comparison between our model and disentangled VAEs on both the dSprties and the downsampled version of the Isaac3D dataset in Table 2. We found that the unsupervised version of our model largely outperforms these baselines in terms of both image quality and disentanglement quality.\n\n- We have added the experimental results on CelebA with resolution 256x256 and showed the effectiveness of our model on the real data  in Figure 5 and 15.\n\n- We have added a supervised baseline which only uses the labelled data in the disentanglement learning. The results in Table 3 showed our semi-supervised method consistently outperforms the supervised baseline in the case where only 1% or 5% labelled data is available.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyezSCNYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1102/Authors|ICLR.cc/2020/Conference/Paper1102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161226, "tmdate": 1576860558189, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment"}}}, {"id": "HyeJ7i_hsH", "original": null, "number": 4, "cdate": 1573845782552, "ddate": null, "tcdate": 1573845782552, "tmdate": 1573845782552, "tddate": null, "forum": "SyezSCNYPB", "replyto": "HJxsmxLE9S", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you very much for your review and helpful comments. We address your specific questions and comments below:\n\n1. The core contributions of new datasets and architectures are not validated or analyzed rigorously enough.\n\n- We have included Table 1 to directly summarize the advantages of our proposed datasets over previous datasets. \n- We have also slightly modified the abstract and introduction for clarifying the main focus points of this paper as the reviewer suggested. \n- To further validate our contributions, we have also done the ablation studies on semi-supervised learning, compared with baselines on both the existing dataset and our proposed dataset, and applied our method to real data.\n\n2. No prior work is used as baselines in order to compare / validate the newly introduced architectures. Especially, why AC-GAN was chosen over the Projection Discriminator of Miyato and Koyama 2018.\n\n- We choose AC-GAN, instead of other conditional GAN methods, such as cGANs (with Projection Discriminator), mainly because only AC-GAN has the property of reconstructing meta code in the discriminator, which can easily be extended to a semi-supervised disentanglement learning framework.\n- We indeed did experiments on comparing AC-GAN, vanilla cGAN and cGANs with Projection Discriminator in the fully supervised case. Our results show that the image quality generated by AC-GAN modified StyleGAN largely outperforms the other two methods. We have made it more clear in the revised paper that only AC-GAN framework can support *semi-supervised* disentanglement learning.\n\n3.  The lack of any results on real, non synthetic, datasets in order to validate these (for instance, CelebA) which significant prior work on disentanglement has been evaluated on is an unfortunate omission.\n\n- One major motivation is that how (semi-)supervision helps disentanglement. In such sense, we need the ground-truth meta code to provide (semi-)supervision and *quantify* our results. However, most real datasets lack the ground-truth meta code for factors of variation. It is why we want to focus on the synthetic datasets with quantitative results to gain useful insights.\n- We agree that many previous works use CelebA to evaluate the disentanglement quality. But they can only show the *qualitative* results of latent traversal since there are only *binary* attributes in CelebA. Commonly-used disentanglement metrics, such as MIG and FactorVAE score, do not apply to factors of *binary* variations.\n- As the reviewer suggested, we have added latent traversal results on CelebA in the revised paper to qualitatively validate our model in real datasets, as shown in Figure 5 and Figure 15.\n\n4. The addition of an ablation / analysis demonstrating the contribution of the second term in settings where labels are also available would strengthen the author's claims and fully demonstrate the model is an effective semi-supervised learner.\n\n- Thanks for the suggestion. We have run the suggested experiments by removing the unsupervised disentanglement term in the discriminator loss function. This serves as a supervised baseline by only using the available labelled data. We found that when only 1% or 5% of labelled data is available, our semi-supervised method consistently outperforms the supervised baseline, as shown in Table 3.\n\nPlease let us know if we have addressed your concerns and if you have further comments. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyezSCNYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1102/Authors|ICLR.cc/2020/Conference/Paper1102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161226, "tmdate": 1576860558189, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment"}}}, {"id": "rJeWSDD5jr", "original": null, "number": 3, "cdate": 1573709624648, "ddate": null, "tcdate": 1573709624648, "tmdate": 1573709624648, "tddate": null, "forum": "SyezSCNYPB", "replyto": "B1ldFEHFdH", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you very much for your review and helpful comments. We address your specific questions and comments below:\n\n1. I don't understand why the proposed algorithm wasn't evaluated on existing datasets. \n\n- We have evaluated our model on dSprites, one of the most commonly disentanglement datasets. The results of comparing with the state-of-the-art unsupervised disentangled VAEs, such as Beta-VAE, FactorVAE and Beta-TCVAE are shown in Table 2(a), and we can see the unsupervised version of our model outperforms all the *generic* disentanglement learning baselines. \n- As the reviewer 3 suggested, we have also provided the latent traversal results of our model on CelebA of resolution 256x256 to demonstrate the effectiveness of our model on real datasets. \n\n2. I don't understand why it wasn't compared against other algorithms that purport to do the same thing.\n\n- Thanks for the suggestion! We agree that there are many works on synthesizing images and controlling various attributes. But many of them are either domain-specific (only for faces [1] or person images [2] or 3D features [3]) or are not able to control the generation of *high-resolution* images (at least 512x512). To the best of our knowledge, there is little prior work on *semi-supervised* *generic* disentanglement learning of *high-resolution* images for us to systematically compare with.\n- More similarly, the disentangled VAEs, such Beta-VAE, FactorVAE and Beta-TCVAE are a class of unsupervised *generic* disentanglement learning models. By setting \\alpha=0, the unsupervised version of our model can be compared with these disentangled VAEs. We have run experiments on a downsampled version of the Isaac3D dataset (with resolution 128x128), and found that the unsupervised version of our model outperforms all of these disentangled VAEs in terms of disentanglement quality and image quality. We have added these comparison results into Section 4.2 in the revised paper.\n\n[1] Tran et al., \"Disentangled representation learning gan for pose-invariant face recognition.\" CVPR 2017.\n[2] Ma et al., \"Disentangled person image generation.\" CVPR. 2018.\n[3] Nguyen-Phuoc et al., \"HoloGAN: Unsupervised learning of 3D representations from natural images.\" ICCV 2019.\n\n3. There are a few things I can think of that are weird about the measurement \u2014 'interpolation variance'. For example, I might have a dimension of the code that causes one really peaky change at one point, and then the change quickly reverts.\n\n- We agree that generally if the number of interpolating points (denoted by S in Eq. (3)) is not large enough, we may miss some peaky changes. However, we didn\u2019t observe this kind of peaky change in experiments, because the latent traversal results of our model are quite smooth. For example, we have increased the value of S in experiments, but the results of identifying fine-grained styles basically remain unchanged.\n\n4. Other representations issues \n\n- We thank the reviewer for pointing them out. We have fixed most representation issues accordingly.\n\nPlease let us know if we have addressed your concerns and if you have further comments. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyezSCNYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1102/Authors|ICLR.cc/2020/Conference/Paper1102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161226, "tmdate": 1576860558189, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment"}}}, {"id": "H1l5PLw5ir", "original": null, "number": 2, "cdate": 1573709410170, "ddate": null, "tcdate": 1573709410170, "tmdate": 1573709410170, "tddate": null, "forum": "SyezSCNYPB", "replyto": "rkxJEUD9ir", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (cont.)", "comment": "4. Clarify the main focus points of this paper and try to substantiate the result. The author wrote \"Our work extends the above works by scaling up the disentanglement learning to high-resolution images, and emphasizing the importance of supervision in controllable generation.\" <---- but (1) high-res images here are synthetic and limited in scene variability; (2) the second part is expected given previous work.\n\n- In the *generic* disentanglement learning community, most commonly used benchmarks are synthetic datasets, such as dSprites and Shape3D (see more in [1]). This is because the factors of variation are *known* in the synthetic datasets and thus we can quantify the disentanglement quality of different models. We think it is good to first study more complex synthetic datasets to gain insights and then apply the insights to real datasets. Furthermore, we have tested a higher-resolution version of CelebA (compared with most previous methods working on 64x64 CelebA) and confirmed the effectiveness of our model on high-resolution real datasets. \n- As far as we know, no much work on the *generic* disentanglement learning has systematically investigated the importance of *limited* supervision, except for [2]. However, [2] only applies to disentangled VAEs on low-resolution synthetic datasets, raising another open question: With very little supervision on more complex and higher-resolution datasets, can we still well disentangle the factors of variation while maintaining the high generation quality? One major goal in our work is to answer this question with a thorough experimental study. We have highlighted this point in the revised paper.\n- Finally, we have slightly modified the abstract and introduction for clarifying the main focus points of this paper as the reviewer suggested. \n\n[1] Locatello et al., \u201cChallenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\u201d. ICML 2019.\n[2] Locatello et al., \u201cDisentangling factors of variation using few labels\u201d. arXiv preprint arXiv:1905.01258.\n\nPlease let us know if we have addressed your concerns and if you have further comments. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyezSCNYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1102/Authors|ICLR.cc/2020/Conference/Paper1102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161226, "tmdate": 1576860558189, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment"}}}, {"id": "rkxJEUD9ir", "original": null, "number": 1, "cdate": 1573709350652, "ddate": null, "tcdate": 1573709350652, "tmdate": 1573709350652, "tddate": null, "forum": "SyezSCNYPB", "replyto": "B1gjLKW0FB", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you very much for your review and helpful comments. We address your specific questions and comments below:\n\n1. This paper presents a fairly incremental advance over what has been done (e.g. HoloGAN, StyleGAN, AC-GAN). The impact of the unsupervised work (e.g. HoloGAN or StyleGAN) is much higher since they are generally applicable to real data without labels.\n\n- We agree that the *unsupervised* disentanglement learning on real data, such as HoloGAN, is a very interesting direction. Mostly, the model inductive bias introduced in these unsupervised methods makes them 1) only applicable to a specific domain, and/or 2) difficult to scale up to high-resolution images. \n- On the contrary, the semi-supervised method can apply to a *generic* disentanglement learning, and it could also be of high practical impact if we only need to use a very limited portion of labelled data. In such sense, our work is complementary to these previous unsupervised methods.\n\n2. The claim that only 5% of supervised labels is required may not carry over to larger-scaled datasets with larger scene variability.\n\n- This is a good point! We share the same concern that with more scene variability, more supervision may be required to achieve the same disentanglement quality. This motivates the creation of two new disentanglement datasets, which are of higher variability and resolution than the existing disentanglement datasets as listed in Table 1 of the revised paper. We think our datasets and results can serve as a good starting point in the direction of scaling up the *generic* disentangling learning to large-scale real data. Based on our work, there is more future work to be done to keep increasing the scale and variability of the disentanglement datasets and models. \n\n3. I don't see any baseline whatsoever being compared with the proposed methods here. Some suggestions: 1) Show how your pre-trained GANs can be fine-tuned or transferred to the real data where we don't have labels. 2) You could also plug in your pre-trained GANs to a separate synthetic-to-real image translation model to show that we could indeed learn to control these factors of variations of the real images. 3) Compare with HoloGAN and StyleGAN.\n\n- These are great suggestions, but the use of transfer learning or a separate synthetic-to-real image translation model is out of the scope of this paper. This is because doing so will introduce another research topic about the cross-domain disentanglement learning, but our main focus is on the *generic* disentanglement learning with deep generative models. \n- Regarding comparison with HoloGAN, we do not think it is a fair comparison: HoloGAN is restricted to only disentangling 3D representations for relatively low-resolution images, however, our model is a generic semi-supervised disentanglement learning model especially designed for controlling high-resolution images. \n- Regarding comparison with StyleGAN, we have already use InfoGAN modified StyleGAN (when \\alpha=0 in Eq (2)) as a stronger unsupervised baseline than the original StyleGAN.\n- Regarding the evaluation on real images, we have run our model on CelebA of resolution 256x256 as Reviewer 3 suggested and showed the good latent traversal results in Section 4.2 of the revised paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyezSCNYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1102/Authors|ICLR.cc/2020/Conference/Paper1102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161226, "tmdate": 1576860558189, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1102/Authors", "ICLR.cc/2020/Conference/Paper1102/Reviewers", "ICLR.cc/2020/Conference/Paper1102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Official_Comment"}}}, {"id": "B1ldFEHFdH", "original": null, "number": 1, "cdate": 1570489472253, "ddate": null, "tcdate": 1570489472253, "tmdate": 1572972512660, "tddate": null, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nThe paper is reasonably clear (though see some of my detailed comments on writing below).\nThe idea seems well-motivated and somewhat new though not revolutionary,\nand the new data sets are nice (though see comments below about how I'm not qualified to evaluate them),\nbut I don't understand why the proposed algorithm wasn't evaluated on existing data sets as well,\nand I don't understand why it wasn't compared against other algorithms that purport to do the same thing.\nBarring certain exceptions (again see detailed comments), it seems like those two things are things\nwe ask of essentially all machine learning paper submissions, and I don't see why this paper is different?\nI am open to being persuaded, but as of now I can't recommend acceptance.\n\nDetailed Comments:\n\n> More importantly, only using 5% of the labelled data significantly improves the disentanglement quality.\nHard to parse.\nDoes this mean using only 5% of data is better for disentangling than using 100%?\n\n\n>  Generative adversarial networks (GANs) (Goodfellow et al., 2014) have achieved great success at generating realistic images, such as StyleGAN\nStrange sentence - styleGAN is not itself a realistic image.\n\n\n>  the controllable generation of high-resolution images is possible\nNit: i would cut the 'the'\n\n\n> which characterizes how significant that the model can change a factor.\nWhat does this mean?\n\n\n> present a state-ofthe-art challenge\nWhat does it really mean for a challenge to be state of the art?\n\n> that enables conditional generation of high-fidelity images\nPretty minor nit, but I don't think it makes sense to refer to samples themselves as high fidelity.\nHigh fidelity to what?\nIt makes a little more sense to refer to a trained generator as having high fidelity to the training data set,\nbut TBH I still don't even like the phrase in that context. \n\nFig 1 is good. \nI felt like I could understand what's going on mostly from looking at the figure, which is nice.\n\nThe section surrounding eqs 1 and 2 is a little hard to read for me. \nLots of single letter variable names and it's hard to keep them all in my head when I read the equation.\nI'd replace e.g. c_r with \\text{code}_r and so forth.\n\n> Note that AC-StyleGAN reduces to an InfoGAN variant in the special case\nThis is helpful.\n\n> AC-StyelGAN\n\n>  FCStyleAGN\n\n> by symmetry\nI don't follow this part.\n\n\n> while its high-resolution blocks accounts for fine styles\nI feel like it's worth making a distinction between high frequency bits of an image and non-essential (or nuisance variables or whatever) bits.\nThey often are the same, but not always (it really just depends how close up your photo is, right?) but this technique is really separating high-freq from low-freq, IIUC.\n\n>  handling complex high-fidelity images\nAgain, I just don't feel like this phrase makes any sense.\n\nRe eq 3:\nI guess you can call this the 'interpolation variance'\nif you want, but it's not really a new thing.\nYou're just getting discretized measurements of the Jacobian of the mapping from code to predicted code, and I don't really think in a way that fits with the\nintuition you describe.\nThere are a few things I can think of that are weird about this measurement, but here's just one:\nI might have a dimension of the code that causes one really peaky change at one point, and then the change quickly reverts.\nIf i measure the variance of the output, I won't see much (in fact, depending on how you discretize, maybe I'll miss it altogether),\neven though the fact that the change is peaky doesn't say anything about the 'importance' of the change.\nI think you want something more like a line integral.\nIt's possible I'm misunderstanding something about the description of this, however.\n\nRe: the experiments:\nI like the new data sets, although I'm not familiar enough w/ the robotics literature to know whether they add something really new.\nWhat I don't understand is why there's not really any attempt to do either of:\na) compare the models from the paper to existing models \nb) evaluate the models from the paper on existing data-sets?\n\nI understand that sometimes one comes up with a thing that does something totally new,\nand then reviewers will (maybe unjustifiably) still insist that the some kind of comparison be made even though it doesn't make sense,\nbut that doesn't seem to be what has happened here?\nThis paper proposes a new algorithm for synthesizing images and controlling various attributes of the images, but this has certainly been done in the past,\nso why can't your technique be compared to those prior techniques?\n\nSimilarly, there are lots of data sets that people are already familiar with that you could have evaluated these models on, right?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1102/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1102/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+"}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+"}}, "expdate": 1576031400000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1102/Reviewers"], "noninvitees": [], "tcdate": 1570237742348, "tmdate": 1576031820964, "super": "ICLR.cc/2020/Conference/-/Official_Review", "final": [], "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Official_Review"}}}, {"id": "B1gjLKW0FB", "original": null, "number": 2, "cdate": 1571850579247, "ddate": null, "tcdate": 1571850579247, "tmdate": 1572972512627, "tddate": null, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "=== A. Summary ===\n\nThis paper proposes to train a new conditional GAN model that allows for controllable image generation by changing the input factors of variations (e.g. object color).\nThe supervised labels for the controllable attributes are obtained from a 3D renderer.\nThat is, the work combines the recent StyleGAN (that learns to generate images with disentangle latent vectors in an unsupervised manner) with AC-GAN (a clas-conditional GAN but here class information is replaced by the attribute information that we want to control).\nThe resultant AC-StyleGAN has essentially two latent vectors, one trained unsupervised and one trained with supervised labels.\n\nThe proposed GANs were thoroughly tested with different factors of variations (lighting, camera, objects) and on two different datasets self-contructed via 3D renderer.\nThe work is a solid demonstration that GANs can be used to synthesize images with fine-grained and coarse controllability if we have supervision signals!\nThe authors also released the anonymous code (which is a plus!).\n\n\n=== B. Decision ===\n\nWeak Reject.\n\nI voted for Weak Reject because this paper presents a fairly incremental advance over what has been done (e.g. HoloGAN, StyleGAN, AC-GAN).\nThe impact of the unsupervised work (e.g. HoloGAN or StyleGAN) is much higher since they are generally applicable to real data without labels.\nHere, although reasonable, the demonstration was done on a relatively small-scale 3D synthetic data (where there is only one scene and one object being manipulated).\nTherefore, the claim that only 5% of supervised labels is required may not carry over to larger-scaled datasets with larger scene variability.\nPlus, I don't see any baseline whatsoever being compared with the proposed methods here. \n\n\n=== C. Suggestions for Improvement ===\n\nI have no problems with the novelty or idea of the work.\nFor me, the key problem with this work is the low impact or significance.\n\nSome suggestions for showing the impact of this work:\n- Show how your pre-trained GANs can be fine-tuned or transferred to the real data where we don't have labels.\n- You could also plug in your pre-trained GANs to a separate synthetic-to-real image translation model to show that we could indeed learn to control these factors of variations of the real images. Hopefully, would the above setups yield better results than HoloGAN or StyleGAN?\n- Clarify the main focus points of this paper and try to substantiate the result. The author wrote \"Our work extends the above works by scaling up the disentanglement learning to high- resolution images, and emphasizing the importance of supervision in controllable generation.\" <---- but (1) high-res images here are synthetic and limited in scene variability; (2) the second part is expected given previous work."}, "signatures": ["ICLR.cc/2020/Conference/Paper1102/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1102/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+"}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+"}}, "expdate": 1576031400000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1102/Reviewers"], "noninvitees": [], "tcdate": 1570237742348, "tmdate": 1576031820964, "super": "ICLR.cc/2020/Conference/-/Official_Review", "final": [], "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Official_Review"}}}, {"id": "HJxsmxLE9S", "original": null, "number": 3, "cdate": 1572261923201, "ddate": null, "tcdate": 1572261923201, "tmdate": 1572972512583, "tddate": null, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "invitation": "ICLR.cc/2020/Conference/Paper1102/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Disentangled GANs for controllable generation of High-Resolution Images introduces two new high resolution synthetic scene datasets for studying disentanglement in generative models and benchmarks two Style-GAN based architectures for controllable generation on these datasets. The datasets, though still synthetic, provide a significant quality boost over some of the simpler toy datasets previously studied in the disentanglement literature. A variety of experiments are conducted looking at 3 metrics (FID, MIG, and latent reconstruction) as well as qualitative analysis of samples. The paper studies how the performance of these architectures varies with hyperparameter and design decisions. The authors demonstrate that AC-StyleGAN performs well in fully supervised settings achieving the desired conditional generation, but, when controlling only a subset of factors, does not correctly disentangle. To address this, the author\u2019s other architecture modification, FC-StyleGAN an image to image model is demonstrated to improve performance in this setting. The paper provides the reader with a useful overview of the behavior of the two models on these two datasets.\n\nMy rating is weak reject. While the paper has a variety of contributions (most notably introducing two new datasets and modifications of StyleGAN) and interesting results (such as relatively high performance with only 5% labeled data and the disentanglement issues when controlling a subset of factors), the core contributions of new datasets and architectures are not validated or analyzed rigorously enough.\n\n1) No prior work is used as baselines in order to compare / validate the newly introduced architectures. The authors dismiss prior work in the introduction but do not provide any direct evidence that prior work is unable to handle the datasets introduced in the paper. In order to consider acceptance based on the value of the architectures proposed in the paper, there should be some direct evidence that the proposed Style-GAN modifications AC/FC, outperform prior architectures. One particular choice which is unclear to the reviewer is why AC GAN was chosen over the Projection Discriminator of Miyato and Koyama 2018 which demonstrated significantly better results than AC GAN and was adopted by major followup work such as BigGAN.\n\n2) Given the core contributions of new architectures for disentanglement, the lack of any results on real, non synthetic, datasets in order to validate these (for instance, CelebA) which significant prior work on disentanglement has been evaluated on is an unfortunate omission. It is unclear how the introduced architectures handle complex natural image distributions with the much more diverse sources of natural variation they contain.\n\n3) The authors claim their method is effective in the semi-supervised setting and demonstrate this by showing relatively strong performance in a low data regime. This demonstrates the importance of the third term in the loss function, but as other work for semi-supervised learning has shown (Oliver et al 2018) care must be take to properly attribute the performance of a semi-supervised algorithm to the actual semi-supervised components and purely supervised baselines are often quite competitive. The addition of an ablation / analysis demonstrating the contribution of the second term in settings where labels are also available would strengthen the author's claims and fully demonstrate the model is an effective semi-supervised learner.\n\nAdditional comments:\n\nThe datasets introduced in the paper do seem like potentially valuable contributions to the community - more discussion on the motivations behind their creation, the differences with prior work, the open difficulties / challenges, as well as the recommended evaluation protocols could add to their value.\n\nThe authors could more clearly motivate the work / applications the paper is interested in and how each contribution / experiment fits in with this. Without a clear sense of the authors mission / goals with the work, it feels a bit difficult to interpret the results. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1102/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1102/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled GANs for Controllable Generation of High-Resolution Images", "authors": ["Weili Nie", "Tero Karras", "Animesh Garg", "Shoubhik Debhath", "Anjul Patney", "Ankit B. Patel", "Anima Anandkumar"], "authorids": ["wn8@rice.edu", "tkarras@nvidia.com", "garg@cs.toronto.edu", "shoubhikdn@gmail.com", "anjul.patney@gmail.com", "abp4@rice.edu", "animakumar@gmail.com"], "keywords": ["Disentangled GANs", "controllable generation", "high-resolution image synthesis", "semantic manipulation", "fine-grained factors"], "TL;DR": "We propose new GAN architectures that enable disentangled and controllable high-resolution image generation as well as new datasets that will serve as benchmarks for the research community.", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "pdf": "/pdf/33496382d110e6f98705351587547d9698259356.pdf", "code": "https://github.com/AnonymousDisentangledGans/Disentangled_GANs", "paperhash": "nie|disentangled_gans_for_controllable_generation_of_highresolution_images", "original_pdf": "/attachment/25aa87207d24685db8ff51aac42f4b6c3c706b8a.pdf", "_bibtex": "@misc{\nnie2020disentangled,\ntitle={Disentangled {\\{}GAN{\\}}s for Controllable Generation of High-Resolution Images},\nauthor={Weili Nie and Tero Karras and Animesh Garg and Shoubhik Debhath and Anjul Patney and Ankit B. Patel and Anima Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=SyezSCNYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyezSCNYPB", "replyto": "SyezSCNYPB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+"}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1102/AnonReviewer[0-9]+"}}, "expdate": 1576031400000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1102/Reviewers"], "noninvitees": [], "tcdate": 1570237742348, "tmdate": 1576031820964, "super": "ICLR.cc/2020/Conference/-/Official_Review", "final": [], "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1102/-/Official_Review"}}}], "count": 10}