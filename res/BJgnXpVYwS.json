{"notes": [{"id": "BJgnXpVYwS", "original": "HJlr40xDDH", "number": 464, "cdate": 1569439012006, "ddate": null, "tcdate": 1569439012006, "tmdate": 1583912023652, "tddate": null, "forum": "BJgnXpVYwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "KA65jj-wX", "original": null, "number": 1, "cdate": 1576798697194, "ddate": null, "tcdate": 1576798697194, "tmdate": 1576800938530, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "Gradient clipping is increasingly popular and it's nice to see a paper theoretically exploring its nice performance. All reviewers appreciated the work and the results.\n\nPlease make sure to incorporate all of their comments for the final version.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712196, "tmdate": 1576800261538, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper464/-/Decision"}}}, {"id": "ryx_yx-HtS", "original": null, "number": 2, "cdate": 1571258335597, "ddate": null, "tcdate": 1571258335597, "tmdate": 1574374589893, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "In this paper, the authors relax the generally used Lipschitz smoothness condition in optimization, to a more general smoothness condition that may depend on norm of the gradient. The authors proved that, with this relaxed condition, under such cases, both GD and clipped GD can converge within O(1/\\epsilon^2) time, but when their exists x where the gradient norm can be very large, the clipped GD will converge provably faster than GD with a proved GD convergence rate lower bound. The authors also generalize their results to SGD. Experiments show that in deep neural network local gradient Lipschitz constant is scale with gradient norm, and use clipped gradient can accelerate convergence while keep good generalization performance as expected, which is well observed by other researchers.\n\nDetailed comments:\n1. The f^* In theorem 3 is not defined. I think it\u2019s not the f^* in Assumption 1? Should be more clear. Also, in non-convex optimization, can the deterministic GD find the global optimum? If f^* is the stationary point the algorithm find, then the Assumption 1 is a little weird. The authors should better clarify this.\n2. Feel the claim of the Assumption can be relaxed to only hold on \\mathcal{S} is a little confusing. Which Assumption can be relaxed? It seems that in the proof of Lemma 9, we need the Assumption 3 holds for the set \\|x^+-x\\| \\leq min{1/L_1, 1}. I think maybe the other Assumption will also be needed globally, unless the authors can prove that the optimization is only on a compact set in S.\n3. Maybe a discussion connected the existing Lipschitz constant based results to the new results in this paper can make the readers more aware of the contribution in this paper. For example, the Lipschitz constant of neural network can be of order O(L_1 M) due to the definition, so if L_1 M is large, for clipped GD we can still have expected decreasing of O(1/L_0) each turn while vanilla GD with the best O(1/L) step size can still only have O(1/L) where L = L_0 + L_1 M due to the traditional results based on Lipschitz smoothness.\n\nI don\u2019t totally go through the whole proof, but I think the results are reasonable and the most of the techniques are standard to the whole community. So there may be no fatal error that violates the conclusion. I find the whole paper interesting and match the intuition from the practice. However, I think it can be polished to make the whole idea more clear and more easy to accept by the potential theorists and practitioner audience.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575659976310, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper464/Reviewers"], "noninvitees": [], "tcdate": 1570237751746, "tmdate": 1575659976325, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Review"}}}, {"id": "rylSdgmeqB", "original": null, "number": 3, "cdate": 1571987565506, "ddate": null, "tcdate": 1571987565506, "tmdate": 1574373985970, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper applies new assumption on smoothness that assume the norm of Hessian is bounded by a scalar plus norm of gradient. The traditional smoothness is only bounded with a scalar, the proposed assumption is more relaxed because now the norm of Hessian can grow with the norm of gradient. Under this assumption, the authors show clipped gradient converges faster than gradient for general nonconvex problems. The authors provide insights on why the proposed assumption is good for describing neural networks, and empirically verify the assumption with ResNet on CIFAR and LSTM on PTB. \n\nI like the paper in general. It is well written and easy to follow. The contributions are clearly described and the techniques seem to be solid. \n\nI want a little bit more discussion to help me better understand the paper. \n\n1) Intuitively, try best in plain English, why does clipped gradient convergence does not have dependency on L_1 M? Could the authors provide more discussion on the learning rate (hyperparameters) used for clipped gradient and gradient?\n\n2) The convergence rates under the proposed assumption are slower than traditional smoothness assumption. Could you verify the slow convergence rate under proposed assumption aligns better with practical training? Could you provide a toy example, for example x^3, to show the advantage of the proposed assumption and the convergence under the assumption? What is the gap between clipped gradient and gradient, for experiments in figure 4, and possible toy problem? Could the authors elaborate the difference of clipped gradient and gradient with the details of theory, instead of simply claiming clipped gradient is faster?\n\nTypo, page 5, (0, L) -> (L, 0)\n\n\n============ after rebuttal ===================\nI am happy to see the paper in the conference. The paper is intuitive and well written. My remaining comments are still on the slower convergence rate comparing to previous assumption. Even before, researchers suggest overparameterized network can converge faster than expected. This paper suggests a generally slower convergence rate if the proposed assumption does fit practice better.   \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575659976310, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper464/Reviewers"], "noninvitees": [], "tcdate": 1570237751746, "tmdate": 1575659976325, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Review"}}}, {"id": "SJgnrqooiS", "original": null, "number": 8, "cdate": 1573792323530, "ddate": null, "tcdate": 1573792323530, "tmdate": 1573838083840, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "BylX_gm8iS", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment", "content": {"title": "Updated again to quantify the gap between theorem and experiment results (Appendix J)", "comment": "We updated the paper (see Appendix J) based on reviewer 1's comments. In particular, we show that the gap predicted by theory is on the same scale as the gap we observe in experiments (including PTB the Language modeling experiment). We thank the reviewer 1 for this interesting suggestion and enjoyed doing the experiments.\n\nWe also added some clarifications based on Reviewer 2's helpful suggestions."}, "signatures": ["ICLR.cc/2020/Conference/Paper464/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgnXpVYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper464/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper464/Authors|ICLR.cc/2020/Conference/Paper464/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171105, "tmdate": 1576860543099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment"}}}, {"id": "HyeE2sosjr", "original": null, "number": 9, "cdate": 1573792683754, "ddate": null, "tcdate": 1573792683754, "tmdate": 1573792683754, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "SylJvkx5oB", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment", "content": {"title": "New experiment and analysis updated in Appendix J", "comment": "Thank you very much for the clarification! \n\nWe did an analysis to quantify the gap between the theory and the experiments for both synthetic data and PTB language modeling. The details are in Appendix J of the updated paper. We found that the theory predictions,  though not precise, are on the same scale as the experiments.\n\nWe thank the reviewer for raising this very interesting question."}, "signatures": ["ICLR.cc/2020/Conference/Paper464/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgnXpVYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper464/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper464/Authors|ICLR.cc/2020/Conference/Paper464/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171105, "tmdate": 1576860543099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment"}}}, {"id": "rkgxn0mosS", "original": null, "number": 7, "cdate": 1573760680470, "ddate": null, "tcdate": 1573760680470, "tmdate": 1573760711341, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "Skei0fQ8ir", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment", "content": {"title": "Thanks for the clarification.", "comment": "I hope the authors can also make these clarifications in the paper so that the potential audience need not to suffer from the questions I first have. Thanks!"}, "signatures": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgnXpVYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper464/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper464/Authors|ICLR.cc/2020/Conference/Paper464/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171105, "tmdate": 1576860543099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment"}}}, {"id": "SylJvkx5oB", "original": null, "number": 6, "cdate": 1573678935110, "ddate": null, "tcdate": 1573678935110, "tmdate": 1573678935110, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "HJgzsbXUsB", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment", "content": {"title": "clarification on \u201cthe gap between clipped gradient and gradient \u201d", "comment": "My question is whether the empirical difference of clipped gradient and gradient aligns with the theoretical difference presented in a probably more quantitative way. There are several curves in figure 4, I am trying to get more info.  Sorry if it makes any confusion."}, "signatures": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgnXpVYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper464/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper464/Authors|ICLR.cc/2020/Conference/Paper464/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171105, "tmdate": 1576860543099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment"}}}, {"id": "rJeemQXLjS", "original": null, "number": 5, "cdate": 1573430040333, "ddate": null, "tcdate": 1573430040333, "tmdate": 1573483168432, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "rkxmDNtEYH", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment", "content": {"title": "Thank you for the comments.", "comment": "We thank the reviewer for the positive comments and helpful notes. The paper is updated as the reviewer suggested. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper464/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgnXpVYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper464/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper464/Authors|ICLR.cc/2020/Conference/Paper464/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171105, "tmdate": 1576860543099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment"}}}, {"id": "HJgzsbXUsB", "original": null, "number": 3, "cdate": 1573429657630, "ddate": null, "tcdate": 1573429657630, "tmdate": 1573430106961, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "rylSdgmeqB", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment", "content": {"title": "Intuitions; A synthetic experiment added in Appendix I; Some clarifications needed", "comment": "We would like to thank the reviewer for the feedback and comments. We address the reviewer\u2019s questions as follows:\n\n(1) Intuition for the faster convergence of clipped GD: On a high level, gradient descent with a fixed step size has an iteration dependency on (L1M/\\epsilon) because the algorithm needs to use a very small step size in the steep regions, which will slow down convergence in the flat regions, which determines the $\\epsilon$ dependent term. This problem is circumvented by clipping because adaptivity allows the gradient descent to automatically use a small step size in steep regions with large gradients. When the iterates enter a flat region, larger learning rates are adopted if the gradient is small. As a result, the convergence rates in the flat region and in the steep region are decoupled as a sum instead of a multiplication.\n\n(2) Hyperparameters: Gradient clipping indeed has one more parameter (clipping threshold) to tune than GD. This parameter determines when the clipping effect should kick in. In fact, within a compact set, for each L1, there is a corresponding L0 to make the assumption hold. In practice, we usually pick one or two values for clipping threshold, and then scan the step size.\n\n(3) Slow convergence due to relaxed assumption: We thank the reviewer for the suggestion. A synthetic experiment is now included in Appendix I. \n\nSince we observe empirically that the local smoothness of a neural network is positively correlated with gradient norm, therefore, our theorem 3 and 6 jusitify the gap by proving that clipped GD converges faster. We do not fully understand what the reviewer mean by the \u201cthe gap between clipped gradient and gradient \u201d. Is the reviewer asking what is the proportion of gradients that are clipped? We are willing to supplement with experiments if the reviewer could elaborate more on this question.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper464/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgnXpVYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper464/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper464/Authors|ICLR.cc/2020/Conference/Paper464/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171105, "tmdate": 1576860543099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment"}}}, {"id": "Skei0fQ8ir", "original": null, "number": 4, "cdate": 1573429970770, "ddate": null, "tcdate": 1573429970770, "tmdate": 1573429970770, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "ryx_yx-HtS", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment", "content": {"title": "Clarification on theorems.", "comment": "We would like to thank the reviewer for the positive feedback and comments. As the reviewer pointed out, our goal is to bridge the gap of theoretical result and practice observations. Hence, the reviewer's suggestion on an additional discussion is much appreciated.\n\n(1) f^* is the global minimum of the nonconvex function. For example, in most neural network training, f^*  = 0 as the network usually is allowed to overfit. However, though f^* is the global minimum, we do not provide bound for convergence of GD to global minimum. Instead, the bound is on gradient norm, which suggests that GD converges to stationary points. In other words, we provide a guarantee for convergence to stationary points instead of global min, though the number of iterations depends on the value of global min. This dependence is required as shown by [Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points].\n\n(2) We apologize for the confusion. In the deterministic case, all the assumptions only need to hold in \\mathcal{S}. In the proof, we show that the function value is nonincreasing in each iteration. Therefore, even though we are solving an unconstrained problem, the iterates never escape the set \\mathcal{S}. \n\n(3) We appreciate the reviewer\u2019s suggestion and will edit accordingly when we have an extra page in the camera-ready version.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper464/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgnXpVYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper464/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper464/Authors|ICLR.cc/2020/Conference/Paper464/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171105, "tmdate": 1576860543099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment"}}}, {"id": "BylX_gm8iS", "original": null, "number": 2, "cdate": 1573429354920, "ddate": null, "tcdate": 1573429354920, "tmdate": 1573429354920, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment", "content": {"title": "Paper updated with an additional synthetic experiments.", "comment": "We thank the reviewers for the positive feedbacks and helpful comments. We believe that aligning the assumptions of theoretical analysis to explain/predict practice is an exciting direction for studying optimization.\n\nBased on the comments, we included an additional synthetic experiment for optimizing polynomial in Appendix I. We also corrected some typos according to Reviewer 3's feedback."}, "signatures": ["ICLR.cc/2020/Conference/Paper464/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgnXpVYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper464/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper464/Authors|ICLR.cc/2020/Conference/Paper464/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171105, "tmdate": 1576860543099, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper464/Authors", "ICLR.cc/2020/Conference/Paper464/Reviewers", "ICLR.cc/2020/Conference/Paper464/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Comment"}}}, {"id": "rkxmDNtEYH", "original": null, "number": 1, "cdate": 1571226714758, "ddate": null, "tcdate": 1571226714758, "tmdate": 1572972592141, "tddate": null, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "invitation": "ICLR.cc/2020/Conference/Paper464/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors provide the definition of a (L_0, L_1)-smooth function.\nThey motivate this class of function based on theoretical arguments and\nempirical observations in the context of neural network training loss.\nThe authors go on to show that for (L_0, L_1)-smooth loss constant\nstep-size gradient descent can perform arbitrarily bad with poor initialization.\nSpecifically, the largest constant step size that can be used without divergence\ndepends on the upper bound of the gradient norm.\nIf the function is steep (high gradient norm) at some point during the\noptimization process the steps need to become arbitrarily small\n(especially relevant in nonconvex loss, where gradient norm does not always decrease).\nThe small constant step size slows down convergence everywhere else.\nThe described phenomenon is an argument for step sizes adaptivity in general\nClipping the gradient based on the known (L_0, L_1)-smoothness is a form of\nadaptivity that makes steps small enough to avoid divergence only where the\nfunction is steep and can thus be more efficient.\n\nOverall, the authors give a theoretical justification for the use of clipped\ngradient descent in the context of training neural networks.\nThe analysis of clipped gradient descent is performed rigorously (proofs are\nprovided in the appendix) and extended to the stochastic gradient descent setting.\nThe motivation for using (L_0, L_1)-smoothness and gradient clipping is also\nshown to be relevant in practical experiments with language and image\nclassification models.\n\nI recommend to accept the paper.\n\nThe paper is a clearly stated contribution to optimization theory as motivated\nby machine learning applications.\nThe authors give a clean theoretical analysis of a previously empirically\nobserved phenomenon and an algorithm that is already used in practice.\n\nNotes:\n- Page 1, first paragraph:\nIf f(x) = E_xi [f(x, xi)] then f(x) the expectation is generally not stochastic,\nlike you write before. It is clear what you mean but Maybe use a different symbol.\n\n- Page 2, ...we observe the function smoothness has a strong correlation with gradient norm 2:\nPerhaps write something like \"(see Figure 2)\" instead of just \"2\" here.\n\n- Page 2, after definition of Lipschitz continuous:\nNitpick: Formal definition based on Hessian norm upper bound should also have, for all x \\in \\mathbb R^d\n\n- General: Some equations that are not referenced seem to be numbered, some are not.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper464/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jzhzhang@mit.edu", "tianxing@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "authors": ["Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie"], "pdf": "/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf", "TL;DR": "Gradient clipping provably accelerates gradient descent for non-smooth non-convex functions.", "abstract": "We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.", "keywords": ["Adaptive methods", "optimization", "deep learning"], "paperhash": "zhang|why_gradient_clipping_accelerates_training_a_theoretical_justification_for_adaptivity", "code": "https://github.com/JingzhaoZhang/why-clipping-accelerates", "_bibtex": "@inproceedings{\nZhang2020Why,\ntitle={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},\nauthor={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgnXpVYwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1216f41ee7983c7595221efeb1a2288a3d6ec2f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgnXpVYwS", "replyto": "BJgnXpVYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper464/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575659976310, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper464/Reviewers"], "noninvitees": [], "tcdate": 1570237751746, "tmdate": 1575659976325, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper464/-/Official_Review"}}}], "count": 13}