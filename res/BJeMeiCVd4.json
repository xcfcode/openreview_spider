{"notes": [{"id": "BJeMeiCVd4", "original": "BygVXqqVOV", "number": 53, "cdate": 1553423082380, "ddate": null, "tcdate": 1553423082380, "tmdate": 1562082113925, "tddate": null, "forum": "BJeMeiCVd4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables", "authors": ["Kate Rakelly*", "Aurick Zhou*", "Deirdre Quillen", "Chelsea Finn", "Sergey Levine"], "authorids": ["rakelly@eecs.berkeley.edu", "azhou42@berkeley.edu", "deirdrequillen@berkeley.edu", "cfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["meta-reinforcement learning"], "TL;DR": "Sample efficient meta-RL by combining variational inference of probabilistic task variables with off-policy RL ", "abstract": "Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.", "pdf": "/pdf/0fefa8e505d91a7e40b27c6c4359a40e2ba7e131.pdf", "paperhash": "rakelly|efficient_offpolicy_metareinforcement_learning_via_probabilistic_context_variables"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "B1x-ykU9FN", "original": null, "number": 1, "cdate": 1554829017361, "ddate": null, "tcdate": 1554829017361, "tmdate": 1555511880134, "tddate": null, "forum": "BJeMeiCVd4", "replyto": "BJeMeiCVd4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper53/Official_Review", "content": {"title": "Important Meta-RL work but unsure if it fits the theme of the workshop", "review": "Note: While I really like the paper, I am not sure if it aligns with the workshop theme. The paper itself mentions \"Under review at the Workshop on \u201cStructure & Priors in Reinforcement Learning\u201d at ICLR 2019\" which I feel might be a better venue for this paper. I will let the organizers take a call and provide an objective assessment of the paper below without the workshop theme in consideration. \n\nThis paper proposes using off-policy RL during the meta-training time to greatly improve sample efficiency of Meta-RL methods. \n\nCurrent meta-RL methods (MAML/ProMP) are terribly sample inefficient since the adaptation phase during training is trained using vanilla policy gradients with a linear feature baseline (which is known to be very sample inefficient). A natural next step would have been to use a learned state dependent baseline (analogous to actor-critic methods) by meta-learning a critic. The authors take a step further and use an off-policy RL method (SAC) which has shown to be better performing than on-policy actor critic methods. The policy is also conditioned on a task embedding, enabling better adaptation. \n\nIf the authors could clarify the following points, I think it would make the paper much better:\n* It's not clear if this method could be directly extendible to partially-observed settings since the factorization of the inference network would no longer hold. If being restricted to fully observed domains is a limitation, it should be noted clearly in the paper.\n* Ablations dissecting which of the two main components of the paper (inference network / using SAC) helped the performance most would be helpful, right now the relative importance of each component is unclear. Relatedly, the authors could also include a baseline a MAML/ProMP baseline where the critic is meta-learned. \n* The experimental details in this version of the paper are severely limited. I realize the page length constraints but the authors could put details in the appendix. ", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper53/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper53/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables", "authors": ["Kate Rakelly*", "Aurick Zhou*", "Deirdre Quillen", "Chelsea Finn", "Sergey Levine"], "authorids": ["rakelly@eecs.berkeley.edu", "azhou42@berkeley.edu", "deirdrequillen@berkeley.edu", "cfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["meta-reinforcement learning"], "TL;DR": "Sample efficient meta-RL by combining variational inference of probabilistic task variables with off-policy RL ", "abstract": "Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.", "pdf": "/pdf/0fefa8e505d91a7e40b27c6c4359a40e2ba7e131.pdf", "paperhash": "rakelly|efficient_offpolicy_metareinforcement_learning_via_probabilistic_context_variables"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper53/Official_Review", "cdate": 1553713412775, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BJeMeiCVd4", "replyto": "BJeMeiCVd4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper53/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper53/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713412775, "tmdate": 1555511822943, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper53/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "ByggwiEAYN", "original": null, "number": 2, "cdate": 1555086167790, "ddate": null, "tcdate": 1555086167790, "tmdate": 1555511875623, "tddate": null, "forum": "BJeMeiCVd4", "replyto": "BJeMeiCVd4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper53/Official_Review", "content": {"title": "Interesting work, re-wraped from the original version a bit too fast", "review": "This paper tackles the problem of meta-learning, more precisely the possibility of considering off-policies. For this, a first network estimates, from the history on the current task (the \"context\"), which kind of task it is (represented by a variable 'z'), and a second network (in a standard actor-critic approach) learns the policy for the current task, thanks to a conditioning on this 'z' (which allows to perform transfer learning between similar tasks within the same network).\n\nExperiments are performed on a standard problem (MuJoCo) and significant gains are observed (from 20 to 100 fewer examples needed for training).\nWhich justifies the submission to this workshop on Limited Labeled Data, even though the primary objective of the article is not really to deal with a given dataset of limited data, but rather to target sampling data efficiency, as the article is about RL tasks.\n\nOverall, the paper is very good, with very interesting ideas; however it is clearly a very hastily made short version of a longer paper: abstract and conclusion were removed (are fully missing), text density maybe too high for a non-specialist.\n\nTherefore I hesitate between \"very good\" and \"borderline\".\n", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper53/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper53/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables", "authors": ["Kate Rakelly*", "Aurick Zhou*", "Deirdre Quillen", "Chelsea Finn", "Sergey Levine"], "authorids": ["rakelly@eecs.berkeley.edu", "azhou42@berkeley.edu", "deirdrequillen@berkeley.edu", "cfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["meta-reinforcement learning"], "TL;DR": "Sample efficient meta-RL by combining variational inference of probabilistic task variables with off-policy RL ", "abstract": "Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.", "pdf": "/pdf/0fefa8e505d91a7e40b27c6c4359a40e2ba7e131.pdf", "paperhash": "rakelly|efficient_offpolicy_metareinforcement_learning_via_probabilistic_context_variables"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper53/Official_Review", "cdate": 1553713412775, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BJeMeiCVd4", "replyto": "BJeMeiCVd4", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper53/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper53/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713412775, "tmdate": 1555511822943, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper53/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "BJgazJpzcE", "original": null, "number": 1, "cdate": 1555382036971, "ddate": null, "tcdate": 1555382036971, "tmdate": 1555510975746, "tddate": null, "forum": "BJeMeiCVd4", "replyto": "BJeMeiCVd4", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper53/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables", "authors": ["Kate Rakelly*", "Aurick Zhou*", "Deirdre Quillen", "Chelsea Finn", "Sergey Levine"], "authorids": ["rakelly@eecs.berkeley.edu", "azhou42@berkeley.edu", "deirdrequillen@berkeley.edu", "cfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "keywords": ["meta-reinforcement learning"], "TL;DR": "Sample efficient meta-RL by combining variational inference of probabilistic task variables with off-policy RL ", "abstract": "Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.", "pdf": "/pdf/0fefa8e505d91a7e40b27c6c4359a40e2ba7e131.pdf", "paperhash": "rakelly|efficient_offpolicy_metareinforcement_learning_via_probabilistic_context_variables"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper53/Decision", "cdate": 1554736074247, "reply": {"forum": "BJeMeiCVd4", "replyto": "BJeMeiCVd4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736074247, "tmdate": 1555510964608, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}