{"notes": [{"tddate": null, "tmdate": 1486482158749, "tcdate": 1486482158749, "number": 11, "id": "SyweiPvOl", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "rkL_hMLdl", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Feedback on final decision", "comment": "Well, the decision comment is a review on its own! Although unfortunate to us, we accept the decision. Thank you for the well informed advices and pointers to relevant litterature, it will certainly help us to make the paper stronger and to relate it better with existing work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396526139, "tcdate": 1486396526139, "number": 1, "id": "rkL_hMLdl", "invitation": "ICLR.cc/2017/conference/-/paper341/acceptance", "forum": "rye9LT8cee", "replyto": "rye9LT8cee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper studies a sparsification method for pre-trained CNNs based on the alternating direction method of multipliers. \n The reviewers agreed that the paper is well-written and easy to follow. The authors were responsive during the rebuttal phase and addressed most of the reviewers questions. \n \n The reviewers, however, disagree with the significance of this work. Whereas R4 is happy to see an established method (ADMM) applied to a nowadays very popular architecture, R1-R3 were skeptical about the scope of the experiments and the usefulness of the sparse approximation. \n \n Pros:\n - Simple algorithmic description using well-known ADMM method. \n - Consistent performance gains in small and mid-scale object classification problems. \n \n Cons:\n - Lack of significance in light of current literature on the topic. \n - Lack of numerical experiments on large-scale classification problems and/or other tasks.\n - Lack of clarity when reporting speedup gains. \n \n Based on these assessments, the AC recommends rejection. Since this decision is not 100% aligned \n with the reviewers, let me expand on the reasons why I recommend rejection. \n \n This paper presents a sound algorithm that sparsifies the weights of a trained-CNN, and it shows that \n the resulting pruned network works well, better than the original one. A priori, this is a solid result. \n My main problem is that this contribution has to be taken in the context of the already large body of \n literature addressing this same question, starting from the seminal 'Predicting Parameters in Deep Learning', by Misha Denil et al., NIPS 2013, which is surprisingly ignored in the bibliography here; and onwards with 'Exploiting linear structure within convolutional networks for efficient evaluation', Denton et al, NIPS'14 (also ignored) and 'Speeding up convolutional neural networks with low rank expansions', Jadergerg et al., '14. These and many other works culminated in two recent papers, 'Deep Compression', by Han et al, ICLR'16 and 'Learning Structured Sparsity in Deep Neural Networks' by Wen et al, NIPS'16.\n Several reviewers pointed out that comparisons with Wen et al were needed; the authors of the present submission now do cite this work, but do not compare their results quantitatively and do not present the reader with compelling reasons to choose their acceleration instead of Wen et al.'s. Moreover, they mention in the rebuttal that the paper was published after the submission, but the work was available on the arxiv well before the ICLR deadline (august 2016). Wen's paper presents (i) source code, (ii) experiments on imagenet, (iii) 3x to 5x speedups in both CPU and GPU, and (iv) accuracy improvements on Cifar of ~1.5%. As far as I can see, the present submission presents none of these. Further compression factors were obtained in 'Deep Compression', also in large-scale models. \n \n In light of these results, the authors should present more compelling evidence (in this case, since this is an experimental paper, empirical evidence in the form of higher accuracy and/or larger speedups/gains) as to why should a practitioner use their model, rather than simply presenting the differences between the approaches."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396527094, "id": "ICLR.cc/2017/conference/-/paper341/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rye9LT8cee", "replyto": "rye9LT8cee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396527094}}}, {"tddate": null, "tmdate": 1484689363447, "tcdate": 1484689363447, "number": 2, "id": "HyiAkzn8x", "invitation": "ICLR.cc/2017/conference/-/paper341/official/comment", "forum": "rye9LT8cee", "replyto": "rJvQGDtUg", "signatures": ["ICLR.cc/2017/conference/paper341/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper341/AnonReviewer4"], "content": {"title": "Response to comments on review", "comment": "I greatly appreciate the effort the authors have spent in polishing their manuscript.  I have updated my review in response."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616849, "id": "ICLR.cc/2017/conference/-/paper341/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper341/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper341/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616849}}}, {"tddate": null, "tmdate": 1484689355635, "tcdate": 1483655902975, "number": 4, "id": "ByDyiBnSl", "invitation": "ICLR.cc/2017/conference/-/paper341/official/review", "forum": "rye9LT8cee", "replyto": "rye9LT8cee", "signatures": ["ICLR.cc/2017/conference/paper341/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper341/AnonReviewer4"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "The authors use the Alternating Direction Method of Multipliers (ADMM) algorithm for the first time on CNN models, allowing them to perform model compression without any appreciable loss on the CIFAR-10, CIFAR-100, and SVHN tasks.  The algorithmic details and the intuition behind their algorithm are generally well presented, (although there are occasional typos).\n\nPros: \n1) Put an old algorithm to good use in a new setting\n2) The algorithm has the nice property that it is partially analytically solvable (due to the separability property the authors mention).  This contributes to the efficient trainability of the model\n3) Seems to dovetail nicely with other results to encourage sparsity--that is, it can be used simultaneously--and is quite generalizable.\n\nCons:\n1) It would be nice to see a more thorough analysis of the performance gains for using this method, beyond raw data about % sparsity--some sort of comparison involving training time would be great, and is currently lacking. EDIT: Authors addressed this by addition of results in Appendix B.\n2) I would very much like to see some discussion about why the sparsity seems to *improve* the test performance, as mentioned in my previous comment.  Is this a general feature?  Is this a statistical fluke? etc.  Even if the answer is \"it is not obvious, and determining why goes outside the scope of this work\", I would like to know it!  EDIT: Authors addressed this by addition of statistical significance tests in the new Appendix A.\n3) Based on the current text, and some of the other reviewer comments, I would appreciate an expanded discussion on how this work compares with other methods in the field.  I don't think a full numerical comparison is necessary, but some additional text discussing some of the other papers mentioned in the other reviews would greatly benefit the paper.  EDIT: Authors addressed this by followup to question and additional text in the paper.\n\nAdditional comments: If my Cons are addressed, I would definitely raise my score to a 6 or even a 7.  The core of this paper is quite solid, it just needs a little bit more polishing.  \n\n\nEDIT: Score has been updated.  \n\nNote: the authors probably meant \"In order to verify\" in the first sentence of Appendix A.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483655903684, "id": "ICLR.cc/2017/conference/-/paper341/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper341/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper341/AnonReviewer3", "ICLR.cc/2017/conference/paper341/AnonReviewer1", "ICLR.cc/2017/conference/paper341/AnonReviewer2", "ICLR.cc/2017/conference/paper341/AnonReviewer4"], "reply": {"forum": "rye9LT8cee", "replyto": "rye9LT8cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483655903684}}}, {"tddate": null, "tmdate": 1484513822829, "tcdate": 1484513822829, "number": 10, "id": "rJvQGDtUg", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "ByDyiBnSl", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Answers to Reviewer 4", "comment": "Thank you very much for your comments.\n\n[More information on training time (cons 1)]: See answers to reviewer 1.\n\n[Why sparsity improves performance (cons 2)]: See the newly added Appendix A, which is reported statistical test values over 15 runs of the approach on NIN model on CIFAR-10. The statistical tests demonstrate that results are statistically significant. We hope these empirical results clarify the question. \n\n[Comparison with other methods (cons 3)]: Standard L1 regularization methods in the literature, like Structural Sparsity Learning (SSL) using group Lasso, suffer from two limitations compared to our proposed method. First, they rely on a rigid framework that avoids incorporation of non-differentiable penalty functions (e.g., not usable with L0-norm). Second, they require training the original full model for performance and sparsity with the same optimization method, while our proposed method allows to decompose the corresponding optimization problems into two sub-problems and exploit the separability of the sparsity-promoting penalty functions to find analytical solutions for one of the sub-problems, while using SGD for solving the other.\n\nThese two properties are highlighted in the statements 1, 2 at the Discussion section and the following text is also added to that section in order to clarify the distinction between our proposed method and closely related SSL approach (text beginning with \"Some methods such as SSL...\" up to the end of the Discussion section).\n\nWe hope the latest version of the paper will provide you with material that will convince you of the soundness of our proposal."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "tmdate": 1484513735177, "tcdate": 1484513735177, "number": 9, "id": "H1yAZPt8e", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "HJcYbIZHe", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Answers to Reviewer 2", "comment": "We thank the reviewer for the comments.\n\nWe have tried to evaluate the proposed sparse CNN approach, extensively, which was not an easy task. The proposed scheme has been validated using a variety of network architectures and on three well-known datasets, the CIFAR-10, CIFAR-100, and SVHN datasets, under two different sparsity penalty term L0-norm and L1-norm and a variety of values assigned to the sparsity strength mu. These configurations are coherent to what many other current work on sparsifying deep neural networks are reporting. Realistically, we think that such an extensive experimental assessment can hardly be done on much more complex datasets and models in a timely manner. We prefer to be more systematic in validating the method than trying to report results on only a handful set of configurations with larger models and datasets, where we would encounter the risk of reporting only anecdotal results.\n\nWe also added more results in terms of speedup changes as we vary the parameter mu to the results of Tables 3, 4, and 5 in Appendix B. As the reviewer noticed, in practice, when the network becomes sparser with fewer parameters, its inference time is reduced. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "tmdate": 1484513672071, "tcdate": 1484513672071, "number": 8, "id": "Bkg9bvK8x", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "HJ_Zh_mEe", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Answer to Reviewer 1", "comment": "Thank you for your review.\n\nTo answer this point, we added the following text to the revised version of the paper, starting in second paragraph of the Experimental Results section.\n\n--- Begin quoted text ---\n\nSince the regularization factor $\\mu$ is selected from gradually increasing values, for the first small values of $\\mu$ the selection of long epochs for performance-promoting step (inner loop) and fine-tuning steps is computationally prohibitive and would result in over-fitting. Instead, we start with one epoch for the first $\\mu$ and increase the number of epochs by $\\delta$ for the next $\\mu$ values up to the $\\nu$-th $\\mu$ value, after which the number of epochs is limited to $\\delta\\nu$. We found that $\\delta=1$ and $\\nu=15$ generally work well in our experiments. We already incorporated the number of training epochs at tables 3, 4, and 5 of Appendix B. If the maximum limiting number of iterations of inner loop is $\\xi$ (suggested value of $\\xi$=10), the training time of the $\\nu$-th $\\mu$ value takes a total of $\\delta\\nu\\xi+\\delta\\nu$ epochs ($\\delta\\nu\\xi$ for performance-promoting step and $\\delta\\nu$ for fine-tuning) under the worst-case assumption, where the inner loop has not converged and completes only at the $\\xi$-th iteration.\n\n--- End quoted text ---\n\nWe hope this will answer well the questions of your last paragraph.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "tmdate": 1484513572253, "tcdate": 1484513572253, "number": 7, "id": "Bk3Q-vtIg", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "rJwYfhFHe", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Answers to additional review questions from Reviewer 4", "comment": "[Elaborate more on point (3) of the conclusion]: Indeed, the ADMM method breaks the minimization of the augmented Lagrangian function into two parts, which brings the differentiability advantage to the proposed method for loss minimization, while making possible the use of L0 regularization.\n\n[Statistical significance of generalization improvements with more sparsity]: In order to demonstrate that the ADMM training performance is statistically significant, we did a t-test by repeating the experiment 15 times on CIFAR-10 using the NIN model. The results demonstrate that ADMM training achieves significant improvements from both the baseline model (t-test result with p<0.001) and standard fine tuning (t-test result with p<0.001). The t-test results are given in the new appendix A.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "tmdate": 1484513460610, "tcdate": 1484513460610, "number": 6, "id": "BJphxDtIg", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "S1Adw4m4x", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Answers to Reviewer 3", "comment": "Thank you for your careful and thoughtful review, that's really appreciated.\n\n[Comparison with group lasso work]: Compared to the closely related Sparse Structure Learning (SSL) method, which learns sparse block structures (e.g. sparse filters) and minimizes the classification error simultaneously, our proposed approach uses ADMM method to provide a separate scheme to optimize the sparse blocks and classification error. ADMM brings the differentiability and separability advantages to the proposed sparse CNN method which are the basis of the distinctive contributions of our proposed approach compared to SSL method. The algorithm has the advantage that it is partially and analytically solvable due to the separability property. This contributes to the efficient trainability of the model. Moreover, the differentiability problem of L0-norm penalty function makes it unusable in the SSL framework, while L0-norm can be incorporated as a mean of sparsity penalty terms in our proposed method.\n\n[Effectiveness of ADMM]: The ADMM method breaks the minimization of the augmented Lagrangian function into two parts which brings the differentiability and separability advantages to the proposed method (claims 1 and 2 in the Discussion section). Although during the training phase ADMM seems to be more demanding in terms of memory (2 copies of the parameters need to be stored), after the convergence only one copy of parameters with sparse structure is retrieved and used in the test phase and the final model benefits from a more compact size (smaller memory footprint).\n\n[Confusion over claimed contributions]: We agree with the reviewer's comment, implying that the statement 1, 2, and 3 may induce some confusion to the readers. Actually, compared against the standard L1 regularization, which is somehow differentiable, the major point is that due to the decoupling property of ADMM, the proposed algorithm makes it possible to use the non-differentiable L0 regularization (differentiability advantage). Furthermore, the decoupling capability of ADMM algorithm along with the separability of the penalty functions provide analytical solutions to the sparsity-promoting problems (separability advantage). In order to clarify this, we revised the statement in the discussion part to elucidate the notions of the /differentiability/ and the /separability/ of the proposed method. \n\n[Discussion on why sparsity helps improve performance]: The advantage of the complicated models is that they can capture highly non-linear relationship between features and output. The drawback of such large models is that they are prone to capture the noise that does not generalize to new datasets, leading to over-fitting and a high variance. The recently presented compression methods of dense networks without losing accuracy show significant redundancy in the trained model and inadequacy of current training methods to find the existing sparse and compact solutions with better generalization properties. We propose ADMM-based training method as a post-processing step, once a good model has been learned, with the solution gradually moving from the original dense network to the sparse structure of interest, as our emphasis on the sparsity-promoting penalty term is increased.\n\nMore specifically, since the target solution is likely to be sparse, we may think that enforcing sparsity right in the beginning, using our proposed method, will provide a way to avoid overfitting for achieving a better performance. However, increasing more the sparsity strength of the solution may lead to over-smoothing the models and drops in the performance. Therefore, in practical design of networks, we figured out that this regularization factor should be increased gradually, until the desired balance between performance and sparsity is achieved.\n\nTherefore, from our preliminary experiments, we observed that sparsifying networks gradually and after a good dense network has been learned is better at improving performance. We conjuncture that the learning method may need to have enough room to first explore and find a good position in the search space of possible models, before applying gradual sparsification over it. Otherwise, a strong sparsification penalty may break the smoothness of the search space, making it much harder to explore sparser solutions in the neighborhood of the current model. Verifying this may be the topic of a future work. \n\n[Report speedup]: We reported speedup as we vary the parameter mu to the results of Tables 3, 4, and 5 in Appendix B. As the reviewer truly noticed, the number of parameters in the network is reduced by the proposed method and in practice, a speedup for all the networks is achieved. \n\n[Cite Han et al. (2016)]: Thank you for suggesting the interesting and relevant paper by Han et al. (2016), which is cited in the latest revision of our paper.\n\n[Correct Eq. 3]: We also edited the augmented Lagrangian equation (3).\n\nWe hope you find the modified paper as a better presentation of the work.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "tmdate": 1484513256491, "tcdate": 1484513256491, "number": 5, "id": "BklegDtIx", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "HkvlWFzNg", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "References added", "comment": "Thank you for these highly relevant references. These were published after we submitted the paper. We are citing both of these references in the latest revision of our paper, they are interesting works on sparse CNNs. Compared to the closely related Sparse Structure Learning (SSL) method from Wen et al. (2016), differentiability and separability are the major benefits obtained by adopting the ADMM framework. These two properties are highlighted in the claims 1 and 2 in the Discussion section in the latest version of the paper. See also in the Discussion section the text starting with \"Some methods such as SSL...\", up to the end of the section, which clarifies the distinction between our proposed method and the SSL method from Wen et al. (2016)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "tmdate": 1484513167702, "tcdate": 1484513167702, "number": 4, "id": "ryuqkwtUl", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "rye9LT8cee", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Revision of the paper (Jan 15, 2017)", "comment": "A revision of the paper has been uploaded, following comments and reviews received."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484513060632, "tcdate": 1478286676569, "number": 341, "id": "rye9LT8cee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rye9LT8cee", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "content": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483494389540, "tcdate": 1483485823525, "number": 2, "id": "rJwYfhFHe", "invitation": "ICLR.cc/2017/conference/-/paper341/pre-review/question", "forum": "rye9LT8cee", "replyto": "rye9LT8cee", "signatures": ["ICLR.cc/2017/conference/paper341/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper341/AnonReviewer4"], "content": {"title": "Additional review questions", "question": "Could the authors elaborate more on point (3) in their conclusion?  Like Reviewer3, I'd like some more intuition as to how this particular point is supposed to be compared against  the more standard L-1 regularization, which is likewise differentiable.  Is the claim that this algorithm is sort of like a differentiable version of L-0 regularization?\n\nAlso, could the authors speculate more about why some of the models become better with more sparsity?  Are there enough experiments to determine if this is just a statistical fluke, or is this more of a general observation?  Or perhaps this is just an artifact of the sparsity allowing for longer training times?  Numerics would be nice here, but really I'm just curious.\n\nEdit: Because I was added late as a reviewer, if the authors could provide an answer by Thursday 1/5/2017, I will incorporate that into my review.  Otherwise, I'll post a full review on Thursday (but will still, of course, consider further comments by the authors in updates to that review before the deadline)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1483485824169, "id": "ICLR.cc/2017/conference/-/paper341/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper341/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper341/AnonReviewer1", "ICLR.cc/2017/conference/paper341/AnonReviewer4"], "reply": {"forum": "rye9LT8cee", "replyto": "rye9LT8cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1483485824169}}}, {"tddate": null, "tmdate": 1482936705971, "tcdate": 1482936705971, "number": 3, "id": "HJcYbIZHe", "invitation": "ICLR.cc/2017/conference/-/paper341/official/review", "forum": "rye9LT8cee", "replyto": "rye9LT8cee", "signatures": ["ICLR.cc/2017/conference/paper341/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper341/AnonReviewer2"], "content": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "rating": "7: Good paper, accept", "review": "This paper presents a framework to use sparsity to reduce the parameters and computations of a pre-trained CNN. \nThe results are only reported for small datasets and networks, while it is now imperative to be able to report results on larger datasets and production-size networks.\nThe biggest problem with this paper is that it does not report numbers of inference time and gains, which is very important in production. And similarly for disk pace. Parameters reduction is useful only if it leads to a large decrease in space or inference time.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483655903684, "id": "ICLR.cc/2017/conference/-/paper341/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper341/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper341/AnonReviewer3", "ICLR.cc/2017/conference/paper341/AnonReviewer1", "ICLR.cc/2017/conference/paper341/AnonReviewer2", "ICLR.cc/2017/conference/paper341/AnonReviewer4"], "reply": {"forum": "rye9LT8cee", "replyto": "rye9LT8cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483655903684}}}, {"tddate": null, "tmdate": 1482030132117, "tcdate": 1482030080265, "number": 2, "id": "HJ_Zh_mEe", "invitation": "ICLR.cc/2017/conference/-/paper341/official/review", "forum": "rye9LT8cee", "replyto": "rye9LT8cee", "signatures": ["ICLR.cc/2017/conference/paper341/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper341/AnonReviewer1"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper reduces the computational complexity of CNNs by minimizing the recognition loss and a sparsity-promoting penalty term together. The authors use ADMM, which is widely used in optimization problems but not used with CNNs before.\nThe paper has a good motivation and well written. The experiments show that the proposed approach increases the sparsity in a network as well as increases the performance.\n\nAs the authors stated, ADMM approach is not guaranteed to converge in non-convex problems. The authors used pre-trained networks to mitigate the problem of trapping into a local optimum. However, the datasets that are used are very small. It would be good to investigate how the proposed approach works on bigger datasets such as ImageNet.\n\nAuthors should compare their results with previous studies that use pruning or sparsity regularizers (Liu et al. (2015); Han et al. (2015); Collins & Kohli (2014)).\n\nIn the discussion section, authors stated that the proposed approach is efficient in training because of the separability property. Could you elaborate on that? Lets say this work uses two phases; phase 1 is pre-training a network, phase 2 is using sparsity and performance promoting steps.  Phase 2 also includes fine-tuning the network based on the new sparse structure. How long does phase 2 take compare to phase 1? How many epochs is needed to fine-tune the network?\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483655903684, "id": "ICLR.cc/2017/conference/-/paper341/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper341/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper341/AnonReviewer3", "ICLR.cc/2017/conference/paper341/AnonReviewer1", "ICLR.cc/2017/conference/paper341/AnonReviewer2", "ICLR.cc/2017/conference/paper341/AnonReviewer4"], "reply": {"forum": "rye9LT8cee", "replyto": "rye9LT8cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483655903684}}}, {"tddate": null, "tmdate": 1482012534183, "tcdate": 1482012534183, "number": 1, "id": "S1Adw4m4x", "invitation": "ICLR.cc/2017/conference/-/paper341/official/review", "forum": "rye9LT8cee", "replyto": "rye9LT8cee", "signatures": ["ICLR.cc/2017/conference/paper341/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper341/AnonReviewer3"], "content": {"title": "Review ", "rating": "5: Marginally below acceptance threshold", "review": "\nThe paper presents a method for sparsifying the weights of the convolutional\nfilters and fully-connected layers of a CNN without loss of performance. Sparsification is achieved by using augmenting the CNN objective function with a regularization term promoting sparisty on groups of weights. The authors use ADMM for solving this optimization task, which allows decoupling of the two terms. The method alternates between promoting the sparsity of the network and optimizing the recognition performance\n\nThe method is technically sound and clearly explained. The paper is well organised and the ideas presented in a structured manner. I believe that sometimes the wording could be improved. \n\nThe proposed method is simple and effective. Using it in combination with other CNN compression techniques such as quantization/encoding is a promising direction for future research. The experimental evaluation is convincing in the sense that the method seems to work well. The authors do not use state-of-the-art CNNs architectures, but I don't see this a requirement to deliver the message. \n\nOn the other hand, the proposed method is closely related to recent works (as shown in the references posted in the public comment titled \"Comparison with structurally-sparse DNNs using group Lasso), that should be cited and compared against (at least at a conceptual level). I will of course consider the authors rebuttal on this matter.\n\nIt would be very important for the authors to comment on the differences between these works and the proposed approach. It seems that both of these references use sparsity regularized training for neural networks, with a very similar formulation. \n\nThe authors choose to optimize the proposed objective function using ADMM. It is not clear to me, why this approach should be more effective than proximal gradient descent methods. Could you please elaborate on this? ADMM is more demanding in terms of memory (2 copies of the parameters need to be stored). \n\nThe claimed contributions (Section 5) seem a bit misleading in my opinion. Using sparsity promoting regularization in the parameters of the model has been used by many works, in particular in the dictionary learning literature but also in the neural network community (as stated above).  Claims 1,2 and 3, are well understood properties of L1-type regularizers. As written in the current version of the manuscript, it seems that these are claimed contributions of this particular work. \n\nA discussion on why sparsity sometimes helps improve performance could be interesting. \n\nIn the experimental section, the authors mainly concentrate on comparing accuracy vs parameter reduction. While this is naturally very relevant property to report, it would be also interesting to show more results in terms of speed-up. which should also be improved by the proposed approach.\n\nOther minor issues:\n- In (3): I think a \"^2\" is missing in the augmented term (the rightmost term).\n\n- The authors could cite the approach by Han et all for compressing DNNS:\nHan, ICLR 2016 \"Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding\"\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483655903684, "id": "ICLR.cc/2017/conference/-/paper341/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper341/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper341/AnonReviewer3", "ICLR.cc/2017/conference/paper341/AnonReviewer1", "ICLR.cc/2017/conference/paper341/AnonReviewer2", "ICLR.cc/2017/conference/paper341/AnonReviewer4"], "reply": {"forum": "rye9LT8cee", "replyto": "rye9LT8cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483655903684}}}, {"tddate": null, "tmdate": 1480911831758, "tcdate": 1480911831751, "number": 2, "id": "S1l1nDGQg", "invitation": "ICLR.cc/2017/conference/-/paper341/public/comment", "forum": "rye9LT8cee", "replyto": "rkjBtPyme", "signatures": ["~Christian_Gagn\u00e91"], "readers": ["everyone"], "writers": ["~Christian_Gagn\u00e91"], "content": {"title": "Answer to AnonReviewer1, ICLR 2017 conference paper341 pre-review question", "comment": "Thank you for your questions, here are our answers:\n1) We prepared and uploaded a new version of the paper with Table 5, which shows joint variations of sparcity and accuracy for increasing \\mu, for the various datasets/models/regularization tested, from single runs that were picked randomly. Please check the updated PDF of the paper.\n2) The proposed method can be seen as a post-processing approach. It can be applied over a trained network where regularization has been used. In fact, all of the baseline networks presented in this paper are pre-trained with a cost function that includes a weight decaying L2 penalty term. The results reported with ADMM are in addition to this L2 regularization."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616985, "id": "ICLR.cc/2017/conference/-/paper341/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rye9LT8cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper341/reviewers", "ICLR.cc/2017/conference/paper341/areachairs"], "cdate": 1485287616985}}}, {"tddate": null, "tmdate": 1480714563258, "tcdate": 1480714563255, "number": 1, "id": "rkjBtPyme", "invitation": "ICLR.cc/2017/conference/-/paper341/pre-review/question", "forum": "rye9LT8cee", "replyto": "rye9LT8cee", "signatures": ["ICLR.cc/2017/conference/paper341/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper341/AnonReviewer1"], "content": {"title": "Table for the results", "question": "1) Can you please add a table that includes datasets/models/methods vs accuracies/sparsities? It would be easier to compare the results. \n2)  Does the proposed sparsity-promoting penalty term provide with a more sparse network than by just using L1,L2 regularizer during training?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks", "abstract": "The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ", "pdf": "/pdf/2a7a79259293dbf09747e4ce766ae78301a3c2f9.pdf", "TL;DR": "A method to sparsify (prune) pre-trained deep neural networks.", "paperhash": "kiaee|alternating_direction_method_of_multipliers_for_sparse_convolutional_neural_networks", "keywords": ["Deep learning", "Computer vision", "Optimization"], "conflicts": ["ulaval.ca", "inria.fr", "sharif.ir", "aut.ac.ir"], "authors": ["Farkhondeh Kiaee", "Christian Gagn\u00e9", "and Mahdieh Abbasi"], "authorids": ["farkhondeh.kiaee.1@ulaval.ca", "christian.gagne@gel.ulaval.ca", "mahdieh.abbasi.1@ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1483485824169, "id": "ICLR.cc/2017/conference/-/paper341/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper341/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper341/AnonReviewer1", "ICLR.cc/2017/conference/paper341/AnonReviewer4"], "reply": {"forum": "rye9LT8cee", "replyto": "rye9LT8cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper341/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1483485824169}}}], "count": 18}