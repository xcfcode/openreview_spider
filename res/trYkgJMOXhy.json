{"notes": [{"id": "trYkgJMOXhy", "original": "V6M1DgMZc9E", "number": 3190, "cdate": 1601308354197, "ddate": null, "tcdate": 1601308354197, "tmdate": 1614985768398, "tddate": null, "forum": "trYkgJMOXhy", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "-iYZ3LvIwxT", "original": null, "number": 1, "cdate": 1610040364568, "ddate": null, "tcdate": 1610040364568, "tmdate": 1610473955001, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper addresses counterfactual fairness learning using generative approach. While acknowledging the importance and potential usefulness of generative approach, the reviewers and AC raised several important concerns that place this paper below the acceptance bar: \n\n(1) low degree of novelty \u2013 see multiple concerns and suggestions by R2, R3, R4;\n\n(2) the model is not justified by a causal mechanism (R4), and it remains unclear under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples (R2); \n\n (3) lack of technical rigor when presenting the model \u2013 see R4\u2019s request to relate to the DAG models, see R1 multiple questions regarding the reinforced data sampler; \n\n(4) lack of empirical evidence (R3) and evaluation details, e.g. on cross validation and more recent methods (see R4\u2019s recommendations); \n\n (5) related work is not discussed in sufficient details \u2013 see R4\u2019s elaborate comment.\n\nAmong these, (4,5) did not have a substantial impact on the decision but would be helpful to address in a subsequent revision. However, (1), (2) and (3) make it very difficult to assess the benefits of the proposed approach and were viewed by AC as critical issues.\nIn the rebuttal it is stated that \u2018Our counterfactual examples are generated using a powerful generator rather than a fixed synthesizer in Kusner et al. (2017)\u2019 \u2013 more rigorous comparison has to be provided to support such statement. AC would urge the authors to contrast and compare their synthetic counterfactual examples with Kusner et al on the datasets where causal graph has been built. [Razieh Nabi and Ilya Shpitser. Fair inference on outcomes., AAAI2018], Figure 2 postulates causal graphs for the Compas and Adult Income datasets evaluated in this paper.\n\nA general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews and encouragements are useful for revising the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040364554, "tmdate": 1610473954984, "id": "ICLR.cc/2021/Conference/Paper3190/-/Decision"}}}, {"id": "ZLwRbypCUzT", "original": null, "number": 2, "cdate": 1603854295920, "ddate": null, "tcdate": 1603854295920, "tmdate": 1606794672857, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Review", "content": {"title": "Interesting direction, but more needs to  be done", "review": "The paper proposes a teacher-student framework to ensure fairness by letting the teacher choose examples for the student from either the training data or from a counterfactual distribution. The main contributions are a counterfactual generative model and an algorithm for learning the teacher policy.\n\nStrengths:\n========\n1. The idea seems interesting and the proposed teacher-student framework is novel in the area of fair learning.\n\n2. The authors have done a good job of modeling various aspects of their complex approach using neural networks.\n\n3. The fact that authors were able to make the complex optimization work is itself a good thing since the objective has a lot of moving parts.\n\n4. The presented evaluations also show some promise.\n\nThings to consider improving:\n=========================\n1. My basic question is regarding the motivation for such a framework. Why is this approach important? Is there any fundamentally new insight that can be obtained with a GFT framework that cannot be obtained using existing fair learning approaches.\n\n2. The argument that causal methods cannot benefit commonly used fairness metrics like demographic parity (DP) or equalized odds (EO) should be elaborated. Isn't it possible to create Structural causal models that subsume conditional independences like DP and EO?\n\n3. There are a number of issues with references:\n- There are a number of additional pre-, in-, and post-processing methods that can be cited. Looking into some recent fairness papers may give the authors an idea of the works.\n- Agarwal et al. 2018 is considered an in-processing approach and not post-processing or pre-processing since it imposes fairness criteria while training a fair model.\n- Demographic parity is a really old concept and has been discussed in many papers before Madras et al. 2018.\n- The authors should also consider citing and perhaps comparing with https://homes.cs.washington.edu/~suciu/sigmod-2019-fairness.pdf if it makes sense, since it also does \"database repair\" or pre-processing.\n- Barring some exceptions, many early works in fairness are not cited. For example https://ieeexplore.ieee.org/document/6175897 is a classic work that discusses a lot of concepts in fairness that we use today (they call it discrimination). I recommend the authors to do a thorough literature survey and include at least the important works.\n\n5. Many crucial algorithmic details are missing. While  the complex optimization is important to cover all the aspects of the framework for data generation, it should also be motivated and explained better. \n- In Sec. 3.2.1, how valid is it to assume U to be independent of A? Technically A is a part of the data that is generated so it may be reasonable to surmise that it should depend on the latent. In general the authors should provide a DAG which encodes their assumptions and justify them.\n- How would this architecture be modified for DP since the authors discuss both DP and EO in the beginning?\n- What is L_att trying to optimize?\n- What does it mean to have attribute labels as auxiliary tasks for D (in L_cls)? What are attribute labels?\n- It may make sense to summarize in a few sentences what each term in the objective does. If space  is less, the authors can move some contents to the appendix.\n- In Sec. 3.3, could the authors provide some insights on why such a training works?\n- Please also discuss what REINCFORCE is, and provide reference/more details.\n\n6. Generally in experiments, cross-validated results are needed. It is also crucial to provide sufficient pre-processing and hyper-parameter details to help reproduction of results.\n\n7. There are also a number of recent post-processing methods that can be compared with besides Hardt et al, 2016 (see http://auai.org/uai2019/proceedings/papers/315.pdf, http://proceedings.mlr.press/v108/wei20a.html)\n\n8. More details needed on how Agarwal et al., 2018 was run for example. In https://arxiv.org/pdf/1906.00066.pdf, pg. 30  Agarwal seems to be more competitive than what is shown here. Look at adult-gender-LR-EO and  adult-gender-GBM-EO, Agarwal et al. 2018 (named \"red\"\")  at EO=0.04 has higher  accuracies than shown here. Similar comments apply to COMPAS.\n\n9.There are  also a few other pre-processing approaches that use GANs. \nhttps://arxiv.org/pdf/1805.11202.pdf\nhttps://arxiv.org/pdf/1805.09910.pdf\n\n10. The authors must identify the caveats  of training a model on CelebA which has a \"western\" and \"celebrity\" bias. Models trained there may not transfer to other general settings.\n\n11. In page 8, the analysis of teacher model needs more details. How can we say original images dominate if only 7% image is chosen at max at any iteration? Why is the teacher behavior of choosing real samples in the beginning and synthesized samples later justified?\n\nIn summary, the paper has identified an interesting direction but this needs to be taken forward a bit more.\n\nPost-rebuttal\n===========\nThanks to the  authors for  their detailed response to  my  questions. Some of the answers are indeed satisfactory, but some questions remain - such as extensive comparisons to other methods (probably using more datasets), how  the method would  behave (practically)  with  a different fairness measure like DP, and more  carefully situating the method in  the  fairness literature. I encourage the authors  to keep pursuing this interesting  direction.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538080576, "tmdate": 1606915762518, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3190/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Review"}}}, {"id": "yjbq-5hpTZ", "original": null, "number": 1, "cdate": 1603557312537, "ddate": null, "tcdate": 1603557312537, "tmdate": 1606783166031, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Review", "content": {"title": "An interesting work", "review": "This paper describes a pre-processing method to reduce certain statistical disparities in the classifier obtained from the training data. The proposed approach involves learning a latent probability model that simulates the training data. The authors then manipulate the learned model to generate \"counterfactual\" samples that belong to the membership of underrepresented demography. A more \"fair\" classifier is trained on the manipulated data mixing with the \"counterfactual\" samples.\n\nThe proposed approach seems sensible. The experiment results support the claims that it reduces the statistical disparities (e.g., equalized odds) of the trained classifier. I like the simplicity of the proposed method while future work is needed to explicate the theoretical condition under which it is sufficient.\n\nAlso, I believe that the clarity of this paper could be improved. For instance, the definition of counterfactual fairness seems to be orthogonal to the \"counterfactual\" samples described in later sections. More specifically, the counterfactual fairness concerns with the potential outcome of prediction $\\hat Y_{a'}$ has the sensitive attribute $A$ been $a'$. On the other hand, the \"counterfactual\" samples is the potential outcomes of the feature $X_{a'}$ had $A = a'$. It does not seem that the concept of counterfactual fairness has be used in later sections. Therefore, I would suggest that the authors could remove the definition of counterfactual fairness and provide a formal introduction to structural causal models and interventions, e.g., see (Causality, Pearl, 2009). \n\n\n------------ Post Rebuttal ------------------\nI read other reviewers' comments and the authors' responses. I like the idea of applying the GAN framework to compute counterfactual distributions. However, I could also see why other reviewers are not particularly excited about it. The authors managed to apply the GAN approach to obtain counterfactual samples in some specific datasets. However, many questions regarding the proposed methods are left unanswered, e.g., under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples. With this being said, I think this paper could be most improved by further elaborating how it contributes to the existing causal inference literature, especially in computing counterfactual probabilities. Due to these reasons, I intend to keep my score but won't strongly champion for it.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538080576, "tmdate": 1606915762518, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3190/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Review"}}}, {"id": "CDdfpJjGi2P", "original": null, "number": 10, "cdate": 1606294989302, "ddate": null, "tcdate": 1606294989302, "tmdate": 1606294989302, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "yjbq-5hpTZ", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment", "content": {"title": "Reply to Reviewer 2", "comment": "We thank the reviewer for the constructive comments. We do believe that the proposed approach can overcome some of the drawbacks of the existing counterfactual fairness training and have much room for future work. \n\nWe also thank the reviewer for the feedback on the quality of the paper. Since the release of the review we have made a comprehensive review of the technical part of the paper and have improved many of the parts that may cause confusion, including:\n- Added graphical model we used in Sec 3.2.1\n- More explanations on our learning paradigm, including the auxiliary loss, the summarization of loss terms in Table 1. \n- A detailed description of RL based teaching method in Sec 3.4  \n- More experimental details in Sec 4.2 and Appendix A.  \n\nPlease refer to the revised manuscript to see our changes. \n\nWe have adopted the reviewer\u2019s suggestion by removing the definition of counterfactual fairness in formal fairness criteria. Additionally, we\u2019ve added another section called \u2018Causal Models and Counterfactual Examples\u2019 in the revised paper. In this section, we presented a formal definition of causal models along with the definition of counterfactual example. We have also removed the majority of the occurrence mentioning counterfactual fairness throughout the paper with the single exception of the introduction. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "trYkgJMOXhy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3190/Authors|ICLR.cc/2021/Conference/Paper3190/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840217, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment"}}}, {"id": "nY2FRKWlwHB", "original": null, "number": 9, "cdate": 1606294953359, "ddate": null, "tcdate": 1606294953359, "tmdate": 1606294953359, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "ZLwRbypCUzT", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment", "content": {"title": "Reply to Reviewer 4, part 3/3", "comment": "### Q11. Teacher behavior\n---\nThe reason why the sampler uses so many counterfactual data is the extremely biased distribution of the CelebA dataset. In our experiments, the target output is highly correlated to the binary sensitive attribute. The distribution is defined as follows: P(A=0, Y=1) : P(A=1,Y=1) = 1 : k, where A and Y represent the protected attributes and output of interest respectively. Empirically, we found when the k is relatively large, the sampler tends to alleviate the intrinsic bias by using many counterfactual samples to reverse the distribution: P(A\u2019=0, Y=1) : P(A\u2019=1,Y=1) \u2248 k : 1. We add experiments in section 4.2 to show the performance of two different settings:\n\n| Method                  |            Error(%)       |      EO |\n| --- | --- | --- |\n| Base5: fix ratio         |           20.7      |        0.242 |\n| Base6: reverse         |          19.2       |        0.171 |\n\nAs shown in the results, The reverse setting (baseline 6), which maintains a part of the real images and adds counterfactual images to reverse the distribution (1:k to k:1) in the training data preprocessing process, could achieve a lower EO compared to other baselines. However, if we used all fake data (baseline 4), the student would never have seen the real image in the training process. Training with all synthetic data does have some limitations. Although these counterfactual images are very authentic to human eyes, there are properties that the synthetic data could not recreate and need to be learned from the real data. In conclusion, the teacher will feed slightly more real data to students to learn these patterns in the beginning and increase the amount of counterfactual data later to strengthen the reverse bias.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "trYkgJMOXhy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3190/Authors|ICLR.cc/2021/Conference/Paper3190/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840217, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment"}}}, {"id": "rPA61lDA4v3", "original": null, "number": 8, "cdate": 1606294920602, "ddate": null, "tcdate": 1606294920602, "tmdate": 1606294920602, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "ZLwRbypCUzT", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment", "content": {"title": "Reply to Reviewer 4, part 2/3", "comment": "### Q6: Pre-processing and hyper parameter details\n---\nThank you for pointing this out, please refer to the revised Appendix for more details.\n\n### Q7: \u201cThere are also a number of recent post-processing methods..\u201d\n---\nThanks for pointing out these papers. The two papers are indeed relevant and we have included in our paper for discussion. Although the two referenced papers used wasserstein distance, there are major differences compared to our paper: Jiang et.al learns to transport predictive distribution p_a to p* for each a, while Wei et.al learns to transform the score r\u2019(x), which is in the reweighting regime. What we are doing is to transform p(X) to p_{A<-a\u2019}(X), e.g.., the generative model of images. This way we can go beyond the images from the original dataset. Technically this is also significantly harder than Jiang et.al or Wei et.al. It is intractable to get the closed form transport, due to the high dimensional space (e.g., 1024-dimensional for 32x32 images). \n\n### Q8. Difference in evaluation results for baselines\n---\nWe follow the implementation of Agarwal et al., 2018 in their original paper (https://arxiv.org/pdf/1803.02453.pdf). The only different setting is the choices of the decision variables. As introduced in section 4.1, for in-processing, post-processing and our methods, we only use age, education number of years, relationship, race, sex, capital-gain and hours-per-week to be the 7 decision variables in the Adult dataset, while 14 variables are used in https://arxiv.org/pdf/1906.00066.pdf. Similarly, we only use age, race, sex, count of prior offences, charge for which the person was arrested and COMPAS risk score to be the decision variables in the COMPAS dataset. It should be the reason that caused the accuracy gaps. Since our generative model is more suitable for image data and tabular data with fewer rows, consistent and fair comparison was the main reason we use the same decision variables in the in-processing and post-processing method. While our work focuses on the entire framework, there are techniques that could be applied to improve the tabular data generation, we will add experiments by modifying our generative model to better accommodate tabular data.\n\n### Q9. Differences between our work and few other pre-processing approaches that use GANs\n---\nThere is a fundamental difference between our work and the line of work that improves fairness in the GAN setting. The lines of work such as FairGAN aim at improving fairness on GAN models. In other words, GAN is used as a problem setting rather than a solution. The purpose of our work, however, is to improve fairness in the general sense with the help of a technique that involves generative models and the idea of counterfactual reasoning. In this case, we are not really restricting the problem space to be GAN, but rather use the core idea of generative models as a solution. To avoid further confusions, we have clearly illustrated this difference in an updated version of the paper.\n\n### Q10. Caveats of \u2018western\u2019 and \u2018celebrity\u2019 bias\n---Our method is a general framework, it is not particularly designed for specific dataset. We could extend it to fit into other general settings. Such extensions include adding the number of teacher actions(flip both the gender and the race) or enriching the training and validation data with our generative model. In practice, tasks related to face attribute classification could be trained and validated on a diverse dataset which involves more sensitive attributes (western / eastern). Note that our method mainly relies on the reward signal from the validation set, which is smaller and easier to collect.\n\n**2/3 OF REPLY**"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "trYkgJMOXhy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3190/Authors|ICLR.cc/2021/Conference/Paper3190/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840217, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment"}}}, {"id": "uYEdNnKix5D", "original": null, "number": 7, "cdate": 1606294785384, "ddate": null, "tcdate": 1606294785384, "tmdate": 1606294804262, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "ZLwRbypCUzT", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment", "content": {"title": "Reply to Reviewer 4, part 1/3", "comment": "Thanks a lot for the constructive feedback! We have addressed them correspondingly in the paper, please also see our response below: \n\n### Q1: New insights and significance of the work\n---\nThe significance of the work lies in the fact that we demonstrated that fairness training can benefit from counterfactual examples generated by a learned generator. We also show that the training can benefit from a carefully tuned ratio of counterfactual examples and real examples as training progresses. Those are the insights we contributed to and are something that related work such as counterfactual fairness can not achieve, as they were using a fixed generator and a fixed counterfactual-real ratio. \n\nThe optimization process of our generator is built on a student-teacher framework. Although much of the generative models are inspired by GAN, as we have illustrated in the response 9 of this rebuttal to the reviewer, our approaches and problem settings are fundamentally different from work that aims at improving the fairness of GAN models such as FairGAN. For those settings, GAN is chosen to be a problem while our generative models are used as a solution. Our approach is generanic to any fairness problem and can be applied to improve the fairness of a wide range of problems that is not limited to GAN models. To the best of our knowledge, we are the first work in the area to demonstrate that a combination of 1) a learned generator for synthesizing counterfactual examples and 2) a dynamic counterfactual-real ratio would benefit fairness training.\n \n\n### Q2: Applicability of causal methods to more common fairness metrics\n---\nWe agree with the reviewer that it is possible to create causal structures that subsume conditional independencies in order to benefit DP or EO. However, we believe the arguments in the paper are still valid, as we will need those structure information to be known in advance. If we want to make those causal structures specific to the metrics, we will have to derive one structure for each metric we find. This is, what we believed, a significant limitation of the current causal methods which we aim to improve. To acknowledge the point that the reviewer is mentioned, we have updated the paper to clarify our arguments. \n\n### Q3: Issues with references\n---\n- Additional citations for additional pre-, in-, and post-processing methods\n\nWe have moved Agarwal et al. 2018 to in-processing as the reviewer suggested. \nWe have also incorporated the database repair paper along with the discrimiantion paper into the related work. \n\n\n### Q5. \u201cMany crucial algorithmic details are missing\u2026\u201d\n---\nPlease see our explanation below. We\u2019ve also made refinements in the paper. \n\n*Q5.1: In Sec. 3.2.1, \u2026 assume U to be independent of A*\n\nWe follow the graphical model from Kusner et.al (see Fig 1(a)(b)  in https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf). It is standard to assume A -> X <- U. This is known as \u201cv-structure\u201d in directed graphical model, where observing X will couple A and U. For example, gender should be independent of educational background, but observing a picture X that shows a girl wearing a doctoral hat would couple the \u201cfemale doctor\u201d. \n\n*Q5.2: how ...modified for DP*\n\nWe only need to modify the reward function. In the paper we use EO, but it is straightforward to replace it with DP objective, as the reinforcement learned teacher can handle the black-box reward function. \n\n*Q5.3: what is L_att*\n\nL_att learns an attribute predictor from U. The learned predictor will be used in Eq (6), where the latent factor U will try to minimize the mutual information between A, so as to decouple the sensitive information A for predicting target label Y. \n\n*Q5.4: What \u2026 for D (in L_cls)? What are attribute labels?*\n\nEach training sample is a  <X, A, Y> triplet, for example, X is an image, A is the gender, and Y is the hair style. The auxiliary task of predicting attributes using D\u2019 on top of D is to help D better distinguish between realistic v.s. fake images. \n\n*Q5.5: summarize \u2026 each term in the objective*\n\nWe\u2019ve added a table that summarizes the roles of different losses in the objective. \n\n*Q5.6: In Sec. 3.3, ... why such a training works?*\n\nFirstly it is expected that learning GAN to generate realistic images would work in general. Also learning an attribute classifier would also be a tractable task. So our training is designed to first train these two parts (Sec 3.3 (a)(b)) separately, to provide a warm start.\nThe procedure in (c) further minimizes the mutual information between U and A given X, and the counterfactual generator would also learn to adapt to the refined U. Since the generator is pretrained, the adaptive refinement procedure would be smoother. \n\n*Q5.7: discuss REINFORCE*\nThe policy gradient is a standard RL approach. Experimentally we found the variance reduction is not very helpful, so we didn\u2019t use actor-critic. We\u2019ve added the explanation in the paper. \n\n**1/3 OF REPLY**\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "trYkgJMOXhy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3190/Authors|ICLR.cc/2021/Conference/Paper3190/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840217, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment"}}}, {"id": "pYvI5Ie-KKf", "original": null, "number": 6, "cdate": 1606294554245, "ddate": null, "tcdate": 1606294554245, "tmdate": 1606294554245, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "cnKmzUPEXan", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment", "content": {"title": "Reply to Reviewer 3", "comment": "We would like to thank R3 for the constructive comments, and we have addressed them accordingly as follows.\n\n### Novelty\n---\nOur main novelty lies in the fact that we adopt a learned generator to synthesize counterfactual examples, which is optimized by a student - teacher framework. We agree with the reviewer that the idea of mitigating biases using counterfactual examples already exists in Kusner et al. (2017). However, there are several key differences between our method and Kusner et al. (2017). 1) Our counterfactual examples are generated using a powerful generator rather than a fixed synthesizer in Kusner et al. (2017). Such an improvement makes it possible to adopt our method on more sophisticated settings such as vision recognition illustrated in the CelebA dataset. The method used in Kusner et al. (2017) is difficult to work beyond tabular dataset. 2) Kusner et al. (2017) uses a fixed k : 1 ratio in training their model, where k represents the proportion between the majority class and the minority class. Our method learns the optimal ratio as training progresses and is different for each dataset. The experimental results in the revised paper (baseline 6 in Figure 4) illustrated that our method largely out-performs the baseline using a fixed ratio and on with a reversed ratio (i.e., k:1 ratio). We note that the latter is used in the Kusner et al. (2017) and is one of the drawbacks of their method compared to ours. \n\n### Applications on datasets other than Adult and COMPAS\n---\nWe agree with the reviewer that tabular data such as Adult and COMPAS do not provide much value in demonstrating our method. As a matter of fact, we have experimented with our method using the CelebA dataset in order to demonstrate the strength of our method beyond the tabular dataset. We also agree with the reviewer that text data could be another venue to demonstrate our model. However, we believe our existing results on the CelebA dataset can also be a good indicator of the applicability of our method.\n\n### Categorization of the method\n---\nWe believe that our method is a cross category one and relates to all of the three major fairness methods. \n\nWe are considered a pre-processing method for fairness because we actively manipulate the distribution of the training data. Methods fall into this category includes the resampling or optimized pre-processing (Calmon 2017) that aims at re-adjusting the distribution of a biased dataset. \n\nWe are considered an In-processing method for fairness as we directly optimize the model to achieve fairness. Methods fall into this category includes equalized odds and demographic parity. As our method builds on a fairness goal, which we rely on as a reward function in our Generative Fairness Teaching framework, we believe our method is related to in-processing. \n\nAnd finally, we are related to the Post-processing method because our teacher model can take any biased student model and train it to be fair. Methods falling into this category includes the learning to defer (Madras 2018) method which mitigates a biased model.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "trYkgJMOXhy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3190/Authors|ICLR.cc/2021/Conference/Paper3190/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840217, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment"}}}, {"id": "Z7_OW9zsnhn", "original": null, "number": 5, "cdate": 1606294472807, "ddate": null, "tcdate": 1606294472807, "tmdate": 1606294472807, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "-60AhWcnsDx", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment", "content": {"title": "Reply to Reviewer 1", "comment": "Thank you for your constructive comments! Please see our detailed response below.\n\n### How is the policy parameterized beyond the inputs? \n---\nPlease refer to section 3.4 for the parameterization details. To clarify the parametrization of the policy network, we have added a new paragraph in Appendix of the revised paper. Please refer to the subsection \u2018\u2018Teacher and student model settings\u2019 for a detailed demonstration of the parametrization of the policy network. The inputs include labels, sensitive attributes, cross entropy and the group fairness on current training batch. The policy is then predicted based on the inputs with the REINFORCE algorithm. \n\n### How many episodes run out to tune the sampler?\n---\nWe train the teacher sampler for 500 episodes, within each episode, the student model is re-initialized and trained for 20 epochs.\n\n### Do you mean a different held out set or the same final evaluation set?\n---\nOfficially, CelebA dataset is partitioned into training, validation, testing sets. Images 1-162770 are training, 162771-182637 are validation, 182638-202599 are testing. The reward during the training process was measured on the held-out validation set, the final result reported in table 3 was measured on the testing set.\n\n### confused by Figure 6\n---\nThe y-axis is indeed the % of samples that left unmodified (the original samples). The reason why the sampler uses so many counterfactual data is the extremely biased distribution of the CelebA dataset. In our experiments, the target output is highly correlated to the binary sensitive attribute. The distribution is defined as follows: P(A=0, Y=1) : P(A=1,Y=1) = 1 : k, where A and Y represent the protected attributes and output of interest respectively. Empirically, we found when the k is relatively large, the sampler tends to alleviate the intrinsic bias by using many counterfactual samples to reverse the distribution: P(A\u2019=0, Y=1) : P(A\u2019=1,Y=1) \u2248 k : 1. In order to further verify that our proposed method can outperform those simple heuristics by reversing the ratio, we\u2019ve added additional experiments in section 4.2 to show the performance of two different baseline settings:\n\n\n| Method     |       Error(%)       |     EO | \n| --- | --- | --- | \n| Base5: fix ratio     |   20.7        |    0.242 |\n| Base6: reverse             |      19.2          |     0.171 |\n\nThe first baseline we want to show is the one with reverse weight. As shown in the results, The reverse setting (baseline 6), which maintains a part of the real images and adds counterfactual images to reverse the distribution (1:k to k:1) in the training data preprocessing process, could achieve a lower EO compared to other baselines. However, if we used all fake data (baseline 4), the student would never have seen the real image in the training process. Training with all synthetic data does have some limitations. Although these counterfactual images are very authentic to human eyes, there are properties that the synthetic data could not recreate and need to be learned from the real data. The second baseline we want to show is the In the fix ratio setting(baseline 5) with very little real image. We fix the original-to-counterfactual ratio at each batch, which has 10% original image and 90% counterfactual image, the EO will also decrease by a large margin compared to the all fake setting (baseline 4).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "trYkgJMOXhy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3190/Authors|ICLR.cc/2021/Conference/Paper3190/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840217, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Comment"}}}, {"id": "cnKmzUPEXan", "original": null, "number": 3, "cdate": 1604025847286, "ddate": null, "tcdate": 1604025847286, "tmdate": 1605024051168, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Review", "content": {"title": "Review of GFT", "review": "This paper combines counterfactual modeling with adversarial training for fair machine learning tasks. For a given fairness metric chosen from a variety of canonical examples, the method ensures fairness by augmenting the data with counterfactual examples during training. The approach has potential, which is best demonstrated on examples where the counterfactual data generation is interesting, like the CelebA data.\n\nI believe the main weakness of the paper is a low degree of novelty. For example, the idea of training with counterfactual data is present already in the Kusner et al. (2017) reference. The current paper expands the uses of that technique and combines it with an adversarial training architecture. So the strength of this work depends on the suitability of the counterfactual model and training architecture.\n\nThe other data examples, Adult and COMPAS, do not contribute much additional value, particularly in light of my previous point. The paper could be improved by replacing these with one or more examples that better leverage the strength of adversarial training. For ICLR it might be best if these examples use types of data where representations can be useful, like text for example. \n\nThe Section 2.2 comments on in/pre/post-processing were confusing. The method is \"related\" to all of them? Does it not fall into any of the categories?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538080576, "tmdate": 1606915762518, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3190/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Review"}}}, {"id": "-60AhWcnsDx", "original": null, "number": 4, "cdate": 1604113017213, "ddate": null, "tcdate": 1604113017213, "tmdate": 1605024051039, "tddate": null, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "invitation": "ICLR.cc/2021/Conference/Paper3190/-/Official_Review", "content": {"title": "Interesting and novel proposal, surprised part of it works and seeking more details", "review": "The paper proposes to pair a GAN based model for generating counterfactual samples given protected attribute labels and a reinforced data sampler for choosing whether to let a model train on generated data or original data.\u00a0 \u00a0The generative model in of itself is interesting, combining a gan component, and VAE component for analyzing an input image and a mutual information penalty for the VAE hidden vector and protected attribute.\u00a0 The key to the method seems to be a reinforced data sampler, which picks, when to use a counterfactual sample versus the original sample. Given that the core of what is making this work is the data sampler, I wish there were more details.\u00a0\n\nHow is the policy parameterized beyond the inputs?\u00a0\nHow many episodes run out to tune the sampler?\nThe paper says the reward was the fairness measure on the held out set. Do you mean a different held out set or the same final evaluation set?\nI am confused by Figure 6. Is the y-axis the % of samples that left unmodified? If so, then seems like the sampler is essentially saying always use a counterfactual? So I would expect results very similar to all-fake baseline, but in terms of EO, the results are very different. If only a sample number of samples are different than that baseline, how is this possible?\n\nPositives:\n+ the paper proposes an interesting way to generate counterfactual samples\n+ the results seem promising\n\nNegatives:\n- mostly I am left confused how the reinforced data selector is making the method work. Answering some of the above questions would help.\u00a0", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3190/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3190/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Fairness Teaching", "authorids": ["~Rongmei_Lin1", "~Hanjun_Dai1", "~Li_Xiong1", "~Wei_Wei15"], "authors": ["Rongmei Lin", "Hanjun Dai", "Li Xiong", "Wei Wei"], "keywords": ["fairness", "student teacher model", "counterfactual generative model"], "abstract": "Increasing evidences has shown that data biases towards sensitive features such as gender or race are often inherited or even amplified by machine learning models. Recent advancements in fairness mitigate such biases by adjusting the predictions across sensitive groups during the training. Such a correction, however, can only take advantage of samples in a fixed dataset, which usually has limited amount of samples for the minority groups. We propose a generative fairness teaching framework that provides a model with not only real samples but also synthesized samples to compensate the data biases during training. We employ such a teaching strategy by implementing a Generative Fairness Teacher (GFT) that dynamically adjust the proportion of training data for a biased student model. Experimental results indicated that our teacher model is capable of guiding a wide range of biased models by improving  the fairness and performance trade-offs significantly.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lin|generative_fairness_teaching", "one-sentence_summary": "Teaching machines to achieve fairness by using a counterfactual generative model", "pdf": "/pdf/55f12e9ada3c5832d3878cbb1631d34fd3ada822.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eTXTcj8Npb", "_bibtex": "@misc{\nlin2021generative,\ntitle={Generative Fairness Teaching},\nauthor={Rongmei Lin and Hanjun Dai and Li Xiong and Wei Wei},\nyear={2021},\nurl={https://openreview.net/forum?id=trYkgJMOXhy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "trYkgJMOXhy", "replyto": "trYkgJMOXhy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3190/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538080576, "tmdate": 1606915762518, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3190/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3190/-/Official_Review"}}}], "count": 12}