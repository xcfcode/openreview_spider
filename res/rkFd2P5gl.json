{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396581709, "tcdate": 1486396581709, "number": 1, "id": "HyAihz8Ox", "invitation": "ICLR.cc/2017/conference/-/paper432/acceptance", "forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "A summary of strengths and weaknesses brought up in the reviews:\n \n Strengths\n -Paper presents a novel way to evaluate representations on generalizability to out-of-domain data (R2)\n -Experimental results are encouraging (R2)\n -Writing is clear (R1, R2)\n \n Weaknesses\n -More careful controls are needed to ascertain generalization (R2)\n -Experimental analysis is preliminary and lack of detailed analysis (R1, R2, R3)\n -Novelty and discussion of past related work (R3)\n \n The reviewers are in consensus that the idea is exciting and at least of moderate novelty, however the paper is just too preliminary for acceptance as-is. The authors did not provide a response. This is surprising because specific feedback was given to improve the paper and it seems that the paper was just under the bar. Therefore I have decided to align with the 3 reviewers in consensus and encourage the authors to revise the paper to respond to the fairly consistent suggestions for improvement and re-submit."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396582201, "id": "ICLR.cc/2017/conference/-/paper432/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396582201}}}, {"tddate": null, "tmdate": 1482002436752, "tcdate": 1481920167269, "number": 3, "id": "B1JhRTbNg", "invitation": "ICLR.cc/2017/conference/-/paper432/official/review", "forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "signatures": ["ICLR.cc/2017/conference/paper432/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper432/AnonReviewer2"], "content": {"title": "review for Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "rating": "5: Marginally below acceptance threshold", "review": "This paper describe an implementation of delayed synchronize SGD method for multi-GPU deep ne training.\nComments\n1) The described manual implementation of delayed synchronization and state protection is helpful. However, such dependency been implemented by a dependency scheduler, without doing threading manually.\n2) The overlap of computation and communication is a known technique implemented in existing solutions such as TensorFlow(as described in Chen et.al) and MXNet. The claimed contribution of this point is somewhat limited.\n3) The convergence accuracy is only reported for the beginning iterations and only on AlexNet. It would be more helpful to include convergence curve till the end for all compared networks.\n\nIn summary, this is paper implements a variant of delayed SyncSGD approach. I find the novelty of the system somewhat limited (due to comment (2)). The experiments should have been improved to demonstrate the advantage of proposed approach.\n\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512587411, "id": "ICLR.cc/2017/conference/-/paper432/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper432/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper432/AnonReviewer3", "ICLR.cc/2017/conference/paper432/AnonReviewer1", "ICLR.cc/2017/conference/paper432/AnonReviewer2"], "reply": {"forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512587411}}}, {"tddate": null, "tmdate": 1481718229333, "tcdate": 1481718229329, "number": 3, "id": "Bka0FhRXg", "invitation": "ICLR.cc/2017/conference/-/paper432/pre-review/question", "forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "signatures": ["ICLR.cc/2017/conference/paper432/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper432/AnonReviewer1"], "content": {"title": "No questions.", "question": "Just to make the annoying message go away."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481718229832, "id": "ICLR.cc/2017/conference/-/paper432/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper432/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper432/AnonReviewer3", "ICLR.cc/2017/conference/paper432/AnonReviewer2", "ICLR.cc/2017/conference/paper432/AnonReviewer1"], "reply": {"forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481718229832}}}, {"tddate": null, "tmdate": 1481716954435, "tcdate": 1481716907064, "number": 2, "id": "S1X2Nn07e", "invitation": "ICLR.cc/2017/conference/-/paper432/official/review", "forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "signatures": ["ICLR.cc/2017/conference/paper432/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper432/AnonReviewer1"], "content": {"title": "Lacks Strong Baselines and Wall-Time Results", "rating": "3: Clear rejection", "review": "The authors present methods to speed-up gradient descent by leveraging asynchronicity in a layer-wise manner.\n\nWhile they obtain up-to 1.7x speedup compared to synchronous training, their baseline is weak. More importantly, they dismiss parameter-server based methods, which are becoming standard, and so effectively just do not compare to the current state-of-the-art. They also do not present wall-time measurements. With these flaws, the paper is not ready for ICLR acceptance.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512587411, "id": "ICLR.cc/2017/conference/-/paper432/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper432/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper432/AnonReviewer3", "ICLR.cc/2017/conference/paper432/AnonReviewer1", "ICLR.cc/2017/conference/paper432/AnonReviewer2"], "reply": {"forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512587411}}}, {"tddate": null, "tmdate": 1481576239282, "tcdate": 1481576239273, "number": 1, "id": "SyP4J9hXe", "invitation": "ICLR.cc/2017/conference/-/paper432/public/comment", "forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "Chen et al., and wallclock time evaluations", "comment": "Full disclosure: I am an author on the follow-up paper to Chen et al. (2016).\n\nAlthough the paper cites Chen et al. (2016) as a starting point of the proposed approach, it does not mention the key message of Chen et al. (2016) of using backup workers to hide straggler effects in synchronous gradient descent. Were backup workers used as part of the synchronous approaches?\n\nAlso, the paper does not evaluate the wallclock time to reach convergence or a given accuracy. Even though the AGD approach has more iterations per second, it also leads to poorer convergence (as acknowledged in Table 1). As a specific example, to reach 0.01688 accuracy requires <2000 iterations of SGD, <3000 iterations of AGD (1 comm), and 5000 iterations of AGD (2 comm). Hence, for AGD to be faster than SGD in wallclock time, AGD must have at 1.5x iterations per second relative to SGD, which does not appear to the case from Figure 1. Of course, this example is only for a low accuracy of 0.01688, and the proposed AGD could very well reach the final convergence of 54% in lesser time. It would be very helpful if the authors could provide that information."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287578986, "id": "ICLR.cc/2017/conference/-/paper432/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkFd2P5gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper432/reviewers", "ICLR.cc/2017/conference/paper432/areachairs"], "cdate": 1485287578986}}}, {"tddate": null, "tmdate": 1481055171220, "tcdate": 1481055171215, "number": 1, "id": "r1jpo9N7x", "invitation": "ICLR.cc/2017/conference/-/paper432/official/review", "forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "signatures": ["ICLR.cc/2017/conference/paper432/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper432/AnonReviewer3"], "content": {"title": "Difficult to read paper. Lack of strong async baseline a major flaw.", "rating": "3: Clear rejection", "review": "This paper is relatively difficult to parse. Much of the exposition of the proposed algorithm could be better presented using pseudo-code describing the compute flow, or a diagram describing exactly how the updates take place. As it stands, I'm not sure I understand everything. I would also have liked to see exactly described what the various labels in Fig 1 correspond to (\"SGD task-wise, 1 comm\"? Did you mean layer-wise?).\nThere are a couple of major issues with the evaluation: first, no comparison is reported against baseline async methods such as using a parameter server. Second, using AlexNet as a benchmark is not informative at all. AlexNet looks very different from any SOTA image recognition model, and in particular it has many fewer layers, which is especially relevant to the discussion in 6.3. It also uses lots of fully-connected layers which affect the compute/communication ratios in ways that are not relevant to most interesting architectures today.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512587411, "id": "ICLR.cc/2017/conference/-/paper432/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper432/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper432/AnonReviewer3", "ICLR.cc/2017/conference/paper432/AnonReviewer1", "ICLR.cc/2017/conference/paper432/AnonReviewer2"], "reply": {"forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512587411}}}, {"tddate": null, "tmdate": 1480560976632, "tcdate": 1480560976628, "number": 2, "id": "rJFUZf6Ml", "invitation": "ICLR.cc/2017/conference/-/paper432/pre-review/question", "forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "signatures": ["ICLR.cc/2017/conference/paper432/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper432/AnonReviewer2"], "content": {"title": "Overlapping techniques already in existing tools", "question": "The overlap of computation and communication appears to be already handled by existing tools, e.g. TensorFlow(Dean et.al) and MXNet(Chen et.al.) Would be nice to have a discussion and comparison with these approaches"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481718229832, "id": "ICLR.cc/2017/conference/-/paper432/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper432/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper432/AnonReviewer3", "ICLR.cc/2017/conference/paper432/AnonReviewer2", "ICLR.cc/2017/conference/paper432/AnonReviewer1"], "reply": {"forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481718229832}}}, {"tddate": null, "tmdate": 1480269949023, "tcdate": 1480269949019, "number": 1, "id": "HyrFgjufx", "invitation": "ICLR.cc/2017/conference/-/paper432/pre-review/question", "forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "signatures": ["ICLR.cc/2017/conference/paper432/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper432/AnonReviewer3"], "content": {"title": "Baselines", "question": "Paper refers to Dean & al and Chen & al as closest related works, yet doesn't appear to provide experimental results against them. Would it be possible to include these baseline figures?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481718229832, "id": "ICLR.cc/2017/conference/-/paper432/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper432/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper432/AnonReviewer3", "ICLR.cc/2017/conference/paper432/AnonReviewer2", "ICLR.cc/2017/conference/paper432/AnonReviewer1"], "reply": {"forum": "rkFd2P5gl", "replyto": "rkFd2P5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481718229832}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478290626415, "tcdate": 1478290545348, "number": 432, "id": "rkFd2P5gl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkFd2P5gl", "signatures": ["~Jeff_Daily1"], "readers": ["everyone"], "content": {"title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "pdf": "/pdf/aeadd246d34d90d3e704cad25d7c6850ee199c18.pdf", "TL;DR": "Overlapping communication and computation for distributed gradient descent.", "paperhash": "daily|leveraging_asynchronicity_in_gradient_descent_for_scalable_deep_learning", "keywords": ["Deep learning"], "conflicts": ["wsu.edu", "pnnl.gov"], "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel"], "authorids": ["jeff.daily@pnnl.gov", "abhinav.vishnu@pnnl.gov", "charles.siegel@pnnl.gov"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}