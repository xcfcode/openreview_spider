{"notes": [{"id": "Wi5KUNlqWty", "original": "LjVtpB8jIg9", "number": 1028, "cdate": 1601308116173, "ddate": null, "tcdate": 1601308116173, "tmdate": 1611607627329, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "yIFsay5YCxn", "original": null, "number": 1, "cdate": 1610040502828, "ddate": null, "tcdate": 1610040502828, "tmdate": 1610474109808, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Two reviewers are very positive about this paper and recommend acceptance, one indicates rejection and one is on the fence. Although all referees appreciate the extensive experiments and analysis presented in the paper, their main concerns are related to the limited superiority of the method wrt state of the art [R1], seemingly arbitrary choices and questionable assumptions [R4]. The rebuttal adequately addresses R1's concerns by highlighting statistical significance of the results, and partially covers R4's concerns. Although the proposed approach may be perceived as incremental [R1, R2, R3, R4], the authors argue that introducing self-supervision to graph attention is not trivial, and emphasize their findings on how/when this is beneficial. Moreover, R2 and R3 acknowledge that the contribution of the paper holds promise, is worth exploring, and may be useful to the research community. Most reviewers are satisfied with the answers in the rebuttal. After discussion, three referees lean towards acceptance and the fourth reviewer does not oppose the decision. I agree with their assessment and therefore recommend acceptance. Please do include your comments regarding the choice of average degree and homophily in the final version of paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040502815, "tmdate": 1610474109792, "id": "ICLR.cc/2021/Conference/Paper1028/-/Decision"}}}, {"id": "TX4JV7-EeLC", "original": null, "number": 1, "cdate": 1603528995076, "ddate": null, "tcdate": 1603528995076, "tmdate": 1606928226280, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Review", "content": {"title": "interesting method for graph attention network; thorough experiments; need to add a picture describing the general framework", "review": "********Summary\nIn this paper, they introduced self-supervised graph attention network (SuperGAT), which is claimed to perform well in noisy graphs. They used information in the edges as an indicator of importance of relations in the graph, then they learn the relational importance using self-supervised attention. After learning the attentions values using their self-supervised method, they can predict the likelihood of an edge between nodes. They worked on two popular attention mechanisms: GO and DP, and showed in their experiments DP has better performance than GO for link prediction task. And Go has better performance in label-agreements between nodes. The other question they answered in their experiments was what the best attention model is to choose. They introduce a recipe based on two graph characteristics: homophily and average degree.\n\n********Positives\n- One thing I liked about this paper was thorough and neat experiments. I enjoyed the way they designed their experiments by mentioning several important questions followed by their answered backed up with their experiments. They used two attention mechanism as the base, then applied their method on these two methods for link prediction and label-agreements tasks and compare their results.\n\n- I also liked that they examined their recommendation for the choice of attention model on real world datasets, and their answer for real-word data was almost similar to synthetics data.\n\n\n- The paper was well-organized and well-written. They clearly explained their method.\n\n********Notes\n- I would recommend adding a picture showing their architecture and compare it with other two attention models\n\n- I sort-of understand the reading as to why \"GO learns label-agreement better than DP.\" Based on the argument on page 6. A strong argument would be helpful to explain why \"DP predicts edge presence better than GO.\"\n\n- (minor note:)No need the parenthesis in this sentence line 8: \"Interestingly, for datasets (CS, Physics, Cora-ML, and Flickr) in which\"\n\n********Reason to accept\nI am in general positive about this paper. The innovation is not significant; however, their experiments were interesting, and they prove how well their method works well empirically. I think this research will be useful for people in this area.\n\n******* After Rebuttal\nI have read the author's response\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128832, "tmdate": 1606915800897, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1028/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Review"}}}, {"id": "nYsyAOUSMI6", "original": null, "number": 10, "cdate": 1606101769049, "ddate": null, "tcdate": 1606101769049, "tmdate": 1606182711583, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment", "content": {"title": "Summary of Revision", "comment": "To all readers, we uploaded the revised paper last week based on reviewers' comments. The revised parts are highlighted in red. In summary, we added the following:\n- Figure 1 to illustrate attention mechanisms of all SuperGAT variants and the original GAT (Veli\u010dkovi\u0107 et al., 2018)\n- Design rationale of SuperGAT MX \n-  Explanation about the sparsity of real-world graphs in response to the limitation of our model for dense graphs\n- Justification of our choice of label-agreement as ground-truth of importance using theoretical analysis of deep GATs (Wang et al., 2019)\n- More details about why we chose the average degree and homophily as the main variables of RQ3 experiments\n- More clear description of t-SNE plots of input graphs in appendix A.3\n\nAgain, thank you for the constructive feedback. If there is an issue that has not been addressed, we would be happy to discuss it."}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wi5KUNlqWty", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1028/Authors|ICLR.cc/2021/Conference/Paper1028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864505, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment"}}}, {"id": "iMkhL_qmEBe", "original": null, "number": 4, "cdate": 1605516381117, "ddate": null, "tcdate": 1605516381117, "tmdate": 1606182699317, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank the reviewers for insightful and constructive feedback. We first answer two issues raised by multiple reviewers: novelty of our models and justification of focusing on two graph properties.\n\n### Contributions and Novelty\n\nR1, R2, and R4 raised an issue of limited novelty of SuperGAT.\n\nOur model improves graph attention by employing link prediction for self-supervision. Each of these building blocks is not novel, but combining them to give self-supervision to attention from edges, is not trivial. This is because simply adding the self-supervision task to the graph attention mechanism does not produce better results (RQ 1-2), and the resulting model performance varies across datasets (RQ 3-4). In addition to the model, this paper presents insightful findings of when and how to use self-supervision for graph attention. We believe these in-depth analyses and insights are important contributions to the community of researchers and practitioners working with graph neural nets. We hope the reviewers will support this view.\n\n### Focusing on Avg. Degree and Homophily\n\nR1 and R4 asked about our choice of two graph properties: the average degree and homophily, among many graph properties (e.g., diameter, degree sequence, degree distribution, average clustering coefficient).\n\nThere are many properties that can characterize graphs. Among them, we choose average degree and homophily as the main variables of our study because they determine the quality and quantity of labels in our self-supervised task (i.e., link prediction). Label quality and quantity directly impact the result of supervised learning [1]. Below is a sentence from our main paper, section 4, RQ3.\n\n\u201cFrom the perspective of supervised learning of graph attention with edge labels, the quality of the result depends on how noisy labels are (i.e., how low the homophily is) and how many labels exist (i.e., how high the average degree is).\u201d\n\nWe revised the explanation under RQ3 to address this concern and clarify our reasoning. Please see the sentences in red on Page 5 of the revised paper.\n\nThere are also practical reasons to use the average degree and homophily. In RQ3, we experiment with various synthetic graphs in the space created by two properties and map the result of real-world graphs. There are three considerations in choosing graph properties for this experiment. First, the graph property can be computed efficiently even for large graphs. Second, there should be an algorithm that can generate graphs by controlling the property of interest only. Third, the property should be a scalar value because if the synthetic graph space is too wide, it would be impossible to conduct an experiment with sufficient coverage. Average degree and homophily satisfy the above conditions and are suitable for our experiment, unlike some of the other graph properties.\n\nUpon reading the reviews, we analyzed two scalar properties, diameter and average clustering coefficient, which reviewers explicitly mentioned, but we could not find a meaningful pattern for the best graph attention model in the space of diameter and average clustering coefficient. In future work, we will explore and analyze some of the other graph properties.\n\n[1] David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive label noise. arXiv preprint arXiv:1705.10694, 2017."}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wi5KUNlqWty", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1028/Authors|ICLR.cc/2021/Conference/Paper1028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864505, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment"}}}, {"id": "efk4WS9qL7", "original": null, "number": 8, "cdate": 1605516589128, "ddate": null, "tcdate": 1605516589128, "tmdate": 1605516589128, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "oGOSyw_tZGG", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment", "content": {"title": "Response to R4", "comment": "Thank you for your review and constructive feedback. We addressed questions about novelty and the choice of graph properties in the [general response](https://openreview.net/forum?id=Wi5KUNlqWty&noteId=iMkhL_qmEBe). We provide answers to your individual questions below.\n\n> In RQ1, the authors claim that \u201cideal node representation can be generated by aggregating only neighbors with the same label\u201d. As the neighborhood information of a node can be also informative when predicting node labels in certain cases, a good node representation does not necessarily need to only aggregates neighbors of the same labels, which makes the proposed method that uses KL divergence to compare label-agreement and graph attention questionable. I suggest the authors to provide more justification on this assumption.\n\nIn the node classification task, we expect that the GNN produces the same representation for nodes with the same label. Mixing neighbors with different labels has the effect of mixing representations that should be different. As theorem 1 in [3] says, if we stack infinite GAT layers, node representations in the connected component will converge to the same value. If there is an edge between nodes with different labels, it will be hard to distinguish the two corresponding labels with GAT of sufficiently many layers. Based on this theorem, although we are using finite-depth GNNs, we argue that the label-agreement distribution is a simple and effective ground-truth of graph attention. \n\nPlease see the red text in RQ1 on Page 4 of the revised paper. \n\n[3] Wang, Guangtao, et al. \"Improving graph attention networks with large margin-based constraints.\" arXiv preprint arXiv:1910.11945 (2019).\n\n> RQ2\u2019s primary goal is to understand how different graph attention methods perform for the link prediction task, it would be better if the authors can justify why they didn\u2019t conduct experiments where only link prediction (self-supervised) loss is used and discard the node classification task.\n\nSuperGAT is focused on the node classification task, in keeping with the focus of the GAT. SuperGAT uses link prediction experiments to understand how attention learns the relational importance from edges for the node classification, but not for the link prediction itself.\n\nFigure 3 in the revised paper shows the results of changing the mixing coefficient $\\lambda_E$. These results show a consistent pattern of significant decrease in the node classification performance as the mixing coefficient increases, which is equivalent to giving a large weight to the link prediction task. So we can extrapolate that when only the link prediction loss is used, the representation learned from the model would be quite poor.\n\n> Another minor question is that why do the authors add an activation function for eij,DP in Eqn. 4, given that eij,DP is already a dot product that indicates the weight of a link. It would be better if the authors can elaborate more on the design rationale\n\nSuperGAT MX (Eq. 4) multiplies GO and DP attention with the sigmoid. The motivation of this form comes from gating mechanisms such as GRU. Since DP attention with the sigmoid represents the probability of an edge, it can softly drop neighbors that are not likely linked while implicitly assigning importance to the remaining nodes. This also prevents the attention value from being highly dependent on DP, the variance of which is larger than those of GO.\n\nWe revised this part to be more clear.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wi5KUNlqWty", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1028/Authors|ICLR.cc/2021/Conference/Paper1028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864505, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment"}}}, {"id": "cLmMGAi42rT", "original": null, "number": 7, "cdate": 1605516552338, "ddate": null, "tcdate": 1605516552338, "tmdate": 1605516552338, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "TX4JV7-EeLC", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment", "content": {"title": "Response to R3", "comment": "Thank you for your review and positive feedback. \n\n> I would recommend adding a picture showing their architecture and compare it with other two attention models\n\nWe added a figure of the model in the revised paper. Figure 1 contains the attention mechanism of all variants of SuperGAT and the original GAT.\n\n> No need the parenthesis in this sentence line 8: \"Interestingly, for datasets (CS, Physics, Cora-ML, and Flickr) in which\"\n\nThank you R3. We fixed this issue.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wi5KUNlqWty", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1028/Authors|ICLR.cc/2021/Conference/Paper1028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864505, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment"}}}, {"id": "4HdyYiYniKR", "original": null, "number": 6, "cdate": 1605516513342, "ddate": null, "tcdate": 1605516513342, "tmdate": 1605516513342, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "GUGjDIJ7MXm", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment", "content": {"title": "Response to R2", "comment": "Thank you for your review and positive feedback. We addressed the first weakness about novelty in the [general response](https://openreview.net/forum?id=Wi5KUNlqWty&noteId=iMkhL_qmEBe). We provide answers to the second weakness and questions.\n\n> In Appendix, A.3, the description and discussion for t-SNE plots is limited or absent. It would be better to add more details to it [\u2026] Also, how the results in the subfigures differ in terms of representations. It is difficult to get any insights from these plots. [...] I would like a comment and clarification from the authors regarding Appendix A.3 Figure 5 (t-SNE plots), even though it is not in the main paper submission.\n\nAppendix A.3. is about t-SNE plots of input features of synthetic graphs, and it is not a qualitative evaluation of the learned node representations. We put in this figure to show what the synthetic datasets look like, as they are not familiar to readers in our field compared to the benchmark datasets. So we are simply showing some (small) examples of the synthetic graphs, to illustrate how the average degree and homophily vary according to $\\delta$ and $p_{in}$.\n\nSorry for the confusion. We added a more detailed description in Appendix A.3.\n\n> My understanding is that the authors are going to release the code upon acceptance, is this correct? In the repository that the code will be released, it will be useful to also add links to all 17 public datasets to ease research in the field.\n\nWe have already uploaded our codes in the supplementary material. We will open our GitHub link if this paper gets accepted. This code contains links to all 17 public datasets, thanks to the open source communities in our research field: PyTorch Geometric & Open Graph Benchmark.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wi5KUNlqWty", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1028/Authors|ICLR.cc/2021/Conference/Paper1028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864505, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment"}}}, {"id": "XV-2nCSD0zx", "original": null, "number": 5, "cdate": 1605516476658, "ddate": null, "tcdate": 1605516476658, "tmdate": 1605516476658, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "bxd5kTltVEU", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment", "content": {"title": "Response to R1", "comment": "Thank you for your review and constructive feedback. We addressed two first bullets in the [general response](https://openreview.net/forum?id=Wi5KUNlqWty&noteId=iMkhL_qmEBe). We provide answers to your questions below.\n\n> In all the experiments that the authors perform, SuperGAT\u2019s performance is only slightly better than older models (e.g. in Table 2, difference between an accuracy of 72.5 vs 72.6 can hardly be considered superior). And even then, SuperGAT does not have the best performance across all datasets.\n\nWhile SuperGAT does not outperform all baselines for all datasets, self-supervision contributes to a statistically significant improvement in 12 out of 17 real-world datasets. Moreover, inconsistency in performance improvement is shown in recent GNN literature including GAT, and in this paper, we reveal how that inconsistency can be explained by focusing on the characteristics of the input graph (RQ 3-4).\n\n> The number of negative cases is a function of the number of edges and not the number of nodes. If a graph contains all possible edges, then the number of all possible negative cases is zero, no matter how many nodes there are.\n\nThe number of negative edges is a function of both the number of edges and nodes: $|V|^2 - |E|$ for $G = (V, E)$. SuperGAT is capable of modeling graphs that are sparse with a sufficiently large number of negative samples (i.e., $|V \\times V| \\gg |E|$), but this is generally not a problem because most real-world graphs are sparse [2]. For dense (or fully connected) graphs, it would be challenging to draw negative samples. These are a special class of graphs and rarely seen in the datasets in the graph neural network research community. We discuss this limitation in the paper.\n\n[2] Chung, Fan. \"Graph theory in the information age.\" Notices of the AMS 57.6 (2010): 726-732.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wi5KUNlqWty", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1028/Authors|ICLR.cc/2021/Conference/Paper1028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864505, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Comment"}}}, {"id": "bxd5kTltVEU", "original": null, "number": 2, "cdate": 1603815492638, "ddate": null, "tcdate": 1603815492638, "tmdate": 1605024548241, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Review", "content": {"title": "The authors present a new self-supervised attention mechanism for graph neural networks and conduct extensive experiments. The analysis and the design however lack some depth and it is uncertain how much this work enhances our understanding (or ability) to use attention in graphs. ", "review": "This paper proposes a new attention mechanism SuperGAT (with various flavours) for graph neural networks that is self-supervised. They exploit the presence/absence of an edge between a pair of nodes to guide the attention. The authors then make the observation that the homophily and average degree of a graph influence the design of the attention mechanism.  Extensive experiments are shown, where the various versions of SuperGAT are tested on 17 real-world dataset and many synthetic ones, and these results are compared against other state-of-the-art models (including the original GAT work). \n\nThe paper is well-written and it is clear that the authors have done extensive experimentation to test their hypothesis. \nHowever, the paper has some weaknesses that I try to summarize below.\n- To obtain SuperGAT, the authors have made some tweaks made to the original GAT formulation. These tweaks are minor and are not surprising or inspired from a deep/novel insight.  \n\n- The choice of studying two graph properties homophily and average degree seem arbitrary. What is the reasoning behind these properties? Were there other properties (e.g. diameter, degree sequence, ...etc) that the authors have studied that did not yield good results? This is particularly of interest due to the fact that experiments do not entirely support/explain the importance of these two properties, such as in the case of Flickr and Crocodile datasets.\n\n- The above reasons would still not be a major disadvantage had the experimental results shown strong superiority of SuperGAT over previous models. In all the experiments that the authors perform, SuperGAT\u2019s performance is only slightly better than older models (e.g. in Table 2, difference between an accuracy of 72.5 vs 72.6 can hardly be considered superior). And even then, SuperGAT does not have the best performance across all datasets.\n\nProposition 1 and its proof are interesting contributions of this work, but they may not be enough. Perhaps instead of concentrating on performance superiority, the authors can look at the explainability aspect of their proposed architecture and look deeper into other graph properties that may guide the design/use of self-supervision.\n\nAnother minor comment: on Page 3, the authors say \u201cif the number of nodes is large, it is not efficient to use all possible negative cases\u201d. The number of negative cases is a function of the number of edges and not the number of nodes. If a graph contains all possible edges, then the number of all possible negative cases is zero, no matter how many nodes there are. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128832, "tmdate": 1606915800897, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1028/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Review"}}}, {"id": "GUGjDIJ7MXm", "original": null, "number": 3, "cdate": 1603886769523, "ddate": null, "tcdate": 1603886769523, "tmdate": 1605024548181, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Review", "content": {"title": "Recommendation for Acceptance", "review": "Summary:\n===========\nThe paper provides an interesting direction for improving the Graph Attention Networks. More specifically, the authors propose a self-supervised graph attention network (SuperGAT) designed for noisy graphs. They encode positive and negative edges so that SuperGAT learns more expressive attention. They focus on two characteristics that influence the effectiveness of attention and self-supervision: homophily and average degree. They show the superiority of their method (4 variations) by comparing it with many state-of-the-art methods, in 144 synthetic datasets (with varying homophily and average node degree) and 17 real-world datasets (again with various ).\n\n\nReasons for score:\n===========\nOverall, I vote for accepting. I find very interesting the idea of using self-supervision to improve graph attention networks and the experiments are nicely done and convincing. The authors do an impressive work to include as much information and results as they can in the given space.\n\n\nStrengths:\n===========\n- The paper is about an interesting problem in the ICLR community. Graph Attention Networks have gained a lot of attention in the recent years from researchers in the field of graph and node representations with applications in node classification and link prediction. The idea of adding the advancements in recent direction of self-supervised learning to improve the learnt representations seems very promising.\n- The authors have done a great job in the structure and the presentation of the paper. The paper is well-written and especially the sections Experiments and Results are well-structured and contain a lot of packed details in the design and the outcome of the experiments. More specifically, Figure 4 stands out as in a very limited space contains the information for the best performed model in all 144+17 graphs!\n- The contributions of this work include the proposed method (GANs with self-supervision), but also an analysis for the selection of the best model depending on two important features of the graph (homophily and average degree).\n- The authors compare their method with all state-of-the-art methods and also four variations of their own model and exhaust their evaluation by testing the performance in 144 synthetic graphs and 17 real-world datasets (including the benchmark datasets that are usually being used in this domain).\n\n\nWeaknesses:\n===========\n- The proposed method uses two known graph attention mechanism as building blocks, they use negative sampling and they add cross-entropy loss for all node labels and self-supervised graph attention losses for all layers. These building blocks and mechanisms are known in the literature, and as a result the proposed method adds incremental novelty compared to the related works.\n- In Appendix, A.3, the description and discussion for t-SNE plots is limited or absent. It would be better to add more details to it, as for example why this is a good representation and how the representations improve or not based on the hyperparameters. Also, how the results in the subfigures differ in terms of representations. It is difficult to get any insights from these plots.\n\n\nQuestions during rebuttal:\n===========\n- Overall, my recommendations for more analysis and insights of the results are all responded from the Appendices. I would like a comment and clarification from the authors regarding Appendix A.3 Figure 5 (t-SNE plots), even though it is not in the main paper submission.\n- My understanding is that the authors are going to release the code upon acceptance, is this correct? In the repository that the code will be released, it will be useful to also add links to all 17 public datasets to ease research in the field. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128832, "tmdate": 1606915800897, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1028/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Review"}}}, {"id": "oGOSyw_tZGG", "original": null, "number": 4, "cdate": 1603970680368, "ddate": null, "tcdate": 1603970680368, "tmdate": 1605024548121, "tddate": null, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "invitation": "ICLR.cc/2021/Conference/Paper1028/-/Official_Review", "content": {"title": "R4", "review": "In this paper, the authors propose an enhanced GAT model named SuperGAT by adding link prediction as an auxiliary task when conducting node classification and compare different methods of attention forms. The authors have also conducted evaluation based on both synthetic datasets and real-world datasets to analyze how different attention methods perform on various data and task types. In general, the paper is written well and easy to follow. However, there are still several issues the authors need to address:\n\nFirst, the authors need to better justify the novelty of the proposed method. The claimed self-supervision task can be considered as general link prediction task where the attention weights is used as features. Though they authors propose two new types of attention forms, i.e., 1) scaled dot-product and 2) mixed GO and DP, they are normalized and combined version of existing attention mechanisms. I suggest the authors to better justify the novelty of the proposed technique and how they differ from existing work.\n\nAnother major concern is the experiment settings. It is good to see that the authors raise several research questions to guide experiment design. However, some assumptions in these research questions are questionable. For example, in RQ1, the authors claim that \u201cideal node representation can be generated by aggregating only neighbors with the same label\u201d. As the neighborhood information of a node can be also informative when predicting node labels in certain cases, a good node representation does not necessarily need to only aggregates neighbors of the same labels, which makes the proposed method that uses KL divergence to compare label-agreement and graph attention questionable. I suggest the authors to provide more justification on this assumption. RQ2\u2019s primary goal is to understand how different graph attention methods perform for the link prediction task, it would be better if the authors can justify why they didn\u2019t conduct experiments where only link prediction (self-supervised) loss is used and discard the node classification task. In RQ3, the authors hypothesize that \u201cdifferent graph attention will have different abilities to model graphs under various homophily and average degree\u201d. This is probably true. However, given the so many graph properties (e.g., degree distribution, graph diameter, and average clustering coefficient) and model configuration (e.g., # of layers and task type), it is unclear why the authors choose these two controlled variables and why they believe they are the most important ones. I suggest the authors provide more rationale on how they choose the controlled variables and how other factors may impact model performance.\n\nAnother minor question is that why do the authors add an activation function for $e_{ij, DP}$ in Eqn. 4, given that $e_{ij, DP}$ is already a dot product that indicates the weight of a link. It would be better if the authors can elaborate more on the design rationale.\n\nIn summary, I think the authors focuses on an interesting problem but need to further address the issues listed above.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1028/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1028/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "authorids": ["~Dongkwan_Kim1", "~Alice_Oh1"], "authors": ["Dongkwan Kim", "Alice Oh"], "keywords": ["Graph Neural Network", "Attention Mechanism", "Self-supervised Learning"], "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "one-sentence_summary": "We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|how_to_find_your_friendly_neighborhood_graph_attention_design_with_selfsupervision", "supplementary_material": "", "pdf": "/pdf/e6fa500cd5a6276fe2d346ccffdc25ddf6cf18d9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nkim2021how,\ntitle={How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision},\nauthor={Dongkwan Kim and Alice Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Wi5KUNlqWty}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Wi5KUNlqWty", "replyto": "Wi5KUNlqWty", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1028/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128832, "tmdate": 1606915800897, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1028/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1028/-/Official_Review"}}}], "count": 12}