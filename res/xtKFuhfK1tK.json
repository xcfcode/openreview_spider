{"notes": [{"id": "xtKFuhfK1tK", "original": "hLPyDRxYqLx", "number": 2045, "cdate": 1601308225256, "ddate": null, "tcdate": 1601308225256, "tmdate": 1614985744750, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1UNxMOZqKqW", "original": null, "number": 1, "cdate": 1610040392330, "ddate": null, "tcdate": 1610040392330, "tmdate": 1610473986810, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper introduces a new locality-aware importance weighted sampling procedure for distributed training of GNNs. While the paper is interesting, the reviewers raised some fundamental concerns about it.\n\nThe focus on the paper is on scalable methods and the experiments or only run on medium-size datasets(<2m nodes). For such a paper larger scale experiments are expected.\n\nFurthermore, the novelty of the paper is limited.\n\nOverall, the paper is below the high acceptance bar of ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040392317, "tmdate": 1610473986791, "id": "ICLR.cc/2021/Conference/Paper2045/-/Decision"}}}, {"id": "oQTzbffAw0", "original": null, "number": 4, "cdate": 1604518427955, "ddate": null, "tcdate": 1604518427955, "tmdate": 1606820142083, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Official_Review", "content": {"title": "Importance weighted sampling for GNN distributed training", "review": "This work considers the challenge of distributed training for GNNs. The approach is a locality-aware importance weighted sampling procedure. I was not given much time to read the paper but it seems like a decent contribution, albeit too minor of a contribution to the existing literature to be considered a bonafide research paper.\n\n### Quality\n\n- There is nothing clearly wrong with the paper. I did not have time to go through all the equations but I can believe the approach.\n\n### Clarity\n\n- The writing is clear and the approach is well described.\n\n### Originality\n\nThere is not a whole lot of originality. Prior work (appropriately cited) has consider a similar approach. The main difference is the locality of sampling to avoid communication.\n\n### Significance of this work\n\n- Important topic, not exciting as a research paper. \n\n### Pros\n\n- Scalability of GNNs is a very important topic that deserves more attention.\n\n- The writing is good; the reader can quickly understand the approach and the main points of the paper. It also helps that the approach is well-known and relatively simple.\n\n### Cons\n\n- Experiments: For a work studying distributed training over graphs with \"billions of nodes\", it is certainly disappointing to see that the datasets contain up to 1.1M nodes. \n\n- The work seems like a direct application of importance weighting and stratified sampling to sampling the neighborhood of node in a GNN. Locality-aware importance sampling is a common approach used in industry, and rather trivial as a method.\n\n\n### Other comments\n\n- Regarding the bound V, it would be nice to get a sense of its magnitude. It does not look very efficient. What if we performed a push sampling operation (where the node that has x_j will sample it with probability ||x_j||^2  and push it to the servers that need it) rather than the proposed pull sampling (where each node requests the samples)? That way we don't need to guess or bound the value of ||x_j||^2. Just a quick thought. \n\n-------\n\nThe rebuttal did not meaningfully addressed my concerns.\n\nApologies to the authors for not providing a reference for my comment on approaches for reducing communication in graph-optimization methods being widely known in industry. GraphLab is an example (https://en.wikipedia.org/wiki/GraphLab)\n\n- Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin and J. Hellerstein. GraphLab: A New Framework for Parallel Machine Learning. In the 26th Conference on Uncertainty in Artificial Intelligence (UAI), Catalina Island, USA, 2010\n- Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin and Joseph M. Hellerstein (2012). \"Distributed GraphLab: A Framework for Machine Learning and Data Mining in the Cloud.\" Proceedings of Very Large Data Bases (PVLDB).\n- Joseph Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, Carlos Guestrin (2012). \"PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs.\" Proceedings of Operating Systems Design and Implementation (OSDI).\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2045/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2045/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105290, "tmdate": 1606915770310, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2045/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2045/-/Official_Review"}}}, {"id": "_0zUho_iufj", "original": null, "number": 5, "cdate": 1606175920428, "ddate": null, "tcdate": 1606175920428, "tmdate": 1606175920428, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Official_Comment", "content": {"title": "Added more experimental results", "comment": "We appreciate the valuable feedback from all the reviewers. To address the concerns and improve the clarity, we made the following changes to the paper. All the changes are marked in yellow in the revised version.\n\n\n1. To address the concern of scalability of our method from Reviewer4 and 5, we added a new set of results on eight machines. (Section 5.3)  \n\n\n2. To address the concern of applicability of our method to other sampling methods from Reviewer4, we incorporated our skewed sampling method into the subgraph sampling procedure in GraphSAINT. The new set of results is collected with GraphSAINT.  (Section 5.3)\n\n\n3. To clarify the setting of our experiments and address the concern of novelty from Reviewer4, we conducted an experiment with centralized storage of the graph on CPU and show that splitting nodes' features among GPU is more efficient. ('Comparison with Centralized Training' in Section 5.2) \n\n\n4. To address the concern of missing baseline from Reviewer1, we added the training loss and accuracy with only local neighbor aggregation in Figure 2,3,4,5. The results show that this baseline has noticeable accuracy losses. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2045/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xtKFuhfK1tK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2045/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2045/Authors|ICLR.cc/2021/Conference/Paper2045/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852902, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2045/-/Official_Comment"}}}, {"id": "wE5octuLDSA", "original": null, "number": 4, "cdate": 1606174374151, "ddate": null, "tcdate": 1606174374151, "tmdate": 1606174374151, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "oQTzbffAw0", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Official_Comment", "content": {"title": "Response to Reviewer5", "comment": "Q1: it is certainly disappointing to see that the datasets contain up to 1.1M nodes. \n\nOur experiments focus on a single-machine multi-GPU platform. On such a platform, this graph is large enough since each GPU cannot hold the feature vectors and intermediate data of the entire graph.  We also added a new set of results with a graph with 1.6M nodes and 132M edges.  \n\nQ2: Locality-aware importance sampling is a common approach used in industry, and rather trivial as a method.\n\nWe are not aware of previous works using this method for reducing communication in GCN training. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2045/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xtKFuhfK1tK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2045/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2045/Authors|ICLR.cc/2021/Conference/Paper2045/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852902, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2045/-/Official_Comment"}}}, {"id": "e07FUIxA0r3", "original": null, "number": 2, "cdate": 1605668674358, "ddate": null, "tcdate": 1605668674358, "tmdate": 1605672586939, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "xtPcPNvrmt", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "Q1: The experiments only explore 4 workers with a compression ratio less than 7. How does the proposed method scale with more workers and larger compression ratios?\n\nAlthough our experiments focus on multiple GPUs on a single machine, our method is applicable to distributed systems with multiple machines. We tested our method on 8 machines and the performance follows a similar pattern. The speedup is even higher since communication is more expensive. We will add the result in the updated version. \n\nQ2: How will the idea work for other sampling methods? The authors compared the proposed method with the LADIES sampling method. Why is the original linear weighted sampling method not compared with the proposed skewed linear weighted sampling?\n\nNote that LADIES, FastGCN and GraphSAINT all use linear weighted sampling. The only difference is the definition of $N(S_l)$ in Formula (7). Therefore, our method is applicable to all these sampling methods. We incorporated our method into GraphSAINT and tested with the graph used in GraphSAINT paper. The performance and accuracy follow a similar pattern as LADIES. We will add the result in the updated version. \n\nQ3: How the graph is distributed in experiments needs more clarification.\n\nThe distribution of the graph is shown in Figure1 and is described in Section3 \"Distributed Training of GCN\". \n\nQ4: Naive distributed training is certainly inefficient involving too much communication of neighborhood nodes' features.\n\nAs we described in Section3 \"Distributed Training of GCN\", we target multi-GPU platforms where each GPU is not large enough to hold the entire graph. We also target multi-machine platforms where the graph is too large to be duplicated on each machine.  The alternatives described in 4.1) and 4.2) do not work for these cases. \n\nQ5: the paper lacks nolvety...\n\nTo the best of our knowledge, our paper is the first to consider reducing the communication overhead of nodes' features in the setting described in Q4. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2045/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xtKFuhfK1tK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2045/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2045/Authors|ICLR.cc/2021/Conference/Paper2045/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852902, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2045/-/Official_Comment"}}}, {"id": "L45TtxWL0A", "original": null, "number": 3, "cdate": 1605669651979, "ddate": null, "tcdate": 1605669651979, "tmdate": 1605669651979, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "-x1PPh8dev", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "Q1:  A good solution should reduce communication costs and try to make the prediction performance as good as possible. However, this method only focuses on the former one.\n\nOur sampling method keeps the estimation variance in the same order as the original sampling method, and thus makes sure the training has the same asymptotic convergence rate. Our experimental results show that our method reduces the communication without impairing the prediction accuracy. \n\nQ2: In the proof of Theorem 1, this paper assumes there exists a constant $D_1$...without any knowledge regarding $D$, the bound for $s$ is useless. \n\nNote that $D$ is a tuning parameter. The point of Formula (12) in Theorem 1 is to show that the scale factor $s$ is dependent on the number of local and remote nodes, instead of being an arbitrary constant value. \n\nQ3: an important baseline is missed. Specifically, the method only using intra-worker neighbor nodes should be used.\n\nThis baseline has significant accuracy losses. We will add the result in the revised version. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2045/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xtKFuhfK1tK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2045/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2045/Authors|ICLR.cc/2021/Conference/Paper2045/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852902, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2045/-/Official_Comment"}}}, {"id": "-x1PPh8dev", "original": null, "number": 1, "cdate": 1603781334398, "ddate": null, "tcdate": 1603781334398, "tmdate": 1605024302124, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Official_Review", "content": {"title": "a new method for distribtued GNN", "review": "This paper proposed a new distributed training method for GNNs. Specifically, unlike traditional distributed training methods for CNNs where data points are independent, nodes in a graph are dependent on each other. Thus, this dependence incurs communication between different workers in the distributed training of GNNs. This paper aims to reduce the communication cost in this procedure. Here, this paper proposed to sample more neighbor nodes within the same worker while reducing the sampling probability for the neighbor nodes on other workers. It also provides some theoretical analysis and conducts the experiments to verify the proposed method. \n\n1. The idea is simple. It is just a trad-off between intra-worker sampling and inter-worker sampling. In fact, it does not address the real challenge in distributed training of GNNs. Even though sampling more intra-worker neighbor nodes can reduce the communication cost, it will impair the prediction performance. A good solution should reduce communication costs and try to make the prediction performance as good as possible. However, this method only focuses on the former one. \n\n2. In the proof of Theorem 1, this paper assumes there exists a constant $D_1$, and further claims that $D$ is small. However, no evidence is provided to verify $D$ is small.  Thus, the claim in Theorem 1 does not hold. Moreover, without any knowledge regarding $D$, the bound for $s$ is useless. \n\n3. Regarding experiments, an important baseline is missed. Specifically, the method only using intra-worker neighbor nodes should be used. Otherwise, the current experimental results cannot support the efficacy of the proposed method. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2045/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2045/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105290, "tmdate": 1606915770310, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2045/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2045/-/Official_Review"}}}, {"id": "xtPcPNvrmt", "original": null, "number": 2, "cdate": 1603908540129, "ddate": null, "tcdate": 1603908540129, "tmdate": 1605024302057, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Official_Review", "content": {"title": "It is a well-written paper. But the paper lacks novelty. Many alternative methods were not discussed and compared.", "review": "The paper presents a simple method to reduce the communication for the distributed sampling-based training of GCN. Specifically, communication is reduced by sampling local nodes more and remote nodes less.\n\nThe paper is well written, and motivation has been clearly conveyed. Related works in sampling-based training of GCN are well summarized and categorized. The communication ratio of the proposed skewed linear weighted sampling method explored in experiments is around x1 ~ x7 without noticeable performance degradation.\n\nMajor concerns:\n1) The experiments only explore 4 workers with a compression ratio less than 7. How does the proposed method scale with more workers and larger compression ratios?\n\n2) The proposed sampling method and the original sampling method.\n\nIn Section 4.2, the authors managed to achieve the same sampling variance for linear weighted sampling and the proposed skewed linear weighted sampling, which may partially due to that the variance of the original linear weighted sampling is not optimal. How will the idea of sampling local nodes more frequently work for other sampling methods?\n\nIn experiments, the authors compared the proposed skewed linear weighted sampling with the LADIES sampling method. Why is the original linear weighted sampling method not compared with the proposed skewed linear weighted sampling?\n\n3) It seems that the authors distribute the graph to workers and each worker only holds the local nodes' original data and intermediate activations during training. How the graph is distributed in experiments needs more clarification.\n\n4) The importance of distributed sampling-based training of GCN is not explained. Naive distributed training as in 3) is certainly inefficient involving too much communication of neighborhood nodes' features. However, there seem to be some simple alternatives of 3) as in 4.1) and 4.2).\n\n4.1) Sampling-based training of GCN already makes it feasible to train GCN on large datasets in the single-machine setting. Why can't we just use multiple workers to do the training as in single-machine settings and then average the gradient or average the model periodically like Local SGD [1]?\n\n4.2) Cluster-GCN may also be another alternative as it also makes it feasible to train large GCN in the single-machine setting. Can we make it work for distributed training the same way as in 4.1)?\n\nReferences:\n[1] Stich, Sebastian U. \"Local SGD converges fast and communicates little.\" arXiv preprint arXiv:1805.09767 (2018).\n[2] Chiang, Wei-Lin, et al. \"Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2019.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2045/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2045/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105290, "tmdate": 1606915770310, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2045/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2045/-/Official_Review"}}}, {"id": "YTs-4WBkV0Y", "original": null, "number": 3, "cdate": 1603934127435, "ddate": null, "tcdate": 1603934127435, "tmdate": 1605024301998, "tddate": null, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "invitation": "ICLR.cc/2021/Conference/Paper2045/-/Official_Review", "content": {"title": "A simple approach to speeding up training of GCNs in distributed systems (but it works)", "review": "The paper presents a sampling-based approach to speeding-up training of GCNs in distributed systems. The key step in this task involves exchanging and aggregating messages sent along the edges of the graph. If the nodes of the graph are partitioned between several machines, then exchanging those messages involve costly communication between the nodes. To reduce this communication, the paper introduces a variant of the node sampling approach of Chen et al. (2018b) and Zou et al. (2019), where the probabilities of nodes in other machines are scaled down by some factor s. The approach is evaluated experimentally, showing that it reduces the amount of communication while (essentially) preserving the accuracy.\n\nPROS:\n- The first implementation of sampling-based training for GCNs in the distributed scenario (this is according to the authors, but I am not aware of any prior work of this type either)\n - Experimental evaluation shows significant improvement to the communication cost\n\nCONS:\n- The new sampling process is a relatively simple modification of prior work, so there isn't much conceptual novelty in here\n\nOverall, a solid contribution to the area.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2045/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2045/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks", "authorids": ["~Peng_Jiang4", "masumaakter-rumi@uiowa.edu"], "authors": ["Peng Jiang", "Masuma Akter Rumi"], "keywords": ["Graph Convolutional Networks", "Sampling", "Distributed Training"], "abstract": "Training Graph Convolutional Networks (GCNs) is expensive as it needs to aggregate data recursively from neighboring nodes. To reduce the computation overhead, previous works have proposed various neighbor sampling methods that estimate the aggregation result based on a small number of sampled neighbors. Although these methods have successfully accelerated the training, they mainly focus on the single-machine setting. As real-world graphs are large, training GCNs in distributed systems is desirable. However, we found that the existing neighbor sampling methods do not work well in a distributed setting. Specifically, a naive implementation may incur a huge amount of communication of feature vectors among different machines. To address this problem, we propose a communication-efficient neighbor sampling method in this work. Our main idea is to assign higher sampling probabilities to the local nodes so that remote nodes are accessed less frequently. We present an algorithm that determines the local sampling probabilities and makes sure our skewed neighbor sampling does not affect much to the convergence of the training. Our experiments with node classification benchmarks show that our method significantly reduces the communication overhead for distributed GCN training with little accuracy loss. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|communicationefficient_sampling_for_distributed_training_of_graph_convolutional_networks", "one-sentence_summary": "A new neighbor sampling method for reducing the communication overhead in distributed GCN training. ", "pdf": "/pdf/10e0e318e5d61d3ee7ce8c8051a820d2e9bb468c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sY-G7a9Xy4", "_bibtex": "@misc{\njiang2021communicationefficient,\ntitle={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},\nauthor={Peng Jiang and Masuma Akter Rumi},\nyear={2021},\nurl={https://openreview.net/forum?id=xtKFuhfK1tK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xtKFuhfK1tK", "replyto": "xtKFuhfK1tK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2045/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105290, "tmdate": 1606915770310, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2045/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2045/-/Official_Review"}}}], "count": 10}