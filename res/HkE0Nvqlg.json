{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487267594058, "tcdate": 1478288588067, "number": 379, "id": "HkE0Nvqlg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HkE0Nvqlg", "signatures": ["~Yoon_Kim1"], "readers": ["everyone"], "content": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396551843, "tcdate": 1486396551843, "number": 1, "id": "S1e92f8dx", "invitation": "ICLR.cc/2017/conference/-/paper379/acceptance", "forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The area chair shares the reviewer's opinion and thinks that this is a very solid paper that deserves to be presented at ICLR. The idea is novel, well described and backed up by solid experiments (that show some empirical gains).", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396552339, "id": "ICLR.cc/2017/conference/-/paper379/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396552339}}}, {"tddate": null, "tmdate": 1482525265081, "tcdate": 1482444697888, "number": 4, "id": "rkfjJCtNx", "invitation": "ICLR.cc/2017/conference/-/paper379/public/comment", "forum": "HkE0Nvqlg", "replyto": "HJF0WZf4e", "signatures": ["~Yoon_Kim1"], "readers": ["everyone"], "writers": ["~Yoon_Kim1"], "content": {"title": "review response", "comment": "Thank you for the comments.\n\n- Right, structured attention could be a way of doing neural phrase-based MT (i.e. segmentation attention at the word-level) and neural syntax-based MT (e.g. attend to word + soft parent during decoding). We are exploring this currently.\n\n- Generalizing to other probabilistic models via the matrix-tree formulation is a very interesting direction of future research! \n\n- We found that working in the log semi-field was enough to mitigate numerical issues.\n\n- Sec 4.1 \"... as it has no information about the source ordering\": For the synthetic experiments, we do not employ an LSTM on the encoder side, and hence the baseline model (without any attention) is just a bag-of-words."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599574, "id": "ICLR.cc/2017/conference/-/paper379/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkE0Nvqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper379/reviewers", "ICLR.cc/2017/conference/paper379/areachairs"], "cdate": 1485287599574}}}, {"tddate": null, "tmdate": 1482444827581, "tcdate": 1482444827581, "number": 6, "id": "r1VXl0Y4x", "invitation": "ICLR.cc/2017/conference/-/paper379/public/comment", "forum": "HkE0Nvqlg", "replyto": "ryQybpb4l", "signatures": ["~Yoon_Kim1"], "readers": ["everyone"], "writers": ["~Yoon_Kim1"], "content": {"title": "review response", "comment": "Thank you for the comments.\n\n- Checking the alignments seems like a great idea. Obtaining ground-truth alignments, as noted, could be tricky--we could potentially use an external alignment model and treat that as the ground truth (as in Luong et al. 2015).  We spot-checked a couple of translations ourselves using Google translate, and we further had a native Japanese speaker annotate the example sentence shown in the figure. The alignments from the sigmoid/structured attention model largely agreed with the hand annotation. This was not the case for the simple (softmax) model.\n\n- We were also surprised to find that the pretrained syntactic attention model does not outperform the non-pretrained model for natural language inference. It is possible that the pretrained attention was too strict for this task. We hope to explore this in the future.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599574, "id": "ICLR.cc/2017/conference/-/paper379/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkE0Nvqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper379/reviewers", "ICLR.cc/2017/conference/paper379/areachairs"], "cdate": 1485287599574}}}, {"tddate": null, "tmdate": 1481946178774, "tcdate": 1481946178774, "number": 3, "id": "r1orV4zNg", "invitation": "ICLR.cc/2017/conference/-/paper379/official/review", "forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "signatures": ["ICLR.cc/2017/conference/paper379/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper379/AnonReviewer2"], "content": {"title": "review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network.\n\nUnder this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures.\n\nThe experiments of the paper prove the usefulness of the model in various level \u2014 seq2seq and tree structure etc. I think it\u2019s solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model.\n\nIn sum, I think this is a solid contribution and the approach will benefit the research in other problems.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604264, "id": "ICLR.cc/2017/conference/-/paper379/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper379/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper379/AnonReviewer3", "ICLR.cc/2017/conference/paper379/AnonReviewer1", "ICLR.cc/2017/conference/paper379/AnonReviewer2"], "reply": {"forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604264}}}, {"tddate": null, "tmdate": 1481933265317, "tcdate": 1481933265317, "number": 2, "id": "HJF0WZf4e", "invitation": "ICLR.cc/2017/conference/-/paper379/official/review", "forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "signatures": ["ICLR.cc/2017/conference/paper379/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper379/AnonReviewer1"], "content": {"title": "solid paper", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.\n\nThe paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them. \n\nThis seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?\n\nMinor comments/typos:\n- last paragraph of sec 1: \"standard attention attention\"\n- third paragraph of sec 3.2: \"the on log-potentials\"\n- sec 4.1, Results: \"... as it has no information about the source ordering\" -- what do you mean here?", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604264, "id": "ICLR.cc/2017/conference/-/paper379/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper379/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper379/AnonReviewer3", "ICLR.cc/2017/conference/paper379/AnonReviewer1", "ICLR.cc/2017/conference/paper379/AnonReviewer2"], "reply": {"forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604264}}}, {"tddate": null, "tmdate": 1481916634726, "tcdate": 1481916634726, "number": 1, "id": "ryQybpb4l", "invitation": "ICLR.cc/2017/conference/-/paper379/official/review", "forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "signatures": ["ICLR.cc/2017/conference/paper379/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper379/AnonReviewer3"], "content": {"title": "Interesting, clearly-written paper which proposes to extend attention over latent structures. However, experimental results on two real-world tasks only show small improvements over the baseline \"simple\" attention system.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The authors propose to extend the \u201cstandard\u201d attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.\n\nThe paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the \u201cstructured attention\u201d models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the \u201csimple\u201d attention models, but the techniques are nonetheless interesting.\n\nMain comments:\n1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I\u2019m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.\n2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?\n\nMinor comments:\n1. Typographical error: Equation 1: \u201cp(z | x, q\u201d \u2192 \u201cp(z | x, q)\u201d\n2. Section 3.3: \u201cPast work has demonstrated that the techniques necessary for this approach, \u2026 \u201d \u2192  \u201cPast work has demonstrated the techniques necessary for this approach, \u2026 \u201d\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604264, "id": "ICLR.cc/2017/conference/-/paper379/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper379/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper379/AnonReviewer3", "ICLR.cc/2017/conference/paper379/AnonReviewer1", "ICLR.cc/2017/conference/paper379/AnonReviewer2"], "reply": {"forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604264}}}, {"tddate": null, "tmdate": 1481862984457, "tcdate": 1480799021954, "number": 2, "id": "ry8VXngQe", "invitation": "ICLR.cc/2017/conference/-/paper379/public/comment", "forum": "HkE0Nvqlg", "replyto": "Hy0E3ByQx", "signatures": ["~Yoon_Kim1"], "readers": ["everyone"], "writers": ["~Yoon_Kim1"], "content": {"title": ".", "comment": "Yes, the CRF is fully connected. While it is true that (in general) fully connected means that inference is intractable, there are many special cases where inference is tractable/efficient. Specifically, in this case we can use the inside-outside algorithm on the data structures of Eisner (1996) to obtain the marginal probability of each word's parent in O(n^3) time (with respect to the sentence length). Forward/backward pass through this inference algorithm is shown in appendix B.\n\nAlso, while we've focused on models that admit exact (and tractable) inference, our approach is applicable to approximate inference methods such as loopy belief propagation (since each step of belief propagation is differentiable). For example, differentiating through approximate inference was explored in Gormley (2015), in the context of minimum risk training of higher-order dependency parsers\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599574, "id": "ICLR.cc/2017/conference/-/paper379/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkE0Nvqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper379/reviewers", "ICLR.cc/2017/conference/paper379/areachairs"], "cdate": 1485287599574}}}, {"tddate": null, "tmdate": 1480802081224, "tcdate": 1480799506416, "number": 3, "id": "r1czrhl7g", "invitation": "ICLR.cc/2017/conference/-/paper379/public/comment", "forum": "HkE0Nvqlg", "replyto": "By6-eKJml", "signatures": ["~Yoon_Kim1"], "readers": ["everyone"], "writers": ["~Yoon_Kim1"], "content": {"title": ".", "comment": "Right--the root symbol is not necessary for 4.1, but we kept it for consistency with 4.3. There is no difference in performance when the root symbol is not used.\n\nRegarding Fig 3, we agree that the alignment of \")\" after \"0 13\" is incorrect. Our claim was that closing parentheses generally have more weight on opening parentheses (not necessarily one particular/correct opening parenthesis). But we agree that the claim could be misleading/confusing--we will correct.\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599574, "id": "ICLR.cc/2017/conference/-/paper379/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkE0Nvqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper379/reviewers", "ICLR.cc/2017/conference/paper379/areachairs"], "cdate": 1485287599574}}}, {"tddate": null, "tmdate": 1480720388976, "tcdate": 1480720388972, "number": 3, "id": "By6-eKJml", "invitation": "ICLR.cc/2017/conference/-/paper379/pre-review/question", "forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "signatures": ["ICLR.cc/2017/conference/paper379/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper379/AnonReviewer1"], "content": {"title": "A couple minor questions", "question": "Why is the root symbol necessary in the tree transduction problem in 4.1?\n\nIn Fig 3 (right) the soft alignments seem wrong -- the row for the closing parenthesis \")\" after \"0 13\" seem to be aligned with the wrong closing parenthesis (despite the claim in the caption). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959312113, "id": "ICLR.cc/2017/conference/-/paper379/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper379/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper379/AnonReviewer2", "ICLR.cc/2017/conference/paper379/AnonReviewer3", "ICLR.cc/2017/conference/paper379/AnonReviewer1"], "reply": {"forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959312113}}}, {"tddate": null, "tmdate": 1480707125704, "tcdate": 1480707125699, "number": 2, "id": "Hy0E3ByQx", "invitation": "ICLR.cc/2017/conference/-/paper379/pre-review/question", "forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "signatures": ["ICLR.cc/2017/conference/paper379/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper379/AnonReviewer3"], "content": {"title": "Clique Structure in Syntactic Tree Selection Example", "question": "In Section 3.2, Example 2: when the model is used for Syntactic Tree Selection, the constraints such as ensuring projectivity appear to require constraints that involve all of the z_ij variables. In this case, isn't the CRF clique structure fully connected? Is it still efficient to perform inference in these cases where you have larger cliques in the CRF?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959312113, "id": "ICLR.cc/2017/conference/-/paper379/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper379/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper379/AnonReviewer2", "ICLR.cc/2017/conference/paper379/AnonReviewer3", "ICLR.cc/2017/conference/paper379/AnonReviewer1"], "reply": {"forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959312113}}}, {"tddate": null, "tmdate": 1480692792869, "tcdate": 1480692792863, "number": 1, "id": "Sk-HVzkmg", "invitation": "ICLR.cc/2017/conference/-/paper379/public/comment", "forum": "HkE0Nvqlg", "replyto": "ryabzGkQl", "signatures": ["~Yoon_Kim1"], "readers": ["everyone"], "writers": ["~Yoon_Kim1"], "content": {"title": ".", "comment": "Thank you for your question. Indeed, the sum is not guaranteed to be 1, and for the Sigmoid attention we found this to be ok, but for structured attention we found training to be unstable due to the issue you pointed out. We therefore had to normalize the marginals. We mention this in the appendix (page 14: \"The marginals are globally normalized for additional stability\"), but we agree that this is an important detail. We will move this to the main part.\n\nHope this clarifies things, thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287599574, "id": "ICLR.cc/2017/conference/-/paper379/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkE0Nvqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper379/reviewers", "ICLR.cc/2017/conference/paper379/areachairs"], "cdate": 1485287599574}}}, {"tddate": null, "tmdate": 1480692228723, "tcdate": 1480692228719, "number": 1, "id": "ryabzGkQl", "invitation": "ICLR.cc/2017/conference/-/paper379/pre-review/question", "forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "signatures": ["ICLR.cc/2017/conference/paper379/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper379/AnonReviewer2"], "content": {"title": "questions", "question": "In this framework, in eq1, \\sum_{i=1}^n p(z=i|x,q) equals 1. In eq2, in my understanding, \\sum_{i=1}^n p(z_i=1) is not guaranteed to be 1. Therefore, the final representation computed from the attention can sometimes be as large as n times the vector representations of x. When the sentence is very long and the attention wants to selected many things, will this be a problem? The vector computed from the attention can be sometimes too large? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "pdf": "/pdf/9b890c1db132551890779413703c91b796588de3.pdf", "TL;DR": "Use a graphical model as a hidden layer to perform attention over latent structures", "paperhash": "kim|structured_attention_networks", "conflicts": ["harvard.edu"], "keywords": [], "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "authorids": ["yoonkim@seas.harvard.edu", "carldenton@college.harvard.edu", "lhoang@g.harvard.edu", "srush@seas.harvard.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959312113, "id": "ICLR.cc/2017/conference/-/paper379/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper379/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper379/AnonReviewer2", "ICLR.cc/2017/conference/paper379/AnonReviewer3", "ICLR.cc/2017/conference/paper379/AnonReviewer1"], "reply": {"forum": "HkE0Nvqlg", "replyto": "HkE0Nvqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper379/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959312113}}}], "count": 13}