{"notes": [{"id": "wb3wxCObbRT", "original": "CLOuvU1IS-a", "number": 2209, "cdate": 1601308243289, "ddate": null, "tcdate": 1601308243289, "tmdate": 1616017034393, "tddate": null, "forum": "wb3wxCObbRT", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KcVk6kpKJ8s", "original": null, "number": 1, "cdate": 1610040423210, "ddate": null, "tcdate": 1610040423210, "tmdate": 1610474022187, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "The paper proposes a method to grow deep network architectures over the course of training. The work has been extremely well received and has clear novelty and solid experiment validation."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040423197, "tmdate": 1610474022169, "id": "ICLR.cc/2021/Conference/Paper2209/-/Decision"}}}, {"id": "FGN0ZqCdH9L", "original": null, "number": 1, "cdate": 1603866044550, "ddate": null, "tcdate": 1603866044550, "tmdate": 1607367818901, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Review", "content": {"title": "Reviews", "review": "Summary:\n\n  This paper proposes a NAS-type work for growing a small network to a large network by adding channels and layers gradually. The authors apply the method to both CNN and LSTM networks. \n\nStrong points:\n\n  1. This paper is well-written and shows good results.\n\n  2. The proposed algorithm is sound and effective. E.g. use less wall-time as compared to other NAS approaches.\n\n\nWeak points:\n  \n  1. It seems that the number of channels and number of layers still need to be predefined (the mask size).\n\n\nQuestions:\n\n\n  1. How does the FLOPs reduction translate to runtime saving?\n\n  2. What is the target sparsity u in the experiments?\n\n  3. When growing with layers, does the author observe any middle layer is dropped and then recovered? If so, does it happen frequently? \n\n  4. At section 4.2 and 4.3, what is the size of the channel/layer mask? I believe the author still needs to define the upper bound of the network can grow. If so, does the upper bound affect the optimization?  Or the proposed method gradually expands the mask?\n\n  5. In table 4, I think Efficient-B0 should be taken into consideration as it is a recent representative approach.\n  \nAfter rebuttal:\n\nThe authors' rebuttal addressed all my questions and I upgrade my rating.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101556, "tmdate": 1606915779013, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2209/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Review"}}}, {"id": "rLzfGjbIcJb", "original": null, "number": 2, "cdate": 1603868252791, "ddate": null, "tcdate": 1603868252791, "tmdate": 1606768235173, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Review", "content": {"title": "Big idea, with extensive experiments", "review": "**Pros**:\n\n1. This paper nicely unifies two different classes of approaches (NAS + sparsity) for determining the topology of neural networks. They are combined into a single optimization problem, with binary indicators on network components and connections.\n\n2. Experiments illustrate the behavior of the method. It is good to see that the experiments dig a big deeper than end-result accuracy. For instance, the \"budget-aware growing\" is shown well to work as described by Fig 3.\n\n**Cons**:\n\n3. No attention to random seeding.\n\nThe sparsification dynamics seem likely to change somewhat from one run to the next. The submission does not describe how random seeding was done for training. Multiple runs with different seeds are not shown, and the distribution of accuracies across runs is unknown. Attention to randomness for this kind of training process seems especially important given the variances in results in the Lottery Ticket hypothesis paper.\n\n4. No comparison to simple random baseline.\n\nA large portion of the method consists of a search method over the space of possible sparse networks, combining it with growing the network to get a NAS-like method. It has been observed, though, that in a sufficiently general space of this kind one can randomly sample connections and see high accuracies. So to identify the sources of empirical gains, it is good to consider experimental baselines, such as random sampling, that separate the contribution of the search space and the search method:\n\n  * Xie et. al. \"Exploring randomly wired neural networks for image recognition\"\n  * Li & Talkwaker \"Random search and reproducibility for neural architecture search\"\n  * Yu et. al. \"Evaluating the Search Phase of Neural Architecture Search\"\n  * Radosavovic et. al. \"On Network Design Spaces for Visual Recognition.\"\n\nNote that the submission's method also is randomly choosing connections, through a somewhat involved process that also accounts for the observed sparsity of G during training. The simplest baseline seems to be the \"uniform pruning\" described in Section 4.4. This only ablates part of the method that doesn't seem to meet the same criterion here.\n\n5. Incomplete illustration of the cost/accuracy tradeoff.\n\nThe gold standard for comparison in both sparse-neural-network papers and NAS is to consider the accuracy at a range of different model costs. See for example:\n\n  * Blalock et. al. \"What is the state of neural network pruning?\" Figs 1, 3\n  * (Yang et. al., 2018) Figs 5-9\n\nThis clearly illustrates whether a method is overall better (i.e. produces better models across the entire pareto frontier), or is only better for some ranges or on one metric. For a result like the first one in Table 5 in the submission, it is unclear which model is better: they may simply be considering different points on the same cost/accuracy curve.\n\n6. As a less important aside, \"budget-aware growing\" seems to be an ad-hoc reinvention of something similar to an Augmented Lagrangian method. Explicitly describing the differences from standard optimization techniques might be good.\n\n**Reasoning for rating**:\n\nWhile the experiments are extensive, I think they miss the key comparisons that show how useful the method and each of its components is. Given that many different innovations are included in the submission, it may be a muddle for follow-up research to sort out how good each individual one is.\n\n**Misc comments**\n\n  * Check spacing around (6) in Algorithm 1\n  * Colon instead of comma after \"trainable variables\" in \u00a74.1\n  * \"For better analyze the growing patterns\" -> \"To better analyze the growing patterns\" on page 14\n  * Wortsman et. al. \"Discovering Neural Wirings\" is another closely related work at the intersection of NAS and pruning. (with major differences from the submission)\n\n**After rebuttal**\n\nThe authors have gone above and beyond in providing additional experimental results. All of the points raised above that deal with methodological issues are completely addressed.\n\nThe sole significant weakness that remains is the lack of the kind of ablation/component studies that would justify individual design decisions. I do not disagree with the authors that this will be difficult for this work, but I still feel they would have been helpful for researchers who will be building upon this method. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101556, "tmdate": 1606915779013, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2209/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Review"}}}, {"id": "VeMByQnUyJ0", "original": null, "number": 6, "cdate": 1606178611843, "ddate": null, "tcdate": 1606178611843, "tmdate": 1606178611843, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment", "content": {"title": "Paper Revision", "comment": "We thank all the reviewers, and answer questions in individual responses to each review below. We have revised the paper, incorporating suggested changes, including:\n\n* For experiments in Table 1 (CIFAR), we now report our method's performance in the fashion of mean +/- standard deviation across 5 runs with different random seeds. In the final version, we will a include similar update for all competing methods.\n\n* We conducted additional experiments on ImageNet to analyze the full accuracy vs FLOPs trade-off curve of our method in comparison to NetAdapt. A newly added Figure 3 shows that our method's trade-off curve dominates that of NetAdapt.\n\n* To the ImageNet results in Table 4, we added EfficientNet-B0. In order to facilitate comparison with ProxylessNet at equal accuracy, we also trained two additional models using our method. We match ProxylessNet's accuracy, while still training faster and producing a smaller model.\n\n* We add a random sampling baseline experiment to Section 4.4, with results displayed in a new Table 5. Our method's learned strategy significantly outperforms this random baseline.\n\n* The main text now contains a brief summary of our LSTM experiments, referring to the Appendix for full details."}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wb3wxCObbRT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2209/Authors|ICLR.cc/2021/Conference/Paper2209/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851026, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment"}}}, {"id": "Z6d22S415ki", "original": null, "number": 5, "cdate": 1606178363661, "ddate": null, "tcdate": 1606178363661, "tmdate": 1606178363661, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "mShRrPsufgN", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment", "content": {"title": "response to AnonReviewer4", "comment": "Thank you for the review and comments. We address your points individually below.\n\n**Q: In Eq. (3), it is said that f is the operation in Eq. (1). However, I couldn\u2019t find f in Eq. (1).**\n\n**A:** $f$ in Eq. (3) denotes the 'conv' operation in Eq. (1). To clarify, we have replaced 'conv' in Eq. (1) with $f$.\n\n**Q: clarify how many temperature parameters $\\beta$ are in the proposed model; recommend channel and layer indicators are denoted as vectors.**\n\n**A:** The total count of temperature parameters is the sum of the total number of channels in the network and the total number of layers. That is, each channel (filter) has an associated temperature parameter, as does each layer. This is identical to the situation with mask variables: per channel and per layer indicators allow for structured pruning of those components. The associated temperature depends upon the age of the corresponding component in the network, as defined by Eq. (7).\n\nEq. (7) and the \"improved temperature scheduler\" subsection introduce this per-component definition of temperature. In Eq. (6), which is presented prior to this discussion, we denote temperature as a scalar, following the conventions in existing pruning or NAS work, where all components share a single temperature parameter.\n\nWe have revised the paper to use bold font for all vector-valued variables.\n\n**Q: The authors should include at least a brief summary of the recurrent neural networks results.**\n\n**A:** We have added a brief summary of our LSTM experiments to Sec 4.1.\n\n**Q: Some figures (and the text inside) are too small while containing many details.**\n\n**A:** We have enlarged Figures 3, 4, and 5 in the updated version of the paper.\n\n**Q: In Table 1, what\u2019s the meaning of the underlines? ...the method with the second-best FLOPs is SoftNet, not Provable.**\n\n**A:** Underlines denote the second best results, while bold denotes the best. In the updated paper, we have added this explanation as well as correctly denoted SoftNet as having the second-best FLOPs for ResNet-20 in Table 1.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wb3wxCObbRT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2209/Authors|ICLR.cc/2021/Conference/Paper2209/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851026, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment"}}}, {"id": "cmrvnWMdM-6", "original": null, "number": 4, "cdate": 1606178298186, "ddate": null, "tcdate": 1606178298186, "tmdate": 1606178298186, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "_HS7uArSE12", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment", "content": {"title": "response to AnonReviewer3", "comment": "Thank you for the review and comments. We address your points individually below.\n\n**Q: Compared to ProxylessNet, the proposed model can reduce half of the training time but does harm to the model performance.**\n\n**A:** To provide a comparison at equal accuracy, we have updated Table 4 with two additional models trained by our method. Using a longer training time, but still faster than ProxylessNet, our method produces a network that matches ProxylessNet in accuracy while having fewer parameters. Here are the results on ImageNet (also included in our reply to AnonReviewer1):\n\n| Method            | Params(M) | Top-1 Acc(%) | Search/Grow Cost |\n| :---------------- | :-------- | :----------- | :--------------- |\n| ProxylessNet(GPU) | 7.1       | 75.1         | 200 GPU hours    |\n| Ours-1            | 6.8       | 74.3         |  80 GPU hours    |\n| Ours-2            | 6.7       | 74.8         | 110 GPU hours    |\n| Ours-3            | 6.9       | 75.1         | 140 GPU hours    |\n\n\nOur method saves 30% of the training time compared to ProxylessNet (140 vs 200 GPU hours), while producing a slightly smaller network (6.9M vs 7.1M params) that has the same accuracy (75.1%).\n\n**Q: Top-1 validation accuracy**\n\n**A:** Top-1 accuracy and Top-1 validation accuracy are synonymous in our usage. Top-1 validation accuracy is exactly the Top-1 accuracy achieved by the network on the ImageNet validation set. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wb3wxCObbRT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2209/Authors|ICLR.cc/2021/Conference/Paper2209/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851026, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment"}}}, {"id": "F6SD7K4HDLz", "original": null, "number": 3, "cdate": 1606178197789, "ddate": null, "tcdate": 1606178197789, "tmdate": 1606178197789, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "FGN0ZqCdH9L", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment", "content": {"title": "response to AnonReviewer2", "comment": "Thank you for the review and comments. We address your points individually below.\n\n**Q: How does the FLOPs reduction translate to runtime savings?**\n\n**A:** Our method performs structured sparsification at two levels, removing entire layers or filters (channels) within a layer. This directly translates into runtime savings as the resulting networks have reduced depth and width. Shallower networks have correspondingly lower latency. Reduced width means less computation is required for a layer; even on a highly parallel GPU, this translates into energy savings or speedup via use of larger batch sizes. Structured sparsification notably differs from methods which prune individual weights. Unlike our method, the latter may depend upon hardware efficiently supporting sparse matrix operations in order to realize savings.\n\n**Q: What is the target sparsity u in the experiments?**\n\n**A:** In Section 4.1, we set *u* as 0.95 (VGG-16), 0.65 (ResNet-20), 0.90 (WRN-28-10), 0.4 (ResNet-50), 0.6 (MobileNetV1), 0.5 (Deeplab-V3-ResNet101) and 0.6 (2-stacked-LSTM) to compare with the channel pruning methods. In Section 4.2 and 4.3, we set *u* to 0.5 when comparing with AutoGrow and NAS methods.\n\n**Q: any middle layer is dropped and then recovered?**\n\n**A:** Yes, we have observed middle layers dropped and then recovered. Here are some statistics.\n\nWe calculate frequency by simply counting: how many layers in the final architecture undergo a drop then recovery, i.e., the associated indicators flip to 0 then back to 1.\n\nIn the CIFAR-10 Basic3ResNet growing experiments in Table 3, the final layer architecture is 23-29-31, we observe that \"dropped and then recovered\" phenomena happen in 4-7-5 of final architecture's layers. The overall frequency is (4+7+5)/(23+29+31) = 19%.\n\nIn the ImageNet Bottleneck4ResNet growing experiments, the frequency is (0+2+1+2)/(5+6+5+7) = 22%.\n\n**Q: size of channel/layer mask, predefined upper bound on number of channels and number of layers?**\n\n**A:** Our method does not require setting any upper bound on the number of channels or layers.\n\nIn experiments, we did predefine a maximum mask size to simplify implementation. This was sufficient to allow comparison with other methods operating on similar target model scales. Here are the upper bounds on mask size, per stage of the network, used in Sec. 4.2 and 4.3:\n\n| Property          | Sec. 4.2 (CIFAR) | Sec. 4.2 (ImageNet) | Sec. 4.3       |\n| :---------------  | :----------------| :------------------ | :------------- |\n| Channel Mask Size | 16,32,64         | 64,128,256,512      | 64,128,256,256 |\n| Layer Mask Size   | 40,40,40         | 8,8,8,8             | 4,4,4,4        |\n\nA straightforward extension of our current implementation would remove any limits on channels or layers, while minimally impacting efficiency: reallocate and expand the appropriate mask and parameter arrays when all of their elements are in use. This is precisely the same as a classic variable-length array implementation that dynamically grows or shrinks via an occasional reallocation and copy procedure.\n\n**Q: Table 4, Efficient-B0**\n\n**A:** Thanks for the suggestion. We have added Efficient-B0 [1] to Table 4. Note that the Efficient-B0 architecture is generated by grid search in the MnasNet [2] search space, implying the search cost is in the same large-scale range as MnasNet, i.e., 40,000 GPU hours. \n\n[1] Mingxing Tan and Quoc V. Le. \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\" ICML 2019.\n\n[2] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. \"MnaNnet: Platform-aware neural architecture search for mobile.\" CVPR 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wb3wxCObbRT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2209/Authors|ICLR.cc/2021/Conference/Paper2209/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851026, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment"}}}, {"id": "_w30sv3yDH", "original": null, "number": 2, "cdate": 1606178065236, "ddate": null, "tcdate": 1606178065236, "tmdate": 1606178065236, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "rLzfGjbIcJb", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment", "content": {"title": "response to AnonReviewer1", "comment": "Thank you for the review and comments. We address your points individually below.\n\n**Q: random seeding**\n\n**A:** We have re-run our pruning method on CIFAR-10 (matching original experiments in Table 1) with 5 different random seeds and report results as follows, written as mean (+/- standard deviation):\n\n| model     | CIFAR Acc(%)      | Params(M)        | FLOPs(%)        | Train-cost savings(x) |\n| :-------  | :---------------- | :--------------- | :-------------- | :------------- |\n| VGG-16    | 92.50 (+/-0.10)   | 0.754 (+/-0.005) | 13.55 (+/-0.03) | 4.95 (+/-0.17) |\n| ResNet-20 | 90.91 (+/-0.07)   | 0.096 (+/-0.002) | 50.20 (+/-0.01) | 2.40 (+/-0.09) |\n| WRN-28-10 | 95.32 (+/-0.11)   | 3.443 (+/-0.010) | 28.25 (+/-0.04) | 3.12 (+/-0.11) |\n\nThese results demonstrate high consistency across random seeds. They indicate that the advantages we originally observed in comparison to other methods, in terms of accuracy improvement, parameter and/or FLOP reduction, and training cost savings, are beyond the range explainable by chance.\n\nTo the final version of the paper, we will include these variance statistics, as well as those for re-running from different random initialization in all competing methods, which requires additional time to complete.\n\n**Q: comparison to simple random baseline**\n\n**A:** Section 4.4 includes comparison to a \"uniform pruning\" baseline; we expected this would be the strongest simple baseline against which to compare. We agree with the suggestion of also comparing to a simple random baseline and have conducted this experiment. This new random baseline replaces the procedure for sampling values of *q* in Equation 6. Instead of using sampling probabilities derived from the learned parameters *s*, it samples with fixed probability. On CIFAR-10 (matching the setting in Table 1), we obtain the following results, averaged across 5 runs:\n\n| Model     | Method | Val Acc(\\%)     | Params(M)        |\n| :-------- | :----- | :-------------- | :--------------- |\n| VGG-16    | random | 90.01 (+/-0.69) | 0.770 (+/-0.050) |\n| VGG-16    | ours   | 92.50 (+/-0.10) | 0.754 (+/-0.005) |\n| ResNet-20 | random | 89.18 (+/-0.55) | 0.100 (+/-0.010) |\n| ResNet-20 | ours   | 90.91 (+/-0.07) | 0.096 (+/-0.002) |\n| WRN-28-10 | random | 92.26 (+/-0.87) | 3.440 (+/-0.110) |\n| WRN-28-10 | ours   | 95.32 (+/-0.11) | 3.443 (+/-0.010) |\n\nFor each model, our method produces trained networks with both substantially higher validation accuracy and comparable or fewer parameters than the random baseline. We have updated Section 4.4 to include these results and corresponding discussion.\n\n**Q: incomplete illustration of the cost/accuracy tradeoff**\n\n**A:** We have conducted additional experiments to illustrate the cost/accuracy tradeoff curve.\n\nTo address the concern about first result in the original Table 5 (MobileNetV1 on ImageNet), for both NetAdapt and our method, we train four model variants of different size, and plot accuracy vs FLOPs trade-offs in Figure 3 of our revised paper. Our method dominates NetAdapt, achieving higher accuracy using fewer FLOPs.\n\nWe have also updated Table 4 with two additional models trained by our method. Using a longer training time, but still faster than ProxylessNet, our method produces a network that matches ProxylessNet in accuracy while having fewer parameters:\n\n| Method            | Params(M) | Top-1 Acc(%) | Search/Grow Cost |\n| :---------------- | :-------- | :----------- | :--------------- |\n| ProxylessNet(GPU) | 7.1       | 75.1         | 200 GPU hours    |\n| Ours-1            | 6.8       | 74.3         |  80 GPU hours    |\n| Ours-2            | 6.7       | 74.8         | 110 GPU hours    |\n| Ours-3            | 6.9       | 75.1         | 140 GPU hours    |\n\n**Q: budget-aware growing and Augmented Lagrangian method**\n\n**A:** It is not our intention to claim any novelty in terms of optimization techniques. Indeed, the budget-aware growing procedure in Appendix A.1 does share a similar spirit to an Augmented Lagrangian method, in that it periodically revises an unconstrained objective function in order to drive the system towards a (budget) constraint.\n\n**Q: many different innovations**\n\n**A:** Though our method has several components, it is not a collection of orthogonal innovations. Rather, these components are designed and woven together to implement a high-level vision: efficiently train deep networks by growing them over time while pruning away useless structure. Continuous sparsification, temperature schedules, and sampling form the technical basis which enables the simultaneous growing and pruning dynamics.\n\n**Q: misc**\n\n**A:** We have revised accordingly and added citation of Wortsman et. al.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "wb3wxCObbRT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2209/Authors|ICLR.cc/2021/Conference/Paper2209/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851026, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Comment"}}}, {"id": "_HS7uArSE12", "original": null, "number": 3, "cdate": 1603871602481, "ddate": null, "tcdate": 1603871602481, "tmdate": 1605024263408, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "This paper proposes a novel NAS method that searches the model architectures by grows the networks. This searching strategy determines the channel and layer configurations by assigning a binary learnable parameter m for each channel or layer. The objective is to optimize a trade-off between the model performance on the given task and the regularization on the binary indicator m.\n\nPros:\n1. The general idea of searching the architectures by growing the networks sounds very interesting. The authors propose a novel framework to achieve their idea, and also apply some tricks to speed up and simplify the optimization (e.g. budget-aware growing and learning by continuation). \n2. The paper is well-written and easy to follow.\n3. The authors conduct a series of solid experiments to verify the effectiveness of their proposed methods. The experiments show the performance of channel pruning, the remarkable improvement on AutoGrow model, and the comparsion with other NAS methods.\n\nCons:\n1. Compared to ProxylessNet, the proposed model can reduce half of the training time but does harm to the model performance.\n\nQuestions:\n1. What's the exact meaning of \"Top-1 valiadation accuracy\"? What's the different with Top-1 accuracy? Is this metric evaluated on the valiadation set?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101556, "tmdate": 1606915779013, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2209/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Review"}}}, {"id": "mShRrPsufgN", "original": null, "number": 4, "cdate": 1603950870462, "ddate": null, "tcdate": 1603950870462, "tmdate": 1605024263346, "tddate": null, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "invitation": "ICLR.cc/2021/Conference/Paper2209/-/Official_Review", "content": {"title": "Principled approach to growing deep networks ", "review": "This paper proposes a new principled approach to growing deep network architectures based on continuous relaxation of discrete structure optimization combined with a sparse subnetwork sampling scheme. It starts from a simple seed architecture and dynamically grows/prunes both the layers and filters during training. Through extensive experiments, the authors show that this method produces more efficient networks while reducing the computational cost of training, still maintaining good validation accuracy, compared to other NAS or pruning/growing methods. \n\nStrength: \n(+) The proposed idea of formulating the problem as a continuous relaxation of discrete structure optimization is interesting. It seems to be a more principled approach than previous NAS or separate pruning/growing approaches. \n(+) Extensive experimental results are provided to verify the superiority over recent other methods and also to show the performance behavior of the proposed method. The overall experimental setup is systematic and comprehensive. \nThe experiments were done on widely used deep networks on various tasks.\n\nI only have concerns about the clarity of the notation and the representation of the figures. Specific examples are as follows: \n\nIn Eq. (3), it is said that f is the operation in Eq. (1). However, I couldn\u2019t find f in Eq. (1).\n\nIt should be clarified how many temperature parameters \u03b2 are in the proposed model. Only one or as many as channels and layers? If it is only one, it does not seem reasonable that all the probabilities growing or pruning channels and layers are the same.  Equation (7) seems to imply \u03b2 to be a vector, but earlier notations (e.g. in Algo 1, Equation 6, etc.) seem to present it as a scalar.\n\nOverall, there are confusing symbols, whether it\u2019s a scalar or a vector. I recommend the channel and layer indicators are denoted as vectors. It seems that each channel and each layer has its unique indicator, respectively. Also, notations should include channel and layer index if they are different depending on channels and layers.\n\nAdditionally, all the experimental results shown in the main manuscript are on convolutional neural networks while the abstract mentions recurrent neural networks. The appendix has some, but very little has the main manuscript. If it\u2019s an important part of this manuscript, the authors should include at least a brief summary of the results. \n\nSome figures (and the text inside) are too small while containing many details, probably because of the space limit. For example, Figure 3 has many lines that are hard to analyze and texts that are not readable. \n\nIn Table 1, what\u2019s the meaning of the underlines? I guess the second best results, but for RestNet-20, the method with the second-best FLOPs is SoftNet, not Provable. And the explanation about the boldface and underlines should be included. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2209/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2209/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "authorids": ["~Xin_Yuan5", "~Pedro_Henrique_Pamplona_Savarese1", "~Michael_Maire1"], "authors": ["Xin Yuan", "Pedro Henrique Pamplona Savarese", "Michael Maire"], "keywords": ["deep learning", "computer vision", "network pruning", "neural architecture search"], "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.", "one-sentence_summary": "We propose an efficient training method that dynamically grows and prunes neural network architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yuan|growing_efficient_deep_networks_by_structured_continuous_sparsification", "pdf": "/pdf/f0340388ab26e079bb52b2e75a594fa25f418c28.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyuan2021growing,\ntitle={Growing Efficient Deep Networks by Structured Continuous Sparsification},\nauthor={Xin Yuan and Pedro Henrique Pamplona Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=wb3wxCObbRT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "wb3wxCObbRT", "replyto": "wb3wxCObbRT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2209/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101556, "tmdate": 1606915779013, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2209/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2209/-/Official_Review"}}}], "count": 11}