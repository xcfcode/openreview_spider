{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028563775, "tcdate": 1490028563775, "number": 1, "id": "S13f_tTjl", "invitation": "ICLR.cc/2017/workshop/-/paper36/acceptance", "forum": "ByXrfaGFe", "replyto": "ByXrfaGFe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss", "abstract": "Training a network with small capacity that can perform as well as a larger capacity network is an important problem that needs to be solved in real life applications which require fast inference time and small memory requirement. Previous approaches that transfer knowledge from a bigger network to a smaller network show little benefit when applied to state-of-the-art convolutional neural network architectures such as Residual Network trained with batch normalization. We propose class-distance loss that helps teacher networks to form densely clustered vector space to make it easy for the student network to learn from it. We show that a small network with half the size of the original network trained with the proposed strategy can perform close to the original network on CIFAR-10 dataset.", "pdf": "/pdf/f359a3ee736cc967bf331e30395912cfaf67964a.pdf", "paperhash": "kim|transferring_knowledge_to_smaller_network_with_classdistance_loss", "conflicts": ["lunit.io"], "authorids": ["swkim@lunit.io", "hekim@lunit.io"], "keywords": [], "authors": ["Seung Wook Kim", "Hyo-Eun Kim"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028564320, "id": "ICLR.cc/2017/workshop/-/paper36/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByXrfaGFe", "replyto": "ByXrfaGFe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028564320}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489474926961, "tcdate": 1487225402738, "number": 36, "id": "ByXrfaGFe", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "ByXrfaGFe", "signatures": ["~Seung_Wook_Kim1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss", "abstract": "Training a network with small capacity that can perform as well as a larger capacity network is an important problem that needs to be solved in real life applications which require fast inference time and small memory requirement. Previous approaches that transfer knowledge from a bigger network to a smaller network show little benefit when applied to state-of-the-art convolutional neural network architectures such as Residual Network trained with batch normalization. We propose class-distance loss that helps teacher networks to form densely clustered vector space to make it easy for the student network to learn from it. We show that a small network with half the size of the original network trained with the proposed strategy can perform close to the original network on CIFAR-10 dataset.", "pdf": "/pdf/f359a3ee736cc967bf331e30395912cfaf67964a.pdf", "paperhash": "kim|transferring_knowledge_to_smaller_network_with_classdistance_loss", "conflicts": ["lunit.io"], "authorids": ["swkim@lunit.io", "hekim@lunit.io"], "keywords": [], "authors": ["Seung Wook Kim", "Hyo-Eun Kim"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489202092428, "tcdate": 1489202092428, "number": 2, "id": "H14hj1Zox", "invitation": "ICLR.cc/2017/workshop/-/paper36/public/comment", "forum": "ByXrfaGFe", "replyto": "BkvUXIlse", "signatures": ["~Seung_Wook_Kim1"], "readers": ["everyone"], "writers": ["~Seung_Wook_Kim1"], "content": {"title": "Reply to AnonReviewer1", "comment": "Thank you for your review.\n\nQuestions:\n- What is the performance of the Teacher on CIFAR10?\n 110-layer 'Baseline' and 'Class-distance loss' resnets in the table 1 refers to the performances of teacher models.\n\n- Did you compare with  knowledge distillation baseline (that matches softmax logits of teacher and student networks) ?\n Yes. We couldn't get the error rate go down below 9% by training a student model with traditional cross-entropy transfer\n \n- Do you use Batch Normalization in your residual networks?\n Yes. All resnets are trained with batch normalization.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss", "abstract": "Training a network with small capacity that can perform as well as a larger capacity network is an important problem that needs to be solved in real life applications which require fast inference time and small memory requirement. Previous approaches that transfer knowledge from a bigger network to a smaller network show little benefit when applied to state-of-the-art convolutional neural network architectures such as Residual Network trained with batch normalization. We propose class-distance loss that helps teacher networks to form densely clustered vector space to make it easy for the student network to learn from it. We show that a small network with half the size of the original network trained with the proposed strategy can perform close to the original network on CIFAR-10 dataset.", "pdf": "/pdf/f359a3ee736cc967bf331e30395912cfaf67964a.pdf", "paperhash": "kim|transferring_knowledge_to_smaller_network_with_classdistance_loss", "conflicts": ["lunit.io"], "authorids": ["swkim@lunit.io", "hekim@lunit.io"], "keywords": [], "authors": ["Seung Wook Kim", "Hyo-Eun Kim"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487225403458, "tcdate": 1487225403458, "id": "ICLR.cc/2017/workshop/-/paper36/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper36/reviewers"], "reply": {"forum": "ByXrfaGFe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487225403458}}}, {"tddate": null, "tmdate": 1489163086754, "tcdate": 1489163086754, "number": 2, "id": "BkvUXIlse", "invitation": "ICLR.cc/2017/workshop/-/paper36/official/review", "forum": "ByXrfaGFe", "replyto": "ByXrfaGFe", "signatures": ["ICLR.cc/2017/workshop/paper36/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper36/AnonReviewer1"], "content": {"title": "Promising Work", "rating": "7: Good paper, accept", "review": "This paper investigates Knowledge Distillation for network compression.  In their approach, the authors propose to match feature vector of the sofmax preactivation. In addition, they introduce a new loss function for training the teacher, i.e. they add a regularisation term so that class-wise clusters of feature vectors are more dense.\n\nAuthors evaluate their approach on the CIFAR10 dataset using Resnet for both teacher and student. Contrary to previous approaches using Knowledge Distillation, they show that their approach is able to leverage the Teacher to improve the Student performances with such network architectures.\n\nQuestions:\n- What is the performance of the Teacher on CIFAR10?\n- Did you compare with  knowledge distillation baseline (that matches softmax logits of teacher and student networks) ?\n- Do you use Batch Normalization in your residual networks?\n\nPros:\n- The paper is clear an easy to follow\n- Authors show that Knowledge Distillation is useful for recent network architecture (Resnets).\nCon:\n- Experiences on only one dataset.\n\n\nI recommend acceptance.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss", "abstract": "Training a network with small capacity that can perform as well as a larger capacity network is an important problem that needs to be solved in real life applications which require fast inference time and small memory requirement. Previous approaches that transfer knowledge from a bigger network to a smaller network show little benefit when applied to state-of-the-art convolutional neural network architectures such as Residual Network trained with batch normalization. We propose class-distance loss that helps teacher networks to form densely clustered vector space to make it easy for the student network to learn from it. We show that a small network with half the size of the original network trained with the proposed strategy can perform close to the original network on CIFAR-10 dataset.", "pdf": "/pdf/f359a3ee736cc967bf331e30395912cfaf67964a.pdf", "paperhash": "kim|transferring_knowledge_to_smaller_network_with_classdistance_loss", "conflicts": ["lunit.io"], "authorids": ["swkim@lunit.io", "hekim@lunit.io"], "keywords": [], "authors": ["Seung Wook Kim", "Hyo-Eun Kim"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489163087476, "id": "ICLR.cc/2017/workshop/-/paper36/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper36/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper36/AnonReviewer2", "ICLR.cc/2017/workshop/paper36/AnonReviewer1"], "reply": {"forum": "ByXrfaGFe", "replyto": "ByXrfaGFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper36/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper36/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489163087476}}}, {"tddate": null, "nonreaders": null, "tmdate": 1488481209008, "tcdate": 1488481170048, "number": 1, "id": "Byc9s1U5e", "invitation": "ICLR.cc/2017/workshop/-/paper36/official/comment", "forum": "ByXrfaGFe", "replyto": "S1Ph1Zr5l", "signatures": ["ICLR.cc/2017/workshop/paper36/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper36/AnonReviewer2"], "content": {"title": "thanks", "comment": "Thanks for the response. It might be worth having the cross-entropy results in there as well for reference.\nThe proposed method seems even better in light of the fact that the usual cross entropy knowledge distillation does not work in this case.\n\n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss", "abstract": "Training a network with small capacity that can perform as well as a larger capacity network is an important problem that needs to be solved in real life applications which require fast inference time and small memory requirement. Previous approaches that transfer knowledge from a bigger network to a smaller network show little benefit when applied to state-of-the-art convolutional neural network architectures such as Residual Network trained with batch normalization. We propose class-distance loss that helps teacher networks to form densely clustered vector space to make it easy for the student network to learn from it. We show that a small network with half the size of the original network trained with the proposed strategy can perform close to the original network on CIFAR-10 dataset.", "pdf": "/pdf/f359a3ee736cc967bf331e30395912cfaf67964a.pdf", "paperhash": "kim|transferring_knowledge_to_smaller_network_with_classdistance_loss", "conflicts": ["lunit.io"], "authorids": ["swkim@lunit.io", "hekim@lunit.io"], "keywords": [], "authors": ["Seung Wook Kim", "Hyo-Eun Kim"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487225403424, "tcdate": 1487225403424, "id": "ICLR.cc/2017/workshop/-/paper36/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "ByXrfaGFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper36/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper36/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper36/reviewers", "ICLR.cc/2017/workshop/paper36/areachairs"], "cdate": 1487225403424}}}, {"tddate": null, "nonreaders": null, "tmdate": 1488481179588, "tcdate": 1488142216259, "number": 1, "id": "H1lqJpgqg", "invitation": "ICLR.cc/2017/workshop/-/paper36/official/review", "forum": "ByXrfaGFe", "replyto": "ByXrfaGFe", "signatures": ["ICLR.cc/2017/workshop/paper36/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper36/AnonReviewer2"], "content": {"title": "good paper", "rating": "8: Top 50% of accepted papers, clear accept", "review": "In this work, the authors propose to transfer knowledge from a teacher model to a smaller student model with two variations:\n1) knowledge is transferred by matching the feature vector before the softmax \n2) the teacher is trained with an additional regularization term to make the feature vectors more dense within the same class.\n\nThis is a solid piece work that should be accepted. One question:\n- Does the TF-baseline refer to student model trained with traditional cross-entropy knowledge transfer? or the feature vector transfer? If the latter, can you please have additional baseline numbers for student models trained with (standard) cross-entropy loss transfer?\n\nMinor comments:\n- Citations: Should really cite Bucila et al. 2006 for knowledge distillation and LeCun et al. 1990 (Optimal Brain Damange) for model compression, as these predate some of the more recent work (Hinton 2015, Han 2016, Jaderberg 2014, etc.)\n\n- \"Mimic learning\": probably best just to stick to \"knowledge distillation\"", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss", "abstract": "Training a network with small capacity that can perform as well as a larger capacity network is an important problem that needs to be solved in real life applications which require fast inference time and small memory requirement. Previous approaches that transfer knowledge from a bigger network to a smaller network show little benefit when applied to state-of-the-art convolutional neural network architectures such as Residual Network trained with batch normalization. We propose class-distance loss that helps teacher networks to form densely clustered vector space to make it easy for the student network to learn from it. We show that a small network with half the size of the original network trained with the proposed strategy can perform close to the original network on CIFAR-10 dataset.", "pdf": "/pdf/f359a3ee736cc967bf331e30395912cfaf67964a.pdf", "paperhash": "kim|transferring_knowledge_to_smaller_network_with_classdistance_loss", "conflicts": ["lunit.io"], "authorids": ["swkim@lunit.io", "hekim@lunit.io"], "keywords": [], "authors": ["Seung Wook Kim", "Hyo-Eun Kim"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489163087476, "id": "ICLR.cc/2017/workshop/-/paper36/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper36/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper36/AnonReviewer2", "ICLR.cc/2017/workshop/paper36/AnonReviewer1"], "reply": {"forum": "ByXrfaGFe", "replyto": "ByXrfaGFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper36/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper36/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489163087476}}}, {"tddate": null, "tmdate": 1488420783031, "tcdate": 1488420783031, "number": 1, "id": "S1Ph1Zr5l", "invitation": "ICLR.cc/2017/workshop/-/paper36/public/comment", "forum": "ByXrfaGFe", "replyto": "H1lqJpgqg", "signatures": ["~Seung_Wook_Kim1"], "readers": ["everyone"], "writers": ["~Seung_Wook_Kim1"], "content": {"title": "reply to AnonReviewer2", "comment": "Thank you for your comment.\n\nRegarding your question, TF-baseline refers to student model trained with feature vector transfer. We couldn't get the error rate go down below 9% by training a student model with traditional cross-entropy transfer (stated in your comment). This is expected as previous works (Srivastava et al. (2015) and Chen et al. (2016)) indicated that the cross-entropy transfer strategy did not outperform baseline networks trained from scratch where baseline networks are sufficiently deep neural networks with strong regularizers such as batch-norm.\n\nWe'll add suggested citations as well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss", "abstract": "Training a network with small capacity that can perform as well as a larger capacity network is an important problem that needs to be solved in real life applications which require fast inference time and small memory requirement. Previous approaches that transfer knowledge from a bigger network to a smaller network show little benefit when applied to state-of-the-art convolutional neural network architectures such as Residual Network trained with batch normalization. We propose class-distance loss that helps teacher networks to form densely clustered vector space to make it easy for the student network to learn from it. We show that a small network with half the size of the original network trained with the proposed strategy can perform close to the original network on CIFAR-10 dataset.", "pdf": "/pdf/f359a3ee736cc967bf331e30395912cfaf67964a.pdf", "paperhash": "kim|transferring_knowledge_to_smaller_network_with_classdistance_loss", "conflicts": ["lunit.io"], "authorids": ["swkim@lunit.io", "hekim@lunit.io"], "keywords": [], "authors": ["Seung Wook Kim", "Hyo-Eun Kim"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487225403458, "tcdate": 1487225403458, "id": "ICLR.cc/2017/workshop/-/paper36/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper36/reviewers"], "reply": {"forum": "ByXrfaGFe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487225403458}}}], "count": 7}