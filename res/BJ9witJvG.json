{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124444855, "tcdate": 1518472034059, "number": 311, "cdate": 1518472034059, "id": "BJ9witJvG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJ9witJvG", "signatures": ["~Jakob_Nicolaus_Foerster2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "DiCE: The Infinitely Differentiable Monte-Carlo Estimator", "abstract": "The score function estimator is widely used for estimating gradients of stochastic objectives in Stochastic Computation Graphs (SCG), eg. in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order gradients is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order gradient involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for higher-order gradient estimators. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct gradient estimators of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and through numerical evaluation of the DiCE gradient estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://goo.gl/xkkGxN.", "paperhash": "foerster|dice_the_infinitely_differentiable_montecarlo_estimator", "keywords": ["reinforcement learning", "deep learning", "multi agent", "meta-learning", "stochastic computation graphs"], "_bibtex": "@misc{\n  foerster2018dice:,\n  title={DiCE: The Infinitely Differentiable Monte-Carlo Estimator},\n  author={Jakob Foerster and Greg Farquhar and Maruan Al-Shedivat and Tim Rockt\u00e4schel and Eric P. Xing and Shimon Whiteson},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ9witJvG}\n}", "authorids": ["jakob.foerster@cs.ox.ac.uk", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "tim.rocktaschel@cs.ox.ac.uk", "epxing@cs.cmu.edu", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Jakob Foerster", "Greg Farquhar", "Maruan Al-Shedivat", "Tim Rockt\u00e4schel", "Eric P. Xing", "Shimon Whiteson"], "TL;DR": "DiCE provides a single objective which can be differentiated an arbitrary number of times to generate gradient estimators of any order in stochastic computation graphs.", "pdf": "/pdf/8dc3ac9e9fc690fd76d1ce11d9a605af3d962082.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582842451, "tcdate": 1520591083124, "number": 1, "cdate": 1520591083124, "id": "Sk7g-1eKG", "invitation": "ICLR.cc/2018/Workshop/-/Paper311/Official_Review", "forum": "BJ9witJvG", "replyto": "BJ9witJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer2"], "content": {"title": "Interesting improvement to the surrogate loss approach", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose a new approach for estimating gradients of stochastic objectives, designed for use in automatic-differentiation toolboxes. It is shown, both analytically and empirically, that it addresses some of the shortcomings in the existing approach based on surrogate loss in estimating higher order gradients.\n\nWhile I did not go through the full content in the appendix, the work seems convincing and is very relevant to the learning community.\n\nI find the random dice symbol distracting.\n", "confidence": "1: The reviewer's evaluation is an educated guess"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DiCE: The Infinitely Differentiable Monte-Carlo Estimator", "abstract": "The score function estimator is widely used for estimating gradients of stochastic objectives in Stochastic Computation Graphs (SCG), eg. in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order gradients is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order gradient involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for higher-order gradient estimators. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct gradient estimators of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and through numerical evaluation of the DiCE gradient estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://goo.gl/xkkGxN.", "paperhash": "foerster|dice_the_infinitely_differentiable_montecarlo_estimator", "keywords": ["reinforcement learning", "deep learning", "multi agent", "meta-learning", "stochastic computation graphs"], "_bibtex": "@misc{\n  foerster2018dice:,\n  title={DiCE: The Infinitely Differentiable Monte-Carlo Estimator},\n  author={Jakob Foerster and Greg Farquhar and Maruan Al-Shedivat and Tim Rockt\u00e4schel and Eric P. Xing and Shimon Whiteson},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ9witJvG}\n}", "authorids": ["jakob.foerster@cs.ox.ac.uk", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "tim.rocktaschel@cs.ox.ac.uk", "epxing@cs.cmu.edu", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Jakob Foerster", "Greg Farquhar", "Maruan Al-Shedivat", "Tim Rockt\u00e4schel", "Eric P. Xing", "Shimon Whiteson"], "TL;DR": "DiCE provides a single objective which can be differentiated an arbitrary number of times to generate gradient estimators of any order in stochastic computation graphs.", "pdf": "/pdf/8dc3ac9e9fc690fd76d1ce11d9a605af3d962082.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582842256, "id": "ICLR.cc/2018/Workshop/-/Paper311/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper311/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper311/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper311/AnonReviewer3"], "reply": {"forum": "BJ9witJvG", "replyto": "BJ9witJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper311/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper311/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582842256}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582790430, "tcdate": 1520628867169, "number": 2, "cdate": 1520628867169, "id": "BJjYEOxtG", "invitation": "ICLR.cc/2018/Workshop/-/Paper311/Official_Review", "forum": "BJ9witJvG", "replyto": "BJ9witJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer1"], "content": {"title": "The authors provide a novel SL objective that is amenable to autograd even when it comes to calculating higher order gradients", "rating": "6: Marginally above acceptance threshold", "review": "Score functions are used to produce Monte Carlo estimates of gradients.  Surrogate loss function based approach provides a way to provide gradient estimates that utilizes auto-diff and stochastic computational graphs to programmatically calculate gradients. However, the SL approach does not provide an easy way to calculate higher order gradients easily, without tedious manual calculations. \nThe authors in this paper provide a way to get around this problem. They do this by producing a novel operator called DICE, which wraps around the set of stochastic nodes that influence the original loss function.  The authors demonstrate this estimator for multi-agent RL  and show that LOLA-DICE is able to differentiate through multiple steps of the opponent,  whereas LOLA and MAML are somewhat unstable and are restricted to only differentiating w.r.t. one gradient step of the opponent. \nThis is interesting work, but I am unable to judge the correctness of this work. For example, the formulation of DICE objective on page 2 is very unclear. For example, there are multiple dice symbols with multiple, varying dots. i guess they refer to different operators. However, it is not clear to me what they mean.", "confidence": "1: The reviewer's evaluation is an educated guess"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DiCE: The Infinitely Differentiable Monte-Carlo Estimator", "abstract": "The score function estimator is widely used for estimating gradients of stochastic objectives in Stochastic Computation Graphs (SCG), eg. in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order gradients is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order gradient involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for higher-order gradient estimators. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct gradient estimators of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and through numerical evaluation of the DiCE gradient estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://goo.gl/xkkGxN.", "paperhash": "foerster|dice_the_infinitely_differentiable_montecarlo_estimator", "keywords": ["reinforcement learning", "deep learning", "multi agent", "meta-learning", "stochastic computation graphs"], "_bibtex": "@misc{\n  foerster2018dice:,\n  title={DiCE: The Infinitely Differentiable Monte-Carlo Estimator},\n  author={Jakob Foerster and Greg Farquhar and Maruan Al-Shedivat and Tim Rockt\u00e4schel and Eric P. Xing and Shimon Whiteson},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ9witJvG}\n}", "authorids": ["jakob.foerster@cs.ox.ac.uk", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "tim.rocktaschel@cs.ox.ac.uk", "epxing@cs.cmu.edu", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Jakob Foerster", "Greg Farquhar", "Maruan Al-Shedivat", "Tim Rockt\u00e4schel", "Eric P. Xing", "Shimon Whiteson"], "TL;DR": "DiCE provides a single objective which can be differentiated an arbitrary number of times to generate gradient estimators of any order in stochastic computation graphs.", "pdf": "/pdf/8dc3ac9e9fc690fd76d1ce11d9a605af3d962082.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582842256, "id": "ICLR.cc/2018/Workshop/-/Paper311/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper311/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper311/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper311/AnonReviewer3"], "reply": {"forum": "BJ9witJvG", "replyto": "BJ9witJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper311/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper311/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582842256}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582752519, "tcdate": 1520651473878, "number": 3, "cdate": 1520651473878, "id": "H15RhplKG", "invitation": "ICLR.cc/2018/Workshop/-/Paper311/Official_Review", "forum": "BJ9witJvG", "replyto": "BJ9witJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer3"], "content": {"title": "Nice paper that fixes the computation of higher-order gradients in stochastic computation graphs, enabling correct auto-diff training for RL policy gradient, soft Q-learning, model-agnostic meta learning,  etc.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper crisply points out the flaw in existing implementations to get gradients in stochastic computation graphs (i.e. they don't generalize nicely for higher-order gradients, which is a natural requirement for many applications). The fix is explained in a simple manner, and easily implemented in TensorFlow, pyTorch etc.\n\nClarity: Average. The Dice symbol for MAGICBOX is distracting notation! (First instinct was that it is cute and clever, but it actually hindered understanding of the equations).\n\nSignificance: Good. Paper has a singular purpose, but it correctly points out bugs in prior work (e.g. MAML) and makes algorithms like LOLA more practical and so, this can be a significant contribution.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DiCE: The Infinitely Differentiable Monte-Carlo Estimator", "abstract": "The score function estimator is widely used for estimating gradients of stochastic objectives in Stochastic Computation Graphs (SCG), eg. in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order gradients is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order gradient involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for higher-order gradient estimators. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct gradient estimators of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and through numerical evaluation of the DiCE gradient estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://goo.gl/xkkGxN.", "paperhash": "foerster|dice_the_infinitely_differentiable_montecarlo_estimator", "keywords": ["reinforcement learning", "deep learning", "multi agent", "meta-learning", "stochastic computation graphs"], "_bibtex": "@misc{\n  foerster2018dice:,\n  title={DiCE: The Infinitely Differentiable Monte-Carlo Estimator},\n  author={Jakob Foerster and Greg Farquhar and Maruan Al-Shedivat and Tim Rockt\u00e4schel and Eric P. Xing and Shimon Whiteson},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ9witJvG}\n}", "authorids": ["jakob.foerster@cs.ox.ac.uk", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "tim.rocktaschel@cs.ox.ac.uk", "epxing@cs.cmu.edu", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Jakob Foerster", "Greg Farquhar", "Maruan Al-Shedivat", "Tim Rockt\u00e4schel", "Eric P. Xing", "Shimon Whiteson"], "TL;DR": "DiCE provides a single objective which can be differentiated an arbitrary number of times to generate gradient estimators of any order in stochastic computation graphs.", "pdf": "/pdf/8dc3ac9e9fc690fd76d1ce11d9a605af3d962082.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582842256, "id": "ICLR.cc/2018/Workshop/-/Paper311/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper311/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper311/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper311/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper311/AnonReviewer3"], "reply": {"forum": "BJ9witJvG", "replyto": "BJ9witJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper311/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper311/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582842256}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573562559, "tcdate": 1521573562559, "number": 86, "cdate": 1521573562216, "id": "r1Q6RCAFz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJ9witJvG", "replyto": "BJ9witJvG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DiCE: The Infinitely Differentiable Monte-Carlo Estimator", "abstract": "The score function estimator is widely used for estimating gradients of stochastic objectives in Stochastic Computation Graphs (SCG), eg. in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order gradients is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order gradient involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for higher-order gradient estimators. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct gradient estimators of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and through numerical evaluation of the DiCE gradient estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://goo.gl/xkkGxN.", "paperhash": "foerster|dice_the_infinitely_differentiable_montecarlo_estimator", "keywords": ["reinforcement learning", "deep learning", "multi agent", "meta-learning", "stochastic computation graphs"], "_bibtex": "@misc{\n  foerster2018dice:,\n  title={DiCE: The Infinitely Differentiable Monte-Carlo Estimator},\n  author={Jakob Foerster and Greg Farquhar and Maruan Al-Shedivat and Tim Rockt\u00e4schel and Eric P. Xing and Shimon Whiteson},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ9witJvG}\n}", "authorids": ["jakob.foerster@cs.ox.ac.uk", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "tim.rocktaschel@cs.ox.ac.uk", "epxing@cs.cmu.edu", "shimon.whiteson@cs.ox.ac.uk"], "authors": ["Jakob Foerster", "Greg Farquhar", "Maruan Al-Shedivat", "Tim Rockt\u00e4schel", "Eric P. Xing", "Shimon Whiteson"], "TL;DR": "DiCE provides a single objective which can be differentiated an arbitrary number of times to generate gradient estimators of any order in stochastic computation graphs.", "pdf": "/pdf/8dc3ac9e9fc690fd76d1ce11d9a605af3d962082.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}