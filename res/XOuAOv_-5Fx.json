{"notes": [{"id": "XOuAOv_-5Fx", "original": "LmmA9tqsYXj", "number": 3535, "cdate": 1601308392318, "ddate": null, "tcdate": 1601308392318, "tmdate": 1614985772097, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 24, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2fp9CQzhQz", "original": null, "number": 1, "cdate": 1610040360049, "ddate": null, "tcdate": 1610040360049, "tmdate": 1610473950173, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work proposes a novel metric for measuring calibration error in classification models.\n\nPros:\n* Novel calibration metric addressing limitations of previously used metrics such as ECE\n\nCons:\n* Limited experimental validation on CIFAR-10/CIFAR-100 only\n* Unclear impact beyond proposing a new calibration metric\n* Unclear value of using the proposed UCE metric for regularization and OOD detection\n\nAll reviewers appreciate the aim of the work to produce a calibration metric that addresses shortcomings of commonly used existing metrics such as expected calibration error (ECE), which is known to be sensitive to discretization choices.  However, all reviewers remain in doubt whether the proposed metric (uncertainty calibration error, UCE) is truly a better metric of calibration than previous proposals.  This doubt comes from two sources: 1. limited experiments that do not convincingly show the usefulness of UCE; and 2. interpretability of UCE not being as intuitive to the reviewers.  The experiments also use UCE as regularizer but the benefit of doing so over simple entropy regularization is not clear.\n\nOverall the work is well-motivated and written and the proposed UCE measure is interesting.  However, the reviewers remain unconvinced of the claimed benefits and the potential impact for measuring or improving calibration."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040360033, "tmdate": 1610473950156, "id": "ICLR.cc/2021/Conference/Paper3535/-/Decision"}}}, {"id": "C7VCzjPHm2j", "original": null, "number": 1, "cdate": 1603897934480, "ddate": null, "tcdate": 1603897934480, "tmdate": 1606809882045, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Review", "content": {"title": "Reviewer3", "review": "The work addresses an important problem in the study of uncertainty estimation: how does one compare model uncertainty at differing accuracy levels? The work proposes a novel uncertainty metric, relates this to existing methods and provides robust evaluation of the various merits of this approach. The paper is easy to follow. \n\nI have the following concerns with the work:\n1. Regarding the use of UCE as a regularizer: how is the described behaviour of UCE different from the NLL loss? The NLL loss should penalize highly confident incorrect predictions and strives for confident predictions in high accuracy. What does the UCE regularizer add here?  Can table 2 include a non-regularized baseline as well to study this? In appendix A.3. it is said that UCE performs on par without regularization; then what is the point of proposing UCE as a regularizer?\n2. What is the point of proposing the use of the normalized entropy as a thresholding factor for OOD detection? It seems that vanilla entropy would behave exactly the same. Is this considered to be a novel contribution in this work? \n3. Why is figure 2 (right) completely flat for UCE? Are there values not shown here where calibration error does change? Perhaps this should be included in the plot. \n4. Am I correct to say that max-p-based metrics might be preferable in very large-class problems such as language models? The paper does not discuss the computational tradeoffs of the method, and I believe this should be included. \n5. It appears that the experiment section does not provide much evidence  that this metric is favourable in selecting the best model for a downstream task where uncertainty is needed. This could be evaluated by e.g. an active learning problem. I believe it makes sense to include such an experiment. Right now, Table 1 and the accompanying discussion does not convince me that UCE is somehow more beneficial.\n\nOverall, the work has merit and of interest to the community. However, the proposal of the use of the metric as a regulariser and a OOD scoring function seems unproductive and if so, distracts from the core contribution. This core contribution is understudied in the work. The work would benefit from more analysis into the computational tradeoffs, and evaluation of the signal that the proposed metric provides on model selection for downstream uncertainty tasks.\n\nNitpick:\n- Table 1 could benefit from a vertical bar between the two datasets to clarify that the numbers are not comparable. \n\nUpdate:\nAlthough the paper has improved, I still vote for rejection. The new insight of binary-classwise v/s multiclass UCE as a regularizer seems poorly explored in the paper and would benefit from closer study. This appears to be the basis of the improved results in table 1.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074233, "tmdate": 1606915761216, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3535/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Review"}}}, {"id": "8nTzC1CtA26", "original": null, "number": 4, "cdate": 1603950330703, "ddate": null, "tcdate": 1603950330703, "tmdate": 1606768121252, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Review", "content": {"title": "Well-motivated metric for uncertainty calibration; novelty is unclear", "review": "Update: After reading the other reviews and responses, and in light of the authors' updates to the paper, I have increased my score to a 6.\n\nThis paper proposes a new metric for uncertainty calibration, based on comparing the entropy of the marginal class probabilities conditioned on predicted class with the entropy of the predicted probabilities. The metric avoids the failure mode of ECE, where predicting the relative frequencies of classes results in perfect calibration, and can be used as a regularizer in a loss function. The paper demonstrates that regularization with UCE yields better-calibrated uncertainty on CIFAR predictions without sacrificing accuracy.\n\nThe paper is well-written and well-motivated. I\u2019m uncertain as to its novelty. In particular, entropy as a basis for uncertainty estimation is well-explored (and was used as a baseline in (Lakshminarayanan et al., 2017) as well as Jie Ren et al., \u201cLikelihood ratios for out-of-distribution detection,\u201d NEURIPS 2017). It\u2019s unclear what the results in Figure 3 contribute in light of these baselines (besides the normalization by the constant C).\n\nWhich loss function was used to produce the results in Table 1? If it\u2019s the loss in (25), it would also be useful to see calibration metrics for NLL loss alone.\n\nFigure 2 shows strong sensitivity of ACE to the number of bins. Quantile ECE (an ECE metric with bins defined by quantiles instead of fixed-width) often shows less sensitivity -- was this metric considered as well?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074233, "tmdate": 1606915761216, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3535/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Review"}}}, {"id": "Fd2OJAEd-AD", "original": null, "number": 19, "cdate": 1606244170719, "ddate": null, "tcdate": 1606244170719, "tmdate": 1606244170719, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "cXlTFYNwq4S", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Additional Experiments", "comment": "Dear AnonReviewer3, please see the rebuttal version for the added toy experiments. We are still working your suggested active learning experiment. However, due to the time constraints of the rebuttal phase, we were not able to provide these results yet. We will include the results in a possible camera-ready version."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "4pFBsGON1Yp", "original": null, "number": 18, "cdate": 1606242242988, "ddate": null, "tcdate": 1606242242988, "tmdate": 1606242242988, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Please See Rebuttal Version", "comment": "Dear reviewers, we thank you again for your valuable feedback that helps us greatly to improve our manuscript. Please see \u00a7 7 in the rebuttal version for new and updated results. We will integrate the parts of \u00a7 7 into the main text and update the rest of the manuscript according to your suggestions in a possible camera-ready version. We hope to have addressed all raised concerns and appreciate an update on the rating."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "qQ0uD5TVX7A", "original": null, "number": 17, "cdate": 1606201356306, "ddate": null, "tcdate": 1606201356306, "tmdate": 1606201356306, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "CvMv16ohWIe", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Additional Experiments", "comment": "We already added the results for SVHN and will add the results for Fashion-MNIST later today."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "CvMv16ohWIe", "original": null, "number": 16, "cdate": 1606159446783, "ddate": null, "tcdate": 1606159446783, "tmdate": 1606159446783, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "b9qljoluQPv", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "quantile ECE", "comment": "I believe you're correct that Ovadia et al. did not compare quantile and fixed-width ECE, and it is unclear in the paper and code which one they used (there is a `  get_quantile_bins`   method, but it doesn't appear to be called). Thank you for updating Table 2.\n\nIn a response to another comment, you mentioned that you were running experiments on Fashion-MNIST and SVHN, are those results in?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "b9qljoluQPv", "original": null, "number": 15, "cdate": 1606077858844, "ddate": null, "tcdate": 1606077858844, "tmdate": 1606077858844, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "UHyloZDTaMf", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Regarding Quantile ECE", "comment": "Please see Sect. 7 for our updated Table 2 as per your suggestion. We compare the regularization methods at optimal temperature as suggested by Ashukha et al. (2020).\nThank you for pointing out this highly relevant paper, which we should have considered in the first place. We have carefully read the paper and did not find any comparison between quantile ECE and fixed-width ECE. After reviewing the provided source code (https://github.com/google-research/google-research/blob/master/uq_benchmark_2019/metrics_lib.py), we think that Ovadia et al. (2019) only used fixed-width bins in ECE computation. The only relevant mentioning seems to be:\n\n> When bins $ \\\\{\\rho_s : s\\in 1\\ldots S \\\\} $ are quantiles of the held-out predicted probabilities, $|B_s|\\approx|B_k|$ and the estimation error is approximately constant.\n\nWe think that this describtion of using quantiles as bin edges is equivalent to ACE. Ovadia et al. (2019) state that the ECE estimation error is constant across all bins when using quantiles. However, we did not find any statement saying that quantile ECE is robust against a varying number of bins. Please let us know if we have misunderstood anything."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "UHyloZDTaMf", "original": null, "number": 14, "cdate": 1605915070787, "ddate": null, "tcdate": 1605915070787, "tmdate": 1605915070787, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "wd8-Yb9VujY", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Appreciate the clarifications", "comment": "Thank you for clarifying the role of unnormalized entropy and the point you're making in Figure 3. Quantile ECE was used in Ovadia et al., 2019, \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift\", and reduces bias as compared with fixed-width ECE. Looking forward to your updated manuscript, after which I'll reevaluate my rating."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "kX4DvwuE8D", "original": null, "number": 13, "cdate": 1605884738799, "ddate": null, "tcdate": 1605884738799, "tmdate": 1605884738799, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "rFTTGLW8FiE", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "UCE", "comment": "In this specific binary classification example with $ C=2 $, our Proposition 1 does not hold and we would rather suggest to use max p instead of $ \\tilde{\\mathcal{H}} $ (which is effectively regularization with classwise ECE (Kull et al., 2019)). Then, we would have\n```\nprint(cross_entropy(pred_a, true))\nprint(cross_entropy(pred_b, true))\n\nprint(classwise_ece(pred_a, true))\nprint(classwise_ece(pred_b, true))\n```\n```\n0.2558\n0.2298\n\n0.0100\n0.0800\n```\nHowever, in a multiclass scenario, the same holds for UCE (plus the benefits of UCE )."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "rFTTGLW8FiE", "original": null, "number": 12, "cdate": 1605879402532, "ddate": null, "tcdate": 1605879402532, "tmdate": 1605879402532, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "4wLgZuv5QVo", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Good catch!", "comment": "Thank you for spotting the mistake! It seems that when I flip the labels in my example, the (over)confident predictions still have the higher loss:\n```true = np.asarray([0] * 9 + [1]```\n('less confident', 0.36177298426628113, 'confident', 0.9211241006851196)\n\n(Although I would agree that your example is more realistic).\n\nI wonder how UCE compares here, but would you agree that this example indicates that the difference between NLL and UCE as a regulariser is perhaps more subtle than described in the paper? \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "TzHBni0BBH", "original": null, "number": 11, "cdate": 1605878491656, "ddate": null, "tcdate": 1605878491656, "tmdate": 1605878491656, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "cXlTFYNwq4S", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you again for your valuable feedback. We are currently working hard on a revision of our manuscript and would appreciate an updated rating if we were able to address all your concerns."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "4wLgZuv5QVo", "original": null, "number": 10, "cdate": 1605877486369, "ddate": null, "tcdate": 1605877486369, "tmdate": 1605877936165, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "iymdlltTViD", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Response to NLL Concerns", "comment": "Dear AnonReviewer3, thank you for this helpful code example.\n\n1. We think that you may have swapped the labels in line 7. Let us prove a specific code example:\n```\npred_a = np.asarray([[0.9,1-0.9]]*9 + [[0.8,1-0.8]])\npred_b = np.asarray([[0.99,1-0.99]]*9 + [[0.89,1-0.89]])\ntrue = np.asarray([0] * 9 + [1])\n'less confident', float(cross_entropy(np.log(pred_a), true)), 'confident', float(cross_entropy(np.log(pred_b), true))\n```\n('less confident', 0.2557682553354537, 'confident', 0.22977279358712344)  \nI.e. NLL further pushes the confidence of the predictions to 1.0, favoring overconfidence.\n2. By classwise we mean $ \\frac{1}{C} \\sum_{c=1}^{C} \\mathrm{UCE}_{c} $, where $ \\mathrm{UCE} _{c} $ is computed for samples of class $ c $. We will add sentences on this to our revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "cXlTFYNwq4S", "original": null, "number": 9, "cdate": 1605866248232, "ddate": null, "tcdate": 1605866248232, "tmdate": 1605866248232, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "ml__1UKzm7g", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "re 2", "comment": "Re 2: This is a good point, agreed. The accompanying proof of this fact is also of value here. I do think it's helpful to acknowledge this in the paper.\n\nRe 3: That sounds good! Perhaps exploring the limit all the way to 1 bins can help clarify that there is a minimum number of bins required to correctly estimate the metric. \n\nRe 4: Great, I think that insight is helpful for the reader.\n\nRe 5: This seems to me to be the missing piece of the story. If you propose that UCE should be the go-to metric for comparing different methods/models on calibration error,  it seems important to demonstrate practical settings in which UCE is more informative than alternative metrics for model selection."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "iymdlltTViD", "original": null, "number": 8, "cdate": 1605864882335, "ddate": null, "tcdate": 1605864882335, "tmdate": 1605864900986, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "ml__1UKzm7g", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "regarding NLL", "comment": "Re 1.\nIt seems that this is an incorrect characterisation of NLL. NLL heavily penalizes overconfident incorrect predictions, see the following:\n```\nimport jax\nfrom jax import nn\nfrom jax import numpy as np\n\npred_a = np.asarray([[0.8,0.2]]*10)\npred_b = np.asarray([[0.9999,1-0.9999]]*10)\ntrue = np.asarray([1] * 9 + [0])\n\ndef cross_entropy(logprobs, target_class):\n  nll = np.take_along_axis(logprobs, np.expand_dims(target_class, axis=1), axis=1)\n  ce = -np.mean(nll)\n  return ce\n\n'less confident', float(cross_entropy(np.log(pred_a), true)), 'confident', float(cross_entropy(np.log(pred_b), true))\n```\n('less confident', 1.470808506011963, 'confident', 8.28931713104248)\n\nI'm not sure what you mean with classwise. Do you mean binary multi-class prediction rather than categorical prediction?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "H6kLa6Z5xi2", "original": null, "number": 7, "cdate": 1605618670015, "ddate": null, "tcdate": 1605618670015, "tmdate": 1605618893025, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "2nu6qeuQS_B", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Response", "comment": "Dear AnonReviewer4, thank you for appreciating our work and for your thorough review. Below we try to respond to each issue raised and hope to meet your expectations.\n\n1. Your main concern seems to be that we do not make a strong statement that our metric is beneficial. As it is difficult to highlight the distinct strengths of our metric in real-world experiments, we additionally conducted toy experiments that clearly show cases where UCE is able to capture miscalibration but other metrics fail. We emphasize that ECE and MMCE can be minimized by models with constant output and that ACE produces arbitrary values for a varying number of bins. We are confident that our metric provides reasonable benefits and can be useful to the community. We will highlight the benefits more in our revision and hope that we can convince you of this as well.\n2. When we compare at optimal temperature (as suggested by Ashukha et al. (2020)), UCE regularization is at least as good as MMCE reg. and outperforms entropy reg. (see Fig. 6 in appendix). We will add a table with metric values at optimal temperature to the main text to highlight the benefits of UCE regularization. However, using the UCE as a regularizer is not our main contribution and, as you have already mentioned, rather an interesting additional feature.\n3. We think that improved model selection is mainly given by avoiding the pathologies of the other metrics. Secondly, we argue that UCE is as interpretable as ECE and easier to understand for practitioners than e.g. MMCE. We would gladly conduct another experiment that can directly measure and compare the metrics, if you have any suggestion.\n\nAnswers to additional comments\n\n1. Thank you for your suggestion. We will include notes about NLL and Brier being strictly proper scoring rules.\n2. Thank you for pointing out that the definition of perfect calibration can be traced back beyond Guo et al., (2017). We will revisit Brier (1950) and update our manuscript accordingly.\n3. Thank you for this comment. We agree that our assumption is strong for small numbers of classes (e.g. $C < 10$). However, we think that this assumption is reasonable in empirical settings, where $ C = 100 $ (CIFAR-10) or $ C = 1000 $ (ImageNet). We explicitly mention the multi-class setting for our metric in the title of our paper. To shed additional light on this, we will add a figure to our manuscript that shows the effect of an increasing number of classes on the normalized entropy and will discuss this caveat in our conclusion.\n4. Thank you for pointing out this confusion. As you suggested above, we will add a note about strictly proper scoring rules to our revision. Further, we will visually separate the NLL and Brier score values in Table 1 in order to not directly compare calibration metrics to strictly proper scoring rules.\n5. As already mentioned above, UCE regularization outperforms entropy regularization when compared at optimal temperature (Ashukha et al., 2020). This can already be seen in Figures 6 & 7 and holds for both CIFAR-10 and CIFAR-100. We will further highlight this in our revision. Additionally, we will follow your suggestion and add the non-regularized baseline to the table.\n6. Thank you for pointing out relevant prior work. We will consider this on our revision. It is correct that the rejection experiments have already been conducted with unnormalized entropy. Our experiments aim at highlighting the interpretability of UCE/normalized entropy: Rejecting test samples where $ \\tilde{\\mathcal{H}} > 0.2 $ will result in a classification error $< 0.2$ for the remaining samples if the model is well-calibrated (see Fig. 9 & 2). We argue that using normalized entropy as uncertainty measure is as interpretable as max p, but avoids the pathologies of max p when used in a calibration metric. We will make this clearer in the text.\n\nThank you again for your detailed review. We hope to have taken all concerns into account and are currently working hard on the revision of our manuscript. We hope that we can convince you of our work and acknowledge an update of your rating. We are open for any further discussion.\n\nReferences\n\n*see paper*"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "ml__1UKzm7g", "original": null, "number": 6, "cdate": 1605436613590, "ddate": null, "tcdate": 1605436613590, "tmdate": 1605436613590, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "C7VCzjPHm2j", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Benefits of UCE", "comment": "Dear AnonReviewer3, thank you for recognizing the importance of our work and for your constructive feedback. In the following we will address your concerns point by point.\n\nAd 1.: First, we want to stress out that the use of UCE as a regularizer is not our main finding, but rather an interesting fact. UCE regularization works best when computed classwise (in similar manner to ACE). Consider the following example: A batch with mainly samples from class 1 and few samples from class 2 are all predicted as class 1 with high confidence. Increasing the confidence of all predictions further reduces the NLL, whereas UCE is only reduced if the confidence of the overconfidently false predictions is reduced. Moreover, when compared at optimal temperature (as suggested by Ashukha et al. (2020)), UCE regularization is at least as good as MMCE reg. and outperforms entropy reg. (see Fig. 6 in appendix). We will add a table with metric values at optimal temperature to the main text to highlight the benefits of UCE regularization.\n\nAd 2.: Thank you for your question. It is correct that the rejection experiments could have been conducted with vanilla entropy. Our experiments aim at highlighting the interpretability of UCE/normalized entropy: Rejecting test samples where $ \\tilde{\\mathcal{H}} > 0.2 $ will result in a classification error $< 0.2$ for the remaining samples if the model is well-calibrated (see Fig. 9 & 2). We argue that using normalized entropy as uncertainty measure is as interpretable as max p, but avoids the pathologies of max p when used in a calibration metric. We will make this clearer in the text.\n\nAd 3.: Thank you for pointing that out. The figure was created by incrementing the number of bins in steps of 5 from 5 to 100. We will recreate this figure using an increment of 1 and a smaller y axis range. Small fluctuations of the UCE values should then become visible. However, this does not change our finding that UCE is not sensitive to the number of bins and provides a consistent ranking of the models.\n\nAd 4.: This is an interesting comment, indeed. For very large-class problems,  max p based metrics and our metric should be equivalent, but max p based metrics are computationally more efficient. We will follow your suggestion and discuss this in our upcoming revision.\n\nAd 5.: Thank you for your assessment of our experiments. We already conducted additional toy experiments that better highlight the benefits of UCE. The experiments show that UCE can measure miscalibration where the other metrics fail. We are currently working hard to also include an active learning experiment. We hope that this will convince you of our work.\n\nWe hopefully have considered all of your concerns and welcome any further discussion.\n\nReferences\n\n*see paper*"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "n75AOuEvCDu", "original": null, "number": 5, "cdate": 1605260754465, "ddate": null, "tcdate": 1605260754465, "tmdate": 1605260754465, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "jyaNcLhjGjG", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Relevant Work", "comment": "Dear Jize Zhang, thank you for pointing out your highly relevant work. We will review your paper and consider it in our upcoming manuscript."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "jYJLKBQhSn_", "original": null, "number": 4, "cdate": 1605260646765, "ddate": null, "tcdate": 1605260646765, "tmdate": 1605260646765, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "uuO5BygoZt-", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Reducing Confusion", "comment": "Dear AnonReviewer1, we thank you very much for your thorough review of our work and the positive comments. In the following we try to address all your concerns point by point.\n\n> If found the experiments to be well aligned to evaluate the approach, although limited in terms of dataset used (only CIFAR-10 and CIFAR-100), a greater variety of datasets would be more convincing in the of overall good performances of the approach, especially if datasets with a varied number of classes can be tested.\n\nThank you for mentioning our limited use of data sets. As of writing this, we conduct additional experiments on SVHN and Fashion-MNIST and provide the results in our revised manuscript. We expect these results to be aligned with the results on CIFAR-10/100.\n\n> Moreover, looking at the results in detail (Table 1), UCE does not appear to be particularly strong, having a worse calibration than ECE and ACE on CIFAR-10, but slightly better on CIFAR-100, assuming that we want it to be increased to reach the real error rate obtained.\n\nAfter reading your comment, we realized that we do not present our results clearly and comprehensibly. Table 1 shows the considered metrics on uncalibrated models and we do not expect any metric to reflect the model error at this point. Moreover, we argue that for well-calibrated models, the normalized entropy (as notion of uncertainty) should reflect the model error. We do not argue that the value of the metric itself reflects the error; it rather shows the deviation of our assumption of perfect calibration, see Eq. (22). We will add details to the caption of Table 1 and rewrite the sentences discussing the results in the main text. Thank you very much for drawing our attention to this.\n\n> Moreover, the presentation of the results in Table 1 is messy: it gets difficult to match the calibration error with the accuracy, providing the classification error instead of accuracy would help to make a direct comparison with calibration error.\n\nWe think that this issue is mainly addressed above. We provide the accuracy as we do not expect the calibration metrics to equal the error in Table 1; and providing the classification error could further add confusion. However, we will rework Table 1 as suggested by AnonReviewer3 and hope that this, in addition to a more detailed description of the results, will meet your expectations.\n\n> Moreover, why the last two columns in Table 1 (Brier and NLL) are provided as floating-point values instead of percentages as with the other columns. That's unnecessary confusion that should be fixed.\n\nThank you for pointing that out. We mainly followed related work where ECE-like metrics were provided as percentages, and Brier and NLL were provided as floating-point values, e.g. see (Kumar et al., 2018). Moreover, as pointed out by AnonReviewer4, NLL and Brier are strictly proper scoring rules and have to be decomposed in order to be directly comparable to other calibration metrics. We will visually seperate the NLL and Brier scores from the other calibration metrics using a vertical bar to reduce this confusion (see also answer to AnonReviewer4).\n\n> Conversely, I am not sure of the relevance of providing all the detailed information on Bayesian methods in the second part of Sec. 2. It can be presented in a more concise way, as it uses a lot of space to explain well-known approaches.\n\nThank you for this suggestion. We will shorten the description of the Bayesian methods and use the free space for the results of the new experiments.\n\n> In terms of potential impact of that paper, I still need to be convinced. What can tell me that this is just not yet another calibration metric. I think that the paper can have been made stronger on that aspect.\n\nMany recent papers have highlighted the need for an appropriate calibration metric (see our response to AnonReviewer2). Our metric reliably detects miscalibration as it avoids various pathologies of other metrics. We are convinced that our work is a valuable contribution to the community. To respond to your comment, we will rephrase the conclusion of our paper to make it stronger. In addition to the expected results from the new experiments, we hope to convince you and would be very grateful if you would update your rating accordingly.\n\nReferences\n\n*see paper*"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "jyaNcLhjGjG", "original": null, "number": 1, "cdate": 1605257340766, "ddate": null, "tcdate": 1605257340766, "tmdate": 1605257340766, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Public_Comment", "content": {"title": "Related work using KDE to mitigate the bias/binning issues in calibration error estimation", "comment": "Please check out our recent work on the use of KDE-based ECE estimator [1]. By replacing histogram with KDE, we provide a more reliable evaluation of the calibration error while mitigating the bias & binning sensitivity of existing histogram ECE estimators. The code is also available online.\n\n[1] Jize Zhang, Bhavya Kailkhura, and T Han. \"Mix-n-Match: Ensemble and compositional methods for uncertainty calibration in deep learning.\", ICML 2020, https://arxiv.org/pdf/2003.07329.pdf\n\n"}, "signatures": ["~Jize_Zhang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Jize_Zhang1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors", "ICLR.cc/2021/Conference/Paper3535/Reviewers", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024954461, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Public_Comment"}}}, {"id": "sa5tXQc0DzB", "original": null, "number": 3, "cdate": 1605192755334, "ddate": null, "tcdate": 1605192755334, "tmdate": 1605192755334, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank all reviewers for their valuable feedback, as it helps us to improve our paper considerably. We are currently conducting additional experiments as requested by the reviewers and will update the manuscript accordingly. We welcome an open discussion and are working hard to address all issues raised."}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "wd8-Yb9VujY", "original": null, "number": 2, "cdate": 1605192254779, "ddate": null, "tcdate": 1605192254779, "tmdate": 1605192254779, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "8nTzC1CtA26", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment", "content": {"title": "Regarding novelty", "comment": "Dear AnonReviewer2, thank you very much for your valuable feedback. In the following, we try to address every raised concern and hope to meet your expectations.\n\nThank you for pointing out two relevant papers, one of which we have already taken into account. We will review the other and consider it in our manuscript. Your main concern seems to be the lack of novelty of our contribution. We do not propose the use of (normalized) entropy for measuring uncertainty as sole contribution, since this has been extensively studied in the papers you mentioned. Rather, the proposed novelty is a metric for measuring calibration (of a classification model) based on normalized entropy. Recent well-recognized papers have highlighted the lack of a suitable calibration metric and we aim to address this issue (Nixon et al., 2019; Ashukha et al., 2020; Kull et al., 2019; Kumar et al., 2019). The perfect calibration metric has yet to be found, and we believe to make a valuable contribution towards it. Our metric avoids pathologies of other metrics and has several favorable properties for measuring calibration (see p. 5). We hope that we have met your requirement for novelty.\n\nThe contribution of Figure 3 is to show that for calibrated models, normalized entropy correlates with top-1 error, thus additionally providing an empirical justification for the use of normalized entropy in our metric (and definition of perfect calibration). The top-1 error decreases monotonically with the normalized entropy. Moreover, the results of Fig. 3 are novel, as Lakshminarayanan et al. (2017) only used max p for their rejection experiments. In our results, Bayesian methods perform much better as reported by Lakshminarayanan et al. (2017) and yield a top-1 error close to 0 for predictions with uncertainty < 0.1. We will add sentences to our manuscript for clarification.\n\nThe results reported in Table 1 were produced using only NLL/cross-entropy loss. We will add the unregularized baseline to Table 2 for better comparison. Many thanks for this advice!\n\nWe did not consider quantile ECE and will gladly include this in our revision. However, we did not find any related work describing quantile ECE in more detail. Can you point out a reference or describe quantile ECE briefly and how it differs from ACE?\n\nWe hope that we have addressed all your concerns and are grateful if you update your rating of our work accordingly.\n\nReferences\n\n*see paper*"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XOuAOv_-5Fx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3535/Authors|ICLR.cc/2021/Conference/Paper3535/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836592, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Comment"}}}, {"id": "uuO5BygoZt-", "original": null, "number": 3, "cdate": 1603941605401, "ddate": null, "tcdate": 1603941605401, "tmdate": 1605023984083, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Review", "content": {"title": "Good proposal but an enhanced set of experiments are required", "review": "This paper proposes a new calibration error measurement named UCE (Uncertainty Calibration Error) for deep classification models. It consists in doing a calibration in order to achieve \"perfect calibration\" (i.e., the uncertainty provided is equivalent to the classification error at all levels in [0, 1]), relying on normalized entropy for multiclass classification. This UCE is well justified for classification problems with several classes to process, where the entropy is demonstrated to be asymptotically equivalent to the classification (top-1) error. A point with this UCE metric is that is has some interpretability properties in terms of its value, and is said to be robust to the number of bins used.\n\nThe proposed metric is well explained, and justified, although I am wondering how well stands the assumption that the normalized entropy approaches the top-1 error for reasonable number of classes (e.g. C=10, as with CIFAR-10, or C=100, as with CIFAR-100). The properties presented are interesting.\n\nIf found the experiments to be well aligned to evaluate the approach, although limited in terms of dataset used (only CIFAR-10 and CIFAR-100), a greater variety of datasets would be more convincing in the of overall good performances of the approach, especially if datasets with a varied number of classes can be tested. Moreover, looking at the results in detail (Table 1), UCE does not appear to be particularly strong, having a worse calibration than ECE and ACE on CIFAR-10, but slightly better on CIFAR-100, assuming that we want it to be increased to reach the real error rate obtained. Moreover, the presentation of the results in Table 1 is messy: it gets difficult to match the calibration error with the accuracy, providing the classification error instead of accuracy would help to make a direct comparison with calibration error. Moreover, why the last two columns in Table 1 (Brier and NLL) are provided as floating-point values instead of percentages as with the other columns. That's unnecessary confusion that should be fixed.\n\nOverall, I found the paper to be correct, relatively well written. I think that more room should have been given to experimentations, like with other datasets and with more space for OoD rejection and detection. Conversely, I am not sure of the relevance of providing all the detailed information on Bayesian methods in the second part of Sec. 2. It can be presented in a more concise way, as it uses a lot of space to explain well-known approaches.\n\nIn terms of potential impact of that paper, I still need to be convinced. What can tell me that this is just not yet another calibration metric. I think that the paper can have been made stronger on that aspect.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074233, "tmdate": 1606915761216, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3535/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Review"}}}, {"id": "2nu6qeuQS_B", "original": null, "number": 5, "cdate": 1604020669634, "ddate": null, "tcdate": 1604020669634, "tmdate": 1605023983946, "tddate": null, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "invitation": "ICLR.cc/2021/Conference/Paper3535/-/Official_Review", "content": {"title": "Review", "review": "Thanks for the interesting paper!\n\nSummary\n\nThe authors focus on the important problem of improved calibration measures as compared to the (now fairly standard) expected calibration error (ECE). More specifically, they define a new \"Uncertainty Calibration Error\" (UCE) metric based on the normalized entropy of the predictive distribution, rather than the max probability (as in ECE). The metric still uses fixed-width binning (as in ECE), and they motivate the interpretation w.r.t. perfect calibration based on a theoretical limit. They provide a set of experiments to show the differences in model ranking, sensitivity to number of bins, etc. between UCE, ECE, etc. on various models.\n\nStrengths\n\n- As noted in previous literature (and referenced in this paper), improved measures of calibration is an important research area.\n- The authors provide a great background section to place their research within the broader area of uncertainty research.\n- The experiments on sensitivity to the number of bins is informative and highly relevant in the context of previous literature (e.g., Nixon et al. (2019)) where it has been shown that ECE is particularly sensitive to this setting.\n\nWeaknesses\n\nAs I have noted in more detail down below, I believe this paper suffers from a few weaknesses. Overall, at the end of the paper as a reader, I'm still left with questions of whether UCE is truly a better calibration metric. As noted above, the insensitivity to number of bins is great and an improvement on ECE and ACE. However, I don't believe the remaining experiments make a strong case that the metric (1) provides a better measure of calibration, (2) yields consistently improved model performance when used as a regularizer (though it's interesting that it can be used as one!), or (3) allows for improved model selection. Additionally, I believe its interpretability is limited. As noted below, the experiments have mixed results or make comparison claims that detract from the overall message, which I find troubling from an experimental rigor standpoint. Furthermore, I think this paper would benefit from experiments that are set up such that they can directly measure and compare the ability of the metrics to measure calibration error.\n\nRecommendation\n\nGiven the above strengths and weaknesses, I'm currently inclined to suggest rejection of the paper in its current form. However, I think this could be a great paper and as a community I don't believe we have yet to devise the perfect calibration metric -- perhaps this could be it! I would highly recommend the authors push on the points above.\n\nAdditional comments\n\n- p. 3, 4: It could be informative to includes notes about NLL and Brier score being strictly proper scoring rules (Gneiting & Raftery, 2007; Parmigiani & Inoue, 2009) that theoretically should be maximized only when the forecaster emits the distribution they believe to be true, and thus should, in theory, be well-calibrated asymptotically. However, we indeed know from Guo et al. (2017) that empirically, models can still overfit, leading to poor calibration.\n- p. 4: The definition of perfect calibration can be traced back to Brier (1950), and, unlike ECE, is not limited to only the max predicted probability. Rather, for any predicted probability $p_k$ for class $k$, the probability that the true class is class $k$ should be equal to $p_k$ for all $p_k$ and all $k$. That is, $\\mathbb{P}(Y = k | P_k = p_k) = p_k, \\forall p_k \\in [0, 1], \\forall k \\in [1, K]$.\n- p. 5: UCE is based on an argument that normalized entropy approaches the top-1 error in the limit of number of class going to infinity. While this is interesting theoretically, this assumption seems too strong for empirical settings, and I think this affects the interpretability of the metric as claimed in the conclusion.\n- p. 6, 7, Section 5.1: This section (and Figures 4 & 5 in the appendix) make claims that NLL and Brier score \"fail at comparing calibration of models with different accuracy, as the metrics are always lower for models with better accuracy\". I find this argument both surprising and confusing in terms of motivation. As strictly proper scoring rules, they should indeed have lower values for better probabilistic models. Although accuracy is a non-proper scoring rule, it should still correlate well with those strictly proper rules, so it is expected that the better models with lower NLL / Brier score will (typically with some variance) have higher accuracy (variance being due to the non-proper nature of accuracy). All strictly proper scoring rules can be decomposed into calibration and refinement terms (Parmigiani & Inoue, 2009; DeGroot & Fienberg, 1983), but in the non-decomposed setting, it is not expected that these rules would directly measure calibration. Therefore, given the focus on calibration measures, I'm confused as to the motivation behind comparing to NLL & Brier score directly (beyond the overconfidence analysis from Guo et al. (2017)) as a means of motivating the usefulness of UCE.\n- p. 8, 12: In the regularization experiment, different regularization approaches are being compared in terms of calibration, but it's difficult to assess the results. In Table 2 (which needs an additional entry for the non-regularized result from Table 1), UCE regularization appears to improve accuracy, NLL, and Brier score over the non-regularized baseline. Interestingly though, NLL, Brier score, ECE, ACE, UCE, and MMCE (i.e, all metrics other than accuracy) point towards entropy-regularization being superior. It does result in a lower accuracy than UCE reg and the baseline, but by the other metrics (including the strictly proper scoring rules NLL and Brier score), it produces a better probabilistic model. For CIFAR-10 UCE reg is worse than the other regularization methods and the baseline.\n- p. 8: Rejection & OOD Detection: This has been studied previously for unnormalized entropy, which should yield the same results. See, e.g., Malinin & Gales (2018), Ren et al. (2019).\n\nMinor\n\n- p. 3: s/as non Bayesian/as a non-Bayesian/\n- p. 7: Figure 1 is too small.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3535/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3535/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty Calibration Error: A New Metric for Multi-Class Classification", "authorids": ["~Max-Heinrich_Laves1", "~Sontje_Ihler1", "kortmann@imes.uni-hannover.de", "ortmaier@imes.uni-hannover.de"], "authors": ["Max-Heinrich Laves", "Sontje Ihler", "Karl-Philipp Kortmann", "Tobias Ortmaier"], "keywords": ["variational inference", "uncertainty", "calibration", "classification"], "abstract": "Various metrics have recently been proposed to measure uncertainty calibration of deep models for classification. However, these metrics either fail to capture miscalibration correctly or lack interpretability. We propose to use the normalized entropy as a measure of uncertainty and derive the Uncertainty Calibration Error (UCE), a comprehensible calibration metric for multi-class classification. In our experiments, we focus on uncertainty from variational Bayesian inference methods and compare UCE to established calibration errors on the task of multi-class image classification. UCE avoids several pathologies of other metrics, but does not sacrifice interpretability. It can be used for regularization to improve calibration during training without penalizing predictions with justified high confidence.", "one-sentence_summary": "We present an uncertainty calibration error metric based on normalized entropy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laves|uncertainty_calibration_error_a_new_metric_for_multiclass_classification", "supplementary_material": "/attachment/ff589741dbb9c6a311b02c079998b18799b6be68.zip", "pdf": "/pdf/29192b96d2d3f83f14cbb305bf70309a512a4852.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nPFas-bQA", "_bibtex": "@misc{\nlaves2021uncertainty,\ntitle={Uncertainty Calibration Error: A New Metric for Multi-Class Classification},\nauthor={Max-Heinrich Laves and Sontje Ihler and Karl-Philipp Kortmann and Tobias Ortmaier},\nyear={2021},\nurl={https://openreview.net/forum?id=XOuAOv_-5Fx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XOuAOv_-5Fx", "replyto": "XOuAOv_-5Fx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3535/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074233, "tmdate": 1606915761216, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3535/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3535/-/Official_Review"}}}], "count": 25}