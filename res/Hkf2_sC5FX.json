{"notes": [{"id": "Hkf2_sC5FX", "original": "r1lYDHPctm", "number": 392, "cdate": 1538087796305, "ddate": null, "tcdate": 1538087796305, "tmdate": 1547031779185, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJxy1kgTh7", "original": null, "number": 1, "cdate": 1541369559441, "ddate": null, "tcdate": 1541369559441, "tmdate": 1545354502419, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "Hkf2_sC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Meta_Review", "content": {"metareview": "\nPros:\n- Great work on getting rid of the need for QP and the corresponding proof of the update rule\n- Mostly clear writing\n- Good experimental results on relevant datasets\n- Introduction of a more reasonable evaluation methodology for continual learning\n\nCons:\n- The model is arguably a little incremental over GEM.  In the end I think all the reviewers agree though that the practical value of a considerably more efficient and easy to implement approach largely outweighs this concern.\n\nI think this is a good contribution in this area and I recommend acceptance.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Useful improvement over GEM and a good evaluation methodology"}, "signatures": ["ICLR.cc/2019/Conference/Paper392/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper392/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353233029, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkf2_sC5FX", "replyto": "Hkf2_sC5FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper392/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353233029}}}, {"id": "H1ltuen11V", "original": null, "number": 10, "cdate": 1543647344936, "ddate": null, "tcdate": 1543647344936, "tmdate": 1543647344936, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "HJe92m0RCm", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "content": {"title": "Updated Review", "comment": "Thank you for your detailed rebuttal and revisions to the paper. I do agree that you have addressed my primary concerns and clarified some areas of confusion for me about the paper. I have updated my score in favor of acceptance after the revisions. "}, "signatures": ["ICLR.cc/2019/Conference/Paper392/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper392/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619873, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkf2_sC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper392/Authors|ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619873}}}, {"id": "rklWHLB9hQ", "original": null, "number": 1, "cdate": 1541195320638, "ddate": null, "tcdate": 1541195320638, "tmdate": 1543646983049, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "Hkf2_sC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Review", "content": {"title": "There are some interesting ideas here. A more efficient version of GEM. ", "review": "This paper proposes a variant of GEM called A-GEM that substantially improves the computational characteristics of GEM while achieving quite similar performance. To me the most interesting insight of this work is the proof that an inner product between gradients can suffice instead of needing to solve the quadratic program in GEM \u2013 which I have found to be a major limitation of the original algorithm.  The additional experiments using task descriptors to enable zero shot learning are also interesting.  Moreover, the discussion of the new evaluation protocol and metrics make sense with further clarification from the authors. Overall, I agree with the other reviewers that this paper makes a clear and practical contribution worthy of acceptance. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper392/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Review", "cdate": 1542234471939, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkf2_sC5FX", "replyto": "Hkf2_sC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335713753, "tmdate": 1552335713753, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BygF0mRAC7", "original": null, "number": 9, "cdate": 1543590865511, "ddate": null, "tcdate": 1543590865511, "tmdate": 1543590865511, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "rJev5wEq6m", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "content": {"title": "Follow-up", "comment": "May we ask the reviewer, if we were able to address the main concerns that the reviewer had through our rebuttal and revision of the paper? Are there any further issues that the reviewer wants us to address? If so, we would appreciate your feedback and further discussion. "}, "signatures": ["ICLR.cc/2019/Conference/Paper392/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619873, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkf2_sC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper392/Authors|ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619873}}}, {"id": "HJe92m0RCm", "original": null, "number": 8, "cdate": 1543590833551, "ddate": null, "tcdate": 1543590833551, "tmdate": 1543590833551, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "B1lVPuVcpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "content": {"title": "Follow-up", "comment": "May we ask the reviewer, if we were able to address the main concerns that the reviewer had through our rebuttal and revision of the paper? Are there any further issues that the reviewer wants us to address? If so, we would appreciate your feedback and further discussion. "}, "signatures": ["ICLR.cc/2019/Conference/Paper392/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619873, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkf2_sC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper392/Authors|ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619873}}}, {"id": "HkeVfvnnhX", "original": null, "number": 3, "cdate": 1541355275730, "ddate": null, "tcdate": 1541355275730, "tmdate": 1543175415888, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "Hkf2_sC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Review", "content": {"title": "A-GEM is a a clear improvement over the previous approach (GEM)", "review": "The paper is well-written, with the main points supported by experiments.  The modifications to GEM are a clear computational improvement.\n\nOne complaint: the \"A\" in A-GEM could stand for \"averaging\" (over all task losses) or \"approximating\" (the loss gradient with a sample).  Both ideas are good.  However, the paper does not address the question: how well does GEM do when it uses a stochastic approximation to each task loss?  (Note I'm not talking about S-GEM, which randomly samples a task constraint; rather, approximate each task's constraint by sampling that task's examples).\n\nAnother complaint: reported experimental results lack any associated idea of uncertainty, confidence interval, empirical variation, etc.  Therefore it is unclear whether observed differences are meaningful.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper392/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Review", "cdate": 1542234471939, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkf2_sC5FX", "replyto": "Hkf2_sC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335713753, "tmdate": 1552335713753, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkepkT_u07", "original": null, "number": 5, "cdate": 1543175396640, "ddate": null, "tcdate": 1543175396640, "tmdate": 1543175396640, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "r1epCSNc67", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "content": {"title": "A model of a good author response", "comment": "I find your response very satisfying and highly professional.  Consequently I am upgrading my review."}, "signatures": ["ICLR.cc/2019/Conference/Paper392/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper392/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619873, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkf2_sC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper392/Authors|ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619873}}}, {"id": "B1lVPuVcpQ", "original": null, "number": 3, "cdate": 1542240347866, "ddate": null, "tcdate": 1542240347866, "tmdate": 1542240347866, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "rklWHLB9hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "content": {"title": "The use of task descriptors to expedite learning, LCA and a new train/ eval protocol are important contributions for lifelong learning ", "comment": "We thank the reviewer for providing the feedback on the draft. Following is our response to the the questions asked  by the reviewer:\n\nScattered Discourse:  \nOur motivation for working on lifelong learning is mostly based on the unprecedented opportunity to learn more quickly new tasks given the experience accumulated in the past. A major reason why catastrophic forgetting is bad is that it prevents the learner from quickly adapting to new tasks that are similar to old tasks.\nThe focus of this work is then on sample and computational efficiency in LLL. It is important to impose the restrictions of learning from few examples in a single pass (and to cross-validate on a different set of tasks to properly assess generalization in this single pass setting) as we really aim at models that learn quickly without iterating multiple times over the same data. Moreover, it is important to be able to measure how quickly one learns, and to improve efficiency of existing algorithms (A-GEM and compositional task descriptors).  The current evaluation framework that other works borrow from supervised learning (multiple passes over the data and cross-validate on the same tasks as used for testing) is often misleading, as the methodology (training protocol and metrics) is inadequate for evaluating continual learning algorithms. With this work, we hope to convince the research community to adopt our proposed training/evaluation protocol and to also consider sample/computational and memory efficiency in their metrics.\nWe hope the reviewer can find the revised paper more coherent and clear in this respect.\n\n1, 2: The reviewer is correct in saying that the use of compositional task descriptors and joint embedding models are not specific to A-GEM. In fact, we apply the joint embedding model also to the baseline methods and show improvements on those as well (see fig. 2 and fig. 4). The reason why we introduce them in this work is because a) there may be applications where an agent is given some sort of *description* of the task to perform, and b) since we focus on efficient learning (meaning, learning quickly from few examples), compositional task descriptors enable the learner to perform well at 0-/few-shot learning (see new fig. 5).\n\n3: \na) Training/ Evaluation Protocol: To the best of our knowledge, standard practice in LLL is to perform several passes over the data of each task, and several passes over the whole stream of tasks to set hyper-parameters, and then report error on the test set. This evaluation protocol is not adequate because the point of LLL is to quickly learn new tasks, and doing multiple passes over the data defeats the original purpose. Moreover, the prevalent protocol greatly puts the baseline,  which simply finetunes parameters from the previous task without any regularization, at disadvantage. The more the passes are done over the data of a given task, the more the model will forget. Therefore, the conclusions drawn from using the \u201csupervised learning\u201d protocol in a LLL setting can be highly misleading, while using the proposed methodology takes us closer to our goal to fairly assess algorithms in the continual learning setting.\n\nb) LCA: In the few shot learning literature, people specify the number of examples they will be given at test time, and use  $Z_b$ as defined in eq. 4, which is the average accuracy after seeing $b$ minibatches (or a certain number of examples). LCA is the area under the $Z_b$ curve. LCA is a better metric because it also contains information of the values of $Z_j$ for $j <= b$. If $b$ is relatively large, all methods produce similar average accuracy. LCA enables us to distinguish those models that have learned fast, because their 0 or few shot accuracy is higher. Since we care about how quickly a model learns, LCA is a useful metric to assess sample efficiency. \n\nc) Measuring performance on few examples: If by measuring performance on few examples, the reviewer mean reporting $Z_b$ numbers (Eq.4) and not taking the area under the $Z_b$ curve, then we would like to highlight that area under the curve (LCA) is capturing the learner's performance up to the $b$-th minibatch, giving the average profile of the complete few-shot region. $Z_b$, on the other hand, would only give the performance at the current mini-batch and will not have the path information. \n\n4: Adding error bars: As suggested, we have added the uncertainty estimates measured across multiple runs and seeds in the updated draft. Please take a look at Figs 1, 2 and Tabs. 4, 5, 6. Our conclusions are confirmed.\nRegarding running GEM with task descriptors, we have shown that GEM and A-GEM have similar performance on MNIST and CIFAR. We did not run it on CUB and AWA because GEM is too computationally expensive to run on larger models.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper392/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619873, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkf2_sC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper392/Authors|ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619873}}}, {"id": "rJev5wEq6m", "original": null, "number": 2, "cdate": 1542240143446, "ddate": null, "tcdate": 1542240143446, "tmdate": 1542240143446, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "BylvMvH5hm", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "content": {"title": "Section 5 is clarified. Train/ Eval protocol, new measure for efficiency and the use of task descriptors to expedite learning are novel contributions", "comment": "We thank the reviewer for providing the feedback on the draft. Here is our response to the questions asked by the reviewer:\n\nClarity About Section 5: We have updated the Section 5 of the paper and tried to add additional details about the model. Here are some clarifications:\n\n1 - Matrix Description t^k: The matrix description is not learnt. It is composed from class attributes. \nFor instance, in CUB each class is described by 312 attributes. If the current task has 10 classes, then the task descriptor is a matrix of size 10x312. The task descriptor is the same for all samples belonging to that task. As noted in the section, each input example consists of (x^k, y^k, t^k).\n\n2 - Variable size of the attribute matrix: Let A be the number of attribute per class (the same across all classes) and C^k the number of classes in task k, then the input task descriptor has size C^k \\times A. Module \\psi_{\\omega} is simply a matrix of size A \\times D embedding each attribute. By multiplying the input task descriptor with this embedding matrix, we obtain a matrix of size C^k \\times D, embedding each class descriptor. The joint embedding model scores each class by computing a dot product between the image features and the class embeddings (each row of the above matrix), and it turns this scores into probability values using a softmax, as shown in eq. 13.\n\nIf the model extracts good image features that reveal the underlying attributes, it can now perform 0 shot learning on unseen classes (as long as their constituent attributes have already been learned for other tasks albeit in different combinations).\n\nIn the rewrite of Section 5, we have clarified this point and the corresponding notation.\n \n3 -  Functions used to represent \\psi_{\\omega}: We use a lookup table whose parameter matrix has size A \\times D. \n\n4 - Confusion between C and C_k: The reviewer is correct. It should be C_k. We have corrected this in the updated draft.\n5 - Eq. 12: The reviewer is correct. We have corrected the equation in the updated draft. \n\nEffect of Representative Sampling on the Performance:  [1] showed that using the existing LLL setups and benchmarks, more sophisticated strategies to populate the memory  do not have an appreciable impact. We did try herding-based sampling [2] and got an improved performance of 1-2%. We leave further exploration to future work.\n\nT^{CV} \\ll T: In the updated draft, the AWA-10 experiments has been replaced with AWA-20. So, now we have 20 tasks for all the datasets. While, comparatively, 3 may not be much less than 20, in general, the idea is to use a small and separate subset of tasks for the cross-validation which will not be used for further training and evaluation. This allows us to conform to our stricter definition of LLL setting. \n\nLegends of Figs 4 and 5: We have fixed the legend in the updated draft. A-GEM is the one with the dashed line. \n\n[1] RWalk: Riemannian walk for incremental learning: Understanding forgetting and intransigence, ECCV2018.\n[2] Incremental Classifier and Representation Learner: CVPR 2016\n\n*Additional comment*: \nWhile A-GEM is an important contribution of this paper as it makes the original GEM algorithm much more practical, we believe that the introduction of the new evaluation protocol, new metric and extension using compositional task descriptors are also significant contributions. \n\nLifelong learning setting entails learning more quickly given the experience accumulated in the past. One reason why catastrophic forgetting is bad is that it prevents the learner from quickly adapting to new tasks that are similar to old tasks.\n\nSince the focus should be on sample and computational efficiency, in this work we considered learning from few examples in a single pass, and cross-validating on a different set of tasks to satisfy that requirement. The metric, the additional efficiency achieved by the use of task descriptors and the new A-GEM algorithm are then part of the same effort to make lifelong learning methods and evaluation protocol more realistic. The current evaluation framework that other works borrow from supervised learning (multiple passes over the data and cross-validate on the same tasks as used for testing) is often misleading. We hope to convince the research community to adopt our proposed training/evaluation protocol and to also consider sample/computational and memory efficiency in their metrics.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper392/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619873, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkf2_sC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper392/Authors|ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619873}}}, {"id": "r1epCSNc67", "original": null, "number": 1, "cdate": 1542239700835, "ddate": null, "tcdate": 1542239700835, "tmdate": 1542239700835, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "HkeVfvnnhX", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "content": {"title": "Stochastic version of GEM has very similar run time and memory complexity as the original GEM algorithm", "comment": "We thank the reviewer for providing the feedback on the draft. Here is our response to the questions asked by the reviewer:\n\nQ1: We tried the version of GEM where each task loss is approximated by the few examples in the memory for that task as suggested by the reviewer. This approximation yielded slightly better numbers than original GEM:\n\nMethod            | DataSet   |   Average Acc   |   Forgetting \n--------------------------------------------------------------------------------------\nApprox-GEM   | MNIST     |   90.1 (+-0.6)   |   0.06 (+-0.01)\n                          | CIFAR      |   61.8 (+-0.5)   |   0.06 (+- 0.01)\n--------------------------------------------------------------------------------------\nGEM                  | MNIST   |   89.5 (+- 0.5)  |   0.06 (+- 0.004)  \n                          | CIFAR     |   61.2 (+-0.8)   |   0.06 (+- 0.01)\n--------------------------------------------------------------------------------------\nA-GEM             | MNIST    |   89.1 (+-0.14) |  0.06 (+-0.001)\n(this paper)    | CIFAR      |   62.9  (+-2.2)  |  0.07 (+- 0.02)\n\nHowever, note that this approximation only makes gradient computation more efficient (although not as much on modern GPUs), but the crux of the computation which is due to the inner optimization problem has the same memory and time complexity as the original GEM; overall, this stochastic version of GEM has very similar run time as the original GEM algorithm. Instead, the proposed A-GEM has much lower time (about 100 times faster) and memory cost (about 10 times lower) while achieving similar performance, as highlighted in the Section 6.1 of the paper. \n\nQ2: As suggested by the reviewer, we have added the uncertainty estimates in the updated draft. As you can see from the updated Figs 1, 2 and Tabs. 4, 5, and 6, conclusions do not change. \n\n*Additional comment*: \nWhile A-GEM is an important contribution of this paper as it makes the original GEM algorithm much more practical, we believe that the introduction of the new evaluation protocol, new metric and extension using compositional task descriptors are also significant contributions. \n\nLifelong learning setting entails learning more quickly given the experience accumulated in the past. One reason why catastrophic forgetting is bad is that it prevents the learner from quickly adapting to new tasks that are similar to old tasks.\n\nSince the focus should be on sample and computational efficiency, in this work we considered learning from few examples in a single pass, and cross-validating on a different set of tasks to satisfy that requirement. The metric, the additional efficiency achieved by the use of task descriptors and the new A-GEM algorithm are then part of the same effort to make lifelong learning methods and evaluation protocol more realistic. The current evaluation framework that other works borrow from supervised learning (multiple passes over the data and cross-validate on the same tasks as used for testing) is often misleading. We hope to convince the research community to adopt our proposed training/evaluation protocol and to also consider sample/computational and memory efficiency in their metrics."}, "signatures": ["ICLR.cc/2019/Conference/Paper392/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619873, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkf2_sC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper392/Authors|ICLR.cc/2019/Conference/Paper392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers", "ICLR.cc/2019/Conference/Paper392/Authors", "ICLR.cc/2019/Conference/Paper392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619873}}}, {"id": "BylvMvH5hm", "original": null, "number": 2, "cdate": 1541195534828, "ddate": null, "tcdate": 1541195534828, "tmdate": 1541534034103, "tddate": null, "forum": "Hkf2_sC5FX", "replyto": "Hkf2_sC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper392/Official_Review", "content": {"title": "Computational improvement of the GEM algorithm for lifelong learning ", "review": "Summary of the paper:\n\nThis paper focuses on the problem of lifelong learning for multi-task\nneural networks. The goal is to learn in a computationally and memory\nefficient manner new tasks as they are encountered while at the same\ntime remembering how to solve previously seen tasks with a focus on\nhaving only one training pass through all the training data. The paper\nbuilds on the GEM method introduced in the paper \"Gradient episodic\nmemory for continuum learning\", NIPS 2017.\n\nThe main novelty over the original GEM paper is that A-GEM simplifies\nthe constraints on what constitutes a feasible update step during its\nSGD training so that GEM's QP problem is replaced by a couple of\ninner-products (and thus makes A-GEM much more computationally\nefficient). This simplification also means that only one gradient\nvector (the average gradient computed from the individual gradients of\nthe task loss of the previously seen tasks) has to be stored at each\nupdate as opposed to GEM where each task specific gradient vector has\nto be stored. Thus the memory requirements of A-GEM is much less than\nGEM and is independent of the number of already learnt tasks.\n\nThe paper then presents experimental evidence that A-GEM does run much\nfaster and uses less memory and results in performance similar to the\noriginal GEM strategy. The latter point is important as the simplified\nA-GEM algorithm - which adjusts the network's parameter to improve\nperformance on the current task while ensuring the average performance\non the previously seen tasks should not decrease - does not guarantee\nas stringently as GEM that the network does not forget how to perform\nall the previous tasks.\n\nThe paper also introduces an extra performance metric is introduced\n  called the \"Learning Curve Area\" which measures how quickly a new\n  task is learnt when it is presented with new material.\n\n\nPros and Cons of the paper:\n\n+/- The paper presents a simple intuitive extension to the original GEM\npaper that is much more computationally efficient and is thus more\nsuited and feasible for real lifelong learning applications. And it\nshows that performance exceeds other methods that have similar\ncomputational demands. The paper can be viewed as somewhat incremental\nbut the increment is probably crucial for any real-world practical\napplication.\n\n+ The validity of the approach is demonstrated experimentally on\n  standard datasets in the field.\n\n\n- Some of the presentation of the material is somewhat vague, in\n  particular section 5. In this section a joint embedding model is\n  described that helps facilitate zero-shot learning. However, not\n  enough detail is given to fully understand or appreciate this\n  contribution, see below for details.\n\n\nRationale for my evaluation:\n\nThe method is somewhat incremental, however, this increment could be\nquite practically important. The presentation is lacking in some regard and would benefit\n from some re-working i.e. section 5. \n \n\nUnclear in the paper:\n\nSection 5 describing the \"Joint Embedding Model Using Compositional Task descriptors\" is very sparse on detail.  Here are some of the details that I feel are missing:\n- In the experiments how is the matrix description (via attributes) of the different tasks $t^k$ learnt/discovered?\n- The size of this attribute matrix is able to vary from one task to the next. How does the function $\\psi_{\\omega}$ deal with this problem?\n- What functions are used in the experiments to represent $\\psi_{\\omega}$?\n- In the second last line of paragraph 2 should $C$ be $C_k$? If it should be $C$ how is $C$ chosen?\n- In equation (12) should the $c$th column of $\\psi_{\\omega}$ be extracted as opposed to the $k$th column?\n\nRepresentative labelled samples from each task are stored in memory\nand these are used when learning for a new task. The system\nhas a fixed memory so when a new task is added then the number of\nimages stored for each task has to be reduced. Then uniform sampling is\nused to randomly decide which images to keep. Could this selection\nprocess be improved upon and would any such improvement have any large\nimpact on performance?\n\nTypos and minor errors spotted:\n\nIn the third paragraph of section 2 it is stated $T^{CV} \\ll T$ in the\nexperiments performed this is not case. I don't think 3 is much less\nthan 10 or 20.\n\nIn figures 4 and 5 it is not entirely clear which curves correspond to\nA-GEM and A-GEM-JE from the legend. In the legend the dashed line with\nthe triangle looks the same the non-dashed line with the triangle. I\npresuming A-GEM is the non-dashed line, but only because that makes\nthings consistent with the previous figures.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper392/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Lifelong Learning with A-GEM", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency", "keywords": ["Lifelong Learning", "Continual Learning", "Catastrophic Forgetting", "Few-shot Transfer"], "authorids": ["arslan.chaudhry@eng.ox.ac.uk", "ranzato@fb.com", "mrf@fb.com", "elhoseiny@fb.com"], "authors": ["Arslan Chaudhry", "Marc\u2019Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "TL;DR": "An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ", "pdf": "/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf", "paperhash": "chaudhry|efficient_lifelong_learning_with_agem", "_bibtex": "@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc\u2019Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper392/Official_Review", "cdate": 1542234471939, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkf2_sC5FX", "replyto": "Hkf2_sC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper392/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335713753, "tmdate": 1552335713753, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper392/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}