{"notes": [{"id": "H1ls_eSKPH", "original": "HklVV6xtwB", "number": 2410, "cdate": 1569439858838, "ddate": null, "tcdate": 1569439858838, "tmdate": 1577168286019, "tddate": null, "forum": "H1ls_eSKPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["butyreld@iis.fraunhofer.de", "georgios.kontes@iis.fraunhofer.de", "christoffer.loeffler@iis.fraunhofer.de", "christopher.mutschler@iis.fraunhofer.de"], "title": "Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates", "authors": ["Leonid Butyrev", "Georgios Kontes", "Christoffer L\u00f6ffler", "Christopher Mutschler"], "pdf": "/pdf/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "TL;DR": "This paper provides an approach to address catastrophic forgetting via Hessian-free curvature estimates", "abstract": "Learning neural networks with gradient descent over a long sequence of tasks is problematic as their fine-tuning to new tasks overwrites the network weights that are important for previous tasks. This leads to a poor performance on old tasks \u2013 a phenomenon framed as catastrophic forgetting.  While early approaches use task rehearsal and growing networks that both limit the scalability of the task sequence orthogonal approaches build on regularization.  Based on the Fisher information matrix (FIM) changes to parameters that are relevant to old tasks are penalized, which forces the task to be mapped into the available remaining capacity of the network. This requires to calculate the Hessian around a mode, which makes learning tractable. In this paper, we introduce Hessian-free curvature estimates as an alternative method to actually calculating the Hessian.  In contrast to previous work, we exploit the fact that most regions in the loss surface are flat and hence only calculate a Hessian-vector-product around the surface that is relevant for the current task. Our experiments show that on a variety of well-known task sequences we either significantly outperform or are en par with previous work.", "keywords": ["catastrophic forgetting", "multi-task learning", "continual learning"], "paperhash": "butyrev|overcoming_catastrophic_forgetting_via_hessianfree_curvature_estimates", "original_pdf": "/attachment/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "_bibtex": "@misc{\nbutyrev2020overcoming,\ntitle={Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates},\nauthor={Leonid Butyrev and Georgios Kontes and Christoffer L{\\\"o}ffler and Christopher Mutschler},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ls_eSKPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "U4ysJBqtAS", "original": null, "number": 1, "cdate": 1576798748450, "ddate": null, "tcdate": 1576798748450, "tmdate": 1576800887575, "tddate": null, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "invitation": "ICLR.cc/2020/Conference/Paper2410/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers have provided thorough reviews of your work. I encourage you to read them carefully should you decide to resubmit it to a later conference.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["butyreld@iis.fraunhofer.de", "georgios.kontes@iis.fraunhofer.de", "christoffer.loeffler@iis.fraunhofer.de", "christopher.mutschler@iis.fraunhofer.de"], "title": "Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates", "authors": ["Leonid Butyrev", "Georgios Kontes", "Christoffer L\u00f6ffler", "Christopher Mutschler"], "pdf": "/pdf/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "TL;DR": "This paper provides an approach to address catastrophic forgetting via Hessian-free curvature estimates", "abstract": "Learning neural networks with gradient descent over a long sequence of tasks is problematic as their fine-tuning to new tasks overwrites the network weights that are important for previous tasks. This leads to a poor performance on old tasks \u2013 a phenomenon framed as catastrophic forgetting.  While early approaches use task rehearsal and growing networks that both limit the scalability of the task sequence orthogonal approaches build on regularization.  Based on the Fisher information matrix (FIM) changes to parameters that are relevant to old tasks are penalized, which forces the task to be mapped into the available remaining capacity of the network. This requires to calculate the Hessian around a mode, which makes learning tractable. In this paper, we introduce Hessian-free curvature estimates as an alternative method to actually calculating the Hessian.  In contrast to previous work, we exploit the fact that most regions in the loss surface are flat and hence only calculate a Hessian-vector-product around the surface that is relevant for the current task. Our experiments show that on a variety of well-known task sequences we either significantly outperform or are en par with previous work.", "keywords": ["catastrophic forgetting", "multi-task learning", "continual learning"], "paperhash": "butyrev|overcoming_catastrophic_forgetting_via_hessianfree_curvature_estimates", "original_pdf": "/attachment/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "_bibtex": "@misc{\nbutyrev2020overcoming,\ntitle={Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates},\nauthor={Leonid Butyrev and Georgios Kontes and Christoffer L{\\\"o}ffler and Christopher Mutschler},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ls_eSKPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724831, "tmdate": 1576800276542, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2410/-/Decision"}}}, {"id": "BJeuYr72jH", "original": null, "number": 1, "cdate": 1573823872085, "ddate": null, "tcdate": 1573823872085, "tmdate": 1573823872085, "tddate": null, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "invitation": "ICLR.cc/2020/Conference/Paper2410/-/Official_Comment", "content": {"title": "Rebuttal Comment", "comment": "We would like to thank the reviewers for their feedback. Unfortunately, we are not able to address all comments to the extent and depth we would like to within the rebuttal period, but we will use the feedback as a guideline for improving on our paper and results and resubmit in the future.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2410/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2410/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["butyreld@iis.fraunhofer.de", "georgios.kontes@iis.fraunhofer.de", "christoffer.loeffler@iis.fraunhofer.de", "christopher.mutschler@iis.fraunhofer.de"], "title": "Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates", "authors": ["Leonid Butyrev", "Georgios Kontes", "Christoffer L\u00f6ffler", "Christopher Mutschler"], "pdf": "/pdf/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "TL;DR": "This paper provides an approach to address catastrophic forgetting via Hessian-free curvature estimates", "abstract": "Learning neural networks with gradient descent over a long sequence of tasks is problematic as their fine-tuning to new tasks overwrites the network weights that are important for previous tasks. This leads to a poor performance on old tasks \u2013 a phenomenon framed as catastrophic forgetting.  While early approaches use task rehearsal and growing networks that both limit the scalability of the task sequence orthogonal approaches build on regularization.  Based on the Fisher information matrix (FIM) changes to parameters that are relevant to old tasks are penalized, which forces the task to be mapped into the available remaining capacity of the network. This requires to calculate the Hessian around a mode, which makes learning tractable. In this paper, we introduce Hessian-free curvature estimates as an alternative method to actually calculating the Hessian.  In contrast to previous work, we exploit the fact that most regions in the loss surface are flat and hence only calculate a Hessian-vector-product around the surface that is relevant for the current task. Our experiments show that on a variety of well-known task sequences we either significantly outperform or are en par with previous work.", "keywords": ["catastrophic forgetting", "multi-task learning", "continual learning"], "paperhash": "butyrev|overcoming_catastrophic_forgetting_via_hessianfree_curvature_estimates", "original_pdf": "/attachment/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "_bibtex": "@misc{\nbutyrev2020overcoming,\ntitle={Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates},\nauthor={Leonid Butyrev and Georgios Kontes and Christoffer L{\\\"o}ffler and Christopher Mutschler},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ls_eSKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1ls_eSKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2410/Authors", "ICLR.cc/2020/Conference/Paper2410/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2410/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2410/Reviewers", "ICLR.cc/2020/Conference/Paper2410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2410/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2410/Authors|ICLR.cc/2020/Conference/Paper2410/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141784, "tmdate": 1576860533006, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2410/Authors", "ICLR.cc/2020/Conference/Paper2410/Reviewers", "ICLR.cc/2020/Conference/Paper2410/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2410/-/Official_Comment"}}}, {"id": "rkelpoAaKS", "original": null, "number": 2, "cdate": 1571838904134, "ddate": null, "tcdate": 1571838904134, "tmdate": 1572972341873, "tddate": null, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "invitation": "ICLR.cc/2020/Conference/Paper2410/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper focuses on alleviating the problem of \"catastrophic forgetting\", exhibited by neural networks learned with gradient-based algorithms over long sequence of tasks. In such learning scenarios, tuning of parameters over the new tasks lead to degradation of performance over the old tasks as the parameters important for the latter are overwritten. The gradient-based algorithms are unable to distinguish between the important and  the not-so-important parameters of the old tasks. Hence, one direction of works, including the proposed one, aim at identifying the most important parameters for all the old tasks and discourage modifications on those parameters during the training of the new tasks. \n\nExisting works like Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) have proposed a Bayesian framework to lessen such forgetfulness by condensing the information of the previous tasks and supplying it as a prior for the new task. In such a framework, Ritter et al. (2018) propose a quadratic approximation of the prior which requires computing (an approximate block-diagonal Kronecker-factored) Hessian. \n\nThe paper employs a recent result (Ghorbani et al., 2019) to argue that most regions of the loss surface are flat. Hence, computing the Hessian in only a few regions (which exhibit high curvature) should suffice. However, computing the exact Hessian for large networks is infeasible in practice. The paper, therefore, uses Hessian-vector-product (Schraudolph, 2002; Pearlmutter, 1994), which is similar to sampling the curvature in the direction of a given vector. The key advantage of the proposed approach is the low storage requirements. Regarding how to chose a suitable direction/vector, the paper suggests two choices: the momentum vector or the eigenvector corresponding to the largest eigenvalue (of the Hessian). The  motivation behind the above choices, especially the former option, is unsatisfactory. Empirically, we observe that the momentum vector is a better option than the eigenvector. However, a (theoretical/empirical) deep-dive into why momentum vector is a good candidate should be done. \n\nEmpirically, the proposed approach with momentum vector performs better than EWC but worse than Ritter et al. (2018). More discussion into the results (esp. Hv-momentum vs Hv-eigenvector) would have shed more light on the proposed approach. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2410/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2410/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["butyreld@iis.fraunhofer.de", "georgios.kontes@iis.fraunhofer.de", "christoffer.loeffler@iis.fraunhofer.de", "christopher.mutschler@iis.fraunhofer.de"], "title": "Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates", "authors": ["Leonid Butyrev", "Georgios Kontes", "Christoffer L\u00f6ffler", "Christopher Mutschler"], "pdf": "/pdf/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "TL;DR": "This paper provides an approach to address catastrophic forgetting via Hessian-free curvature estimates", "abstract": "Learning neural networks with gradient descent over a long sequence of tasks is problematic as their fine-tuning to new tasks overwrites the network weights that are important for previous tasks. This leads to a poor performance on old tasks \u2013 a phenomenon framed as catastrophic forgetting.  While early approaches use task rehearsal and growing networks that both limit the scalability of the task sequence orthogonal approaches build on regularization.  Based on the Fisher information matrix (FIM) changes to parameters that are relevant to old tasks are penalized, which forces the task to be mapped into the available remaining capacity of the network. This requires to calculate the Hessian around a mode, which makes learning tractable. In this paper, we introduce Hessian-free curvature estimates as an alternative method to actually calculating the Hessian.  In contrast to previous work, we exploit the fact that most regions in the loss surface are flat and hence only calculate a Hessian-vector-product around the surface that is relevant for the current task. Our experiments show that on a variety of well-known task sequences we either significantly outperform or are en par with previous work.", "keywords": ["catastrophic forgetting", "multi-task learning", "continual learning"], "paperhash": "butyrev|overcoming_catastrophic_forgetting_via_hessianfree_curvature_estimates", "original_pdf": "/attachment/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "_bibtex": "@misc{\nbutyrev2020overcoming,\ntitle={Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates},\nauthor={Leonid Butyrev and Georgios Kontes and Christoffer L{\\\"o}ffler and Christopher Mutschler},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ls_eSKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575495779153, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2410/Reviewers"], "noninvitees": [], "tcdate": 1570237723217, "tmdate": 1575495779164, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2410/-/Official_Review"}}}, {"id": "BJlV6dostB", "original": null, "number": 1, "cdate": 1571694780509, "ddate": null, "tcdate": 1571694780509, "tmdate": 1572972341828, "tddate": null, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "invitation": "ICLR.cc/2020/Conference/Paper2410/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "1. Summary:\nThe paper considers neural network training in the continual learning setting -- data arrive sequentially and we can not revisit past data. The paper proposes an approximate Laplace\u2019s method, in which the Hessian the log likelihood of the data is approximated by some form of Hessian-vector project (? - I will get to this question mark below). The paper considers some benchmark continual learning datasets and compares the proposed approach to EWC and Kronecker-factored online Laplace. The performance of the proposed approach is similar to that of EWC and worse than Kronecker-factored Laplace in most cases. Another sales pitch that the paper brings up a lot is the low space complexity, however this benefit has not been fully demonstrated, given the small-scale network/experiments.\n\n2. Opinion and rationales\n\nI\u2019m leaning towards \u201cstrong reject\u201d as I think the presentation needs another round of polishing and that the technical contributions need to be clarified / unpacked. I explain my thinking below.\n\na. The presentation/explanation/flow are not clear.\nThe abstract does not read well. For example: \u201cThis requires to calculate the Hessian around a mode, which makes learning tractable. In this paper, we introduce Hessian-free curvature estimates as an alternative method to actually calculating the Hessian.\u201d This sentence makes it sound like current approaches are tractable, so what this paper is trying to address? The technical summary is also not precise, the Hessian-free methods used in the paper is to compute Hessian-vector products, not the actual Hessian.\n\nThe introduction motivates the continual learning problem using generalisation of neural networks leading to the need for multi-task learning; however multi-task learning is not scalable given the large number of tasks and thus we need to learn sequentially. However, I find this motivation not clear: if multi-task learning and its scalability issue are the reasons why we need continual learning, with the scale of the experiments considered in the paper, wouldn\u2019t it always more beneficial to use multi-task learning instead of continual learning?\n\nThe prior work section is also not clear, in my opinion. The paper starts out by describing EWC as Bayesian updates and cites MacKay (1992), then talks about the Kronecker-factored Laplace approximation as \u201caddress this shortcoming by adopting the Bayesian online learning approach\u201d, as if these methods are very different while in fact, these methods are some variants of the Laplace approximation, with different ways to approximate the Hessian. The issues described in section 2.2 \u201ctwo problems that stem from eq 1\u201d are not very clear, for example, \u201cwithout storing the information from all previous tasks there is no easy solution to update the posterior\u201d (?). I would follow the presentation/explanation in Ritter et al (2018), Huszar (2018) [a note on the quadratic penalty of EWC] and section 5 of the variational continual learning paper (Nguyen et al 2018) to provide a more succinct connection between these methods.\nThe connections between this work and MAML in section 3 is not clear to me. The continual learning and meta learning settings are also quite different.\n\nb. The technical contribution is not clear and if correct, if of limited novelty.\n\nWhat is not clear from reading section 3 is what quantity is being approximated, at what point a Hessian-vector product appears and thus we can use Hessian-free methods to approximate it. The paper talks about flat loss surface and sampling a small subset of the Hessian -- I\u2019m not sure I understand these connections. In eq 11, the paper replaces the Hessian values with results of the Hessian-vector-product approximations -- this seems very odd to me, especially in terms of semantics and units, Hessian and hessian-vector-products are two very different things. Again, it is perhaps just me not understanding what is being approximated in the first place. The technical contribution of this paper is thus limited: using Hessian-free methods to approximate Hessian-vector products in the continual learning context.\n\nc. The performance of the proposed method is not super exciting. Pragmatically speaking, it is not clear why practitioners should be using this in the near future given Kronecker-factored Laplace works and scales well in practice and there are a plethora of other recent methods (e.g. VCL) that are also developed from the Bayesian principle and work much better than EWC.\n\n\n3. Minor details:\n\na. In eq 1, the denominator should be p(D_{t+1} | D_{1:t}).\n\nb. Figs 1 and 2, I would use the same colour scheme throughout to be consistent.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2410/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2410/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["butyreld@iis.fraunhofer.de", "georgios.kontes@iis.fraunhofer.de", "christoffer.loeffler@iis.fraunhofer.de", "christopher.mutschler@iis.fraunhofer.de"], "title": "Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates", "authors": ["Leonid Butyrev", "Georgios Kontes", "Christoffer L\u00f6ffler", "Christopher Mutschler"], "pdf": "/pdf/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "TL;DR": "This paper provides an approach to address catastrophic forgetting via Hessian-free curvature estimates", "abstract": "Learning neural networks with gradient descent over a long sequence of tasks is problematic as their fine-tuning to new tasks overwrites the network weights that are important for previous tasks. This leads to a poor performance on old tasks \u2013 a phenomenon framed as catastrophic forgetting.  While early approaches use task rehearsal and growing networks that both limit the scalability of the task sequence orthogonal approaches build on regularization.  Based on the Fisher information matrix (FIM) changes to parameters that are relevant to old tasks are penalized, which forces the task to be mapped into the available remaining capacity of the network. This requires to calculate the Hessian around a mode, which makes learning tractable. In this paper, we introduce Hessian-free curvature estimates as an alternative method to actually calculating the Hessian.  In contrast to previous work, we exploit the fact that most regions in the loss surface are flat and hence only calculate a Hessian-vector-product around the surface that is relevant for the current task. Our experiments show that on a variety of well-known task sequences we either significantly outperform or are en par with previous work.", "keywords": ["catastrophic forgetting", "multi-task learning", "continual learning"], "paperhash": "butyrev|overcoming_catastrophic_forgetting_via_hessianfree_curvature_estimates", "original_pdf": "/attachment/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "_bibtex": "@misc{\nbutyrev2020overcoming,\ntitle={Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates},\nauthor={Leonid Butyrev and Georgios Kontes and Christoffer L{\\\"o}ffler and Christopher Mutschler},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ls_eSKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575495779153, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2410/Reviewers"], "noninvitees": [], "tcdate": 1570237723217, "tmdate": 1575495779164, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2410/-/Official_Review"}}}, {"id": "BklI585CtH", "original": null, "number": 3, "cdate": 1571886734494, "ddate": null, "tcdate": 1571886734494, "tmdate": 1572972341784, "tddate": null, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "invitation": "ICLR.cc/2020/Conference/Paper2410/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method for tackling catastrophic forgetting. Similar to previous methods such as EWC (Kirkpatrick et al., 2017), they penalize parameter updates that align with the Fisher information matrix of the previous tasks. This will prevent the model from changing the previously useful parameters. They try to match the result of previous fisher-based methods but at a lower computational cost. They propose using a low-rank approximation to the Hessian using Hessian-vector-product with two types of vectors: the momentum velocity vector and the largest eigen-vector of the hessian. Then they build a diagonal approximation to the Hessian.\n\nCons:\n- Eq 11, there is no justification for forming a curvature matrix by putting the absolute value of the hessian-vector-product with the proposed vectors on the diagonal. Particularly considering the largest eigen-value, Hv will be a vector of zeros with exactly one 1. This does not seem to be a good estimate of the hessian.\n- Fig 1, the proposed method seem to perform poorly compared to the kfac-based method on permuted mnist.\n- Figure 2 mainly compares to EWC as a baseline. In Farquhar & Gal (2019), other methods such as VGR perform significantly better. The proposed method is not competitive with state-of-the-art."}, "signatures": ["ICLR.cc/2020/Conference/Paper2410/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2410/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["butyreld@iis.fraunhofer.de", "georgios.kontes@iis.fraunhofer.de", "christoffer.loeffler@iis.fraunhofer.de", "christopher.mutschler@iis.fraunhofer.de"], "title": "Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates", "authors": ["Leonid Butyrev", "Georgios Kontes", "Christoffer L\u00f6ffler", "Christopher Mutschler"], "pdf": "/pdf/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "TL;DR": "This paper provides an approach to address catastrophic forgetting via Hessian-free curvature estimates", "abstract": "Learning neural networks with gradient descent over a long sequence of tasks is problematic as their fine-tuning to new tasks overwrites the network weights that are important for previous tasks. This leads to a poor performance on old tasks \u2013 a phenomenon framed as catastrophic forgetting.  While early approaches use task rehearsal and growing networks that both limit the scalability of the task sequence orthogonal approaches build on regularization.  Based on the Fisher information matrix (FIM) changes to parameters that are relevant to old tasks are penalized, which forces the task to be mapped into the available remaining capacity of the network. This requires to calculate the Hessian around a mode, which makes learning tractable. In this paper, we introduce Hessian-free curvature estimates as an alternative method to actually calculating the Hessian.  In contrast to previous work, we exploit the fact that most regions in the loss surface are flat and hence only calculate a Hessian-vector-product around the surface that is relevant for the current task. Our experiments show that on a variety of well-known task sequences we either significantly outperform or are en par with previous work.", "keywords": ["catastrophic forgetting", "multi-task learning", "continual learning"], "paperhash": "butyrev|overcoming_catastrophic_forgetting_via_hessianfree_curvature_estimates", "original_pdf": "/attachment/ab15c22442b0c99c538ce4e98ef2c265e2cbf12b.pdf", "_bibtex": "@misc{\nbutyrev2020overcoming,\ntitle={Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates},\nauthor={Leonid Butyrev and Georgios Kontes and Christoffer L{\\\"o}ffler and Christopher Mutschler},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ls_eSKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ls_eSKPH", "replyto": "H1ls_eSKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2410/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575495779153, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2410/Reviewers"], "noninvitees": [], "tcdate": 1570237723217, "tmdate": 1575495779164, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2410/-/Official_Review"}}}], "count": 6}