{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1367449740000, "tcdate": 1367449740000, "number": 5, "id": "i4E0iizbl6uCv", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0W7-W0EaA4Wak", "replyto": "0W7-W0EaA4Wak", "signatures": ["Ian J. Goodfellow, Aaron Courville, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have posted an update to the arXiv paper, containing new material that we will present at the workshop."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Training Deep Boltzmann Machines for Classification", "decision": "conferenceOral-iclr2013-workshop", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel inpainting-based objective function that facilitates second order optimization and line searches.", "pdf": "https://arxiv.org/abs/1301.3568", "paperhash": "goodfellow|joint_training_deep_boltzmann_machines_for_classification", "keywords": [], "conflicts": [], "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363360620000, "tcdate": 1363360620000, "number": 4, "id": "_B-UB_2zNqJCO", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0W7-W0EaA4Wak", "replyto": "0W7-W0EaA4Wak", "signatures": ["anonymous reviewer 55e7"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Indeed I didn't notice this was a workshop paper, which then doesn't have to be as complete.\r\n\r\nStandard way to train nade is go in the fixed order. However you can also choose a random for each input (it leads to worse likelihood though). This is then equivalent to blanking random m pixels and predicting remaining n-m where n is the input size and m is chosen randomly from 0..n-1 with appropriate weighting."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Training Deep Boltzmann Machines for Classification", "decision": "conferenceOral-iclr2013-workshop", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel inpainting-based objective function that facilitates second order optimization and line searches.", "pdf": "https://arxiv.org/abs/1301.3568", "paperhash": "goodfellow|joint_training_deep_boltzmann_machines_for_classification", "keywords": [], "conflicts": [], "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363234680000, "tcdate": 1363234680000, "number": 6, "id": "uu7m3uY-jKu9P", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0W7-W0EaA4Wak", "replyto": "0W7-W0EaA4Wak", "signatures": ["Ian J. Goodfellow, Aaron Courville, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The arXiv link now contains the second revision."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Training Deep Boltzmann Machines for Classification", "decision": "conferenceOral-iclr2013-workshop", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel inpainting-based objective function that facilitates second order optimization and line searches.", "pdf": "https://arxiv.org/abs/1301.3568", "paperhash": "goodfellow|joint_training_deep_boltzmann_machines_for_classification", "keywords": [], "conflicts": [], "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363214940000, "tcdate": 1363214940000, "number": 2, "id": "g6eHAgMz5csdN", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0W7-W0EaA4Wak", "replyto": "0W7-W0EaA4Wak", "signatures": ["Ian J. Goodfellow, Aaron Courville, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have updated our paper and are waiting for arXiv to make the update public. We'll add the updated paper to this webpage as soon as arXiv makes the public link available.\r\n\r\nTo anonymous reviewer 55e7:\r\n-We'd like to draw to your attention that this paper was submitted to the workshops track. We agree with you that the results are very preliminary, which is why we did not submit it to the conference track. We know that the web interface for reviewers doesn't make it clear which track a paper was submitted to.\r\n-We don't find the connection to NADE to be particularly meaningful, for the following reasons: \r\n1) You can think of *any* model trained with maximum likelihood as learning to predict subsets of the inputs from each other. This is just a consequence of the chain rule of probability, p(x,y,z) = p(x)p(y|x)P(z|y,x). \r\n2) For NADE, each variable appears only in one term of the cost function, and is always predicted given the same subset of other variables as input. In our algorithm each variable appears in an exponential number of terms, each with a different input set.\r\n3) NADE defines the model such that P(v_i | v_1, ..., v_{i-1}) is just specified to be what you'd get by running one step of mean field in an RBM. NADE thus uses exact inference in the model that it is training. We use approximate inference, and we also run the mean field to convergence, rather than just doing one step.\r\n4) A trained JDBM can easily predict any subset of variables given any other subset of variables, but NADE runs into problems with intractable inference for most queries. NADE is based on designing a model so that exact inference can compute P(v) easily, but this does not translate into estimating one half of v given the other half, because so many states need to be summed out. ie, to estimate P(v_n | v_1, ... v_k) NADE must explicitly sum over all joint assignments to v_k, ..., v_n-1. This is the case even for queries that follow the same structure as the NADE model. \r\n5) NADE is based on exact maximum likelihood learning. Our algorithm is based on an approximation to pseudolikelihood learning.\r\n\r\nTo anonymous reviewer b31c:\r\n-Yes, I wrote the wrong expression for the KL divergence. It's fixed now.\r\n-Regarding denoising autoencoders, yes, it would be interesting to connect them. Some denoising autoencoders can be understood as doing score matching on RBMs. It's not clear how to extend that view of denoising autoencoders to the setting we explore in this paper (discrete rather than continuous variables, multiple hidden layers rather than one hidden layer).\r\n-CG can overfit the minibatch, but you can compensate for this by using big minibatches. The original DBM paper already uses CG for the supervised fine tuning. Our best results were with 5 CG steps per minibatch of 1250 examples. We have updated the workshop paper to specify these details.\r\n-The size of the network is the same as in the original DBM paper.\r\n We have updated the paper to specify this."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Training Deep Boltzmann Machines for Classification", "decision": "conferenceOral-iclr2013-workshop", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel inpainting-based objective function that facilitates second order optimization and line searches.", "pdf": "https://arxiv.org/abs/1301.3568", "paperhash": "goodfellow|joint_training_deep_boltzmann_machines_for_classification", "keywords": [], "conflicts": [], "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362265800000, "tcdate": 1362265800000, "number": 3, "id": "ua4iaAgtT2WVU", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0W7-W0EaA4Wak", "replyto": "0W7-W0EaA4Wak", "signatures": ["anonymous reviewer b31c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Joint Training Deep Boltzmann Machines for Classification", "review": "This breaking-news paper proposes a new method to jointly train the layers of a DBM. DBM are usually 'pre-trained' in a layer-wise manner using RBMs, a conceivably suboptimal procedure. Here the authors propose to use a deterministic criterion that basically turns the DBM into a RNN. This RNN is trained with a loss that resembles that one of denoising auto-encoders (some inputs at random are missing and the task is to predict their values from the observed ones).\r\n\r\nThe view of a DBM as special kind of RNN is not new and the inpainting criterion is not new either, however their combination is. I am very curious to see whether this will work because it may introduce a new way to train RNNs that can possibly work well for image related tasks. I am not too excited about seeing this as a way to improve DBMs as a probabilistic model, but that's just my personal opinion.  \r\nOverall this work can be moderately original and of good quality.\r\n\r\nPros\r\n-- clear motivation\r\n-- interesting model\r\n-- good potential to improve DBM/RNN training\r\n-- honest writing about method and its limitation (I really like this and it is so much unlike most of the work presented in the literature). Admitting current limitations of the work and being explicit about what is implemented helps the field making faster progress and becoming less obscure to outsiders. \r\n\r\nCons\r\n-- at this stage this work seems preliminary\r\n--  formulation is unclear\r\n\r\nMore detailed comments:\r\nThe notation is a bit confusing: what's the difference between Q^*_i and Q^*? Is the KL divergence correct? I would expect something like:\r\nKL(DBM probability of (v_{S_i} | v_{-S_i}) || empirical probability of ( v_{S_i} | v_{-S_i}) ). I do not understand why P(h | v_{-S_i}) shows up there.\r\n\r\nIt would be nice to relate this method to denoising autoencoders. In my understanding this is the analogous for RNN-kind of networks.\r\n\r\nDoesn't CG make the training procedure more prone to overfitting on the minibatch? How many steps are executed?\r\n\r\nImportant details are missing. Saying that error rate on MNIST is X% does not mean much if the size of the network is not given.\r\n\r\nOverall, this is a good breaking news paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Training Deep Boltzmann Machines for Classification", "decision": "conferenceOral-iclr2013-workshop", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel inpainting-based objective function that facilitates second order optimization and line searches.", "pdf": "https://arxiv.org/abs/1301.3568", "paperhash": "goodfellow|joint_training_deep_boltzmann_machines_for_classification", "keywords": [], "conflicts": [], "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362172860000, "tcdate": 1362172860000, "number": 1, "id": "nnKMnn0dlyqCD", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "0W7-W0EaA4Wak", "replyto": "0W7-W0EaA4Wak", "signatures": ["anonymous reviewer 55e7"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Joint Training Deep Boltzmann Machines for Classification", "review": "The authors aim to introduce a new method for training deep Boltzmann machines. Inspired by inference procedure they turn the model into two hidden layers autoencoder with recurrent connections. Instead of reconstructing all pixels from all (perhaps corrupted) pixels they reconstruct one subset of pixels from the other (the complement).\r\n\r\nOverall this paper is too preliminary and there are too few experiments and most pieces are not new. However with better analysis and experimentation this might turn out to be very good architecture, but at this point is hard to tell.\r\n\r\nThe impainting objective is similar to denoising - one tries to recover original information from either subset of pixels or from corrupted image. So this is quite similar to denoising autoencoders. It is actually exactly the same as the NADE algorithm, which can be equivalently trained by the same criterion (reconstructing one set of pixels from the other - quite obvious) instead of going sequentially through pixels. The architecture is an autoencoder but a more complicated one then standard single layer - it has two (or more) hidden layers and is recurrent. In addition there is the label prediction cost. The idea of a more complicated encoding function, including recurrence, is interesting but certainly not new and neither is combining unsupervised and supervised criterion in one criterion. However if future exploration shows that this particular architecture is a good way of learning features, or that is specifically trains well the deep bolzmann machines, or it is good for some other problems then this work can be very interesting. However as presented, it needs more experiments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Joint Training Deep Boltzmann Machines for Classification", "decision": "conferenceOral-iclr2013-workshop", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel inpainting-based objective function that facilitates second order optimization and line searches.", "pdf": "https://arxiv.org/abs/1301.3568", "paperhash": "goodfellow|joint_training_deep_boltzmann_machines_for_classification", "keywords": [], "conflicts": [], "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358404200000, "tcdate": 1358404200000, "number": 53, "id": "0W7-W0EaA4Wak", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "0W7-W0EaA4Wak", "signatures": ["goodfellow.ian@gmail.com"], "readers": ["everyone"], "content": {"title": "Joint Training Deep Boltzmann Machines for Classification", "decision": "conferenceOral-iclr2013-workshop", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel inpainting-based objective function that facilitates second order optimization and line searches.", "pdf": "https://arxiv.org/abs/1301.3568", "paperhash": "goodfellow|joint_training_deep_boltzmann_machines_for_classification", "keywords": [], "conflicts": [], "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 7}