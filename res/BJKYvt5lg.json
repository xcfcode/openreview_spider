{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487870042429, "tcdate": 1478297472658, "number": 503, "id": "BJKYvt5lg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJKYvt5lg", "signatures": ["~Ishaan_Gulrajani1"], "readers": ["everyone"], "content": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396623870, "tcdate": 1486396623870, "number": 1, "id": "S1u02MIOg", "invitation": "ICLR.cc/2017/conference/-/paper503/acceptance", "forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper provides a solution that combines best of latent variable models and auto-regressive models. The concept is executed well and will make a positive contribution to the conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396624920, "id": "ICLR.cc/2017/conference/-/paper503/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396624920}}}, {"tddate": null, "tmdate": 1484934533507, "tcdate": 1482175154477, "number": 3, "id": "BJq3MnrEg", "invitation": "ICLR.cc/2017/conference/-/paper503/official/review", "forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "signatures": ["ICLR.cc/2017/conference/paper503/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper503/AnonReviewer1"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.\n--------------\nThis paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.\nI think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.\n\nI have the following suggestions/concerns about the paper:\n\n1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.\n\n2) A similar idea is also presented in a concurrent ICLR submission \"Variational Lossy Autoencoder\". It would be interesting to have a discussion included in the paper and compare these works.\n\n3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.\n\n4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.\n\nI will gladly raise the score if the authors address my concerns.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512563136, "id": "ICLR.cc/2017/conference/-/paper503/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper503/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper503/AnonReviewer4", "ICLR.cc/2017/conference/paper503/AnonReviewer5", "ICLR.cc/2017/conference/paper503/AnonReviewer1"], "reply": {"forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512563136}}}, {"tddate": null, "tmdate": 1484768285885, "tcdate": 1484768285885, "number": 7, "id": "HyUQNSaLx", "invitation": "ICLR.cc/2017/conference/-/paper503/public/comment", "forum": "BJKYvt5lg", "replyto": "HJf5zCW4g", "signatures": ["~Ishaan_Gulrajani1"], "readers": ["everyone"], "writers": ["~Ishaan_Gulrajani1"], "content": {"title": "Response", "comment": "Thanks for your thoughtful comments. We've taken your suggestions into account and updated our paper accordingly:\n\n- Re. our model's efficiency compared to PixelCNN with comparable likelihood, we've updated Table 2 to list each model's computational complexity alongside its likelihood (for those models whose papers contained enough details for us to calculate this). In particular, we show that our model achieves comparable likelihood at a fraction of the computational cost of PixelRNN/PixelCNN. All current models underfit on this dataset and performance is mostly limited by computational cost (as observed by both us and PixelRNN); we think it's quite likely that further increasing model size could improve our likelihood.\n\n- Re. details of our model's structure, we've added tables detailing specifics of our architecture in an appendix, and we've also made code to reproduce our experiments available online and referenced it within the paper.\n\n- Re. further investigation of our model's latent representation, we've added experiments to demonstrate our model's ability to learn more useful low-dimensional representations than VAE (section 4.1.3 and figure 5).\n\n- Re. our visualization of sample variations at different levels, our purpose is to provide a qualitative demonstration of our model behaving the way we design it to perform -- we use fewer autoregressive layers (with a smaller receptive field) which should force the model to only focus on low-level details such as precise positioning and color, and leave the more semantic features to be captured by the latent layers. In a sense, this should not be \"surprising\" since the model has been designed to capture variations according to this hierarchy, but it does act as a qualitative check for alignment of model design with actual model behavior."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549457, "id": "ICLR.cc/2017/conference/-/paper503/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJKYvt5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper503/reviewers", "ICLR.cc/2017/conference/paper503/areachairs"], "cdate": 1485287549457}}}, {"tddate": null, "tmdate": 1484645282190, "tcdate": 1484645282190, "number": 6, "id": "BJ5imPjIg", "invitation": "ICLR.cc/2017/conference/-/paper503/public/comment", "forum": "BJKYvt5lg", "replyto": "BJq3MnrEg", "signatures": ["~Ishaan_Gulrajani1"], "readers": ["everyone"], "writers": ["~Ishaan_Gulrajani1"], "content": {"title": "Re: 'Review'", "comment": "Thanks for your thoughtful comments; we agree that all of these are good suggestions. We've updated our paper to address them as follows:\n\n1) We've run this experiment and included the resulting plots in the paper (figure 5).\n\n2) We've added a discussion to our 'Related Work' section and also included VLAE in our MNIST results table for comparison.\n\n3) We've added tables of our model architecture, along with more detailed descriptions, in an appendix; we've also made our code open-source (https://github.com/igul222/PixelVAE) and referenced it in the paper.\n\n4) We've added plots of MNIST samples from PixelVAE and an equal-depth PixelCNN in Appendix B. We've also included samples from a PixelCNN with greater depth, but roughly equal computational complexity."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549457, "id": "ICLR.cc/2017/conference/-/paper503/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJKYvt5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper503/reviewers", "ICLR.cc/2017/conference/paper503/areachairs"], "cdate": 1485287549457}}}, {"tddate": null, "tmdate": 1481945629737, "tcdate": 1481945629737, "number": 5, "id": "HySXz4MVl", "invitation": "ICLR.cc/2017/conference/-/paper503/public/comment", "forum": "BJKYvt5lg", "replyto": "SkLvT70Ql", "signatures": ["~Ishaan_Gulrajani1"], "readers": ["everyone"], "writers": ["~Ishaan_Gulrajani1"], "content": {"title": "Re: 'some questions'", "comment": "Thanks for your comments. \n\n> I think it would be great if you could add an appendix section to the paper and explain the exact architecture details of all the experiment. Also open sourcing the code would help me and other people to understand the paper better.\n\nWe agree and are in the process of doing both of these.\n\n> Could you explain more about how PixelCNN is used in the hierarchical posterior case: \"At each level, the generator is a conditional PixelCNN over the latent features in the level below.\" Is it like the normalizing flow or inverse autoregressive flow idea? Have you included the log det-Jacobian term? What is the exact architecture? Isn't the diagonal Gaussian output layers limiting for pixelcnn as it can't capture multi-modal distributions like the way softmax does?\n\nOur latent variables are split into multiple levels z_1, ..., z_n. The joint posterior over all of these is a simple fully factorized Gaussian (e.g. conditioned on x, z_2 is independent of z_1), unlike normalizing flows (and IAF, which is a special case of NF) which are used to make the posterior distribution more flexible. Our contribution is complementary to NF/IAF, and using them together would likely further improve performance. Our hierarchy exists entirely in the generator (a.k.a. decoder, a.k.a. conditional likelihood function): if we have n levels of latent variables, we (jointly end-to-end) train n \"decoders\", each of which models a prior over the next level of latent variables downward conditioned on the current level (e.g. if our latent variables are z_1 and z_2, with z_2 at the top, one decoder models p(z_1 | z_2) and the other models p(x | z_1) ). Each of these functions is a PixelVAE decoder (see figure 2). At sampling time, we first sample from p(z_2) [which in our experiments is just unit Gaussian], then sample from p(z_1 | z2) autoregressively, then sample from p(x | z_1). The log det-Jacobian term used in NF comes from the rule for change of variables, which our model doesn't use (we don't transform a simple posterior into a more flexible one, rather we directly output a flexible *prior*) hence that term isn't needed in our likelihood lower bound. As you mention, for all but the lowest layer we use a diagonal Gaussian output across channels instead of a softmax (we found that using a more flexible distribution here didn't help much).\n\n> What do you exactly mean by \"Gated PixelVAE without upsampling\"? I suppose this must be using the bias as a function of conditioning vector. If it works better than \"concatenated feature map\" version of PixelVAE on MNIST, how does it work on other datasets? How does the two different ways of conditioning compare in general?\n\nThat's correct; we bias a Gated PixelCNN with a linear transform of the conditioning vector. Though it achieves slightly better likelihood on MNIST (where models tend to overfit easily), we find this architecture doesn't scale as well to large datasets (where we are mainly compute-bound) because it requires a large number of PixelCNN layers. We also suspect it would perform less well with higher image resolutions; for this reason we choose to use upsampling layers in our large-scale models, and demonstrate that they work well.\n\n> What do you mean by spatial resolution in \"two-level PixelVAE with latent variables at 1\u00d71 and 8\u00d78 spatial resolutions\"?\n\nThat model has two levels of latent variables, z_1 and z_2. z_2 is a set of 1x1 feature maps with 512 channels (i.e. a tensor with shape n,512,1,1 in \"NCHW\" format). Similarly z_1 is a set of 8x8 feature maps with 64 channels (i.e. a tensor with shape n,64,8,8 in \"NCHW\" format). We didn't specify the number of channels because we found our model wasn't very sensitive to that choice.\n\n> It would be great to also include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.\n\nGood idea! We'll do that."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549457, "id": "ICLR.cc/2017/conference/-/paper503/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJKYvt5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper503/reviewers", "ICLR.cc/2017/conference/paper503/areachairs"], "cdate": 1485287549457}}}, {"tddate": null, "tmdate": 1481921162055, "tcdate": 1481921162055, "number": 2, "id": "HJf5zCW4g", "invitation": "ICLR.cc/2017/conference/-/paper503/official/review", "forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "signatures": ["ICLR.cc/2017/conference/paper503/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper503/AnonReviewer5"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The paper combines a hierarchical Variational Autoencoder with PixelCNNs to model the distribution of natural images. \nThey report good (although not state of the art) likelihoods on natural images and briefly start to explore what information is encoded by the latent representations in the hierarchical VAE.\n\nI believe that combining the PixelCNN with a VAE, as was already suggested in the PixelCNN paper, is an important and interesting contribution. \nThe encoding of high-, mid- and low-level variations at the different latent stages is interesting but seems not terribly surprising, since the size of the image regions the latent variables model is also at the corresponding scale. Showing that the PixelCNN improves the latent representation of the VAE with regard to some interesting task would be a much stronger result. \nAlso, while the paper claims, that combining the PixelCNN with the VAE reduces the number of computationally expensive autoregressive layers, it remains unclear how much more efficient their whole model is than an PixelCNN with comparable likelihood.\n\nIn general, I find the clarity of the presentation wanting. For example, I agree with reviewer1 that the exact structure of their model remains unclear from the paper and would be difficult to reproduce. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512563136, "id": "ICLR.cc/2017/conference/-/paper503/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper503/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper503/AnonReviewer4", "ICLR.cc/2017/conference/paper503/AnonReviewer5", "ICLR.cc/2017/conference/paper503/AnonReviewer1"], "reply": {"forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512563136}}}, {"tddate": null, "tmdate": 1481802106177, "tcdate": 1481802106172, "number": 1, "id": "BJMF-WxNl", "invitation": "ICLR.cc/2017/conference/-/paper503/official/review", "forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "signatures": ["ICLR.cc/2017/conference/paper503/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper503/AnonReviewer4"], "content": {"title": "Nice paper", "rating": "7: Good paper, accept", "review": "All in all this is a nice paper.\n\nI think the model is quite clever, attempting to get the best of latent variable models and auto-regressive models. The implementation and specific architecture choices (as discussed in the pre-review) also seem reasonable.\nOn the experimental side, I would have liked to see something more than NLL measurements and samples - maybe show this is useful for other tasks such as classification?\n\nThough I don't think this is a huge leap forward this is certainly a nice paper and I recoemmend acceptance.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512563136, "id": "ICLR.cc/2017/conference/-/paper503/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper503/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper503/AnonReviewer4", "ICLR.cc/2017/conference/paper503/AnonReviewer5", "ICLR.cc/2017/conference/paper503/AnonReviewer1"], "reply": {"forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512563136}}}, {"tddate": null, "tmdate": 1481682269716, "tcdate": 1481682269708, "number": 2, "id": "SkLvT70Ql", "invitation": "ICLR.cc/2017/conference/-/paper503/pre-review/question", "forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "signatures": ["ICLR.cc/2017/conference/paper503/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper503/AnonReviewer1"], "content": {"title": "some questions", "question": "I think it would be great if you could add an appendix section to the paper and explain the exact architecture details of all the experiment. Also open sourcing the code would help me and other people to understand the paper better. I read the answer to AnonReviewer5, but I still find some parts confusing:\n\nCould you explain more about how PixelCNN is used in the hierarchical posterior case: \"At each level, the generator is a conditional PixelCNN over the latent features in the level below.\" Is it like the normalizing flow or inverse autoregressive flow idea? Have you included the log det-Jacobian term? What is the exact architecture? Isn't the diagonal Gaussian output layers limiting for pixelcnn as it can't capture multi-modal distributions like the way softmax does?\n\nWhat do you exactly mean by \"Gated PixelVAE without upsampling\"? I suppose this must be using the bias as a function of conditioning vector. If it works better than \"concatenated feature map\" version of PixelVAE on MNIST, how does it work on other datasets? How does the two different ways of conditioning compare in general?\n\nWhat do you mean by spatial resolution in \"two-level PixelVAE with latent variables at 1\u00d71 and 8\u00d78 spatial resolutions\"?\n\nIt would be great to also include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481682270347, "id": "ICLR.cc/2017/conference/-/paper503/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper503/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper503/AnonReviewer5", "ICLR.cc/2017/conference/paper503/AnonReviewer1"], "reply": {"forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481682270347}}}, {"tddate": null, "tmdate": 1481558187788, "tcdate": 1481558187781, "number": 4, "id": "ryEhOrnme", "invitation": "ICLR.cc/2017/conference/-/paper503/public/comment", "forum": "BJKYvt5lg", "replyto": "B1ljPvOQe", "signatures": ["~Ishaan_Gulrajani1"], "readers": ["everyone"], "writers": ["~Ishaan_Gulrajani1"], "content": {"title": "Re: 'Model architectures and questions'", "comment": "Thanks for your questions. Our answers are below; we'll also revise the paper to include these.\n\n> What exactly are the architectures for the VAE and PixelCNN layers used in the LSUN and ImageNet experiments (in terms of number of layers and units per layer)?\n\nThe LSUN and ImageNet models use the same architecture: all encoders and decoders are residual networks; we use pre-activation residual blocks (He et al. 2016) with two 3x3 conv layers each and ELU nonlinearity. Some residual blocks perform either downsampling (using a 2x2 stride in the second convolutional layer) or upsampling (using subpixel convolution in the first convolutional layer). Weight normalization is used throughout the model. We optimize using Adam with learning rate 5e-4 for LSUN and 3e-4 for ImageNet. Training proceeds for 400K iterations using batch size 64.\n\nThe residual blocks in the encoders are as follows (by number of output channels):\nx -> h1 encoder: 128 -> 256 [downsample] -> 256 -> 512 [downsample] -> 512 -> 512 [downsample] -> 512 -> 512\nh1 -> h2 encoder: 512 -> 512 -> 512 [downsample] -> 512 -> 512\n\nThe decoder architectures are symmetrical to the encoders, with the addition of the following extra masked convolutions at the end (see figure 2):\nz2 -> z1 decoder: 512 5x5 linear conv -> 512 residual block -> 512 residual block -> 512 residual block with 1x1 convs\nz1 -> x decoder: 256 5x5 linear conv -> 256 residual block -> 256 residual block -> 256 residual block\n\n> Do you have any guarantees for the robustness of the estimate of the NLL on the MNIST dataset?\n\nWe estimate the marginal likelihood of our MNIST model using the importance sampling technique in Importance Weighted Autoencoders (Burda et al. 2016), which computes a lower bound on the likelihood whose tightness increases with the number of importance samples per datapoint. We use N=1000 samples per datapoint; higher values don't appear to significantly affect the likelihood estimate.\n\n> Could you please further explain in what sense \u201cThe KL divergence term can be interpreted as a measure of the information content in the posterior distribution q(z|x)\u201d \n\nWe mean it in the sense that in expectation, samples from q(z|x) require KL(q||p) fewer bits to code under a code optimized for q than under one optimized for p. Burnham & Anderson (\"Model Selection and Multimodal Inference\", 2nd ed., p78) refer to the KL divergence KL(g||f) as \"information lost when f is used to approximate g\"; conversely it is also the information gained when revising a belief in the opposite direction."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549457, "id": "ICLR.cc/2017/conference/-/paper503/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJKYvt5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper503/reviewers", "ICLR.cc/2017/conference/paper503/areachairs"], "cdate": 1485287549457}}}, {"tddate": null, "tmdate": 1481303959600, "tcdate": 1481303959592, "number": 1, "id": "B1ljPvOQe", "invitation": "ICLR.cc/2017/conference/-/paper503/pre-review/question", "forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "signatures": ["ICLR.cc/2017/conference/paper503/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper503/AnonReviewer5"], "content": {"title": "Model architectures and questions", "question": "- What exactly are the architectures for the VAE and PixelCNN layers used in the LSUN and ImageNet experiments (in terms of number of layers and units per layer)?\n\n- Do you have any guarantees for the robustness of the estimate of the NLL on the MNIST dataset? \n\n- Could you please further explain in what sense \u201cThe KL divergence term can be interpreted as a measure of the information content in the posterior distribution q(z|x)\u201d \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481682270347, "id": "ICLR.cc/2017/conference/-/paper503/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper503/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper503/AnonReviewer5", "ICLR.cc/2017/conference/paper503/AnonReviewer1"], "reply": {"forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper503/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481682270347}}}, {"tddate": null, "tmdate": 1479167493959, "tcdate": 1479167493955, "number": 2, "id": "HJAbRpwZg", "invitation": "ICLR.cc/2017/conference/-/paper503/public/comment", "forum": "BJKYvt5lg", "replyto": "SkYXtuAle", "signatures": ["~Ishaan_Gulrajani1"], "readers": ["everyone"], "writers": ["~Ishaan_Gulrajani1"], "content": {"title": "Re. 'Good paper'", "comment": "Thanks for your feedback Xun; we greatly appreciate the comments. In order:\n\n1. That's not quite all, but the big differences between the two models come from the two things you mentioned. The KL cost term lets us sample from p(z), estimate likelihoods, and learn compressed representations, but directly applying it to the architecture in van der Oord et al. 2016b is inefficient unless we adjust the number of PixelCNN layers (and consequently the PixelCNN's receptive field) (sec 4.1.1); we demonstrate how varying the decoder architecture like this also affects the learned latent representations (sec 4.1.2, fig 4b). Hierarchical PixelVAE is an entirely different model; its main novelty is  using PixelCNN layers in autoregressive distributions over multiple higher levels of latent featuremaps, we found it greatly outperforms non-hierarchical PixelVAE on complex datasets.\n\n2. You're right that we can no longer compute the exact likelihood; this is a disadvantage of variational inference models in general, but probably not a very big one, at least in our experiments as the likelihood bound approaches the performance of previous models (tables 1 & 2), and estimating the marginal NLL with importance sampling (table 1) suggests the bound is fairly tight.\n\n3. Good question! We think PixelVAE gives us a great degree of control over the properties of learned representations. We explore this in sections 4.1.2 and 4.2.1, figures 4b and 5. We're actively working on more experiments to this end and will update the paper with our findings.\n\n4. Good suggestion; we'll include these in the next update.\n\nThanks again for taking the time to read and provide detailed feedback."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549457, "id": "ICLR.cc/2017/conference/-/paper503/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJKYvt5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper503/reviewers", "ICLR.cc/2017/conference/paper503/areachairs"], "cdate": 1485287549457}}}, {"tddate": null, "tmdate": 1478566671759, "tcdate": 1478555936652, "number": 1, "id": "SkYXtuAle", "invitation": "ICLR.cc/2017/conference/-/paper503/public/comment", "forum": "BJKYvt5lg", "replyto": "BJKYvt5lg", "signatures": ["~Xun_Huang1"], "readers": ["everyone"], "writers": ["~Xun_Huang1"], "content": {"title": "Good paper", "comment": "This is a very interesting paper. It combines the best of both world: PixelCNN and VAE. State-of-the-art LL and visually appealing images.\n\nBelow are some comments and suggestions.\n\n1. In van dern Oord et al. 2016b they use an auto-encoder architecture with conditional PixelCNN as the decoder. It is not a variational one, though. It would be interesting to give a comparison between the model in this paper and their model. Are there any other differences, besides the KLD loss on latent variables and the (optional) hierarchical extension?\n\n2. The introduction of latent varibles z definitely helps modeling global structure, however it also breaks the chain rule so the exact likelihood cannnot be derived.\n\n3. How is the quality of representations learned by the encoder? Will the PixelCNN decoder help the encoder learn better representations, compared to Kingma et. al., \"Semi-supervised learning with deep generative models\"?\n\n3. Also, it would be great if you can show some reconstruction results, in addition to generated samples.\n\nBut overall I like this paper very much."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.\n", "pdf": "/pdf/a6ad48900a6ffb46df482e9ceefefe063349a26f.pdf", "TL;DR": "VAE with an autoregressive PixelCNN-based decoder with strong performance on binarized MNIST, ImageNet 64x64, and LSUN bedrooms.", "paperhash": "gulrajani|pixelvae_a_latent_variable_model_for_natural_images", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "iitk.ac.in", "polimi.it", "cvc.uab.es"], "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "authorids": ["igul222@gmail.com", "kundankumar2510@gmail.com", "faruk.ahmed.91@gmail.com", "adrien.alitaiga@gmail.com", "francesco.visin@polimi.it", "dvazquez@cvc.uab.es", "aaron.courville@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287549457, "id": "ICLR.cc/2017/conference/-/paper503/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJKYvt5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper503/reviewers", "ICLR.cc/2017/conference/paper503/areachairs"], "cdate": 1485287549457}}}], "count": 13}