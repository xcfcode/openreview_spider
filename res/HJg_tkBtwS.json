{"notes": [{"id": "HJg_tkBtwS", "original": "rJeUtIROPB", "number": 1846, "cdate": 1569439615854, "ddate": null, "tcdate": 1569439615854, "tmdate": 1577168290808, "tddate": null, "forum": "HJg_tkBtwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5z8aGOxQ4", "original": null, "number": 1, "cdate": 1576798733965, "ddate": null, "tcdate": 1576798733965, "tmdate": 1576800902431, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "HJg_tkBtwS", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents an approach to feature selection. Reviews were mixed and questions whether the paper has enough substance, novelty, the correctness of the theoretical contributions, experimental details, as well as whether the paper compares to the relevant literature.  ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJg_tkBtwS", "replyto": "HJg_tkBtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711539, "tmdate": 1576800260764, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Decision"}}}, {"id": "rkgGvZb3KS", "original": null, "number": 1, "cdate": 1571717466203, "ddate": null, "tcdate": 1571717466203, "tmdate": 1574254356982, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "HJg_tkBtwS", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "This paper proposes a practical improvement of the conditional randomization test (CRT) of (Candes et al., 2018).\nIn the study of (Candes et al., 2018), the choice of the test statistic as well as how one estimates conditional distributions were kept open.\nThe authors proposed \"proper test statistic\" as a promising test statistic for CRT, and proved that f-divergence is one possible choice.\nThey further shown that KL-divergence has a nice property among possible f-divergences: KL-divergence cancels out some of the conditional distributions, and thus the users need to estimate only two conditional distributions to compute the test statistic.\nFor estimating those conditional distributions in the test statistic, the authors proposed fitting regression models.\n\nOverall, I think the paper is well-written and the idea is stated clearly.\nThe use of KL-divergence for CRT seems to be reasonable.\nThe proposed algorithms look simple and easy to implement.\n\nMy only concern is on the practical applicability of the proposed algorithms (which, however, may be not a unique problem for this paper, but for all the CRT methods).\nThey require fitting regression models for each feature xj.\nFor high-dimensional data with more than thousands of features, fitting regression models for all the features seem to be impractical.\nFor the imagenet data experiment, the authors successfully avoided this problem by using an inpainting model.\nHowever, this approach is apparently limited to image data.\nI am interested in seeing if there is any promising way to make the algorithms scalable to high-dimensional data.\n\n\n### Updated after author response ###\nThe authors have partially addressed my concern on the scalability of the proposed algorithm to high-dimensional data.\nI therefore keep my score unchanged.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1846/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1846/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJg_tkBtwS", "replyto": "HJg_tkBtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575625412805, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1846/Reviewers"], "noninvitees": [], "tcdate": 1570237731461, "tmdate": 1575625412820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Official_Review"}}}, {"id": "BkxJ_TjqoB", "original": null, "number": 7, "cdate": 1573727591367, "ddate": null, "tcdate": 1573727591367, "tmdate": 1573727591367, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "B1gDv4VYiB", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment", "content": {"title": "Thanks for the update", "comment": "Thanks for the updated version and your responses. The responses did clarify some questions I had. I still wonder about the motivation of the proposed approach (as opposed to, say, using some conditional dependence test statistic which does not require estimating conditional densities many times). "}, "signatures": ["ICLR.cc/2020/Conference/Paper1846/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1846/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg_tkBtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1846/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1846/Authors|ICLR.cc/2020/Conference/Paper1846/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150051, "tmdate": 1576860530975, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment"}}}, {"id": "SyxvyrNtiH", "original": null, "number": 4, "cdate": 1573631199179, "ddate": null, "tcdate": 1573631199179, "tmdate": 1573631199179, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "rkgGvZb3KS", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment", "content": {"title": "Updates and clarifications regarding the paper", "comment": "We thank the reviewer for their comments.\n\n*Computational expense*\nThe reviewer points out correctly that fitting a model on each feature is computationally expensive in high-dimensional settings. While our algorithm is embarrassingly parallel, we added discussion about a faster version, the fast-ami-crt, which requires only 1 extra regression onto response per feature. While the fast version may not enjoy the theoretical guarantees of ami-crt, it proves effective empirically. We also explore the computational vs statistical tradeoffs in choosing between the regular and fast versions.\n\nIn practice, knowledge about the data-generating process can be used. For example, in genetics, genes are independent of other genes located far away. Further, given groups of features, the inpainting method we use for images would apply to any data. We believe that automatically choosing such groups in a domain-agnostic manner is an interesting direction for the future extensions building on top of our methods.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1846/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg_tkBtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1846/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1846/Authors|ICLR.cc/2020/Conference/Paper1846/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150051, "tmdate": 1576860530975, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment"}}}, {"id": "S1l9qV4YjH", "original": null, "number": 3, "cdate": 1573631122075, "ddate": null, "tcdate": 1573631122075, "tmdate": 1573631122075, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "H1xZGoOhFS", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment", "content": {"title": "Updates and clarifications regarding the paper (Cont.)", "comment": "*Instance-wise feature selection sufficient condition*\nIn practice, this condition is achieved in tasks like image-recognition where the distribution y | x is sharp for most samples. This is reflected in the test error. As discussed in the instance-wise section, when perfect prediction does not hold, it may not be possible to distinguish between important and unimportant features for all samples. The original draft discusses this issue with an example in Section 3. In our experiments we have a noisy-selector, we show that while noise degrades performance across all methods, ami-iw still performs best.\n\n\n*Experiments*\n**Simulations**\nD is the dimension of the x variable. We include particular parameter choices for each experiment in the updated version. We have also added further discussion explaining the performance of our method. Briefly, the simulated examples demonstrate that while other methods do perform well with respect to certain metrics, only ami-crt satisfies all desiderata: high power, uniformity of p-values for null features, and control of False-discovery rate (captured by AUROC). \n\n*# Minor but does affect the evaluation*\n\n* conditional distributions after lemma 1 vs. conditional distributions in Eq. 3*.\n\nAll f-divergences use the ratio of the conditional distributions in Eq. 3. This ratio simplifies to include the conditional distributions in the paragraph after lemma 1. We have noted that this might be hard to follow, and clarified this fact in the updated draft (final paragraph of section 2.1), rather than in the referenced appendix section.\n\n* section E.1 (appendix), Mixture of gaussians vs. gaussian density*\n\nThere seems to be a misunderstanding here. In the original draft, section E.1 (appendix), page 16, N(mu, sigma) is a gaussian density, not a gaussian random variable. The average of two gaussian densities is not a gaussian density, rather a mixture of gaussians. (In the new draft, the derivation is moved to section C.1)\n\n*Things that can be improved. Did not affect the score.*\n\n* Section 1.1: the sentence about permutation tests is vague.*\n\nWe have updated this sentence from \u201cfail in the case of\u201d to \u201cfail to test conditional independence\u201d for added clarity.\n\n* Page 2, our contributions: \"necessary\" should be \"sufficient\"?*\n\nWe believe that proper-test statistics are necessary for feature selection without making further assumptions about the relationship between outcome and response.\n\n* Section 2, conditional randomization tests.*\n\nWe have simplified the introduction to section 2 for clarity. The revised version better sets up the need for such tests in finite sample settings.\n\n* Eq 1: that (i) is unclear. Should state that for i=1,...,N.*\n\nThis equation states that all samples in the dataset have their jth feature replaced by a sample from q(x_j | x_{-j}). The equation has been updated to further clarify this.\n\n* Eq 2: rewrite the second line. The left hand side states that the p-value \"converges in distribution to\". The second line should be just 0.*\n\nEquation 2 describes the convergence in distribution of the p-values. If the jth feature was not important, then this distribution is Uniform(0,1). Otherwise, it converges to a distribution where observing 0 has probability 1. We have clarified this in the draft.\n\n* After eq.6, how to choose T (the number of bins) in practice?*\n\nThe choice of bins has been explored in the histogram approximation literature. We point the reviewer to the discussion in (Wasserman, 2006) and (Miscouridou, 2018).\n\n* Definition 3 is actually a proposition? It is unclear what is being defined there.*\n\nDefinition 3 has been updated to Proposition 1.\n\n* The word \"complete conditional knockoffs (CCKs)\" appears for the first time in Section 3.2 without any explanation.*\n\nWe have removed the terminology CCK and instead refer to the object q( x_ j | x_{ -j } ) itself as necessary.\n\n* Orange skin on page 8: what is \"~ exp(...)\"? An exponential distribution, or just exponential function?*\n\nThe discussion in the experiments about the data generation processes has been clarified. \n\nWe have updated the paper to clarify all definitions and added required information.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1846/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg_tkBtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1846/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1846/Authors|ICLR.cc/2020/Conference/Paper1846/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150051, "tmdate": 1576860530975, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment"}}}, {"id": "B1gDv4VYiB", "original": null, "number": 2, "cdate": 1573631070764, "ddate": null, "tcdate": 1573631070764, "tmdate": 1573631070764, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "H1xZGoOhFS", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment", "content": {"title": "Updates and clarifications regarding the paper", "comment": "We thank the reviewer for their comments.\n\n*Overall*\nWe have made edits in response to this review and reviewer 1. Your summary helped us identify the necessary edits. We clarify our contributions here:\n\nWe introduce the notion of proper test statistics. These are test statistics that, when used in a randomization test for feature selection, yield power that approaches 1 in the limit of data, and are uniformly distributed under the null hypothesis.\nWe show estimators of expected divergences are proper test statistics. We develop AMI-CRT which uses the KL-divergence for a computational speedup over other divergences, and enables the reuse of model code for computing a null distribution. We also show that unlike the 0-1 loss, the log probability in the KL-divergence is smooth and therefore results better calibrated p-values.\nWe develop sufficient conditions to perform instance-wise feature selection, show how our estimates can be adapted to this setting, and compare our methods to several state-of-the-art baselines on simulated and real datasets.\n\n*Proper-tests and AMI-CRT*\n\nIn the original draft, we mention that the definition of a proper test statistic mirrors that of a proper scoring rule. So the advantages that proper test statistics offer are analogous to the advantages that proper scoring rules offer in supervised learning.\nProper test statistics make minimal assumptions about the true data generating process and are asymptotically as powerful as any other test.\nWe have since clarified the discussion to highlight the value of the KL divergence\nUsing the KL-based statistic does not require computing  q(y | x_{-j}). This provides two important benefits: a) avoids learning from x and x_{-j} which have different dimensions and may require different model structures. For example, convolutional-networks require additional padding when learning from x_{-j}. This advantage also allows use to reuse our AMI-CRT framework to perform instance-wise feature selection. b) We compute one less conditional distribution per feature, thereby decreasing the amount of required computation.\n\n*Fitting regressions vs. conditional density models*\nWe want to offer a clarification here. As discussed in the paper, feature selection involves testing conditional independence properties of the true data generating distribution in general. In this setting, knowledge of the true conditional density is required for feature selection.\nMoreover, the conditional density of interest is over a scalar response variable which we can solve via supervised learning. When supervised learning is hard, fitting a single model from the features to the response which all but the most basic methods require might be hard, thereby ruling out both prediction and selection.\n\n\n*Confusion regarding model-agnostic*\nWe used the phrase 'Model-agnostic' to mean that our testing procedure does not depend on a specific model. Instead, we use whichever model fits best for a particular task.\n\nWe have retitled our paper : \u2018Black-box feature selection with Additional Mutual Information\u2019 to resolve this confusion and clarify our contributions.\n\n*Refitting for each draw from the null*\nIn the updated draft we include discussion about fast-ami-crt. Fast-ami-crt fits only a single null model for each feature (rather than one per draw from the null), and uses a mixture of the full model and the null model to compute the test-statistic. In our experiments, relative to baselines we show that the mixture model guards against errors poor quality samples from the null.\n\n*Lemma 1 details*\nThe reviewer points out correctly that the proof works for any divergence that can detect equality of distribution. However, we suggest f-divergences are simply an example of proper test statistics. Therefore, our proof need not rely on f-divergences. Regardless, we updated the writing to make this fact more apparent.\n\nWe note we only need the generalized inverse cumulative distribution function to guarantee uniformity of p-values. This exists when the cumulative distribution function of the test-statistic is continuous everywhere. We have clarified the discussion regarding this in the paper and updated the proof.\n\nUnder the null hypothesis H0, for any sample size N, the distribution of the p-value will be uniform(0,1). The fact that the estimators converge to a single number does not have any implication on the convergence of the distribution of the p-value. The reason behind this is that the limit and comparison (indicator function) do not commute.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1846/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg_tkBtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1846/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1846/Authors|ICLR.cc/2020/Conference/Paper1846/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150051, "tmdate": 1576860530975, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment"}}}, {"id": "S1x8sQEKiB", "original": null, "number": 1, "cdate": 1573630878169, "ddate": null, "tcdate": 1573630878169, "tmdate": 1573630878169, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "r1eV9y2k5r", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment", "content": {"title": "Updates and clarifications regarding the paper", "comment": "We thank the reviewer for their comments.\n\n*Proper tests and AMI-CRT*\n\nThank you for the comment about divergences. We have updated the discussion about divergences. It now includes the derivation of delta_j and highlights the advantages of the KL divergence. See section 2.2 for the full derivation.\n\nWe added explanations for each step in the derivation for delta_j. \\tilde{x}_j is sampled anew from x_j | x_{-j}. Any dependence between y and \\tilde{x}_j is therefore broken by conditioning on x_{-j}.\n\n*Experiments*\n\nIn response to the review, we have reorganized the paper to include the presentation of these important results in the main text.\n\n*FDR controlling methods*\n\nThe Benjamini-Hochberg correction (1995) requires p-values to control the FDR. This table shows only those methods that produce p-values so that FDR can be controlled. We have made this fact more explicit in the main text.\n\n*Hospital readmission *\n\nYes, we reference a citation (Strack et. al. 2014) that clinically validates features from the hospital readmissions dataset and selects a subset as important. These features are used to compute our ROC curves.\nThe dataset is from Strack et al (2014). Here is the filtering criteria they apply:\n(1)\tIt is an inpatient encounter (a hospital admission).\n(2)\tIt is a diabetic encounter, that is, one during which any kind of diabetes was entered to the system as a diagnosis.\n(3)\tThe length of stay was at least 1 day and at most 14 days.\n(4)\tLaboratory tests were performed during the encounter.\n(5)\tMedications were administered during the encounter.\nNo additional samples were dropped from this dataset in our experiments. These steps have been added to the appendix.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1846/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg_tkBtwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1846/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1846/Authors|ICLR.cc/2020/Conference/Paper1846/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150051, "tmdate": 1576860530975, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1846/Authors", "ICLR.cc/2020/Conference/Paper1846/Reviewers", "ICLR.cc/2020/Conference/Paper1846/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Official_Comment"}}}, {"id": "H1xZGoOhFS", "original": null, "number": 2, "cdate": 1571748617085, "ddate": null, "tcdate": 1571748617085, "tmdate": 1572972416016, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "HJg_tkBtwS", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "# Paper summary\n\nThis paper addresses supervised feature selection: given a D-dimensional input variable x = (x_1, ..., x_D), and a response variable y, the goal is to find a subset of \"useful\" features in x. Here, a feature x_j is useful if it is dependent on y even when conditioning on all other input variables (denoted by x_{-j}, which is a set). A generic procedure that can produce a p-value for each feature (allowing on to test each feature whether it is useful) is the conditional randomization test (CRT) proposed in Candes et al., 2018.  For the CRT to produce a valid p-value for each feature (input dimension) x_j, one needs to specify a test statistic that measures conditional dependence between x_j and y given the rest of the features.\n\nThis paper contributes the following results:\n\n1. Propose using an estimate of the f-divergence for the conditional dependence measure and use it with the CRT (section 2.2).  \n\n2. Measuring the conditional dependence with an f-divergence requires estimating a few conditional density functions. The paper considers the KL divergence as a special of f-divergence. This particular choice turns out the reduce the number of conditional density functions that have to be estimated (section 2.3). The paper also shows that the resulting conditional measure coincides with what is known as the Additional Mutual Information (AMI) studied in Ranganath & Perotte, 2018.  \n\n3. The paper also studies instance-wise feature selection i.e., selecting a subset of input features which can explain the response specifically for one instance (example) x.  Yoon et al., 2019 proposed a criterion to decide the importance of a feature for instance-wise feature selection (definition 2). Briefly, a feature x_j is deemed important if q(y | full x) > q(y | x without the jth feature), where q is the conditional density function of y given x. _Contribution_: The paper notes that this criterion may fail and derive sufficient conditions (Definition 3) under which this approach will always work.  \n\nIn simulation on toy problems, the paper shows that the proposed method (KL divergence + CRT) has the highest mean under the ROC curve (Table 2), compared to competing methods. In real problems on images, the paper shows that the proposed instance-wise feature selection can be used to select relevant image patches (features) that explain the class of the input images. The paper also conducts experiments on hospital readmission data (Section 4.3), and genomics data (Section 4.2).\n\n\n\n# Review\n\nThe paper is overall well written with some parts that can be improved (details below). Introduction and related work in section 1 are easy to follow. The paper is also mostly self-contained and friendly to non-specialists who may not work on feature selection primarily. My concerns are\n\n1. I find that the amount of contribution is not sufficient. CRT is known from Candes et al., 2018. The present paper proposes using KL-divergence with it. This can be interesting if the combination gives some clear advantages.  Unfortunately I do not find that this is the case. It turns out that one still needs to learn two conditional density functions (see lines 3-4 in Algorithm 1). Further and even more concerning, one has to refit another conditional density function *for each draw from the null distribution* (see \"Fit regression\" in the loop in Algorithm 1). As an intermediate step for solving the original feature selection problem, I find that learning conditional density functions is a much more difficult problem. All these limit the novelty of the idea. While the title of the paper contains \"model-agnostic\", the idea of fitting conditional density functions seems to contradict it. The paper could have considered some nonparametric conditional dependence measures but did not. For instance, see \n\nKernel-based Conditional Independence Test and Application in Causal Discovery\nKun Zhang, Jonas Peters, Dominik Janzing, Bernhard Schoelkopf\n2012\n\nand other papers that extend this paper.\n\nWhy was the approach of fitting conditional density functions chosen?\n\n2. Related to the previous point, refitting a conditional density model for each draw from the null distribution must be very costly computationally. This point is never addressed in the paper.\n\n3. Lemma 1 states that the expected f-divergence is a \"proper statistic\" (in the sense of Definition 1) i.e., p-value is uniformly distributed if the feature is not useful, and vanishes (asymptotically) if the feature is useful. This result unfortunately relies on a strong assumption that there is a consistent estimator for the f-divergence. In fact, the proof does not even rely on the fact that it is an f-divergence. It can be any divergence D(p,q) such that D(p,q) > 0 if  p!=q and D(p,p) = 0. In the proof in section C.2 in the appendix, existence of the quantile function $(F^{-1}_N)$ is never discussed. I can see the first part of the proof (under the alternative H1). But I do not see the second part (under H0). Since $\\hat{f}$ is a consistent estimator by assumption, as N goes to infinity, the two quantities in the indicator function (in expectation) should both go to the same constant. Isn't this the case? \n\n4. As a contribution, the paper states a sufficient condition in Definition 3 under which instance-wise feature selection with the approach in Definition 2 is *always* possible. When does the condition hold in practice? How do we know if it holds or not? If it does not, what can go wrong?\n\n5. Toy experiments: What is D in Xor and Orange? Where is the \"selector\" problem in Table 2? In table 2, \"lime\" and \"shap\" also seem to perform well. The paper never explains why the proposed approach is better than other methods (only reporting higher mean are under the ROC curve). This should be possible for toy problems.\n\n\n\n# Minor but does affect the evaluation\n\n* Paragraph after Lemma 1: it is unclear why those conditional distributions are required instead of conditional distributions in Eq. 3.\n\n* Section E.1 (appendix), page 16: I think you should have $N( 0.5x_1 + 0.5x_2, \\sigma^2_\\epsilon)$ instead of \n$0.5N(x_1, \\sigma^2_\\epsilon) + 0.5N(x_2, \\sigma^2_\\epsilon)$.\n\n\n\n# Things that can be improved. Did not affect the score.\n\n* Section 1.1: the sentence about permutation tests is vague.\n\n* Page 2, our contributions: \"necessary\" should be \"sufficient\"?\n\n* Section 2, conditional randomization tests: This paragraph is unfortunately not well written even though it is a very important prerequisite of this work. For instance, at \" ... replaced by samples of $\\tilde{x}_j^{(i)}$ that is conditionally independent of the outcome...\", at that point, it is unclear \"conditioning on what\". Following this sentence, one approach might be to replace $\\tilde{x}_j^{(i)}$ with a constant (which is independent of everything else). It is not until definition 1 that this becomes clearer. Also, the \"null hypothesis\" (which is in the first line of equation 2) is never stated throughout the paper.\n\n* Eq 1: that (i) is unclear. Should state that for i=1,...,N.\n\n* Eq 2: rewrite the second line. The left hand side states that the p-value \"converges in distribution to\". The second line should be just 0.\n\n* After eq.6, how to choose T (the number of bins) in practice?\n\n* Definition 3 is actually a proposition? It is unclear what is being defined there.\n\n* The word \"complete conditional knockoffs (CCKs)\" appears for the first time in Section 3.2 without any explanation.\n\n* Orange skin on page 8: what is \"~ exp(...)\"? An exponential distribution, or just exponential function?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1846/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1846/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJg_tkBtwS", "replyto": "HJg_tkBtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575625412805, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1846/Reviewers"], "noninvitees": [], "tcdate": 1570237731461, "tmdate": 1575625412820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Official_Review"}}}, {"id": "r1eV9y2k5r", "original": null, "number": 3, "cdate": 1571958667629, "ddate": null, "tcdate": 1571958667629, "tmdate": 1572972415977, "tddate": null, "forum": "HJg_tkBtwS", "replyto": "HJg_tkBtwS", "invitation": "ICLR.cc/2020/Conference/Paper1846/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method to provide some level of interpretation on the influence of input features on the response of a machine level model all the way down to the instance level. The proposed method is model agnostic. Quoting the authors, they advocate for methods that look at interpretability \u201cas understanding the population distribution through the lens of the model\u201d without restriction on the models fit. The problem is posed as a hypothesis testing problem. The paper proposes \u201cproper test statistics\u201d for model agnostic feature selection. It is argued that f-divergence tests are proper statistic tests, with the KL being particularly interesting as it provides computational advantages. \n\nI have found the paper interesting. The topic is relevant and the approach is interesting. However, I have two main reservations for this work. First, I have found the method difficult to follow and sometimes unclear. Important results are only explained in the appendix. For instance, the derivation of Equation 5 is important but only shown in the appendix. Furthermore, that derivation in the appendix needs to be clarified in my view. For instance, on page 15, for the derivation of $\\delta_I$, can you explain how you went from the second equality to the third equality where references to \\tilde{x}_j are removed from one line to another? It could be due to your definition for the term with a conditional independence\twith the outcome assumed but I suggest that you clarify this as it is important for the paper and for the use of the KL. Also, in this equation, should it be $q(x_j|x_{-j}) instead of $q(x_j,x_{-j})$?\n\nThe second issue that I have is with the experiments. Any reason why the key results on the interpretability of the approach are mostly shown in the appendix (e.g., table 4,5,6)?  Why does table 6 not show results for all the baselines? For the hospital readmission use case, were you able to also get percentages of important features and have it compared with the baselines and vetted for clinical significance? This is more minor but worth double checking in my opinion. For this experiment on re-admission, the paper claims to have data from 130 hospitals for 10 years. Yet the n numbers seems pretty small to me. Total number of events < 100 000 for 130 institutions over 10 years. That would mean that we are dealing with less than an average of 80 admissions per institutions per year. Please confirm or explain if any filtering was done beyond what is described in appendix I. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1846/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1846/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ms7490@nyu.edu", "apm470@nyu.edu", "lakshmi@cs.nyu.edu", "sriram@cs.ucla.edu", "rajeshr@cims.nyu.edu"], "title": "Model-Agnostic Feature Selection with Additional Mutual Information", "authors": ["Mukund Sudarshan", "Aahlad Manas Puli", "Lakshmi Subramanian", "Sriram Sankararaman", "Rajesh Ranganath"], "pdf": "/pdf/bd14218f99d6b7dcd035ddba394b2dc038c7b01c.pdf", "TL;DR": "We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.", "abstract": "Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.  We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. ", "code": "https://drive.google.com/file/d/13ShNmzQV-rI1DQN2AXpFrNAUKe7rmC0l/view?usp=sharing", "keywords": ["feature selection", "interpretability", "randomization", "fdr control", "p-values"], "paperhash": "sudarshan|modelagnostic_feature_selection_with_additional_mutual_information", "original_pdf": "/attachment/3dffb3c71032b977658e3a4821196cb59ecbd136.pdf", "_bibtex": "@misc{\nsudarshan2020modelagnostic,\ntitle={Model-Agnostic Feature Selection with Additional Mutual Information},\nauthor={Mukund Sudarshan and Aahlad Manas Puli and Lakshmi Subramanian and Sriram Sankararaman and Rajesh Ranganath},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg_tkBtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJg_tkBtwS", "replyto": "HJg_tkBtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1846/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575625412805, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1846/Reviewers"], "noninvitees": [], "tcdate": 1570237731461, "tmdate": 1575625412820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1846/-/Official_Review"}}}], "count": 10}