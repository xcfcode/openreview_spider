{"notes": [{"id": "TJSOfuZEd1B", "original": "l_sMek5qi1G", "number": 2536, "cdate": 1601308280383, "ddate": null, "tcdate": 1601308280383, "tmdate": 1614985734672, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ZZ1a4RF7jxK", "original": null, "number": 1, "cdate": 1610040403942, "ddate": null, "tcdate": 1610040403942, "tmdate": 1610474000246, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Reviewer #2 has written a nice summary of the paper which I quote below.\n\n\u201cThe core idea is simple - which is a strength in my view - and does not require retraining the base language model, which could be important as language models become more expensive to train. However, the clarity and experiments in this paper fall short: the experimental setup has issues, the effect on perplexity is quite large but relegated to the Appendix, several claims are speculative and lacking corresponding experimental evidence, and it is unclear how the additional heuristics affect performance.\n\nThe method seems promising, but with the current experiments it is difficult to draw conclusions about how the method affects performance and which parts of it are necessary; given that this is an empirical paper, I would therefore not recommend acceptance in its current form.\u201d\n\nKey Strengths\n+ Well-motivated problem of considerable interest \n+ A relatively straightforward Bayesian solution\n+ Proposed solution is computationally efficient compared to other competing approaches.\n\nThe paper has been thoroughly reviewed by the reviewers and as a result numerous questions has surfaced. While the authors addressed most of the questions adequately, there are still many unanswered questions. They include:\n- Readability issues highlighted by Reviewer #1\n- Reviewer #1: \u201c\"how did you measure model confidence about the toxicity label\"\n- Reviewer #4: The perplexity gets much worse as the gedi training is introduced (i.e. \u039b decreases), e.g. going from 25 to 45 on IMDb. This result is in the Appendix, and perplexity is never evaluated/reported in the other experiments.\n- Reviewer #4: Crucially, the GEDI training does not appear to help over just re-weighting with the conditional LM ( vs. ). Could the authors comment on this result? How well does domain transfer work for less similar domains? How is perplexity affected for the models reported in Table 2?\n- Reviewer #4: How small can the conditional LM be? Why was medium used instead of small? What if large was used? Does the conditional LM need to be a large-scale pretrained model (it would be nice to see a baseline of a simpler conditional LM)?\nSeveral heuristics are used:  weighting, nucleus filtering, keeping tokens over a threshold, repetition penalty, and rescaling the logits to positive (used in only one experiment).\n- How does each of these affect performance? There are no ablations, and given the small differences in some of the experiments it is unclear whether performance would actually be worse if we changed one of the heuristics. One outcome may be that the method only works for a careful balance of hyperparameters, which could be fine, but we don't have a sense of the variation.\n- Reviewer #2: The output in Table 6 makes me doubt how the experiments are badly controlled. The outputs from positive and negative sentiment are totally different and almost random text, meaning that the content of the generators is not controlled properly.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040403929, "tmdate": 1610474000229, "id": "ICLR.cc/2021/Conference/Paper2536/-/Decision"}}}, {"id": "I-co72wBxTp", "original": null, "number": 8, "cdate": 1606237020139, "ddate": null, "tcdate": 1606237020139, "tmdate": 1606237020139, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "p8HCu7H0TtP", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment", "content": {"title": "Reply to RSA related work", "comment": "Thanks for pointing out these papers. The RSA framework idea of an interactive game between a listener and speaker does conceptually relate to our work, and uses Bayes rule in similar way.  It seems that the biggest difference between GeDi and previous RSA based approaches is that GeDi use a separate discriminator (that acts as a listener) trained to isolate a specific attribute, and generalizes this attribute by guiding a more general base language model. We now mention several of these papers in the related work."}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TJSOfuZEd1B", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2536/Authors|ICLR.cc/2021/Conference/Paper2536/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847282, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment"}}}, {"id": "qhDpSsbklYE", "original": null, "number": 7, "cdate": 1606236366679, "ddate": null, "tcdate": 1606236366679, "tmdate": 1606236366679, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "54vRiyEa7kF", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks for your review. Our update to the paper was largely aimed at improving our experimental set up, and we now include baselines, significance tests, and ablation studies, as we will describe in our replies below and also in our general comment that gave an overview of our update.\n\n*\"The contribution of controllability and producing safe output should be separated out. Safer LMs seem to be the outcome of the controllability of the LM by canceling out the opposite part of the target label using the Bayes rule. Highlighting this point might be helpful for showing out the novelty and value of this work. The current frame of the work seems quite distributed between applications and architectural contributions.\"*\n\nIt is indeed true that safer LMs are the outcome of the controllability, and that this is a consequence of using Bayes rule to cancel out predictions resulting from opposing control codes. We chose to also emphasize LM safety since it is such a big concern right now, and we think this application will be valuable.\n\n\n*\"I would recommend using a more exact term to describe this rather than contrastive generation\u201c*\n\nWe have removed this term\n\n\n*\"In Section 3.1.1, various heuristics are used. I expected to see the effect or ablation of each heuristic and how important each of them is in terms of generation quality. Also, the baseline models such as CTRL, CC-LM, and PPLM seem to be not using the same heuristics, which seems to be not fair.\u201d*\n\nWe\u2018ve added ablation studies for the weighted decoding and filtering heuristic we use. PPLM also had many heuristics in its design, including KL-scaling, post-geometric norm-fusion, early stopping of latent updates, and adaptive gradient normalization. Also note that GeDi in its current simplified form only has 2 hyper-parameters, whereas PPLM has 7. Also, these heuristics would not be applicable to CC-LM or CTRL, we are using heuristics specific to weighted decoding schemes. \n\n\n*\u201cThe output in Table 6 makes me doubt how the experiments are badly controlled. The outputs from positive and negative sentiment are totally different and almost random text, meaning that the content of the generators is not controlled properly.\u201d*\n\nIn unconstrained text generation from language models like GPT-2 with short prompts, it is normal for generations to be very different even under slightly different settings. In this case, we would definitely expect the positive and negative to be very different as we are guiding towards different attributes. \n \n\n*\u201cIn preparation for prompts for GeDis (5.2) or for measurement of label fidelity (5.3), authors used the pre-trained BERT or RoBERTa on the target attribute like toxicity and topics. As far as I know, these automatic classifiers are not correlated with human judgment, in fact, leading to huge wrongly-predicted labels. I wonder why human annotations are not used here.\u201d*\n\nUnfortunately it is impossible to run human evaluation experiments for every possible design decision and hyper-parameter setting. We did find across tasks that classification labels given by RoBERTa were well but not perfectly correlated with human evaluation annotations.\n\n*\"In Table 2, I don\u2019t see any significant improvements of GeDi against PPLM in its attribution score (i.e., positivity) and transferability to the target domain. Similar to the comment above, none of the experiments are controlled in content. Measuring how the text is similar to the domain (e.g., book-like) sounds interesting but there are no further details of how the human evaluation is studied, what kinds of guidelines are provided to annotators, how the output looks like, etc.\"*\n\nWe now provide statistical significance p-values in Appendix E .  GeDi achieves statistically significantly better sentiment control than PPLM for top-p sampling (both negative and positive), and statistically significantly better negative sentiment control than PPLM in the greedy setting, while also achieving generation speeds 30 times faster. We also now provide the exact annotator guidelines in Appendix G.\n\n*\u201cIn Table 3 and 4, have you performed the same experiments with PPLM and CTRL?\u201d*\n\nWe\u2019ve added PPLM and CTRL-style baselines to our detoxification experiments, and CTRL style baselines to our topic experiments. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TJSOfuZEd1B", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2536/Authors|ICLR.cc/2021/Conference/Paper2536/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847282, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment"}}}, {"id": "oHhqhn926kj", "original": null, "number": 6, "cdate": 1606234169203, "ddate": null, "tcdate": 1606234169203, "tmdate": 1606234169203, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "OdilMghmAcE", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your review!\n\n*\"I have read through the paper, but I could not find any significant test (e.g., annotator agreement, t-test etc.), are the reported human evaluation results significant? could you provide p-values for the results?\"*\n\nAll of the larger improvements in the previous version of the paper were statistically significant, but since we rerun them we\u2019ve added significance tests for the paper in Appendix E. We used a Wilcoxon signed rank test for statistical significance to compare matched pairs since all models generate from the same set of prompts (and because a non-parametric test is appropriate for ordinal data). \n\n*\u201cBoth detoxification and topic control has no baselines to compare with. For instance, CC-LM-non-tox, CTRL, PPLM could have been used to detoxify the generation. For detox, why not using Universal Triggers (Eric et.al. 2019) for making the model generate toxic text, instead of using prompt from the dev set of the same dataset used for training GEDI?\u201d*\n\nThanks for the suggestions. To address these concerns about baselines and triggers, we added a new detoxification experiment that uses triggers from Real Toxicity Prompts and uses PPLM and CTRL-style baselines. We used triggers from Real Toxicity Prompts that were non-toxic but resulted in a high probability of GPT-2 generating toxic text. We found that for detoxification, GeDi performed on par with PPLM for greedy decoding (with repetition penalty for both), and performed statistically significantly better than PPLM with top-p sampling (while being 30x faster). We also found GeDi could detoxify generation from GPT-3 in this new experiment (PPLM can't be applied to GPT-3 with OpenAI's API, but GeDi can).\n\n*\u201cZero-Shot Topics: why PPLM and CTRL cannot do zero-shot on a topic (from the conclusion)? PPLM can use a bag-of-word discriminator, so no training required and generate any kind of topics, and CTRL can use different link/prompt to generate unseen topics?\u201d*\nWe took out our more general claims and focus specifically on comparing with class conditional LMs. For PPLM, bag of words list still needs to be specified somehow and likely would not be available for any imaginable topic. But presumably, it might be possible to choose words that are close to the desired zero-shot topic in embedding space. For CTRL, we attempted to use zero shot control codes using prompting words. It\u2019s possible that links could work better, but one would need to specify a full link (which could influence the generation in other ways) and couldn\u2019t simply use a word. \n\n*\"Could you please add the inline formula, p\u03b8(c|x)\u2248p\u03b8(x|c)p(c), it saves one jump to the paper and makes the paper more readable :)\u201d*\n\nGood idea, :) We added this.\n\n*\u201cGreedy decoding: why using greedy decoding for a language generation task? it is well known that top-p and top-k greatly improve the model generation, are there performance drop if using top-p? how GEDI compare to CTRL, PPLM in this setting?\u201d* \n\n\nGreedy decoding on its own tends to lead to degenerate text, but Keskar et al. (2019) suggests  that greedy decoding works well with a repetition penalty. To examine the effect of sampling, we decided to include results with top-p sampling with p=0.9 for GPT-2, PPLM, and GeDi, and found that GeDi could work well in that setting too. The samples in our new experiments resulting from greedy decoding with a repetition penalty were rated in human annotations as having a higher fluency than the top-p samples for all models where we compared both settings.\n\n*\"Could you please elaborate on why GEDI would be 10k fold less computation as compared with a unidirectional classifier? Could you include a more detailed computational cost analysis?\"*\n\nWe've added pseudo code for GeDi in Section 3 that might help make it more clear how GeDi can do this efficiently. The objective is to classify the resulting sequence that would occur for every possible next token for a partial sequence. To compute this with a standard classifier, every token in the vocabulary would need to be passed through the classifier as an input, each in a separate forward pass. For a vocab size of 50k, this would require 50k forward passes. However, GeDi can compute these classification probabilities for every possible next token via Bayes rule ($p(c|x) \\propto p(x|c)p(c)$ as you mentioned) using only element wise operations in the output layer. This requires only 2 forward passes through the GeDi, one for each of the two control codes.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TJSOfuZEd1B", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2536/Authors|ICLR.cc/2021/Conference/Paper2536/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847282, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment"}}}, {"id": "kkeYHFjfNuX", "original": null, "number": 5, "cdate": 1606232383374, "ddate": null, "tcdate": 1606232383374, "tmdate": 1606232383374, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "giSNeP00gTD", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment", "content": {"title": "Response to Reviewer 4 ", "comment": "Thanks for your in-depth feedback on our work! \n\n**re: \"Human evaluation\u201d**\n\nFor our latest experiments, we\u2019ve added p-values for all important significance tests to Appendix E  and screen shots of annotator instructions to Appendix G.\n\n**re: \"Automatic eval/perplexity\"**\n\nThose perplexity numbers in the appendix of the original submission (now removed because we no longer consider GeDi training) are actually measuring perplexities of real data under the discriminator LM, whereas table PPLM measured the perplexities of their model\u2019s samples under GPT-2. We\u2019ve added perplexity scores and automatic metrics to Tables 2 and 3 to compare with human evaluations as per your suggestion. \n\n**re: \u201cDecoding algorithms\"**\n\nWe\u2019ve now added results for the most critical models with top-p sampling, and find that GeDi works well in this setting. Our human evaluation experiments also found higher fluency scores for our original  generation method all models that compared both settings, so we still focus primarily on that setting.\n\n**re: \u201cDetoxifying\u201d**\n\nIn the previous toxicity experiments the reductions to GPT-2 toxicity were strongly statistically significant. We\u2019ve run new detoxification experiments to compare many more settings with more baselines, with significance results in appendix G. \n\n**re: \u201cDomain transfer\"**.\n\nOur main point about domain transfer is that CTRL-style models (or any model that finetunes to new data) will only be able to generate text that resembles text from the training domain. So a CTRL-style model trained for sentiment on movie reviews will only be able to control sentiment on movie reviews, which we show by the high percentage of movie reviews generated by the model even when given unrelated prompts. This significantly reduces the utility of CTRL-style models. These experiments show that Discriminator based methods like GeDi and PPLM do not appear to have this problem. We may not be able to generalize sentiment to every possible domain, but the point is that the model rarely if ever reverts back to the domain where the discriminator was trained on, allowing more of the breadth of the original LM (in this case GPT-2) to be retained.  \n\n**re: \"Zero-shot control codes\"**\n\nWe added evaluations in Section 5.3 where we train the GeDi on only 3 out of 4 AG news classes and hold one out (We do this separately for all 4 classes). We then evaluate generations on the held out zero-shot control codes by measuring how often a classifier (trained on all 4 classes) classifies them as the same class as the control code, and show that zero-shot control codes are able to bias generation towards the desired class.\n\n**re: \u201cSmaller language models guiding larger language models.\"**\n\nOur main point here is about the size of the base-LM rather than the size of the GeDi, since the base-LM can potentially be huge. Finetuning large language models is very expensive, and it is much more convenient to be able to finetune a smaller model and use it to control a large LM. To better illustrate the effectiveness of this, we included results using our GPT-2-medium based GeDi  (345M parameter) to detoxify GPT-3 (175B parameter).\n\n\n**re: \"Heuristics\"**\n\nWe\u2019ve simplified our method as it was more complicated than it needed to be before. It now only has two hyper-parameters (in comparison, PPLM has 7), since we removed GeDi training ($\\lambda$), decided to omit the bias parameter (which was only used with a GeDi trained model in the original submission), and decided to remove the heuristic that protects tokens from filtering ($\\tau$). We\u2019ve also added an ablation study (Appendix D) comparing applying our filtering heuristic, our weighted decoding heuristic, and our combined filtering weighted decoding heuristic. Our results show that combining these heuristics likely isn\u2019t critical but may be slightly helpful in some settings. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TJSOfuZEd1B", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2536/Authors|ICLR.cc/2021/Conference/Paper2536/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847282, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment"}}}, {"id": "BU9hnFjOHaD", "original": null, "number": 4, "cdate": 1606231059280, "ddate": null, "tcdate": 1606231059280, "tmdate": 1606231059280, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "fmdR4Q9luZZ", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your review and your questions!\n\n*\u201ccan you please elaborate the constraint on \u2018same tokenization\u2019?\u201d*\n\nThe GeDi and the base language model that the GeDi guides must share the same vocabulary in order to apply Bayes rule to compute the attribute classification probabilities for every candidate next word. If the output vocabularies were different, then it wouldn\u2019t be possible to use the classification probabilities given by GeDi to re-weight the language models predictions. Fortunately for us, all GPT-2 models (as well as GPT-3, which we use in some of our new experiments) use the same vocabulary.  \n\n*\u201cis there any possibility that pushing the sentiment towards positive might degrade the accuracy of the overall generation?\u201d*\n\nIn controllable generation methods like GeDi and PPLM, there is a tradeoff between how aggressively you steer generation, and the quality of the text that is generated (where the aggressiveness of the steering is controlled by hyperparameters). So over aggressively steering generation can negatively affect generation quality. We set hyper-parameters to maintain generation quality while also steering towards the desired attribute.\n\n*\u201cHow the value for \u03bb = 0.6 was chosen? What is the impact of other values for this hyper-parameter?\u201d*\n\nWe decided to remove GeDi-training from the paper to simplify the method (so all of the new experiments are equivalent to  \u03bb = 1. In the previous version, lower values of lambda used a stronger discriminative loss, and we found  \u03bb =0.6 and 0.8 to give the most reliable attribute control in the GeDi training settings that we tested. We chose 0.6 because it was more different from generative training.\n\n*\"how did you measure model confidence about the toxicity label\"* \n\nWe used the confidence of the RoBERTa model in terms of its output probabilities and took the prompts that it was least sure about the output class. Our toxicity experiments have changed though, and we have a new procedure for selecting prompts. We use prompts from RealToxicityPrompts that are classified as non-toxic, but have a high probability of resulting in toxic generation from GPT-2. We changed this due to another reviewer concern that the prompts came from the dev set of the same data we used to train the GeDi. We still found that GeDi was very effective for detoxifying GPT-2 (and also GPT-3) in our new experiments in this setting.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TJSOfuZEd1B", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2536/Authors|ICLR.cc/2021/Conference/Paper2536/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847282, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment"}}}, {"id": "5uqzaj7YuEO", "original": null, "number": 3, "cdate": 1606230516919, "ddate": null, "tcdate": 1606230516919, "tmdate": 1606230516919, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "Z51fEqNVNcQ", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment", "content": {"title": "Response to Reviewer 5", "comment": "Thanks for your review! Here are responses to your main concerns: \n\n*\u201cI think some critical ablations are missing.\u201d*\n\nWe have simplified some aspects of our method to make it easier to study the components. We removed \u201cGeDi training\u201d as it complicates the method and was not critical to making the main idea of the paper (GeDi-guided generation) work. We then provided an ablation study of the methods we do use in Appendix D.\n\n\n*\u201cTo improve the presentation of the idea, I would encourage the authors to distill the central idea into a pseudo code which goes along with Section 3\u201d*\n\nThanks for the suggestion. We\u2019ve added pseudo code in Section 3.\n\n*\u201cThe experiments on detoxification are critical to the thesis of the paper, however it seems that experiments in Section 5.2 consider only GPT-2 baselines? I think a strong baseline based on prior-work, like a CTRL generator conditioned on the positive label (as mentioned in Introduction)\u201d*\n\nWe decided to run new more extensive detoxification experiments to better support the thesis of our paper. Our new toxicity experiments use prompts from Real Toxicity Prompts. We include a PPLM (which is still 30x slower for generation than GeDi in this setting) and a CTRL-style (CC-LM) baseline conditioned on the non-toxic label. We find in these experiments that the CTRL-style baseline, while somewhat effective for detoxification, significantly reduces the fluency of generation in human evaluation, which is likely a result of finetuning the CC-LM to new data (which also severely limits the diversity of the model since the toxicity dataset we use is composed mostly of Wikipedia comments). We also included some results detoxifying GPT-3, where existing baselines such as CTRL-style or PPLM cannot be applied through Open AI\u2019s API (but GeDi can).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TJSOfuZEd1B", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2536/Authors|ICLR.cc/2021/Conference/Paper2536/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847282, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment"}}}, {"id": "_dOUANjsj8_", "original": null, "number": 2, "cdate": 1606229914826, "ddate": null, "tcdate": 1606229914826, "tmdate": 1606229914826, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment", "content": {"title": "Overview of changes", "comment": "For convenience, here is an overview of the main changes we have made to the paper:\n\n* We removed GeDi training from the paper because we felt it subtracted from the core idea of GeDi guided generation. GeDi training results in GeDis that are stronger classifiers, but it complicates the method and the actual advantages in controllability are very minor at best, so we decided to exclude it and focus on GeDi-guided generation with generatively trained models. This also makes it easier to do ablation studies, which several reviewers asked for.\n\n* We include new detoxification experiments (Section 5.2) using triggers from RealToxicityPrompts [1] that lead to high toxicity in GPT-2. We include PPLM and CC-LM (CTRL-style) as baselines. Overall, GeDi was effective (very strongly statistically significant) for detoxifying GPT-2, and performed similarly to PPLM in the greedy setting and better than PPLM in the top-p setting (while also achieving generation speeds 30 times faster). To emphasize our point about efficiently controlling very large language models, we also show that we can use a 345M parameter GeDi to greatly reduce toxicity in generations 175B parameter GPT-3, which we accomplish by using GeDi to modify decoding using OpenAI\u2019s GPT-3 API.\n\n* We include ablation studies of our two main heuristics using automatic metrics in Appendix D.\n\n* We include pseudocode to describe the method in Section 3.\n\n* We include statistical significance tests for human evaluation experiments in Appendix E.\n\n* We give screen shots of exact annotator instructions in Appendix G.\n\n* We include top-p sampling in our sentiment and detoxification experiments for our GPT-2, GeDi, and PPLM models. Our main findings were 1. Our original generation method (greedy with a repetition penalty) results in higher fluency scores across models and tasks. 2. GeDi can still control generation well compared with baselines when using top-p sampling.\n\n* We include new topic experiments in Section 5.3 to quantitatively show (using automatic metrics) that zero-shot control codes can bias generation towards unseen topics. We do this by training GeDi models with a class held out from the training set, and showing that we can bias generation towards held out classes by conditioning on an unseen control code.\n\n[1] Gehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). Realtoxicityprompts: Evaluating neural toxic degeneration in language models. EMNLP 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TJSOfuZEd1B", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2536/Authors|ICLR.cc/2021/Conference/Paper2536/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847282, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Comment"}}}, {"id": "p8HCu7H0TtP", "original": null, "number": 1, "cdate": 1605538885767, "ddate": null, "tcdate": 1605538885767, "tmdate": 1605539090550, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Public_Comment", "content": {"title": "The Rational Speech Acts framework", "comment": "Hello, I read the paper very interestingly! \n\nThe idea of controlling language models by computing the posterior distribution with a generative discriminator is very widely studied as the Rational Speech Acts (RSA) framework [1] in computational pragmatics. \n\nThe RSA framework coins the generative discriminator as a listener and the language model as the speaker. \nIt treats language use as reasoning between probabilistic speaker and listener. \nThis idea has been applied to various tasks in NLP, including text-generation [2], reference games [3], image captioning [4-6], instruction following [7], translation [8], referring expression generation [9], and dialogue. \n\nIt would be really great if the authors align the paper with this long line of work in computational pragmatics!\n \n \n\n* [1] Michael C Frank and Noah D Goodman. 2012. Predicting Pragmatic Reasoning in Language Games. Science, 336(6084):998\u2013998.\n* [2] Sheng Shen, Daniel Fried, Jacob Andreas, and Dan Klein. 2019. Pragmatically Informative Text Generation. In NAACL-HLT.\n* [3] Jacob Andreas and Dan Klein. 2016. Reasoning about Pragmatics with Neural Listeners and Speakers. In EMNLP.\n* [4] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. 2016. Generation and Comprehension of Unambiguous Object Descriptions. In CVPR.\n* [5] Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. 2017. Context-Aware Captions from Context-Agnostic Supervision. In CVPR.\n* [6] Reuben Cohn-Gordon, Noah Goodman, and Christopher Potts. 2018. Pragmatically Informative Image Captioning With Character-level Inference. In NAACL-HLT.\n* [7] Daniel Fried, Jacob Andreas, and Dan Klein. 2017. Unified Pragmatic Models for Generating and Following Instructions. In NAACL-HLT.\n* [8] Reuben Cohn-Gordon and Noah Goodman. 2019. Lost in Machine Translation: A Method to Reduce Mean- ing Loss. In NAACL-HLT.\n* [9] Sina Zarrie\u00df and David Schlangen. 2019. Know What You Don\u2019t Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories. In ACL."}, "signatures": ["~Hyunwoo_Kim3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Hyunwoo_Kim3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TJSOfuZEd1B", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/Authors", "ICLR.cc/2021/Conference/Paper2536/Reviewers", "ICLR.cc/2021/Conference/Paper2536/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024961966, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Public_Comment"}}}, {"id": "OdilMghmAcE", "original": null, "number": 1, "cdate": 1603598131317, "ddate": null, "tcdate": 1603598131317, "tmdate": 1605024189742, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review", "content": {"title": "Review", "review": "[Summary]\nIn this paper, the authors propose an efficient method for controllable language generation of large pre-trained LMs (e.g., GPT2). The main idea is to use a smaller, compared to the LM to control, language model trained with control code (Keskar et al., 2019) to generate a per-token score $P(c|x_{1:t}$) to steer the original Language Model distribution. The author proposes two ways, contrastive and discriminative, to approximate $P(c|x_{1:t})$ using bayesian rules. Experiments on open-ended language generation have been shown for positive/negative, detoxification, and topic-control, including a zero-shot topic-control. \n\n[Pros]\n- the proposed methodology is novel for the task and it is effective in controlling the desired attributes. \n- the proposed method is more efficient than WD (Ghazvininejad et al., 2017) since it does not require a forward to the discriminator for each to token in the vocabulary, and computationally more efficient then PPLM (Dathathri et al. 2020) which requires several updates per token. \n- the paper is well-written and easy to follow, except for some minor (later for more info). To the best of my knowledge, the paper is technically correct and reproducible.\n\n[Cons/Question for the authors]\n- I have read through the paper, but I could not find any significant test (e.g., annotator agreement, t-test etc.), are the reported human evaluation results significant? could you provide p-values for the results? \n- Both detoxification and topic control has no baselines to compare with. For instance, CC-LM-non-tox, CTRL, PPLM could have been used to detoxify the generation. For detox, why not using Universal Triggers (Eric et.al. 2019) for making the model generate toxic text, instead of using prompt from the dev set of the same dataset used for training GEDI? \n- Zero-Shot Topics: why PPLM and CTRL cannot do zero-shot on a topic (from the conclusion)? PPLM can use a bag-of-word discriminator, so no training required and generate any kind of topics, and CTRL can use different link/prompt to generate unseen topics?\n- Greedy decoding: why using greedy decoding for a language generation task? it is well known that top-p and top-k greatly improve the model generation, are there performance drop if using top-p? how GEDI compare to CTRL, PPLM  in this setting? \n\n[Reason to accept]\nThe proposed method is a simple and effective way to control the generation of large language models. This is an important and timely problem, especially for language detoxification.\n\n[Reason to reject]\nThe experiments are a bit unclear, looking forward to the author response\n\n[Suggestions and some more questions]\n- With reference to the sentence: \" In addition to class-conditional generation, CC-LMs can be used as generative classifiers by applying Bayes rule to compute $P(c|x_{1:T})$, as is done by Keskar et al. (2019) for source attribution.\" Could you please add the inline formula, $p_\u03b8(c|x) \\approx p_\u03b8(x|c)p(c)$, it saves one jump to the paper and makes the paper more readable :)\n- Could you please elaborate on why GEDI would be 10k fold less computation as compared with a unidirectional classifier? Could you include a more detailed computational cost analysis?\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094269, "tmdate": 1606915773698, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2536/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review"}}}, {"id": "54vRiyEa7kF", "original": null, "number": 2, "cdate": 1603675807363, "ddate": null, "tcdate": 1603675807363, "tmdate": 1605024189684, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review", "content": {"title": "A novel approach but limited experiments with a lack of valid experimental setups. ", "review": "##########################################################################\nSummary:\nThis paper proposed using small-sized LM as a generative discriminator to guide large-sized LM for better controllability and decoding efficiency. \n \n##########################################################################\nReasons for score: \n \nMy score is marginally below the acceptance threshold. \n \nPros:\n\n1. The most exciting part of this paper is to factor out the opposite labels of each token (e.g., positive and negative, or toxic or non-toxic) using Bayes rule in generative models. \n\n \nCons: \n\n1. The contribution of controllability and producing safe output should be separated out. Safer LMs seem to be the outcome of the controllability of the LM by canceling out the opposite part of the target label using the Bayes rule. Highlighting this point might be helpful for showing out the novelty and value of this work. The current frame of the work seems quite distributed between applications and architectural contributions. \n \n2. The main concern of this work is the lack of focused contributions and their validations. The authors claim that this model is good at almost everything; efficiency, controllability while maintaining linguistic quality, reducing the toxicity of GPT2, zero-shot topical generation, etc. However, in fact, most of the experiments in Section 5 are very shallow, uncontrolled, and lack statistical significance. I appreciate the general effectiveness of the model and I don\u2019t doubt it. However, as a conference paper with limited pages, it would be better to make one or two points among them and providing more in-depth with valid setups of experiments. I put additional notes about the experiments below. \n \n3. As mentioned above, the novelty of this comes from using the Bayes rule to make positive and negative labels far from each other. The authors also mention that this is a sort of contrastive learning, and they used contrastive generation. However, contrastive learning is often used to refer to learning by separating two different instances out far each other. In this work, there is no such auxiliary optimization during training time, but the posteriors are re-weighted using the Bayes rule. I would recommend using a more exact term to describe this rather than contrastive generation. \n \n4. In Section 3.1.1, various heuristics are used. I expected to see the effect or ablation of each heuristic and how important each of them is in terms of generation quality. Also, the baseline models such as CTRL, CC-LM, and PPLM seem to be not using the same heuristics, which seems to be not fair. \n \n5. The output in Table 6 makes me doubt how the experiments are badly controlled. The outputs from positive and negative sentiment are totally different and almost random text, meaning that the content of the generators is not controlled properly. In preparation for prompts for GeDis (5.2) or for measurement of label fidelity (5.3), authors used the pre-trained BERT or RoBERTa on the target attribute like toxicity and topics. As far as I know, these automatic classifiers are not correlated with human judgment, in fact, leading to huge wrongly-predicted labels. I wonder why human annotations are not used here. \n\n6. In Table 2, I don\u2019t see any significant improvements of GeDi against PPLM in its attribution score (i.e., positivity) and transferability to the target domain. Similar to the comment above, none of the experiments are controlled in content. Measuring how the text is similar to the domain (e.g., book-like) sounds interesting but there are no further details of how the human evaluation is studied, what kinds of guidelines are provided to annotators, how the output looks like, etc. \n\n7. In Table 3 and 4, have you performed the same experiments with PPLM and CTRL? \n \n##########################################################################\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094269, "tmdate": 1606915773698, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2536/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review"}}}, {"id": "giSNeP00gTD", "original": null, "number": 3, "cdate": 1603896406178, "ddate": null, "tcdate": 1603896406178, "tmdate": 1605024189621, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review", "content": {"title": "Review: GeDi: Generative Discriminator Guided Sequence Generation", "review": "#### Summary\n\nThe authors propose a method for controlling attributes of generated text (sentiment, topic, toxicity, etc) by reweighting a base language model's token-level distributions with auxiliary conditional language models, bayes rule, and additional heuristics.\n\nThe core idea is simple - which is a strength in my view - and does not require retraining the base language model, which could be important as language models become more expensive to train. However, the clarity and experiments in this paper fall short: the experimental setup has issues (detailed below), the effect on perplexity is quite large but relegated to the Appendix, several claims are speculative and lacking corresponding experimental evidence, and it is unclear how the additional heuristics affect performance.\n\nThe method seems promising, but with the current experiments it is difficult to draw conclusions about how the method affects performance and which parts of it are necessary; given that this is an empirical paper, I would therefore not recommend acceptance in its current form.\n\n#### Experimental setup\n\n- **Human evaluation**. There is little information given about how the human evaluation prompts are written (and why they are written that way), how many evaluators are used, and there are no significance tests (e.g. \"We run human evaluation to measure toxicity\" does not given enough details). This is concerning since all of the results in the main text use human evaluation, sometimes with small differences between methods.\n\n- **Automatic eval/perplexity**. The authors only measure perplexity in one set of experiments in the appendix (and it gets worse by introducing GeDI training). It would be good to have perplexity and automatic metrics to compare against the human evaluation (e.g. see Table 4 in the PPLM paper).\n\n- **Decoding algorithms**. The authors only show results for greedy decoding with a repetition penalty (with no ablation on the choice of penalty parameter). Results with a sampling method (e.g. nucleus) are needed for this open-ended setting, or an argument for why these aren't considered.\n\n#### Effect of the method\n- **Effect on perplexity**. The perplexity gets much worse as the gedi training is introduced (i.e. $\\lambda$ decreases), e.g. going from 25 to 45 on IMDb. This result is in the Appendix, and perplexity is never evaluated/reported in the other experiments.\n\n- **Gedi training**. It's unclear whether the gedi training (i.e. the $\\mathcal{L}_d$ loss) is beneficial: in some experiments $\\lambda=1.0$ performs similarly, and on the IMDb/MNLI/QNLI experiments decreasing $\\lambda$ either hurts, has no effect, or improves performance (i.e. no consistent trend).\n\n- **Detoxifying**. It's unclear how significant the results in Table 3 are, and there are no baselines; in general it it difficult to draw conclusions from these results.\n\n#### Speculative claims or conclusions\n- **Domain transfer**. Figure 1 gives an intuition for why domain transfer might be possible, but only an anecdote (first paragraph of 5.1, \"we noticed that\") and a single experiment is done, where the method performs similarly to PPLM. Crucially, the GEDI training does not appear to help over just re-weighting with the conditional LM ($\\lambda=1.0$ vs. $\\lambda=0.6$). Could the authors comment on this result? How well does domain transfer work for less similar domains? How is perplexity affected for the models reported in Table 2?\n\n- **Zero-shot control codes**. The authors only provide anecdotes for evaluating the Zero-shot control codes. Based on the evaluation it's quite speculative to say \"GeDi\u2019s ability to generalize to new control codes zero-shot gives the ability to generate text corresponding to many topics and subtopics.\".\n\n- **Smaller language models guiding larger language models**. To be fair, the authors use GPT-2 medium as the conditional language model, and GPT-2 XL as the base language model, which is larger, but there was no investigation of this aspect of size difference. How small can the conditional LM be? Why was medium used instead of small? What if large was used? Does the conditional LM need to be a large-scale pretrained model (it would be nice to see a baseline of a simpler conditional LM)?\n\n#### Heuristics\n- Several heuristics are used: $\\alpha/T_i$ weighting, $\\omega$ weighting, nucleus filtering, keeping tokens over a threshold, repetition penalty, and rescaling the logits to positive (used in only one experiment).\n- How does each of these affect performance? There are no ablations, and given the small differences in some of the experiments it is unclear whether performance would actually be worse if we changed one of the heuristics. One outcome may be that the method only works for a careful balance of hyperparameters, which could be fine, but we don't have a sense of the variation.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094269, "tmdate": 1606915773698, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2536/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review"}}}, {"id": "fmdR4Q9luZZ", "original": null, "number": 4, "cdate": 1603980105279, "ddate": null, "tcdate": 1603980105279, "tmdate": 1605024189559, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review", "content": {"title": "The paper is written well", "review": "The paper proposed a method \u2014- GeDi \u2014 to generate guided and controlled text from a large language model (LM). The method utilizes smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. By safer and controllable they emphasis on the toxicity, hate, bias, and negativity contains in the training of the large LM. The proposed method guides generation at each time step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions (i.e. contrastive discrimination); one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute (i.e. contrastive attribute), or anti-control code. \n\nThe paper explores ways to increase generation speed and claimed that with the proposed techniques the generation speeds more than 30 times faster compared to PPLM model. The paper explores different heuristics to impose the guided generation including bias parameter, weighted decoding and filtering heuristics. The findings are that GeDi gives stronger controllability than the state of the art method (i.e.PPLM, CC-LM, CTRL).  \n\nExperiments show that, training GeDi on four topics (i.e. Business, Science/Tech, Sports, World ) allows the controlled generation of new topics zero-shot from just a keyword. They also demonstrate that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality.\n\n\nRe: \u201cso long as the LM and GeDi share the same tokenization\u201d: can you please elaborate the constraint on \u2018same tokenization\u2019?\n\nRe: \u201cIf the GeDi was trained on movie reviews for sentiment control, its direct class-conditional predictions will be biased towards predicting movie review words (illustrated by next word prediction of \u201ccinematic\u201d). However, by contrasting the predictions of opposing control codes via Bayes rule, the bias towards movie reviews can be cancelled out.\u201d: The word cinematic can reveal a neutral/negative sentiment, is there any possibility that pushing the sentiment towards positive might degrade the accuracy of the overall generation?\n\nRe: GeDi training (\u03bb < 1 in Equation (10)) and standard generative training(\u03bb = 1 in Equation (10)). : How the value for \u03bb = 0.6 was chosen? What is the impact of other values for this hyper-parameter? \n  \nRe: \u201cIn order to have prompts that are more likely to trigger aggressive generations but less likely to be explicitly toxic, we pass candidate prompts through a RoBERTa (Liu et al., 2019) model trained to classify toxicity, and only kept prompts where RoBERTa was less confident about the toxicity label.\u201c: how did you measure model confidence about the toxicity label?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094269, "tmdate": 1606915773698, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2536/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review"}}}, {"id": "Z51fEqNVNcQ", "original": null, "number": 5, "cdate": 1604775272302, "ddate": null, "tcdate": 1604775272302, "tmdate": 1605024189499, "tddate": null, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "invitation": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review", "content": {"title": "Proposes algorithm for controllable sequence generation", "review": "Summary\nThe paper considers the problem of attribute-based sequence generation, particularly in language models. Authors propose a framework \u201cGeDi\u201d which learns a generative classifier for controlling generation from a large language model. With experiments on publicly available datasets and models, and including human-evaluation, the authors empirically demonstrate that the algorithm is computationally efficient and is competitive against strong baseline algorithms like CTRL, Plug&Play language models (PPLM).\n\nReason for the score\nI vote for rejecting the current version of the paper (marginally below acceptance threshold). While the premise of the problem is well motivated, I think several sections of the paper are difficult to follow. I would strongly encourage the authors to include a pseudo code of the algorithm to improve the presentation of the central idea. The paper includes several experiments, though I think some critical ablations are missing.\n\nStrengths\n+ The problem is practically well motivated and is very relevant to the language learning community. \n+ The proposed algorithm of using generative classifiers is computationally efficient compared to strong baselines like CTRL and PPLM. The experimental results suggest that the algorithm also allows for better control over generation from a LM while maintaining linguistic quality.\n\nWeaknesses\n- Several sections of the paper are hard to follow. To improve the presentation of the idea, I would encourage the authors to distill the central idea into a pseudo code which goes along with Section 3.\n- The experiments on detoxification are critical to the thesis of the paper, however it seems that experiments in Section 5.2 consider only GPT-2 baselines? I think a strong baseline based on prior-work, like a CTRL generator conditioned on the positive label (as mentioned in Introduction), would help evaluating the gap between proposed approach and current algorithms.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2536/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2536/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authorids": ["~Ben_Krause1", "~Akhilesh_Deepak_Gotmare1", "~Bryan_McCann1", "~Nitish_Shirish_Keskar1", "~Shafiq_Joty1", "~richard_socher1", "~Nazneen_Rajani1"], "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "richard socher", "Nazneen Rajani"], "keywords": ["Language modeling", "controllable generation", "decoding schemes", "auto-regressive models", "language modeling safety"], "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krause|gedi_generative_discriminator_guided_sequence_generation", "one-sentence_summary": "We use smaller language models as generative discriminators to guide generation from larger language models towards desirable attributes. ", "pdf": "/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KgGcObtZWy", "_bibtex": "@misc{\nkrause2021gedi,\ntitle={GeDi: Generative Discriminator Guided Sequence Generation},\nauthor={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and richard socher and Nazneen Rajani},\nyear={2021},\nurl={https://openreview.net/forum?id=TJSOfuZEd1B}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TJSOfuZEd1B", "replyto": "TJSOfuZEd1B", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094269, "tmdate": 1606915773698, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2536/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2536/-/Official_Review"}}}], "count": 15}