{"notes": [{"id": "-qB7ZgRNRq", "original": "Yjyxf3DAOGc", "number": 1520, "cdate": 1601308168835, "ddate": null, "tcdate": 1601308168835, "tmdate": 1614985664107, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "7y5Hu8_-Ek7", "original": null, "number": 1, "cdate": 1610040497199, "ddate": null, "tcdate": 1610040497199, "tmdate": 1610474103628, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors propose a dataset and a method for the task of SpokenQA. The dataset is generated by using Google TTS to generate audio segments corresponding to the CoQA dataset and then using an ASR system to generate (noisy) transcripts of these speech segments. The authors then propose a method which uses a combination of various known techniques. \n\nPros:\n- A good first attempt at creating an interesting dataset for a useful task\n\nCons:\n- Lack of clarity in writing\n- Use of original clean text as input to the model which beats the purpose (in a natural setting such clean text will not be available)\n\nAll reviewers have appreciated the effort and attempt at creating a new dataset for this task. However, they have also pointed out that paper is not very clearly written and some important issues (use of clean text as input to the model) need to be adequately addressed before the paper is ready for acceptance. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040497186, "tmdate": 1610474103612, "id": "ICLR.cc/2021/Conference/Paper1520/-/Decision"}}}, {"id": "qFzbdVDzRh6", "original": null, "number": 4, "cdate": 1604020248221, "ddate": null, "tcdate": 1604020248221, "tmdate": 1607370045879, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Official_Review", "content": {"title": "Review", "review": "This paper proposes a new task: spoken conversational question answering, which combines conversational question answering (e.g. CoQA) with spoken question answering (e.g. Spoken-SQuAD). The task is to answer a question (in written text) given a question that is given in both audio form and text form. They create a dataset for this task by combining CoQA with some off-the-shelf text-to-speech and speech-to-text models. They then propose a new model, DDNet, which obtains improved performance on their dataset.\n\nPros: \n\n-I buy the general argument made in the paper that relying solely on transcribed text from audio for answering the questions could be problematic, so I can see the value in the proposed dataset. I also thought the way the dataset is produced, by running CoQA through a text-to-speech model, was fairly clever. I\u2019m not very familiar with the literature on spoken question answering to know if this is a common practice. \n\n-Another positive of the paper is that the DDNet architecture does seem to improve the performance on their dataset by a fairly large amount (though no error bars are given). \n\n-I appreciate that the paper conducts ablations on the different model components.  \n\nCons:\n-In my view, a drawback of this paper is that it is a bit difficult to read (partially due to grammatical errors). I found much of the motivation for this new task to not be clear from reading the paper. For example, I found the following excerpt difficult to parse:\n\u201cUnlike existing SQA datasets, Spoken-CoQA is a multi-turn conversational SQA dataset, which is more challenging than single-turn benchmarks. First, every question is dependent on the conversation history in the Spoken-CoQA dataset. It is thus difficult for the machine to parse. Second, errors in ASR modules also degrade the performance of machines in tackling contextual understanding with context paragraph.\u201c\n\n-Similarly, I found Figure 1 to be very confusing.\n\n\n-As far as I can tell, the paper doesn't compare to any other baselines that incorporate the audio information in a different way than DDNet. For example, the method from Serdyuk et al. (2018) could be considered. Strangely, the paper claims that this paper was \u2018concurrent\u2019, despite the fact that it was published in 2018, which is very confusing to me.\n\n-It\u2019s unclear why the Spoken-CoQA dataset has to include text transcripts as well as the audio --- to me, it makes more sense for that to be part of the model solving the dataset.\n\n- Ideally the dataset would have natural speech, instead of synthetic speech, but I don\u2019t consider this a major limitation.\n\nOverall:\nI think this is a borderline paper, erring on the side of rejection. My main concern is the lack of audio-based baselines other than DDNet, and the clarity of the paper.\n\n******* Update after reading rebuttal ********\nI appreciate the additional comparisons to prior work added by the authors. I've raised my score to a 6, but I still view this as a borderline paper due to concerns about clarity and impact, along with the  concerns raised by Reviewer 3.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1520/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116725, "tmdate": 1606915799381, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1520/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1520/-/Official_Review"}}}, {"id": "zg8cWd3L90r", "original": null, "number": 3, "cdate": 1603969455492, "ddate": null, "tcdate": 1603969455492, "tmdate": 1606807488165, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Official_Review", "content": {"title": "Propose a spoken conversational question answering system", "review": "The authors tackle the problem of spoken conversational question answering involving multiple turns of a dialogue, where the documents and questions are both in spoken and text form. The authors also compile a new dataset for spoken conversational QA with the help of text-to-speech systems.\n\nIn its current form, I think this work is fairly limited in its scope and is not yet ready to be published at ICLR. Other than ablations of the proposed technique and different choices of backbone networks for the proposed DDNet framework, since there are no comparisons made to prior approaches it is hard to assess the merits of the proposed approach. The authors also do not mention any plans of releasing the new dataset described in this work.\n\nThe draft will also benefit from a thorough editing pass; there are many typos (e.g., \"tailed for a specific domain\", \"nature language processing\", \"BRET-base\", etc).\n\nThree other comments:\n* It might be interesting to show how F1 scores vary on the test instances as a function of the number of turns in the conversation. Do the ASR errors compound to hurt performance on conversations with a large number of turns or is that not much of an issue? Similarly, showing how test F1 scores vary as a function of ASR accuracy of the spoken documents/questions would also be interestin\ng.\n* What is the error rate of the ASR system on the spoken documents and spoken questions? This will give the reader an idea of the accuracy of the transcriptions fed as input to the student model.\n* In Table 4, the F1 scores using SDNet are higher for S-CoQA compared to CoQA which is unexpected. Could the authors comment on why this might be?\n\n------------\n\nUpdate after author response: \n\nI've increased my score to 5. However, I still think this work is not ready to be published at ICLR in its current form. One of the other reviewers had raised an important point about the reliance of the proposed system on clean text which the authors should consider addressing in an updated version of this work.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1520/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116725, "tmdate": 1606915799381, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1520/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1520/-/Official_Review"}}}, {"id": "aHj_PXJqPaE", "original": null, "number": 7, "cdate": 1605413051088, "ddate": null, "tcdate": 1605413051088, "tmdate": 1605597319896, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "3swdK-jWmVG", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your review. We understand your sentiment and found that the comments have some significant misread, which we want to point out right away:\n - The reviewer mentioned, quote *\u201c Note that the dataset constructed does not update the answer spans based on the noisy ASR text and continues to assume answer-spans as per the actual text.\u201d* We very appreciate you point out this, but in fact we have done the data filtering in our investigation as follows: If the answer spans to questions do not exist in the referenced ASR transcriptions, we removed question-answer pairs from our Spoken-CoQA dataset because these examples are too complicated for spoken machine comprehension. Finally, we collected more than 40k question-answer pairs. As a result, we believe it will alleviate the reviewer's concerns, such as mentioned *\u201cit cannot learn anything meaningful.\u201d* And we feel that this is critical to the understanding, so we have added a detailed description in the manuscript.  \n - We will release our dataset and all the codes after the publication.\n - Regarding the comment, quote *\u201crelying on the clean text kind of defeats the purpose of speech-based conversational QA\u201d.* Inspired by the recent work [1,2], we use clean text as an auxiliary tool to solve this speech-based conversational question answering task.\n\n[1] Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension\n\n[2] ODSQA: Open-domain Spoken Question Answering Dataset"}, "signatures": ["ICLR.cc/2021/Conference/Paper1520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-qB7ZgRNRq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1520/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1520/Authors|ICLR.cc/2021/Conference/Paper1520/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858743, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1520/-/Official_Comment"}}}, {"id": "cTZ8XWb2En4", "original": null, "number": 4, "cdate": 1605409775516, "ddate": null, "tcdate": 1605409775516, "tmdate": 1605596858544, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "qFzbdVDzRh6", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thanks for the review. Below we want to address several questions that we think are key points to clarify first:\n - We found out that we had a proofreading mistake in Figure 1, where the $A_2$ should be $A_3$. We have updated it in the revised manuscript.\n - To our best knowledge, we are the first attempt for spoken conversational question answering task. There is no specific baseline for this task. We have conducted a comparison with the method proposed by Serdyuk et al. (2018) (See Table 4). The method is designed to address the spoken language understanding (SLU) task. In comparison, our spoken conversational question answering task includes spoken/natural language understanding, and conversational question answering, where is much more challenging than the SLU task. Furthermore, our proposed *DDNet* shares a similar network architecture but has a more considerable amount of parameters than the method proposed by Serdyuk et al. (2018). We sincerely appreciate the reviewer to point out this. We compared several state-of-the-art spoken question answering methods [1,2,3] in Table 4 of the revised manuscript. Our results suggest that the proposed *DDNet* achieves remarkable performance on SCQA tasks.\n - We agree that the Spoken-CoQA dataset is necessary to include text transcripts as well as the audio. However, inspired by the recent success (Spoken-SQuAD [1] and ODSQA [2]), we aim at leveraging text transcripts to improve model performance. As shown in Tables 4 and 5, it demonstrates that the text-audio features indeed enable the model to achieve better performance compared with the same model trained on the audio-only Spoken-CoQA dataset.\n\n[1] Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension\n\n[2] Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation.\n\n[3] ODSQA: Open-domain Spoken Question Answering Dataset"}, "signatures": ["ICLR.cc/2021/Conference/Paper1520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-qB7ZgRNRq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1520/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1520/Authors|ICLR.cc/2021/Conference/Paper1520/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858743, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1520/-/Official_Comment"}}}, {"id": "QahrLY37Us7", "original": null, "number": 6, "cdate": 1605412543945, "ddate": null, "tcdate": 1605412543945, "tmdate": 1605595473151, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "evkZQIWqsX1", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks for the review. We found that the comments have some significant misread, which we want to point out right away:\n - Regarding the comment, quote *\u201cnot entirely sure if the contributions and its novelty warrant an accept.\u201d*, we want to point out that our work is the first attempt to propose a new task - **spoken conversational question answering**, and there exist many applications related to this in the real life. Furthermore, we release a new dataset - Spoken-CoQA, and present a novel end-to-end distillation model for the spoken conversational question answering task.\n - Regarding the comment, quote *\u201cit seems the dataset isn't made available.\u201d*,  we will release our dataset and all the codes after the publication.\n - Regarding the comment, quote *\u201cCan these two mechanisms be combined (to reach an even better performance)?\u201d* We sincerely appreciate the reviewer to point out this. Actually, we have done the combination work. However, since we want to demonstrate the effectiveness of the distillation strategy, we report the results of cross attention and knowledge distillation in Table 4, separately. We have added the results of a combination of two mechanisms in Table 4. Our results demonstrate that the combination of two mechanisms can bring significant performance improvements. Meanwhile, we compared several state-of-the-art spoken question answering methods [1,2,3] in Table 4 of the revised manuscript. This suggests that the proposed DDNet achieves remarkable performance on SCQA tasks.\n - Regarding the comment, quote *\"Is it correct to say that the textual input is more useful than the audio input in this dataset?\u201d*, we present the results in Table 5 in the Appendix. As shown in the Table, we argue that it is hard to quantify the importance of textual input or audio input for the spoken conversational question answering (SCQA) task. Our main goal is to find an effective way to address the SCQA task. And some previous works [4,5] may shed light on this.\n\n[1] Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension\n\n[2] Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation.\n\n[3] ODSQA: Open-domain Spoken Question Answering Dataset\n\n[4] SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering\n\n[5] An Audio-enriched BERT-based Framework for Spoken Multiple-choice Question Answering\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-qB7ZgRNRq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1520/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1520/Authors|ICLR.cc/2021/Conference/Paper1520/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858743, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1520/-/Official_Comment"}}}, {"id": "9lF44RwBYvh", "original": null, "number": 5, "cdate": 1605410582550, "ddate": null, "tcdate": 1605410582550, "tmdate": 1605594875494, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "zg8cWd3L90r", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for the detailed questions. Please see our responses below, and we would appreciate follow up discussions if anything remains unclear.\n - To our best knowledge, we are the first attempt for spoken conversational question answering task. There is no specific baseline for this task. We sincerely appreciate the reviewer to point out this. We compared several state-of-the-art spoken question answering methods [1,2,3] in Table 4 of the revised manuscript. Our results suggest that the proposed DDNet achieves remarkable performance on SCQA tasks.\n - We will release our dataset and all the codes after the publication.\n - Sorry for the typos. We have revised in the manuscripts.\n - As mentioned, quote *\u201cDo the ASR errors compound to hurt performance on conversations with a large number of turns or is that not much of an issue? \u201c*  We have observed this phenomenon and experiments are underway. As for the comment, quote *\u201cSimilarly, showing how test F1 scores vary as a function of ASR accuracy of the spoken documents/questions would also be interesting\u201d*, our research mainly focuses on the distillation approach to directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. We will investigate this in our future work. \n - The word error rate of the ASR system is 17.3% on the spoken documents and spoken questions in our investigation.\n - As mentioned, quote *\u201cthe F1 scores using SDNet are higher for S-CoQA compared to CoQA\u201d*, the main possible reason is SDNet employs BERT as its encoders since the latter is trained on a large benchmark dataset with clean corpus, which may obtain the performance improvements to ASR transcriptions.\n\n[1] Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension\n\n[2] Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation.\n\n[3] ODSQA: Open-domain Spoken Question Answering Dataset\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-qB7ZgRNRq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1520/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1520/Authors|ICLR.cc/2021/Conference/Paper1520/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858743, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1520/-/Official_Comment"}}}, {"id": "3swdK-jWmVG", "original": null, "number": 1, "cdate": 1603708153693, "ddate": null, "tcdate": 1603708153693, "tmdate": 1605024423077, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Official_Review", "content": {"title": "Unfortunately falls short of delivering a usable dataset for spoken conversation QA ", "review": "---------------\nSummary\n---------------\n\nIn this paper, the authors release a new dataset -  Spoken-CoQA which includes an ASR based version of the popular CoQA dataset. The dataset has been created by running the Google TTS system followed by ASR using CMU Sphinx, to create a speech-transcribed versions of the dataset. The dataset includes the corresponding TTS audio recordings. Since the transcribed dataset has transcription errors, existing reading comprehension models do not work well. Thus, the paper introduces a joint audio-textual model for QA on the Spoken-CoQA dataset that uses TTS recordings its corresponding ASR output. \n\nThe model encodes audio data using the recent Speech-BERT model while the text is encoded using regular BERT. Similar to visio-linguistic models such as VilBERT, the model uses cross-attention on the intermediate audio and textual representations. This is done by using query matrix of a transformer block from one modality and applying it for attention using the key and value matrices on the other. That is, query keys from text are used to attend over keys and values from audio and vice-versa. The cross-attended representations are concatenated and used as input to any \"CMRC\" module. The authors experiment with two \"CMRC\" modules - SDNet and FlowQA. These are existing models developed for QA on CoQA, to return span-extracted answers from the input passage. Note that the dataset constructed does not update the answer spans based on the noisy ASR text and continues to assume answer-spans as per the actual text. Thus, to address this the authors include a knowledge distillation (KD) layer where the teacher network uses the gold speech transcriptions with audio, while the student layer uses the noisy transcriptions.  \n\nThe experiments have been presented using SDNeT, FlowQA as the CMRC modules and also by using BERT and ALBERT as the alternative CMRC modules. Models have been trained and cross-tested using CoQA and Spoken-CoQA. As expected, configurations with cross-testing report a deterioration in performance. For models trained using Spoken-CoQA, all models benefit from using audio-textual cross-attention as well as KD. \n\n--------------------------------------\nStrengths and Weaknesses\n---------------------------------------\nThe paper is interesting overall - while the model by itself is a trivial combination of existing multi-model methods, they result in upto 2pt improvements. I'd be willing to overlook this aspect, but to me the biggest weakness of this paper is in its data construction. It appears, when the ASR output is noisy -- the spans refer to ghost token positions (based on the clean text) -- example in Table 2 where the first question is unanswerable in span positions since the word \"white\" does not exist in the ASR text. The authors also allude to this when they motivate the KD layer, but my concern is what does one learn with this data? Any existing CMRC model trained on this data is obviously going to be bad -- it cant learn anything meaningful. Similarly, even when one employs the audio-textual model why should the model learn to predict the wrong span? I guess that is why the models also dont improve much - with the use of audio-textual encoding and KD. I'm not sure what can be done about this -- perhaps instead of using the text-based spans, one could return the audio segments  as answers? That might be more meaningful but will require annotation. In fact doing so may also remove the need of the KD layer which in some ways tries to (incorrectly) fix the problem by also showing it the gold clean transcription (original passage) . However, I'd argue that  relying on the clean text kind of defeats the purpose of speech-based conversational QA as motivated in this paper.\n\nSpeech based conversation QA is an important problem and the authors make a good first attempt, but unfortunately the paper falls short of delivering a usable dataset for this task. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1520/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116725, "tmdate": 1606915799381, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1520/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1520/-/Official_Review"}}}, {"id": "evkZQIWqsX1", "original": null, "number": 2, "cdate": 1603930930113, "ddate": null, "tcdate": 1603930930113, "tmdate": 1605024422875, "tddate": null, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "invitation": "ICLR.cc/2021/Conference/Paper1520/-/Official_Review", "content": {"title": "Good paper, could be better", "review": "This paper studies spoken conversational QA and its main contributions includes:\n\n- It proposes a task for end-to-end spoken QA. The new dataset Spoken-CoQA is derived from CoQA with additional features including audio data and ASR transcripts. \n\n- It proposes a method utilizing data distillation to learn from speech and text jointly. \n\nThe paper is technically sound and well structured. However, maybe I missed something, but I am not entirely sure if the contributions and its novelty warrant an accept. \n\nWhile the two methods, (1) the cross attention mechanism for speech and text embedding fusion and (2) knowledge distillation for combatting ASR errors, both bring improvement to the base models, they are existing, well-studied methods. The most important contribution of this paper, in my opinion, is the construction of the Spoken-CoQA dataset. But it seems the dataset isn't made available. \n\nBetween knowledge distillation and cross attention, it seems the former brings a larger improvement while the latter generally only has a marginal effect (Table 4). On this result, I have two questions: (1) Can these two mechanisms be combined (to reach an even better performance)? (2) Is it correct to say that the textual input is more useful than the audio input in this dataset? This might be an interesting question especially as the audio input is much larger and hence more difficult to process.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1520/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1520/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Data Distillation for End-to-end Spoken Conversational Question Answering", "authorids": ["~Chenyu_You1", "~Nuo_Chen1", "~Fenglin_Liu1", "~Dongchao_Yang1", "~Zhiyang_Xu1", "~Yuexian_Zou1"], "authors": ["Chenyu You", "Nuo Chen", "Fenglin Liu", "Dongchao Yang", "Zhiyang Xu", "Yuexian Zou"], "keywords": ["spoken question answering", "natural language processing", "speech and language processing", "knowledge distillation"], "abstract": "In spoken question answering, QA systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling QA systems to model complex dialogues flow given the speech utterances and text corpora. In this task, our main objective is to build a QA system to deal with conversational questions both in spoken and text forms, and to explore the plausibility of providing more cues in spoken documents with systems in information gathering. To this end, instead of adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which directly fuse audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions. In addition, to evaluate the capacity of QA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 120k question-answer pairs. Experiments demonstrate that our proposed method achieves superior performance in spoken conversational question answering.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "you|towards_data_distillation_for_endtoend_spoken_conversational_question_answering", "pdf": "/pdf/51ca73e48ac5682b291ddbaa96b0072285b0cf4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sCG7rSU34D", "_bibtex": "@misc{\nyou2021towards,\ntitle={Towards Data Distillation for End-to-end Spoken Conversational Question Answering},\nauthor={Chenyu You and Nuo Chen and Fenglin Liu and Dongchao Yang and Zhiyang Xu and Yuexian Zou},\nyear={2021},\nurl={https://openreview.net/forum?id=-qB7ZgRNRq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-qB7ZgRNRq", "replyto": "-qB7ZgRNRq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116725, "tmdate": 1606915799381, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1520/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1520/-/Official_Review"}}}], "count": 10}