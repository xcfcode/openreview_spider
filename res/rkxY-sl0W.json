{"notes": [{"tddate": null, "ddate": null, "tmdate": 1521494254630, "tcdate": 1509106039829, "number": 365, "cdate": 1518730180175, "id": "rkxY-sl0W", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rkxY-sl0W", "original": "S1yYbjxR-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": ["H1xSvMh8M"], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260091504, "tcdate": 1517249628790, "number": 370, "cdate": 1517249628772, "id": "BJHPNJTrM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rkxY-sl0W", "replyto": "rkxY-sl0W", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "the problem is interesting, and the approach is also interesting. however, the reviewers have found that this manuscript would benefit from more experiments, potentially involving some real data (even at least for evaluation) in addition to largely synthetic data sets used in the submission. i also agree with them and encourage authors to consider this option.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1515658355064, "tcdate": 1515658355064, "number": 6, "cdate": 1515658355064, "id": "HJsu29V4G", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "forum": "rkxY-sl0W", "replyto": "rkGWRHCMM", "signatures": ["ICLR.cc/2018/Conference/Paper365/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper365/AnonReviewer2"], "content": {"title": "Response to Authors", "comment": "- Linearization\nMake sense, and I recommend to add at least 1 conversion example to the paper to guarantee reproducibility, because some accidental style errors in linearized texts may affect the results.\n\n- \"Meaningless\" test sets\nThe point of my concern is the problem which possibly not-few auto-generated codes have (not \"all\" of them). CFG rules basically represent a wider language than actual specification of \"executable\" codes, and auto-generating codes based on only CFG rules may include actually incorrect ones (e.g., the use-before-define error can occur using the BNFs in Appendix A, but this is one of the critical problems in many programming languages).\nConsidering about the case that there are non-ignorable amount of incorrect codes in the evaluation data, it becomes hard to declare the expressiveness of the proposed model in real problems too.\n\n- CoffeeScript and Javascript\nComplexity class is not the point. CoffeeScript has an explicit map (compiler) to Javascript which is guaranteed by the concept of itself, so converting CoffeeScript to Javascript is too trivial to generalize the effectiveness of the program translation models for other languages, which do not have explicit maps between each other (even when they share some programming paradigms, such as Java and Python). I think that, the translation task in opposite side, i.e., translating \"raw\" Javascript codes (which are gathered or generated from scratch, not generated from CoffeeScript) to CoffeeScript, was more effective to discuss about this point."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825734883, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkxY-sl0W", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper365/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper365/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper365/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers", "ICLR.cc/2018/Conference/Paper365/Authors", "ICLR.cc/2018/Conference/Paper365/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825734883}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642439471, "tcdate": 1511634669638, "number": 1, "cdate": 1511634669638, "id": "BkLxPNweG", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Review", "forum": "rkxY-sl0W", "replyto": "rkxY-sl0W", "signatures": ["ICLR.cc/2018/Conference/Paper365/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a tree-to-tree neural network for translating programs\nwritten in one Programming language to another. The model uses soft attention\nmechanism to locate relevant sub-trees in the source program tree when \ndecoding to generate the desired target program tree. The model is evaluated\non two sets of datasets and the tree-to-tree model outperforms seq2tree and\nseq2seq models significantly for the program translation problem.\n\nThis paper is the first to suggest the tree-to-tree network and an interesting\napplication of the network for the program translation problem. The evaluation\nresults demonstrate the benefits of having both tree-based encoder and decoder. \nThe tree encoder, however, is based on the standard Tree-LSTM and the application\nin this case is synthetic as the datasets are generated using a manual rule-based \ntranslation. \n\nQuestions/Comments for authors:\n\nThe current examples are generated using a manually developed rule-based system. \nAs the authors also mention it might be challenging to obtain the aligned examples\nfor training the model in practice. What is the intended use case then of \ntraining the model when the perfect rule-based system is already available?\n\nHow complex are the rules for translating the programs for the two datasets and what\ntype of logic is needed to write such rules? It would be great if the authors can \nprovide the rules used to generate the dataset to better understand the complexity\nof the translation task.\n\nThere are several important details missing regarding the baselines. For the \nseq2seq and seq2tree baseline models, are bidirectional LSTMs used for the encoder?\nWhat type of attention mechanisms are used? Are the hyper-parameters presented in\nTable 1 based on best training performance?\n\nIn section 4.3, it is mentioned that the current models are trained and tested on \nprograms of length 20 and 50. Does the dataset contain programs of length upto \n20/50 or exactly of length 20/50? How is program length defined -- in terms of \ntree nodes or the number of lines in the program?\n\nWhat happens if the models trained with programs upto length 20 are evaluated on \nprograms of larger length say 40? It would be interesting to observe the \ngeneralization capabilities of all the different models.\n\nThere are two benefits of using the tree2tree model: i) use the grammar of the\nlanguage, and ii) use the structure of the tree for locating relevant sub-trees\n(using attention). From the current evaluation results, the empirical benefit\nof using the attention is not clear. How would the accuracies look when using \nthe tree2tree model without attention or when attention vector e_t is set to the\nhidden state h of the expanding node?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642439377, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper365/AnonReviewer3", "ICLR.cc/2018/Conference/Paper365/AnonReviewer1", "ICLR.cc/2018/Conference/Paper365/AnonReviewer2"], "reply": {"forum": "rkxY-sl0W", "replyto": "rkxY-sl0W", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642439377}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642439434, "tcdate": 1511747881439, "number": 2, "cdate": 1511747881439, "id": "r1-4-eYlf", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Review", "forum": "rkxY-sl0W", "replyto": "rkxY-sl0W", "signatures": ["ICLR.cc/2018/Conference/Paper365/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Some interesting components, but overstates contribution", "rating": "4: Ok but not good enough - rejection", "review": "This paper aims to translate source code from one programming language to another using\na neural network architecture that maps trees to trees. The encoder uses an upward pass of\na Tree LSTM to compute embeddings for each subtree of the input, and then the decoder \nconstructs a tree top-down. As nodes are created in the decoder, a hidden state is passed\nfrom parents to children via an LSTM (one for left children, one for right children), and\nan attention mechanism allows nodes in the decoder to attend to subtrees in the encoder.\n\nExperimentally, the model is applied to two synthetic datasets, where programs in the \nsource domain are sampled from a PCFG and then translated to the target domain with a\nhand-coded translator. The model is then trained on these pairs. Results show that the\nnproposed approach outperforms sequence representations or serialized tree representations\nof inputs and outputs.\n\nPros:\n\n- Nice model which seems to perform well.\n\n- Reasonably clear explanation.\n\nA couple questions about the model:\n\n- the encoder uses only bottom-up information to determine embeddings of subtrees. I wonder \nif top-down information would create embeddings with more useful information for the attention\nin the decoder to pick up on.\n\n- I would be interested to know more details about how the hand-coded translator works. Does\nit work in a context-free, bottom-up fashion? That is, recursively translate two children nodes\nand then compute the translation of the parent as a function of the parent node and\ntranslations of the two children? If so, I wonder what is missing from the proposed model\nthat makes it unable to perfectly solve the first task?\n\nCons:\n\n- Only evaluated on synthetic programs, and PCFGs are known to generate unrealistic programs, \nso we can only draw limited conclusions from the results.\n\n- The paper overstates its novelty and doesn't properly deal with related work (see below)\n\nThe paper overstates its novelty and has done a poor job researching related work. \nStatements like \"We are the first to consider employing neural network approaches \ntowards tackling the problem [of translating between programming languages]\" are\nobviously not true (surely many people have *considered* it), and they're particularly\ngrating when the treatment of related  work is poor, as it is in this paper. For example, \nthere are several papers that frame the code migration problem as one of statistical \nmachine translation (see Sec 4.4 of [1] for a review and citations), but this paper \nmakes no reference to them. Further, [2] uses distributed representations for the purpose \nof code migration, which I would call a \"neural network approach,\" so there's not any \nsense that I can see in which this statement is true. The paper further says, \"To the best \nof our knowledge, this is the first tree-to-tree neural network architecture in the \nliterature.\" This is worded better, but it's definitely not the first tree-to-tree \nneural network. See, e.g., [3, 4, 5], one of which is cited, so I'm confused about \nthis claim.\n\nIn total, the model seems clean and somewhat novel, but it has only been tested on \nunrealistic synthetic data, the framing with respect to related work is poor, and the\ncontributions are overstated.\n\n\n[1] https://arxiv.org/abs/1709.06182\n[2]  Trong Duc Nguyen, Anh Tuan Nguyen, and Tien N Nguyen. 2016b. Mapping API elements for code migration with\nvector representations. In Proceedings of the International Conference on Software Engineering (ICSE).\n[3] Socher, Richard, et al. \"Semi-supervised recursive autoencoders for predicting sentiment distributions.\" Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2011.\n[4] https://arxiv.org/abs/1703.01925\n[5] Parisotto, Emilio, et al. \"Neuro-symbolic program synthesis.\" arXiv preprint arXiv:1611.01855 (2016).\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642439377, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper365/AnonReviewer3", "ICLR.cc/2018/Conference/Paper365/AnonReviewer1", "ICLR.cc/2018/Conference/Paper365/AnonReviewer2"], "reply": {"forum": "rkxY-sl0W", "replyto": "rkxY-sl0W", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642439377}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642439393, "tcdate": 1511809023780, "number": 3, "cdate": 1511809023780, "id": "Bk_WgJqgM", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Review", "forum": "rkxY-sl0W", "replyto": "rkxY-sl0W", "signatures": ["ICLR.cc/2018/Conference/Paper365/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "New but trivial model, poor experiments", "rating": "4: Ok but not good enough - rejection", "review": "Authors proposed a neural network based machine translation method between two programming languages. The model is based on both source/target syntax trees and performs an attentional encoder-decoder style network over the tree structure.\n\nThe new things in the paper are the task definition and using the tree-style network in both encoder and decoder. Although each structure of encoder/decoder/attention network is based on the application of some well-known components, unfortunately, the paper pays much space to describe them. On the other hand, the whole model structure looks to be easily generalized to other tree-to-tree tasks and might have some potential to contribute this kind of problems.\n\nIn experimental settings, there are many shortages of the description. First, it is unclear that what the linearization method of the syntax tree is, which could affect the final model accuracy. Second, it is also unclear what the method to generate train/dev/test data is. Are those generated completely randomly? If so, there could be many meaningless (e.g., inexecutable) programs in each dataset. What is the reasonableness of training such kind of data, or are they already avoided from the data? Third, the evaluation metrics \"token/program accuracy\" looks insufficient about measuring the correctness of the program because it has sensitivity about meaningless differences between identifier names and some local coding styles.\n\nAuthors also said that CoffeeScript has a succinct syntax and Javascript has a verbose one without any agreement about what the syntax complexity is. Since any CoffeeScript programs can be compiled into the corresponding Javascript programs, we should assume that CoffeeScript is the only subset of Javascript (without physical difference of syntax), and this translation task may never capture the whole tendency of Javascript. In addition, authors had generated the source CoffeeScript codes, which seems that this task is only one of \"synthetic\" task and no longer capture any real world's programs.\nIf authors were interested in the tendency of real program translation task, they should arrange the experiment by collecting parallel corpora between some unrelated programming languages using resources in the real world.\n\nGlobal attention mechanism looks somewhat not suitable for this task. Probably we can suppress the range of each attention by introducing some prior knowledge about syntax trees (e.g., only paying attention to the descendants in a specific subtree).\n\nSuggestion:\nAfter capturing the motivation of the task, I suspect that the traditional tree-to-tree (also X-to-tree) \"statistical\" machine translation methods still can also work correctly in this task. The traditional methods are basically based on the rule matching, which constructs a target tree by selecting source/target subtree pairs and arranging them according to the actual connections between each subtree in the source tree. This behavior might be suitable to transform syntax trees while keeping their whole structure, and also be able to treat the OOV (e.g., identifier names) problem by a trivial modification. Although it is not necessary, it would like to apply those methods to this task as another baseline if authors are interested in.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642439377, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper365/AnonReviewer3", "ICLR.cc/2018/Conference/Paper365/AnonReviewer1", "ICLR.cc/2018/Conference/Paper365/AnonReviewer2"], "reply": {"forum": "rkxY-sl0W", "replyto": "rkxY-sl0W", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642439377}}}, {"tddate": null, "ddate": null, "tmdate": 1515139010226, "tcdate": 1515139010226, "number": 5, "cdate": 1515139010226, "id": "r1Kak22Xz", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "forum": "rkxY-sl0W", "replyto": "B1LYcrRMz", "signatures": ["ICLR.cc/2018/Conference/Paper365/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper365/Authors"], "content": {"title": "We have added results of our model without the attention mechanism", "comment": "We have updated the revision to include results of our tree2tree model without the attention mechanism, and observe that the performance decreases significantly. In particular, the program accuracy drops to nearly 0%. More details can be found in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825734883, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkxY-sl0W", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper365/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper365/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper365/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers", "ICLR.cc/2018/Conference/Paper365/Authors", "ICLR.cc/2018/Conference/Paper365/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825734883}}}, {"tddate": null, "ddate": null, "tmdate": 1515138542240, "tcdate": 1515138542240, "number": 4, "cdate": 1515138542240, "id": "BJ8e0oh7M", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "forum": "rkxY-sl0W", "replyto": "rkxY-sl0W", "signatures": ["ICLR.cc/2018/Conference/Paper365/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper365/Authors"], "content": {"title": "A revision of the paper has been uploaded", "comment": "We have updated the paper with the following changes:\n\n(1) We include more discussion of related work in Section 5, especially addressing the relationship between our work with previous program translation work using statistical machine translation methods, and with tree-structured autoencoder work.\n(2) We provide more details about our experimental setup in Section 4, and include the implementation of the translator between two synthetic languages in the appendix.\n(3) We have included results of our tree2tree model without the attention mechanism, and observe that the performance degrades dramatically.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825734883, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkxY-sl0W", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper365/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper365/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper365/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers", "ICLR.cc/2018/Conference/Paper365/Authors", "ICLR.cc/2018/Conference/Paper365/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825734883}}}, {"tddate": null, "ddate": null, "tmdate": 1514196474526, "tcdate": 1514196474526, "number": 3, "cdate": 1514196474526, "id": "rkGWRHCMM", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "forum": "rkxY-sl0W", "replyto": "Bk_WgJqgM", "signatures": ["ICLR.cc/2018/Conference/Paper365/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper365/Authors"], "content": {"title": "Response and some clarifications", "comment": "Thank you for your valuable comments! We clarify some confusions below, and we would greatly appreciate it if the reviewer could provide more feedbacks based on our response.\n\nWe have updated our paper to provide more details about our experimental setup. We employ the S-expression to serialize the tree. For example, the parse tree of source program in Figure 1 (i.e., the parse tree of x = 1 if y == 0)\nis represented as\n\n(Block(If(Op===(Value(Identifier Literal(y))Value(Number Literal(0)))Block(Assign(Value(Identifier Literal(x))Value(Number Literal(1))))))\n\nThis is the de facto standard approach used in the literature such as [1] and [2]. To the best of our knowledge, we are not aware of more effective ways to encode a tree. We would greatly appreciate it if the reviewer could provide alternatives that have been examined in the literature, and we would be happy to try them out.\n\nAs described in Section 4.1, we use a pCFG to generate train/dev/test programs, while guaranteeing their lengths are equal to the value we specify. We think testing on randomly generated cases can effectively examine the correctness of the learned translator. Different from natural language translation, program translation task requires to handle all corner cases that may not be frequently seen in practice. Thus, using random test cases can effectively reach to all such corner cases, and we cannot agree with the reviewer that doing so is meaningless.\n\nOur program accuracy is an under-approximation of semantic equivalence, while token accuracy can provide a detailed measurement to understand an approach when the program accuracy is low. In this sense, these two metrics can capture some meaningful information about approaches\u2019 effectiveness. Note that verifying if two programs are semantic-equivalent definition is a turing-complete problem, thus all metrics have to be an approximation to some degree. We consider proposing a better metric as future work.\n\nThe reviewer comments on the syntax of CoffeeScript and JavaScript, and argues that CoffeeScript is a subset of JavaScript, on which we do not agree. The syntactical grammars of two languages do not imply their complexity class. Both of these two languages are Turing-complete, meaning any program in one language has a correspondence in another. Also, for both comments on the syntax and the complexity class, we do not see the direct implication on the later comments on that our synthetic task does not capture the real world programs. For the later comment, as we have explained above, our task is designed to capture different corner cases of a program translator, and we consider handling longer real-world examples as an important future direction.\n\nWe are happy to try some traditional statistical machine translation baselines. Thanks for the suggestion!\n\n[1] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton. Grammar as a foreign language. NIPS 2015.\n[2] Li Dong and Mirella Lapata. Language to logical form with neural attention. ACL 2016.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825734883, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkxY-sl0W", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper365/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper365/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper365/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers", "ICLR.cc/2018/Conference/Paper365/Authors", "ICLR.cc/2018/Conference/Paper365/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825734883}}}, {"tddate": null, "ddate": null, "tmdate": 1514196013652, "tcdate": 1514196013652, "number": 2, "cdate": 1514196013652, "id": "BkIEnSAff", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "forum": "rkxY-sl0W", "replyto": "r1-4-eYlf", "signatures": ["ICLR.cc/2018/Conference/Paper365/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper365/Authors"], "content": {"title": "Response and some clarifications", "comment": "Thank you for the valuable comments!\n\nThank you for pointing out these related work! We have revised our paper to carefully compare with more prior work. From a high level, the papers cited in Section 4.4 of [1] are not neural network models; [2] uses word2vec, which is simply to learn a lookup table. Therefore, we also do not consider [2] as a deep neural network approach; [3, 4] propose tree-structured autoencoder, which is a generative model, rather than a translation model. The key difference is that a translation model has access to the source tree, while a generative model does not. Therefore, we think it is fair to claim our work as \u201cthe first deep neural network approach for the tree-to-tree translation problem.\u201d\n\nIn addition, the reviewer mentions [5] as a tree-to-tree model, which is definitely not true. In fact, [5] is a sequence-to-tree model: the input of the model proposed in [5] is a sequence rather than a tree.\n\nWe clarify the questions below.\n\nWe use the bottom-up fashion to aggregate the information so that each tree node contains all information of its descendants. Propagating information from top to bottom does not match our intuition that the attention is allocated based on the sub-trees of the source tree.\n\nThe hand-coded translator is in a bottom-up fashion, but not context-free. To construct some parents, the translator may need to manipulate its two children. We have added the code of our translator for the synthetic task in the appendix.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825734883, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkxY-sl0W", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper365/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper365/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper365/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers", "ICLR.cc/2018/Conference/Paper365/Authors", "ICLR.cc/2018/Conference/Paper365/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825734883}}}, {"tddate": null, "ddate": null, "tmdate": 1514195582382, "tcdate": 1514195582382, "number": 1, "cdate": 1514195582382, "id": "B1LYcrRMz", "invitation": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "forum": "rkxY-sl0W", "replyto": "BkLxPNweG", "signatures": ["ICLR.cc/2018/Conference/Paper365/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper365/Authors"], "content": {"title": "Response", "comment": "Thank you for your valuable comments! We respond to the questions below.\n\nThe reviewer asks about the meaning of the two program translation tasks studied in our work. We have explained that this is a first step to understand the problem of using a deep neural network approach to solve the program translation problem, and we consider the more challenging task without aligned input-output pairs as an important future direction. Also, although we do not evaluate it in our work, we believe the study of tree-to-tree translation model may have applications to other tree-to-tree translation tasks.\n\nThe CoffeeScript-to-JavaScript compiler is available online, which is too complex to explain in the paper. We have added the code of our translator between two synthetic languages in the appendix.\n\nTo clarify some implementation details, our seq2seq model and seq2tree model faithfully implement [1] and [2]. In particular, we only use uni-directional LSTMs for the encoder, and the attention mechanism is the same as described in the original papers as well. We did grid search for hyper-parameters, and chose the best one based on their performance on the validation set.\n\nA program\u2019s length is defined to be the total number of tokens in the program, and is guaranteed to be equal to 20/50.\n\nWhen we train the model on shorter programs (e.g., programs of length 20), then evaluate on longer programs (e.g., programs of length 50), the test accuracy is 0 for all models, including our proposed tree-to-tree model and the baseline models. We consider solving the generalization issue as the next important problem that we want to address in the future. \n\nWe have clarified all above details in our revised version as well.\n\nWe are running more experiments to provide a complete ablation study to understand the effectiveness of attention. In some preliminary results, we observe that the performance decreases dramatically when attention is not used. We will update the results once we finish the experiments.\n\n[1] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton. Grammar as a foreign language. NIPS 2015.\n[2] Li Dong and Mirella Lapata. Language to logical form with neural attention. ACL 2016.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "pdf": "/pdf/67db57e7b4bece96eca53c4d589c7f85a8f7b862.pdf", "paperhash": "chen|treetotree_neural_networks_for_program_translation", "_bibtex": "@misc{\nchen2018treetotree,\ntitle={Tree-to-tree Neural Networks for Program Translation},\nauthor={Xinyun Chen and Chang Liu and Dawn Song},\nyear={2018},\nurl={https://openreview.net/forum?id=rkxY-sl0W},\n}", "keywords": [], "authors": ["Xinyun Chen", "Chang Liu", "Dawn Song"], "authorids": ["xinyun.chen@berkeley.edu", "liuchang@eecs.berkeley.edu", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825734883, "id": "ICLR.cc/2018/Conference/-/Paper365/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkxY-sl0W", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper365/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper365/Authors|ICLR.cc/2018/Conference/Paper365/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper365/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper365/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper365/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper365/Reviewers", "ICLR.cc/2018/Conference/Paper365/Authors", "ICLR.cc/2018/Conference/Paper365/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825734883}}}], "count": 11}