{"notes": [{"id": "ijJZbomCJIm", "original": "2Qa_Dg__3b1", "number": 2366, "cdate": 1601308260837, "ddate": null, "tcdate": 1601308260837, "tmdate": 1616049615338, "tddate": null, "forum": "ijJZbomCJIm", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "mLBsEEdiLQ", "original": null, "number": 1, "cdate": 1610040441964, "ddate": null, "tcdate": 1610040441964, "tmdate": 1610474043128, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The premise of the work is simple enough: investigate if networks that are trained with an adversarial objective end up being more suitable for transfer learning tasks, especially in the context of limited labeled data for the new domain. The work uncovers the fact that shape-biased representations are learned this way and this helps for the tasks they considered.\n\nThere was rather robust back and forth between the authors and the reviewers. The consensus is that this work has merit, has good quality experiments and investigates something with high potential impact (given the importance of transfer learning in general). I hope that most of the back and forth findings are incorporated in the final version of this work (especially the discussion and comparison with Shafahi et al., as well as all the nuances of the shape bias)."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040441950, "tmdate": 1610474043111, "id": "ICLR.cc/2021/Conference/Paper2366/-/Decision"}}}, {"id": "5uGD54LYltl", "original": null, "number": 2, "cdate": 1603891847616, "ddate": null, "tcdate": 1603891847616, "tmdate": 1606823930954, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Review", "content": {"title": "Review", "review": "1, Summary of contribution:\nThis paper claims that the pre-trained model trained adversarially can achieve better performance on transfer learning, and conducted extensive experiments on the efficacy of the adversely trained pre-trained models.\nAlso, the paper conducts an empirical analysis of the trained models and shows that the adversarially pre-trained models uses the shape of the images rather than the texture to classify the images. \nUsing the influence function (Koh 2017), the paper reveals that each influential image on the adversarially trained model is much more perceptively similar to its test example.\n \n\n2, Strengths and Weaknesses:\nThe paper is well-written and organized, and the experiments look fair and well support the claim.   The analysis is interesting and insightful. \nMeanwhile, the transfer is done to the domain of lower complexity, and some important comparative ideas are not extensively investigated. \n\n\n3, Recommendation:\nWhile the paper\u2019s empirical results are solid, there seems to be a substantial room left for comparative studies.  More ablation studies shall be done for other regularization methods.\nI believe that the paper is marginally above the acceptance threshold. \n\n4, Reasons for Recommendation:\nThe reader will benefit more from the paper if the authors can justify their use of adversarial training as the regularization in the pretraining process.  I believe that this research warrants some comparative study for dropout, weight decay, as well as random perturbations.  I think the paper can be more insightful if it shows whether the other classical regularization performs better or worse on transfer learning than the proposed approach. \n\n5,  Additional feedback:\nIn addition to the suggestions made in 4, I also believe that comparison shall be made against the model trained without pretraining. \n\n\\\n---Post rebuttal---\n\nThank you for the response, and thank you for checking the performance comparison against the white-noise perturbation. It would be interesting to see a future work involving means other than Adversarial training (e.g. including other simple mechanisms like weight decay and dropout) to help reduce the overfitting effects in the pretraining phase. I would like to keep my score as is. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098031, "tmdate": 1606915784341, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2366/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Review"}}}, {"id": "YBgAeVWzaRF", "original": null, "number": 1, "cdate": 1603270209059, "ddate": null, "tcdate": 1603270209059, "tmdate": 1606484871074, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Review", "content": {"title": "Review for \"Adversarially-Trained Deep Nets Transfer Better\"", "review": "### Contributions ###\n* The paper proposes that models that were adversarially trained transfer better to other datasets in that they increase _clean_ performance on this target dataset if there are only few labeled datapoints for the target task or only few training epochs are conducted\n* The authors test their hypothesis for ResNets pretrained on Imagenet in different threat models and transfer these models to 6 different target datasets. Generally, results provide sufficient evidence for the paper's main hypothesis (robust models transfer better)\n* Additional experiments provide evidence that better transferability of robust models is partly due to relying more on shape rather than texture cues. Moreover, an additional analysis using influence functions leads to the hypothesis that  robust neural networks might have learned to classify using example-based concept learning like in human beings.\n\n### Significance ###\nTransfer learning is a topic of high relevancy for practitioners since it can reduce both data label effort and training time. Improving upon the baseline of transferring models pretrained on Imagenet with a non-adversarial loss is thus a potential significant result. \nHowever, the paper's review of the transfer learning literature is superficial and misses some relevant related work such as \"Rethinking ImageNet Pre-training\" by He et al., ICCV 2019.  Additionally, Geirhos et al. (ICLR 2019) also showed that models pretrained on stylized ImageNet (and thus having a stronger shape bias) transfer better to object detection tasks. This should be mentioned in Section 5.  And more generally: if stronger transferability is mainly due to increased shape bias, wouldn't it make sense to pretrain explicitly for strong shape bias rather than achieving this indirectly via adversarial training as proposed in this paper?\nA more thorough review of the transfer learning literature and relating the obtained results to this would generally strengthen the paper.\n\n### Originality ###\nThe work is a purely empirical work studying the stated hypotheses, no novel methods are proposed. Originality can thus only come from the hypotheses.\nThe main hypothesis (robust models transfer better) was also proposed by Salman et al. (2020). However, this work should be seen as concurrent since it was released on arXiv only four months ago. \nThe main prior work is Shafahi et al. (ICLR 2020), which also studies transferring adversarially pretrained models to other tasks. However, their focus is on the robustness gains of transferred models rather than on the effect on clean performance.\nIn summary, I think the main hypothesis studied in the paper is original. However, it is also clearly only a relatively small incremental step beyond what Shafahi et al. 2020 did.\n\n### Clarity ###\nExperimental setup, training pipeline, and analysis are outlined clearly. Releasing code for finetuning and replicating the experiments would further strengthen reproducibility\n\n### Quality ###\nGenerally, the experiments are well conducted, covering a broad range of threat models, target datasets, training image and epochs regimes, and finetuning strategies. Additionally, Section 5 and 6 shed additional light onto why robust models might transfer better, and by this further strengthening the main message of the paper.\nOne shortcoming is that all target tasks are image classification tasks. Whether robust models also transfer better to task such object detection or semantic segmentation remains unclear. \n\n### Recommendation ###\nIn summary, I think the paper is a nice experimental study of a clearly stated hypothesis with potential practical impact. I thus lean towards acceptance, even though novelty is clearly borderline.\n\n### Final Recommendation after Author Response###\nThe authors have addressed several of my main concerns. It would have been helpful to study transferability to tasks beyond image recognition, but overall, I think the paper has been considerably improved. I increase my score to 7.\nTwo remarks regarding new content:\n * I find it misleading to denote PGD as a targeted adversary and additive Gaussian noise as a random adversary Section 7. \"Targeted\" usually refers to an adversary that aims at achieving a specific misclassification (target class). Gaussian noise is not really an adversary, rather a distortion/image corruption. I would recommend clarifying the naming to avoid confusion of readers.\n * Is there any particular reason to use PGD(3) in Table 1b for evaluation? Would the effect hold also against stronger attacks (more iterations etc.)?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098031, "tmdate": 1606915784341, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2366/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Review"}}}, {"id": "z4wC18pj3eD", "original": null, "number": 17, "cdate": 1606294072240, "ddate": null, "tcdate": 1606294072240, "tmdate": 1606294096576, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "gifQPfyWUFY", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Influence functions", "comment": "The effect of early stopping resulting in an implicit regularization/bias is well-known. Inclusion of such an additional change in the model would make it hard to isolate the effect of robust training on the induced prior.\n\nIt is unclear how to effectively control all the other hyperparameters to control only the accuracy and at the same time not bias the learnt representations and hence the induced prior. Further, even if this was possible somehow, if the difference in accuracy is small (close to 0), and the difference in top-k for a given k is say some x% (x will likely be smaller than the current top-k difference), we feel that qualitatively this singular experiment would not be a bigger evidence of the conclusion we arrive at by saying a relatively small difference of 5% in accuracy vs a such a stark difference in top-k influences especially for small k suggests that semantically more meaningful representations being built internally and is supportive of Engstrom et al's view. Recall that the representations we refer to above  are the frozen layers of the model which have been learnt on the source dataset and transferred to the target datasets.\n\nIf the reviewer still feels we should scale back our claims, we are open to considering suggestions about this. Our current conclusion regarding learning the concepts of image classes in neural networks is (quote from the paper):\n\n\".. the robust neural network has learned the image labels by creating strong associations to semantically-similar examples (akin to example based concept learning in human beings) in its internal representations. Thus, reinforcing the human\nprior bias hypothesis in robust representations observed by Engstrom et al. (2019).\"\n\nWould it please the reviewer if we change it to \"... creating stronger associations than non-robust neural network to semantically-similar examples ... Thus, supporting the human prior hypothesis ...\" ? We are also open to other suggestions the reviewer might have. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "lU6DaEkDOyN", "original": null, "number": 15, "cdate": 1606110311259, "ddate": null, "tcdate": 1606110311259, "tmdate": 1606157858831, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "75tO7y-1OpL", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Adding fairness as a future direction and emphasizing that we use the term \u201cnatural\u201d to be consistent with prior works in the introduction.", "comment": "We\u2019re glad that you feel that we\u2019ve addressed your concerns. Also, we\u2019re grateful for the additional context on face recognition. We\u2019ve made the changes described below.\n\n* We changed the wording and the format in the intro, as you suggested. \n* We added a fourth bullet point to the future work section discussing fairness, which is expanded in the appendix to mention face recognition.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "eCg39bSrGiS", "original": null, "number": 16, "cdate": 1606110350747, "ddate": null, "tcdate": 1606110350747, "tmdate": 1606157786270, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "Rmd2lKDRedo", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Shape bias does improve transferability, how to pretrain adversarially, and why our results are not the result of randomness ", "comment": "Thanks for the follow-up. Please see a response to your questions below:\n\n**Does shape bias improve transferability?**\n\nYes. We have evidence to support the fact that the model trained on both stylized imagenet and imagenet imagenet is slightly *more transferable than the natural model but less transferable than the adversarially trained one*, in the low-data regime. This is consistent with our previous comment, and thus we see evidence that shape bias helps improve transferability. \n\nBelow is a table with the mean accuracy for the natural, texture-robust, and adv-robust models, showing only the ones with statistically significant (95% confidence interval) differences between the natural and texture-robust models for 800 training images in the target dataset:\n\n|                 | Natural | Texture-Robust | Adv-Robust | \n|------------|:-------------:|:-------------:|:-------------:|\n| SVHN      | 49.2 | 55.6 | *71.2* |  \n| KMNIST  | 63.0 | 70.2 | *79.7* |  \n| MNIST    | 91.8 | 94.7 | *96.1* |  \n\n\n**Effect of epsilon on transferability vs other randomness in training** \nTable 7 in the appendix shows a larger set of models and constraint setups, and it consistently shows the effect of epsilon on transferability. Further, if the reviewer is interested in deeper study of the effect epsilon has transferability, please refer to Figure 5(b) [A]. All of these experiments show that better transferability is dependent on choosing a good epsilon (which is a hyperparameter, and hence can be different for different datasets), and not on just other random variations in the training process.\n\n[A] https://arxiv.org/pdf/2007.08489.pdf\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "0LZ0jeHFt1L", "original": null, "number": 13, "cdate": 1606110173444, "ddate": null, "tcdate": 1606110173444, "tmdate": 1606157752684, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Update", "comment": "We\u2019ve uploaded a new version where we added a new column for models trained from scratch on the target datasets to Table 7 of the appendix, as suggested by R4. Please note that the models trained from scratch do not freeze any convolutional blocks. Thus, it is a somewhat unfair comparison to the other four columns, which freeze all, but the three last fine-tuned convolutional blocks. \n\nSeparately, thanks to R2\u2019s suggestion, we transferred less sensitive models to texture to confirm further that shape bias is partially responsible for improving transferability. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "mP7RDMIgSq4", "original": null, "number": 3, "cdate": 1603930947585, "ddate": null, "tcdate": 1603930947585, "tmdate": 1606080870461, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Review", "content": {"title": "Nice and clear with interesting experiments, but lacking context, a bit over-claimed, and a few other potential issues I'm optimistic can be addressed", "review": "**Summary of paper:** This paper investigates how \"robust\" (adversarially trained) models can improve the transfer of representations, finding that they transfer better. Additionally, they investigate some reasons this could be the case, examining the biases robust models appear to induce.\n\n**Pros/strong points:**\n - interesting, well-explained experiments with mostly clear nice figures\n - nice extra investigations giving insight into the bias(s) conferred by adversarial training\n\n**Cons/weak points:**\n - overstatement as though the results apply to any type of data/model, but only image data and convolutional nets are tried\n - potential issue with influence function experiments\n - no analysis of computational cost of adversarial training or other information on tradeoffs\n - background lacking/potential issues with related work\n\n**Summary of review\u00a0+ recommendation:** Good paper with nice, thoughtful experiments, mostly well written, although I think the biggest issue there is over-claiming results applying to all data when only image data/convnets are studied. I'm also a bit worried about the thoroughness of the background research, and would like to see an analysis of the computational cost of adversarial training. If these and my question about influence function experiments are addressed, I would be happy to raise my score.\n\nDETAILED REVIEW:\n\n**Quality:**\n - Generally well written and organized, although the link between successive sections could be made a bit more strongly / flow could be better\n - Overstatement of scope (transfer learning in general, when only image data is studied) combined with misstatements in the\u00a0first parts (about the origins of transfer learning) makes me skeptical of the depth of background research done and makes me suspect there may be some very relevant things which have not been surveyed/cited.\u00a0 (see Specific questions/recommendations for details on both points, below) \u00a0\u00a0\n - Comparison of computational cost / other tradeoffs in adversarial vs. 'natural' training not discussed\n - Potential weakness/misleading conclusions in influence function experiments if I've understood correctly (see specifics below)\n\n**Clarity:**\n - The abstract and introduction would benefir from more technical, specific language to avoid ambiguity and establish flow of the sections\n - Clarity within sections is pretty good., some specific suggestions below.\n - Figures are mostly nice and clear, not so much Table 1 and 7a though.\n\n**Originality:**\n - This is the largest potential problem that I am most unsure about. I'm very familiar with work on statistical learning theory and generalization overall, but I'm not an expert in transfer learning or adversarial method and I'm not sure how well these works are reviewed, so I'm not sure how novel this work is. The experiments are well done and well explained though, and I think this is a good contribution even if it is less original than it is made to seem due to the lack of context.\n\n**Significance:**\n - Nice insights and interesting experiments for those wanting to understand the impact of robust training on transfer\n - Limited practical insights without an analysis/discussion of tradeoffs e.g. overall computational time and stability.\n - Directions proposed for future work are concrete and interesting\n\n\n**Specific questions/recommendations:**\n - the first sentence of abstract and the title talk about DL generically, but the second-sentence is about images specifically. If you add non-image data, I'd suggest rephrasing to make the 2nd sentence generic, and maybe mention this technique has been particularly successful for images. However since all experiments are with images, I'd suggest making the title and abstract specific to that domain. e.g. \"Evidence from image data that adversarially-trained deep nets transfer better\" or \"Adversarially-trained deep nets transfer to new images better\"\u00a0\u00a0\n - Overstatement of results: If you want the strong/general claims about transfer learning, I would strongly recommend doing experiments with at least MLPs in addition to convnets, and at least one other type of data in addition to\u00a0 images. Otherwise the statements in title/abstract/intro should be circumscribed to more accurately reflect the nature of the experiments.\u00a0\u00a0\n - mention what is adversarially vs. naturally trained. although I strongly suggest using a different term than 'naturally', including in diagrams/elsewhere as it is confusing. It makes it seem like you are comparing adversarial training to natural gradient methods.\n - first sentence putting data hogs in quotes misleadingly suggests that it's a commonly used phrase. Suggest removing this i.e. \"they are known to require large...\" (and suggest adding a reference which quantifies this rather than referring to hearsay).\n - \"similarly, \"stunning\" is an opinion, suggest \"remarkable\" or something like \"excellent\" which can be derived from empirical results\u00a0-\u00a0Comparison of computational cost / other tradeoffs :even just a reference where this is done with a sentence summarizing those results / other tradeoffs, e.g. \"it's usually at least 2x as slow and more likely to be unstable\" or something like that might be enough, but I would prefer to see full training curves and computational cost numbers in appendix, with a line or two summarizing these in the main text.\n - Stating that Caruana (1995) proposed transfer learning seems incorrect to me. There was a NeurIPS workshop that same year on the subject, suggesting it was already an established term at that time. I don't know the full history, but from a quick google it seems to have been very common in psychology / education literature in the 70s and 80s, here's a book that talked about it in the context of ML in the 80s :https://pdfs.semanticscholar.org/b547/c5837bff9347dc76330a72fd7cbc517ee08c.pdf and here's Rich Sutton talking about it in 92:\u00a0https://link.springer.com/content/pdf/10.1007/978-1-4615-3618-5.pdf\n - Related work:\u00a0 \u00a0 - covariate shift could also mention risk extrapolation\u00a0https://arxiv.org/pdf/2003.00688.pdf (how does the extrapolation done there differ from the adversarial training?)\u00a0 \u00a0 - it seems like a lot of works on adversarial and contrastive training and the relationship to generalization are missing; I'm not an expert in this but starting way back hard negative mining and other contrastive methods (e.g. word2vec) have been used and their properties discussed\u00a0\n - The first subheading in section 4 is the conclusion drawn from that paragraph: \"Adversarially-trained models transfer better and faster.\", but subsequent headings do not have the same 'syntax' (they are more like titles than conclusions to be drawn). I like the conclusion-as-title format; I find it very engaging and helpful for skimming especially since there are many experiments. But most of all I would strongly suggest that all titles have the same 'syntax' i.e. if you can't think of conclusions-as-titles for the other bold p headers (although I think you can and should!), I would recommend rephrasing this one to be like the others (e.g. Comparison of adversarial and non-adversarial transfer)\n - formatting of table 1, with captions both below and above the tables, is hard to read. Put it all above or all below.\n - Could have been a nice opportunity to investigate whether robust models are more or less susceptible to the types of bias people worry about in real-world image datasets (e.g. face recognition); maybe worth mentioning this in future work\u00a0\n - Unless I'm mistaken, the experiments with influence functions do not distinguish the effect of performance from that of training (bias toward the \"human prior\"). To do so, the robust and natural methods would have to have the same accuracy (i.e. this might involve very early stopping of the robust method to match the natural model performance). Without this, the qualitative and quantitative results could just be due to the higher accuracy of the robust method,\u00a0not the particular form of prior it\u00a0induces.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098031, "tmdate": 1606915784341, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2366/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Review"}}}, {"id": "gifQPfyWUFY", "original": null, "number": 12, "cdate": 1606077579334, "ddate": null, "tcdate": 1606077579334, "tmdate": 1606077579334, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "mP7RDMIgSq4", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Last remaining issue: distinguishing effect of training from that of bias/prior in influence function experiments", "comment": "Thank you for quite thoroughly addressing concerns; I think the paper is looking good and I would recommend acceptance if this point is addressed.\n\nI appreciate the results of the analysis in Figure 7 and find them interesting, but I'm afraid I still don't understand how these results demonstrate what you claim - I don't understand why a relatively small difference in accuracy couldn't explain even a relatively large difference in top-k influential images. I don't see how this can be shown except by controlling for accuracy (and even doing so would not tell the full story, but it's a step at least). I also don't understand why early stopping would give an undesirable bias. I'm not suggesting to remove the existing experiments and replace them with early-stopped ones; the existing ones are certainly informative, but if you don't make some attempt to control for accuracy or compare models with similar accuracy, I think the claim about this should be dialed back."}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "75tO7y-1OpL", "original": null, "number": 11, "cdate": 1606076481513, "ddate": null, "tcdate": 1606076481513, "tmdate": 1606076481513, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "Gps-VzJyu4u", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Response to response", "comment": "* Thanks for the more specific name change; this addresses my concern about over-claiming.\n* I understand the motivation for using the term \"natural\" if that is what is used in previous work. In the sentence of the intro where it says  \"non-adversarially-trained (i.e., natural)\" I suggest instead saying \"non-adversarially-trained (henceforth denoted **natural** as in previous works e.g. \\cite{engstrom, ilyas, tspiras}\" to communicate that, or if space is a concern, simply bold the word \"natural\" (also suggest bolding the word \"robust\" where it is defined; this makes it easy to scan back to see what those terms mean if a reader forgets later).\n* The suggestion about facial recognition (or other) bias I was making was not in a prior work; similar to your experiments on imagenet, I was suggesting you could train a model to recognize faces or something both naturally and robustly and investigate whether / how the robust model is more or less biased toward certain categories (e.g. many works have shown that classifiers are poorer at identifying people of colour and women (http://gendershades.org/ has some associated papers); it would be interesting to know if adversarial training helps or even hurts - I can imagine that a shape bias would be helpful for recognizing faces regardless of colour, but perhaps if the shape bias is too 'specific' to male faces it could actually worsen performance on female or unusually-shaped faces."}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "ifIQu3KBAuI", "original": null, "number": 10, "cdate": 1606075602275, "ddate": null, "tcdate": 1606075602275, "tmdate": 1606075602275, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "Sc8uykGmWXF", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Appreciate name change, suggest \"in\" instead of \"on\"", "comment": "Whether for \"Adversarially-Trained Models Transfer Better in Image Classification\" or Adversarially-Trained Models Transfer Better in Image Recognition\", I think the \"in\" reads a bit better, but no big deal either way. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "Rmd2lKDRedo", "original": null, "number": 9, "cdate": 1605788082800, "ddate": null, "tcdate": 1605788082800, "tmdate": 1605788082800, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "0QpY0OKWinI", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "On shape bias and the choice of epsilon", "comment": "Thanks for the quick response. I understand that the paper is mainly about \"an unintended but positive side-effect of adversarial training\". But part of this also (a) what causes this side-effect (shape bias)? and (b) what follows from this observation?\nAnd I think there is still room for clarification: \n * is it actually increased shape bias that causes increased transferability? If a model trained on stylized ImageNet does not transfer as well, this seems questionable. \n * if transferability increases with smaller epsilon, but epsilon=0 (clean training) is also suboptimal, how should one pretrain models adversarially to ensure one benefits from increased transferability? At the moment, this benefit seems to be quite brittle and dependent on the model. Can the authors make a clear point that transferability is dependent on epsilon and not on randomness in training, that is: do models trained with the same epsilon transfer equally well?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "0QpY0OKWinI", "original": null, "number": 8, "cdate": 1605747307608, "ddate": null, "tcdate": 1605747307608, "tmdate": 1605747307608, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "KjSPomVvU5l", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Explanation for the contradiction with Shafahi et al. (2020) and transferring the stylized ImageNet model", "comment": "We appreciate the thoughtful response and that you recognize the novelty of some of our contributions. Additionally, we\u2019d like to highlight two additional novel contributions. First, inspired by R4, we added a new section studying if other adversarial attacks improve transferability (PGD(1) and random Gaussian noise). Second, we explain, at least partly, why robust models transfer better, with the use of influence functions and shape vs. texture bias.\n\n**What is the author's explanation for this disagreement with Shafahi et al. (2020)?**\n\nThe goal of Shafahi et al. (2020) is different from ours. They want to show that robust models are still resilient to adversarial attacks after fine-tuning (i.e., robustness transfers), that is why we believe that they chose larger perturbation bounds. In contrast, our goal is to show that robust models transfer better, which is why we chose smaller perturbation bounds. The different choice of perturbation bound size led to the disagreement, i.e., Shafahi et al. (2020) concluded that robust models transfer worse, while we conclude that robust models transfer better. \n\nWe have already clarified this discrepancy in the related works section: \u201cIt might seem to contradict our thesis that they also notice that an ImageNet robust model with a $\\|\\delta\\|_\\infty \\leq 5$ constraint has lower accuracy on the target datasets, CIFAR-10 and CIFAR-100, compared to a natural ImageNet model. However, we show that the robust model transfers better than the natural one when we use a $\\|\\delta\\|_2 \\leq 3$ constraint to adversarially train the source model.\u201d\n\nAnd later, in Figure 5(c), we provide evidence that source models trained with a smaller adversarial constraint outperform source models trained with a larger one: The source model trained with $\\|\\delta\\|_\\infty \\leq 4/255$ outperforms the one trained with $\\|\\delta\\|_\\infty \\leq 8/255$, on CIFAR-10 and CIFAR-100. \n\n**Did the authors consider transferring the model from Geirhos et al. trained on stylized ImageNet?**\n\nOur original motivation was to present and study an unintended but positive side-effect of adversarial training, not to perform an extensive search to find the best pre-training process for transfer learning. Motivated by your question, we ran this experiment, but preliminary results showed that the model trained on stylized ImageNet does not transfer as well as the robust model. These experimental results agree with Salman et al. (2020). However, if you think that this is an important addition, we\u2019re happy to add a sentence in the shape bias section, with detailed results in the appendix. What are your thoughts on this? \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "KjSPomVvU5l", "original": null, "number": 7, "cdate": 1605705988737, "ddate": null, "tcdate": 1605705988737, "tmdate": 1605705988737, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "S4a3XD-t6mV", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Further Clarification ", "comment": "Thanks for the feedback. I did acknowledge in my review that the author's study and findings are different to Shafahi et al. (ICLR 2020), so there is no strong disagreement here. However, I would maintain my stance that both works are closely related and the author's work builds upon Shafahi et al. (ICLR 2020) in studying transferability of adversarially trained model. The novelty lies in studying transferability to  novel tasks in terms of clean performance and in terms of number of labeled datapoints in target task. This makes it in my assessment an incremental, however sufficiently novel contribution.\n\nI would like to ask the authors two questions that would be helpful for the final review:\n * The authors state that their results contradict  Shafahi et al. (ICLR 2020): \"We show that robust models transfer better, while Shafahi et al. (ICLR 2020) show that robust models transfer worse\". What is the authors explanation for this disagreement? (if it is discussed in the paper, could the authors point me to the respective section?)\n * Repeating my question from the review: \"if stronger transferability is mainly due to increased shape bias, wouldn't it make sense to pretrain explicitly for strong shape bias rather than achieving this indirectly via adversarial training as proposed in this paper?\" To make this more specific: did the authors also consider evaluating the transfer properties of the model from Geirhos et al. \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\"that was trained on stylized ImageNet?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "Sc8uykGmWXF", "original": null, "number": 6, "cdate": 1605683318091, "ddate": null, "tcdate": 1605683318091, "tmdate": 1605683560871, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "General response", "comment": "We appreciate all the feedback from all the reviewers because it allows us to refine and improve our work. We\u2019ve added a new section to our paper, reworded our abstract and introduction, and are considering changing the title of our paper, as described below.\n\n**New Section 7: Do other adversarial attacks improve transferability?**\n* Added a new section to analyze the transferability of two new ImageNet models trained with PGD(1) and random Gaussian noise, as proposed by R4.\n* This Section becomes an additional contribution, addressing the concern for R2 on our work being more than an incremental step beyond Shafahi et al. (2020).\n* Added five related works in this section, addressing R3\u2019s concern about related works.\n\n**Re-wording of title, abstract and introduction to clarify scope.**\n\nWe\u2019re considering changing our title to \u201cAdversarially-Trained Models Transfer Better On Image Classification\u201d, since R1, R2, and R3 raised concerns related to our scope. We would sincerely appreciate feedback on this subject.\n\n**Work in progress.**\n\nWe are computing results for the non-transferred ResNet-50 models with randomly initialized weights on the target datasets, which will be included in Table 7 of the Appendix by Friday.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "Gps-VzJyu4u", "original": null, "number": 2, "cdate": 1605595854320, "ddate": null, "tcdate": 1605595854320, "tmdate": 1605636080485, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "mP7RDMIgSq4", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Clarifying scope to avoid overstating results, adding computational cost analysis, addressing accuracy differences in influence function experiments, and improving related works", "comment": "Thank you for such a detailed review and suggestions to improve our paper. We are incorporating several reviewer suggestions, as stated below.\n\n**Overstatement of results**\n* **Can you make the title and abstract more specific to image classification?** We will clarify in our abstract and introduction that our focus is only on image classification tasks and other transfer tasks are out of scope in this work. Moreover, we're also willing to change our title to \"Adversarially-Trained Models Transfer Better On Image Classification\" if our paper gets accepted. \n* **Can you perform experiments with MLPs if you want to make general claims on transfer learning?** By changing the abstract, as described above, we make it more precise that our focus is on state-of-the-art deep neural networks for image classification tasks. \n\n**Can you add an analysis of the computational cost of adversarial training?**\nYes. We will add a sentence on the computational cost trade-off in the main text and a new appendix section. \n\n**Influence function experiments**\n* **Are the results in the influence function experiments driven by the accuracy differences?** We address this question in our paper: \"This vast gap is not explainable solely from only ~5% difference in target test accuracy, shown in Table 7 in Appendix A.5.\"\n* **Can you control for accuracy between natural and robust models in the influence experiments with early stopping?** Early stopping would add another bias, making it more challenging to make a fair comparison. We feel that the stark quantitative difference in Figure 7, and top-1/top-3 statistics given only 5% difference in accuracy suffices to support the qualitative claim of human prior bias suggested by Engstrom et al. (2019).\n\n**Background lacking/potential issues with related work**\n* **On origins of transfer learning.** We will reword the sentence to avoid suggesting that Caruana proposed transfer learning, and add other relevant citations.\n* **Can covariate shift also mention risk extrapolation?** There are many ways to handle covariate shift, but we are mainly concerned with using transfer learning. Thus, even though risk extrapolation is one method to handle covariate shift, it's not related to transfer learning. Therefore, the relevance to our work  is unclear to us. \n* **Missing works on adversarial and contrastive training and the relationship to generalization** We could add many weakly related works on adversarial training, contrastive learning, and their relationship to generalization. They're quite interesting, and we're well aware of them, but we don't see a strong connection to our work. It does seem that you feel quite strongly about expanding the related works section. We have purposely included only closely related works so far. If you could provide us with specific references that you feel improve the exposition during the discussion period, we are happy to add and discuss these.  \n\n**Can you add reducing bias in real-world image datasets to future work?**\n\nWe looked into facial recognition bias, but we couldn't see a direct relation to this work. However, we sincerely appreciate the idea and plan to investigate it. Could you please propose a few works that you think are worthwhile considering? As we gain more clarity on this idea, we might add this to the future works section.\n\n**Can you use a word other than \"natural\"? It can be confusing with natural gradient methods.**\n\nWe will change \"naturally-trained\" to \"non-adversarially-trained\" both in the abstract and in the beginning of the introduction to avoid confusion. However, we feel strongly about using the term \"natural\" and \"naturally-trained\" throughout the paper to be consistent with the two most directly related works by Shafahi et al. (ICLR 2020) and Salman et al. (2020), as well with other Madry-PGD adversarial works such as Engstrom et al. (2019), Ilyas et al. (2019), and Tsipras (2018).\n\n**Other issues**\n* We plan on removing the \"data hog\" reference.\n* We plan on placing subtable captions on top for consistency.\n* We plan on changing the titles of multiple subsections to reflect takeaways/conclusions."}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "QRKkRC49kts", "original": null, "number": 5, "cdate": 1605596635911, "ddate": null, "tcdate": 1605596635911, "tmdate": 1605596635911, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "5uGD54LYltl", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Comparing our approach (PGD(20)) to other robust training approaches and adding results for models trained from scratch with random initialization", "comment": "Thank you for the positive feedback and constructive comments. We will add a new section to the main paper and add a column to Table 7 in the Appendix. Please see our comments below for more details.\n\n**Do source models trained with PGD(20) outperform other robust training approaches?**\n\nThis is an excellent question, and to answer this, we are adding a new Section to the updated manuscript. To answer this question, we trained two ImageNet models: One with random Gaussian noise and another one with PGD(1). Our results show that PGD(20) and PGD(1) are similar to each other, and significantly better than Gaussian perturbations. \n\n**Can you compare your results to the model trained from scratch?**\n\nYes, we will add a column to Table 7 in the appendix for reference but prefer to keep these results outside of the main paper. Here's our rationale: We know that both natural and robust transferred models will massively outperform models trained from scratch with random initialization, especially as the number of training images in the target dataset decreases. Showing this comparison in our figures in Section 4 would obscure our research's focus, which is not to show that transfer learning works, but that robust models transfer better than natural ones. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "S4a3XD-t6mV", "original": null, "number": 4, "cdate": 1605596502449, "ddate": null, "tcdate": 1605596502449, "tmdate": 1605596502449, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "YBgAeVWzaRF", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Making the case for more than an incremental step beyond Shafahi et al. 2020 and also clarifying our scope", "comment": "Thank you for the positive feedback and constructive comments.\n\n\n**Although the main hypothesis is original, it is only a small incremental step beyond Shafahi et al. 2020.**\n\nWe strongly disagree that the results in our paper can be seen as only a small incremental step. In fact, our results contradict Shafahi et al. (ICLR 2020) results. We show that robust models transfer better, while Shafahi et al. (ICLR 2020) show that robust models transfer worse: \"an ImageNet robust model with a \u2225\u03b4\u2225\u2082 \u2264 5 constraint has lower accuracy on the target datasets, CIFAR-10 and CIFAR-100, compared to a natural ImageNet model.\" \n\nWe hope the reviewer agrees that since our conclusion is diametrically opposite, it should be shared with the community. An additional contribution of our work is looking into the transfer learning behavior with less data, which was previously unexplored and quite valuable from a practical standpoint. Furthermore, we explain, at least partly, why robust models transfer better, with the use of influence functions and shape bias. And to the best of our knowledge, this approach to understanding the transfer performance of models is novel. \n\n**Not clear if robust models also transfer better on tasks different from image classification.**\n\nWe will clarify in our abstract and introduction that our focus is only on image classification tasks. Moreover, we're also willing to change our title to \"Adversarially-Trained Models Transfer Better On Image Classification\" if our paper gets accepted. Separately, we believe that this approach will also work on a broader range of tasks, such as image segmentation, which future works could explore. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "LMzjUSUDMqX", "original": null, "number": 3, "cdate": 1605596217080, "ddate": null, "tcdate": 1605596217080, "tmdate": 1605596217080, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "AfnWPt9tDU", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment", "content": {"title": "Explaining specific configurations, discussing shape bias experiment, and clarifying our scope", "comment": "Thank you for the positive feedback and constructive comments. We've addressed all of your concerns below.\n\n**Why do specific configurations work better than others?**\n\nWe address why the number of fine-tuned convolutional blocks and adversarial constraints affect the transferability of robust models. \n\n* Number of fine-tuned convolutional blocks: In Section 4, we say that: \"In particular, even though all other datasets transfer better when fine-tuning one or three blocks, it seems that models transfer better to CIFAR-10 and CIFAR-100 when fewer blocks are fine-tuned, as shown in Figure 4(b). This suggests that because these datasets are close to ImageNet, fine-tuning of early blocks is unnecessary.\" Yosinski et al., (2014) support the last statement: \"Transferability is negatively affected by \u2026 the specialization of higher layer neurons to their original task at the expense of performance on the target task\". We will rephrase this statement in the updated manuscript to make it more precise.\n\n* Adversarial constraints: In Section 4, we say that: \"... a larger perturbation would destroy low-level features, learned from ImageNet, which are useful to discriminate between labels in CIFAR-10 and CIFAR-100. Finally, for datasets that are most distinct from ImageNet (SVHN and KMNIST), we find that robustness yields the largest benefit to classification accuracy and learning speed, as seen in Figure 2(b) and Figure 3(b), respectively. These discrepancies are even more noticeable when smaller fractions of the target dataset are used.\"\n\nFrom our empirical results, we can draw similarities between transfer learning configurations and hyperparameter tuning. To a large extent, the correct configurations will depend on the situation, and it's often difficult to know which configuration will work best ahead of time. Thus, we explored the landscape of fine-tuning configurations to avoid incorrectly concluding that our method works just because a specific configuration has been chosen.\n\n**If the shape is more important than texture, can robust models outperform natural models?**\n\nWe address this question in Section 5, where we find that adversarially-trained models are less sensitive to texture variations. The setup that we use consists of training both a natural and a robust model on ImageNet-1K and testing on Stylized ImageNet before and after fine-tuning. This allows us to observe what happens when shape is more important than texture, which addresses the situation described in your question. Our results for this experiment show that the robust model significantly outperforms the natural one. Hence, robust models can outperform natural ones when shape is more important than texture.\n\n**Adding a deeper theoretical understanding of why robust models transfer better.**\n\nIn this work, we focus on conducting an empirical investigation of the largely unexplored phenomenon of robust models transferring better. To explain, at least partially, why this happens, we also study the effect of shape bias and influence functions (Sections 5 and 6). However, we agree that a more theoretical understanding of why robust models transfer better is useful, and we hope that our work motivates future theoretical work.\n\n**Can you use target datasets with different and non-overlapping classes relative to ImageNet?**\n\nBesides CIFAR-100 and CIFAR-10, we use two non-digit target datasets with non-overlapping classes: Fashion-MNIST (i.e., FMNIST) and Kuzushiji-MNIST (i.e., KMNIST). FMNIST and KMNIST have clothing and cursive Japanese character classes, which are not contained in ImageNet. \n\nSimilar to R2 and R4, we strongly believe that our experimental setup is thorough. This is because we use six target datasets (where four of them are non-digit and non-overlapping with the source dataset), which represent a wide variety of domains. R2 agrees, saying that \"Generally, the experiments are well conducted, covering a broad range of threat models, target datasets, training image and epochs regimes, and fine-tuning strategies\". Also, R4 also agrees, saying that \"the experiments look fair and well support the claim.\"\n\n**Clarifications**\n* Table 5 shows accuracy differences larger than ~5%: We will fix the link to the correct table, which should be Table 7. The accuracy on the target dataset is our frame of reference, not the accuracy on the source dataset.\n* We will fix the typo on page 3.\n\nFull citation: Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Neural Information Processing Systems (NeurIPS), 2014.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ijJZbomCJIm", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2366/Authors|ICLR.cc/2021/Conference/Paper2366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849250, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Comment"}}}, {"id": "AfnWPt9tDU", "original": null, "number": 4, "cdate": 1604186779228, "ddate": null, "tcdate": 1604186779228, "tmdate": 1605024227878, "tddate": null, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "invitation": "ICLR.cc/2021/Conference/Paper2366/-/Official_Review", "content": {"title": "Good paper to understand that robust models transfer better, but can be better.", "review": "This paper tries to investigate and understand if and how adversarial training helps the models trained on the source domain transfer easier and faster to target domains. With extensive different configurations (such as fine-tuning strategies) in experiments, the authors show that robust models transfer better than natural models with less training data from the target domain. Also they demonstrate the intuition behind through experiments, such as capturing shapes than textures or using influence functions. \n\nStrengths\n- The idea is interesting and have a potential for impacts in the community.\n- Extensive experiments and investigations how and when the robust models works better than natural models is good to demonstrate the main ideas of this paper.\n- Paper is easy to understand.\n\nWeaknesses\n- Even though it was shown by the experiments, it might need to have more theoretical understanding why the robust models transfer better or have better representations than natural models. \n- Even though it seems to provide some explanations, it lacks more thorough investigation why the specific configuration choices yield better performances than others.\n- The presented dataset choices seem limited, which could limit its potential impacts and applications in real-world problems (see the comments below).\n\nDetailed comments:\n- If the shape is indeed more important than texture for human-like performance, is it possible make the model even works on par with the natural models? \n- Why specific configuration works better than others, such as fine-tuning three conv. blocks and \u2225\u03b4\u22252 \u2264 3?\n- In CIFAR-100 and (especially) CIFAR-10, fine-tuning one conv. block is better than zero conv. block and why?\n- The target domains except CIFAR-100 and CIFAR-10 are all digit datasets, so its application to real-world problems may be limited. How about using different and non-overlapping classes than those in the source domain in other image datasets as target domains, such as CALTECH-256? It could make the paper stronger.\n- In Table 5, the accuracy differences seem larger than ~5% as written in the text.\n\nTypo:\nPage 3: nx\u2018on-negative -> non-negative", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2366/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2366/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification", "authorids": ["~Francisco_Utrera1", "kravitz@berkeley.edu", "~N._Benjamin_Erichson1", "~Rajiv_Khanna1", "~Michael_W._Mahoney1"], "authors": ["Francisco Utrera", "Evan Kravitz", "N. Benjamin Erichson", "Rajiv Khanna", "Michael W. Mahoney"], "keywords": ["transfer learning", "adversarial training", "influence functions", "limited data"], "abstract": "Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.", "one-sentence_summary": "We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "utrera|adversariallytrained_deep_nets_transfer_better_illustration_on_image_classification", "supplementary_material": "/attachment/e46803e040306aa3819f36dfd6d59f5d2dcea6ed.zip", "pdf": "/pdf/566e8902b7a749d4525ff5f0933ffdae3a9bec39.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nutrera2021adversariallytrained,\ntitle={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},\nauthor={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ijJZbomCJIm}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ijJZbomCJIm", "replyto": "ijJZbomCJIm", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2366/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098031, "tmdate": 1606915784341, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2366/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2366/-/Official_Review"}}}], "count": 21}