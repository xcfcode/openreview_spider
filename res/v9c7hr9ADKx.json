{"notes": [{"id": "v9c7hr9ADKx", "original": "WT77qR2YNFg", "number": 298, "cdate": 1601308040955, "ddate": null, "tcdate": 1601308040955, "tmdate": 1612693480459, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Ug2QHw3gcJ", "original": null, "number": 1, "cdate": 1610040469208, "ddate": null, "tcdate": 1610040469208, "tmdate": 1610474073072, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "Reviewers all agree on acceptance for this paper. The initial issues with clarity seem to have been addressed by the authors.\n\nThe paper introduces a new transformer-based architecture for MARL that enables variable input and output sizes, which is used to train the agent in a more general setting and on more diverse tasks for multi-task training. The method also produces more interpretable agents.\nThe paper shows results on the Starcraft multi-agent challenge (not the full game of Starcraft, but still a recognised and widely used multi-agent benchmark). The method produces solid results both in terms of final training performance and zero-shot generalisation.\n\nAlthough reviewers are generally supportive of this paper, they mention that the Starcraft challenge used is somewhat simple (only few units used), and that the transformer-based architecture may not be applied to domain which lack the proper structure. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040469194, "tmdate": 1610474073056, "id": "ICLR.cc/2021/Conference/Paper298/-/Decision"}}}, {"id": "wZpIlaSEAGi", "original": null, "number": 1, "cdate": 1603764204186, "ddate": null, "tcdate": 1603764204186, "tmdate": 1606766981622, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Official_Review", "content": {"title": "A promising transformer-based MARL architecture with convincing results on SMAC. Clarity and readability need to be improved.", "review": "\n### Summary and claims of the paper\n\nThe authors propose several Transformer-based architectures that can be used in combination with a MARL (multi-agent reinforcement learning) algorithm to tackle multi-agent environments. They identify a particularly suitable architecture they call UPDeT which they claim combines the following advantages:\n 1) Can be used in the multi-task setting, where the number of entities and therefore the inputs changes from one task to the next.\n 2) Can handle a varying number of actions per entity (e.g. the controlled units could have N actions and each enemy unit corresponds to 1 action (which one to target), allied units correspond to no actions).\n 3) Is more explainable because the attention weights of the transformer can be inspected to see which information a given output depends on.\n 4) Outperforms the standard RNN approach on specific tasks.\n 5) Contrary to the standard RNN approach, the architecture is able to zero-shot generalize to tasks with different numbers of entities and learns significantly faster than the RNN architecture when fine-tuning on a new task. (This is in my opinion the most important and interesting claim).\n\nI have collected these claims from different parts of the paper and had to add some of my own interpretation to disambiguate what was meant. Please correct me if I misunderstood any of the claims. It would be nice if all of these were stated clearly in one place in the paper.\n\nPoints 1-3 can be argued based on the design of the architecture alone, while points 4) and 5) are supported by experiments using the SMAC (StarCraft Multi-Agent Challenge) environment.\n\nA summary of UPDeT from my understanding:\nIn MARL a group of independently acting agents needs to be trained to maximize a reward. Each agent has a separate set of Q values for each action that it can issue at a given time. In UPDeT, the Q values for one agent are computed by applying a transformer to all of the entities that the given agent can see. The resulting transformer output is then mapped into Q values depending on the positions in the transformer output. E.g. in the case of SMAC the position of the currently controlled unit corresponds to many actions, like moving left/right/up/down. The position of enemy units corresponds to 1 action each (targeting that unit) and the position of allied units (also controlled by the network, but not by this application of the transformer) corresponds to 0 actions. The mapping from transformer output to N actions is handled through a linear layer that is shared between entity groups (e.g. outputs for all enemy units get transformed with the same linear layer). A recurrent state through time can be added either globally for the whole transformer, or added to the individual units.\n\n### Discussion of prior work\n\nThe section on prior work is split between references to transformer and MARL literature.\nFrom what I can tell it is complete enough to position the paper sufficiently in the literature.\nThe one thing that I think is missing is a reference to the AlphaStar paper (https://www.nature.com/articles/s41586-019-1724-z) since this is an example of an agent that uses a transformer and is also applied to StarCraft.\nIn contrast to this work it doesn't use a MARL algorithm to control multiple units.\n\n### Are the claims supported?\n\nClaims 1) and 2) are supported directly by the design of the architecture.\nI struggled to understand the details of the architecture for some time. In my opinion this part of the paper could really benefit from a clearer presentation of the architecture (more details and some suggestions for improvement are in the next section).\n\nI'm somewhat unsatisfied with how claim 3) is handled in the paper, because the authors don't include any demonstration of explainability (e.g. a particular instance where the attention weights make it clear how a certain decision is made). Simply claiming that the architecture is explainable because a transformer is used is not enough, because in my experience there can be plenty of instances where we can't interpret how a given decision was made through the attention weights. It would be good if the authors would either give an interpretable example, or soften their claims of explainability and remove it from the main set of claims.\n\nClaims 4) and 5) are handled pretty well in my opinion.\nIn my experience the SMAC challenges are decently complex and allow meaningful differences in performance between algorithms to be measured.\nThe results in figure 4 support the design decisions in the architecture and show that the proposed architecture outperforms the standard RNN approach.\nThe only thing I'm unsure about here is the \"vanilla transformer\" architecture, which doesn't seem to be working at all. Is it possible that the transformer output is not conditioned on the different action types? E.g. this could be achieved by adding a positional encoding to the transformer corresponding to the different actions that can be issued. In any case such an architecture doesn't make a lot of sense since the actions are not related to the entities processed by the transformer. Maybe the aggregation transformer should be the \"vanilla\" version and the vanilla version can just be removed to make the paper simpler?\n\nClaim 5) is supported by a separate set of experiments where the agents are trained with a given number of units, and then switched to a task with a different number of units. UPDeT still performs reasonably well after the switch (zero-shot generalization) and quickly recovers to full performance during fine-tuning. I found this result genuinely interesting and I like how it is presented in the paper, e.g. by including the from-scratch curves as well.\nAs pointed out in the paper, the GRU baseline had to have some of its weights reset when switching from one task to another due to the changed action space. I wonder what would happen if a similar technique to UPDeT was used on the GRU, e.g. by emitting actions from different action heads depending on the identity of a unit as the GRU is unrolled (ideally it should be bidirectional). That way the resetting could be avoided and a fairer comparison between RNN and transformer could be made. This is not a crucial addition to the paper, but could be interesting.\n\n### Presentation and clarity\n\nI think the presentation and clarity of the paper should be improved significantly.\nThere are currently many typos, grammatical mistakes and missing words that made it hard for me to understand the paper.\nThe sections 3.2, 3.3 and 3.4 in particular are really crucial for understanding the paper and have some mistakes that confused me significantly.\nFor example in the bottom of formula (5) the attention is applied to three identical variables $R_i^l$ (this is done in other places in the paper as well, I'm not sure how to interpret it).\nIn Figure 3 the embedding outputs are labeled $e_{B,1}\\cdots o_{B,k}$, but I think this should be $e_{B,1}\\cdots e_{B,k}$.\nI found Figures 2 and 3 very difficult to understand, maybe because they show a very general version of the idea rather than the specific one that was tested by the authors.\nThe most confusing part of Figure 2 for me was the alternate white/colored filling scheme on the various tensors. What does it mean for a position to be colorless or colored here?\nI found Figure 6 in the appendix extremely useful for understanding the architecture. Maybe this should be part of the main text?\nI can appreciate that the authors want to present a general view of their idea (a \"framework\" rather than a specific architecture), but personally I would have understood the paper faster if it would focus on the specific case used for SMAC.\nThere are some references to \"training speed\" in the paper. It took me some time to figure out that this refers to the number of update steps rather than the actual speed of training the network. It would be good to clarify that.\n\nOverall I think the impact of the paper could really benefit from improving presentation and clarity.\nI have not taken this aspect into account in my rating.\n\n### Conclusion\n\nPros:\n - Benefit of using a transformer in a MARL setup is demonstrated in a non-trivial environment.\n - The proposed architecture goes beyond \"obvious\" approaches like the pooling the output of the transformer and the benefit of this is demonstrated.\n\nCons:\n- The paper is currently difficult to understand in several important places.\n- I think the use of transformers as presented here is only valid for environments with structured observations, e.g. where actions map nicely to specific entities in the observation.\n\nOverall I think the paper presents a promising approach to building MARL architectures. The approach makes sense and the experiments are insightful, but the presentation of the paper needs to be improved.\n\n*Edits after author comments and revision:*\nThe authors have greatly improved the readability of the paper. I also appreciate the addition of section 4.4. which seems like a reasonable attempt at supporting the increased interpretability of the architecture. I feel that the paper has improved, but it wasn't quite enough for me to raise the rating from 7 to 8, so I'm leaving it at \"Good paper\".", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper298/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper298/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146272, "tmdate": 1606915791840, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper298/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper298/-/Official_Review"}}}, {"id": "pP5ISHhmCuu", "original": null, "number": 3, "cdate": 1604101782589, "ddate": null, "tcdate": 1604101782589, "tmdate": 1606096461970, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Official_Review", "content": {"title": "A novel work, but lack of strong experiments", "review": "1. In this paper the authors proposed a transferrable framework for multi-agent RL, which enables the learned policies easily generalize to more challenging scenarios. This seems to be a good contribution to the community of multi-agent RL. It bears a potential to handle large-scale tasks with only limited training data, while also demonstrates more explanable policies.\n2. However, the experiments seem to be insufficient. The authors only investigate scenarios for 3 vs. 3, 5 vs. 7, which are still the easiest cases in the StarCraft II combat tasks. I suggest the authors to try more on 20 vs. 30 StarCraft combat task or more challenging scenarios, or the hundres or thousands levels of multi-agent tasks like that provided by MAgent environment[1]. And a comprehensive comparison with a similar work [2] following the curriculum learning pipeline is also worth a trial. This will make this work a strong one.\n3. A more profound analysis is needed for the experiment part. Besides the performance gains, insightful understanding of how the designed model works is also necessary.\n\n[1] Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, Jun Wang. Mean Field Multi-Agent Reinforcement Learning. ICML 2018.\n[2] Kun Shao, Yuanheng Zhu, Dongbin Zhao. StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning. IEEE Transactions on Emerging Topics in Computational Intelligence, 2018.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper298/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper298/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146272, "tmdate": 1606915791840, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper298/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper298/-/Official_Review"}}}, {"id": "pwq0A6LlcBY", "original": null, "number": 6, "cdate": 1605867017040, "ddate": null, "tcdate": 1605867017040, "tmdate": 1605867017040, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "vdEi2TVEor", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment", "content": {"title": "The revisions make this submission a stronger one", "comment": "Based on the supplementary experiments&analysis provided by the authors, I feel the submission now is a stronger one. The proposed framework does bear its superiority and can be inspiring for future research. In the new experiments, the framework also demonstrates its advantage over related works. Hence I decide to promote my rating to 6. I suggest the authors have a full typo-fixing over the paper to avoid readibility concerns."}, "signatures": ["ICLR.cc/2021/Conference/Paper298/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v9c7hr9ADKx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper298/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper298/Authors|ICLR.cc/2021/Conference/Paper298/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872522, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment"}}}, {"id": "lZdIJE70RUD", "original": null, "number": 5, "cdate": 1605842255453, "ddate": null, "tcdate": 1605842255453, "tmdate": 1605842255453, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment", "content": {"title": "General Response to Reviewers' Comments", "comment": "We thank all reviewers for their thoughtful feedback. We have posted a revision incorporating these feedback. \n\nThe first important revised part of the paper is in Section 4.4: \u201cAttention Based Strategy: An Analysis\u201d, which describes the explainity of UPDeT using a real case from SMAC.\n\nThe second main revised part is in Appendix E: \u201cResults of Extensive Experiment on Large Scale\u201d, which guarantees the robustness of UPDeT in large-scale scenarios.\n\nBesides, we have invited a native speaker to proofread this paper and corrected all the typos and grammar errors. Formulation [5][7][8] in Section 3 have been rewritten. Other details and discussions can be found in each of our response.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper298/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v9c7hr9ADKx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper298/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper298/Authors|ICLR.cc/2021/Conference/Paper298/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872522, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment"}}}, {"id": "LSOTd-6s4ty", "original": null, "number": 4, "cdate": 1605619806111, "ddate": null, "tcdate": 1605619806111, "tmdate": 1605619806111, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "wZpIlaSEAGi", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment", "content": {"title": "Response to AnonReviewer1:", "comment": "Thanks for your explicit and helpful comments on almost every aspect of our paper. Here, we address your 3 main concerns.\n1. Point 3 of 5 claims is not satisfied and well explained. \nWe admit that the claim point 3 is not handled very well in our paper. Therefore, we have extended the paper to 9 pages and discussed the attention based policy generation process in Section 4.4, including a complete analysis on 3 marines vs 3 marines demo game using UPDeT as backbone. \n2. Is \u201cvanilla transformer\u201d necessary to the paper ?  \nWe agree with you that aggregation transformer can serve as a better baseline compared to vanilla transformer. The only weakness of aggregation transformer is that it can not deal with multi-task setting. Once the embeddings are aggregated, we can not recover the specific entity number. However, we admit that the vanilla transformer needs to be revised for a stronger baseline. We agree with you that this could be achieved by adding a positional encoding to the transformer corresponding to the different actions that can be issued. We will make further improvement on that. Thanks for your constructive advice.\n3. Bi-directional GRU may be another choice. \nThank you for this insightful comment. We believe a bi-directional GRU can be a strong competitor against UPDeT as both two architectures perform well in many sequence to sequence tasks. While one limitation of Bi-GRU is obvious: it requires more time to inference as the GRU is serial while the Transformer is parallel.  We are now working on this and will provide a Bi-GRU based \u201cUPDeG\u201d in the future. Many thanks for your suggestion on this point.\n\nFurthermore, we correct the typos, grammatical mistakes and missing words as much as we can and revise the confusing formula (5)(7)(8). We have added AlphaStar paper to the paper reference. It is true that we want to present a more general framework rather than a specific architecture which makes Fig 2 & 3 a bit hard to understand. In figure 2, we have adopted the alternate white/colored filling scheme to indicate that observations are from different entities and should be treated separately. In figure 3, we have corrected the wrong typos. To help better understand the model architecture, we have added a figure reference for figure 7 (a real case of UPDeT) in the figure 3 captions. Besides, we have clarified that the training speed we mentioned in this paper refers to the total steps cost from step zero to convergence. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper298/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v9c7hr9ADKx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper298/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper298/Authors|ICLR.cc/2021/Conference/Paper298/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872522, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment"}}}, {"id": "oEx3PEM3KJI", "original": null, "number": 3, "cdate": 1605619578229, "ddate": null, "tcdate": 1605619578229, "tmdate": 1605619578229, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "NgPR_0a3iAy", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment", "content": {"title": "Response to AnonReviewer2:", "comment": "Thank you for your recognition of our work. Upon acceptance, we will release our code to benefit the research community and expand the coverage area of our paper to make it more practical and easy to understand.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper298/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v9c7hr9ADKx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper298/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper298/Authors|ICLR.cc/2021/Conference/Paper298/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872522, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment"}}}, {"id": "vdEi2TVEor", "original": null, "number": 2, "cdate": 1605619529916, "ddate": null, "tcdate": 1605619529916, "tmdate": 1605619529916, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "pP5ISHhmCuu", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment", "content": {"title": "Response to AnonReviewer4:", "comment": "Thanks for your comments. Below we will mainly address your concerns on point 2 and point 3.\n\nPoint 2 Comment 1: Only easiest scenarios like 3 vs 3 5 vs 5 are tested.\nReply 1: The reviewer might have overlooked that we have investigated the model performance on both easy (3v3 8v8) and hard/hard+ (4v5 5v6) scenarios. The related experiment results can be found in Fig 4 (d)(e). In hard+ scenarios, RNN based architecture can only get less than 50% winning rate while UPDeT gets significant improvement on winning rate performance. Therefore, the level of \u2018easy\u2019 or \u2018hard\u2019 does not depend on agent number but the final winning rate. However, we fully understand that large-scale scenarios are also important to model robustness. We therefore provide 10m_vs_11m and 20m_vs_21m experiment results from SMAC in the Fig.8 (a) at Appendix E. We are not able to do very large scale (>30) experiments on SMAC due to rebuttal time and resource limitations. \n\nPoint 2 Comment 2: MAgent environment is another benchmark on MARL.\nReply 2: MAgent by GeekAI is another good MARL environment which has been employed in several papers. Though the setting of MAgent is different from SMAC, we have added additional experiments in Section 4.3 and the experiment results with discussion in section E of Appendix. We have also included the recommended references [1][2] in the updated version.\n\n\nPoint 2 Comment 3: Comprehensive comparison with a similar work [2] following the curriculum learning pipeline is also worth a trial.\nReply 3: Thanks for pointing this out. If there is a work similar to ours, [2] is the earliest one. And lately, there is another related work [3] which also adopts a curriculum style to transfer the knowledge from few agent tasks to more agent tasks. However there are two significant weaknesses of these approaches compared to UPDeT: First, UPDeT has no need to be trained in \u2018curriculum style\u2019 like [2] and [3] did. As mentioned by AnonReviewer 1, our proposed model shows great zero-shot generalization ability with no restrictions on \u2018few\u2019 to \u2018more\u2019 or \u2018easy\u2019 to \u2018hard\u2019 training pipeline. Second, both [2][3] are methods based on a fixed action number which means we need to re-initialize the output layer of the source model to fit the new task under SMAC. Therefore, the performance is far lower than UPDeT and nearly has no zero-shot generalization ability. Corresponding proof of this claim can be found in Fig.5 in [3] and Fig.9 in [2]. In their experiments, all the training curves of winning rate start at zero when transferred to new tasks. We compare these two related works in Section 1 to make UPDeT a stronger and more attractive framework to real world application. Thanks for your helpful advice.\n\n[3] Wang W, Yang T, Liu Y, et al. From Few to More: Large-Scale Dynamic Multiagent Curriculum Learning[C]//AAAI. 2020: 7293-7300.\n\nPoint 3 Comment: Insightful understanding behind performance gains is needed.\nReply: AnonReviewer 1 & 4 both point out that we should do additional analysis on experiment results to better understand the UPDeT. However, due to page number limitation, we haven\u2019t explicitly explained the explainity of UPDeT on decision making as claimed in point 3 section 3.3. We have now provided an analysis on self-attention based policy generation process in Section 4.4 at page 8 & 9.  We take 3 marines vs 3 marines as an example for simplicity and show 3 policy generation processes on different stages in one episode. "}, "signatures": ["ICLR.cc/2021/Conference/Paper298/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v9c7hr9ADKx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper298/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper298/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper298/Authors|ICLR.cc/2021/Conference/Paper298/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872522, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper298/-/Official_Comment"}}}, {"id": "NgPR_0a3iAy", "original": null, "number": 2, "cdate": 1603906356984, "ddate": null, "tcdate": 1603906356984, "tmdate": 1605024721159, "tddate": null, "forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "invitation": "ICLR.cc/2021/Conference/Paper298/-/Official_Review", "content": {"title": " This paper studies the interesting problem of universal multi-agent reinforcement learning for multiple tasks, utilizing a new transformer-based model approach. It has been demonstrated that the new approach proposed in this paper has significant advantages over the existing state-of-art methods in terms of final performance, training time, as well as transfer capability for multiple tasks.", "review": "In my opinion, this paper is a very good paper: novel in the approach, high impacts in both theoretic sense and practical sense, and well-written. The problem of universal multi-agent reinforcement learning for multiple tasks is very interesting and challenging, and the methodology proposed in this paper is inspiring and the demonstrated experimental results are very impressive. \n\nMain contributions of this paper are as follows, which are very significant and impressive:\n [1] The proposed UPDeT-based MARL framework outperforms RNN-based frameworks on state-of-the-art centralized functions by a large margin in terms of final performance.\n[2] The proposed model has strong transfer capability and can handle a number of different task at a time.\n[3] The proposed model accelerates the transfer learning speed so that it is about 10 times faster compared to RNN-based models in most scenarios.\n\nThe paper presentation is in good quality, the concepts and the methodologies were explained clearly. The provided experiments section is also convincing and somewhat extensive, described and presented very clearly, which supports the main claimed contribution very well.\n\nThe authors also point out some promising future research direction on top of this work, which is also helpful.\n\nOverall, I strongly support this paper to be accepted and published in ICLR. The experiment results are impressive, and the proposed methodologies are inspiring and novel. I believe that many people in the research community would find it valuable/inspiring and benefit from this paper.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper298/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper298/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "authorids": ["~Siyi_Hu1", "~Fengda_Zhu1", "~Xiaojun_Chang3", "~Xiaodan_Liang2"], "authors": ["Siyi Hu", "Fengda Zhu", "Xiaojun Chang", "Xiaodan Liang"], "keywords": ["Multi-agent Reinforcement Learning", "Transfer Learning"], "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hu|updet_universal_multiagent_rl_via_policy_decoupling_with_transformers", "pdf": "/pdf/1f24b0b3a09ad8484d3887053d6c4c6a87d96ba1.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhu2021updet,\ntitle={{\\{}UPD{\\}}eT: Universal Multi-agent {\\{}RL{\\}} via Policy Decoupling with Transformers},\nauthor={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v9c7hr9ADKx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "v9c7hr9ADKx", "replyto": "v9c7hr9ADKx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper298/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146272, "tmdate": 1606915791840, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper298/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper298/-/Official_Review"}}}], "count": 10}