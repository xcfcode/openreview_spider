{"notes": [{"id": "BylaUTNtPS", "original": "S1eP2CKvPS", "number": 577, "cdate": 1569439061156, "ddate": null, "tcdate": 1569439061156, "tmdate": 1577168244798, "tddate": null, "forum": "BylaUTNtPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DIe-_Bx2n6", "original": null, "number": 1, "cdate": 1576798700250, "ddate": null, "tcdate": 1576798700250, "tmdate": 1576800935663, "tddate": null, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Decision", "content": {"decision": "Reject", "comment": "This paper has, at its core, a potential for constituting a valuable contribution. However, there was a shared belief among reviewers (that I also share) that the paper still has much room for improvement in terms of presentation and justification of the claims. I hope that the authors will be able to address the feedback they received to make this submission get where it should be.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720496, "tmdate": 1576800271337, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper577/-/Decision"}}}, {"id": "SkxGMecLFB", "original": null, "number": 2, "cdate": 1571360777545, "ddate": null, "tcdate": 1571360777545, "tmdate": 1574437746736, "tddate": null, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper proposes a neural network architecture consisting of multiple independent recurrent modules that interact sparingly. These independent modules are not all used simultaneously, a subset of them is active at each time step. This subset of active modules is chosen through an attention mechanism. The idea behind this architecture is that it would allow the different modules to specialize in different mechanisms and that would allow compositionality. The empirical results suggest that the proposed approach is able to generalize better than traditional architectures (which all have the implicit assumption that all processes interact).\n\nThis paper is well-written and it provides a very thorough empirical analysis of the proposed idea. Because it is not in my area of expertise I\u2019m not confident that I can assess its novelty or its relationship to other existing approaches.\n\nIn terms of presentation, I recommend the authors to enlarge some of the figures in the paper (e.g., I can\u2019t read the small box in Figure 1) and to not use citations as nouns (e.g., \u201cThe mechanisms of this attention mechanism follow (Vaswani et al., 2017; Santoro et al., 2018), with the \u2026\u201d). I would also like to point out that although fairly different in how they tackle the problem, the work of Arjovsky et al. (2019) seems to be related to this one.\n\nThree questions I believe were not answered in the paper are:\n\n1) How is the performance related to the total number of subsystems (and the number of *active* ones). I can only see results related to that in Table 1, but the variation in the number of modules is pretty small (4-6). The results also don\u2019t give any indication whether we want to have more modules active at each time step, if there\u2019s a sweet spot, etc. It is said that the method seems to be robust to this choice but this claim is made because it performs similarly for the values 5 and 6 if I recall correctly.\n\n2) Is there any incentive in this architecture for a module to not simply \u201cgive up\u201d? I mean, the modules are not necessarily incentivized to be used as often as possible, so could it be the case that a module learns to set its weights to zero?\n\n3) Would it make sense to present baseline results for an architecture that uses attention? It seems to me that LSTM was often the baseline of choice but RIMs have two important components: multiple LSTMs and an attention mechanism. Could the attention mechanism be explaining some of the results we are seeing?\n\nFinally, despite the very long appendix, I feel there are important details missing with respect to the empirical setup, at least in the Atari experiments which I\u2019m more familiar with. Was stochasticity used, that is, sticky actions (Machado et al., 2018)? Moreover, for how long was PPO (and RIMs-PPO) trained in terms of number of frames? Finally, I\u2019d recommend the authors to include a table with the actual average (and standard deviation) performance in each Atari games. It is really hard to know how well a method is doing by just squinting at learning curves. It is hard to know if the results are significant without a notion of variance.\n\n\nReferences:\n\nMart\u00edn Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, David Lopez-Paz: Invariant Risk Minimization. CoRR abs/1907.02893 (2019)\n\nMarlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, Michael Bowling: Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. J. Artif. Intell. Res. 61: 523-562 (2018)\n\n\n------\n\n\n>>> Update after rebuttal: I stand by my score after the rebuttal. \n\nUnfortunately I'm not an expert in this area and I don't feel confident in having a very strong opinion about this paper, willing to fight for its acceptance. I also agree with concerns raised by other reviewers. As I stated in the discussion with the authors, the clarifications and additional experiment does improve the paper a bit.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575780511355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper577/Reviewers"], "noninvitees": [], "tcdate": 1570237750118, "tmdate": 1575780511369, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Review"}}}, {"id": "HJeCqprooH", "original": null, "number": 14, "cdate": 1573768597963, "ddate": null, "tcdate": 1573768597963, "tmdate": 1573768639811, "tddate": null, "forum": "BylaUTNtPS", "replyto": "H1eVR3Hsor", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "Update 2", "comment": "Anything else which we can do which can help the reviewer in improving the perception and understanding of the work ?"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "H1eVR3Hsor", "original": null, "number": 13, "cdate": 1573768396346, "ddate": null, "tcdate": 1573768396346, "tmdate": 1573768526149, "tddate": null, "forum": "BylaUTNtPS", "replyto": "BkxoZnrsjS", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "Update.", "comment": "Hello Reviewer, \n\nThanks for your reply. We agree with you. We apologize for not mentioning. Sticky actions were used.\nWe'll open source our code. \n\nThanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "BkxoZnrsjS", "original": null, "number": 12, "cdate": 1573768195196, "ddate": null, "tcdate": 1573768195196, "tmdate": 1573768195196, "tddate": null, "forum": "BylaUTNtPS", "replyto": "rkgM0skisH", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "Perception", "comment": "My questions have been addressed. Thanks for that. I am leaning towards accepting this paper and my score reflects that. With respect to additional details, you never answered my question of whether you used sticky actions or not in the Atari experiments. This is not going to change my assessment of the paper, but it is important for reproducibility and for understand the consequences of the presented results. Without sticky actions it is quite easy to memorize successful trajectories in Atari games.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "rkgM0skisH", "original": null, "number": 11, "cdate": 1573743561629, "ddate": null, "tcdate": 1573743561629, "tmdate": 1573743561629, "tddate": null, "forum": "BylaUTNtPS", "replyto": "SkxGMecLFB", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "More feedback ?", "comment": "Dear Reviewer, \n\nYour feedback has also helped in improving the presentation of the paper. We have done new experiments and analysis related to your questions. Since the review discussion period is going to end, we would appreciate any feedback that you might have. We would be happy to provide further revisions or experiments to address any remaining issues, and would appreciate a response from you on the points that we raised. \n\nThanks for your time. "}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "B1x4z2rcir", "original": null, "number": 10, "cdate": 1573702668165, "ddate": null, "tcdate": 1573702668165, "tmdate": 1573702668165, "tddate": null, "forum": "BylaUTNtPS", "replyto": "r1xY9hm9sB", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "Thanks for the explanation", "comment": "The authors have answered several of my comments. I do agree with some intuitions behind this approach. Since the entire field is searching for good philosophy and innovative approaches, trying to get inspirations from other domains such as the causal community is the natural thing to do. But I still think the algorithmic implementations do not necessarily  realize the high-level ideas. The \"gap\" comment still holds. Nonetheless, I adjusted the score to \"weak accept\". "}, "signatures": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "SyxeOPO8tr", "original": null, "number": 1, "cdate": 1571354472118, "ddate": null, "tcdate": 1571354472118, "tmdate": 1573702257066, "tddate": null, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "This work seems to propose an alternative to general RNN so that  the dynamics of sequential data can be better captured. The work is based on a hypothesis that a causal process can be modeled by \"independent\" modules and sparse interactions.\n \nThe paper is written in very fluent English, but the style is less technical. The impression is that the philosophical arguments and the machine learning realization have a gap in between. There is no much rigorous mathematical definition or derivation to back up the entire development. The mathematical symbols are a bit loosely defined. For example, it is unclear to the reviewer if h is a scalar or a vector.\n\nThe RIM idea seems to be derived from some ideas from the ``'causality community' . But the authors did not elaborate how significant will this structure change the state of art. In particular, reading the abstract or the introduction does not shed much light on what are the challenges of now the ML community is facing, and how this proposed RIM idea is going to help. This may have been obvious to the authors, but spelling them out may help the reviewer/readers to understand the contribution. The related work section helped a bit, but still unclear.\n\nThe reviewer feels that the paper stands at a high level in general, but lacks concrete examples/applications for general readers to appreciate the significance. Perhaps trying to re-organize this part could greatly help the readability.\n\nThe mathematical descriptions in 2.2, 2.3 and 2.4 are very hard to follow. There is no cost function (for training) around, but there are discussions of  'gradient'. Gradient of which function? ", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575780511355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper577/Reviewers"], "noninvitees": [], "tcdate": 1570237750118, "tmdate": 1575780511369, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Review"}}}, {"id": "SylHp2mciB", "original": null, "number": 8, "cdate": 1573694652568, "ddate": null, "tcdate": 1573694652568, "tmdate": 1573694984101, "tddate": null, "forum": "BylaUTNtPS", "replyto": "SkxGMecLFB", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "Updated Impression ?", "comment": "Hello,\n\nWe believe, we have addressed your concerns and clarified some of your points. Do you have an updated impression of our paper? Thanks for your consideration and time. Appreciate it. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "Byg1WAQcjB", "original": null, "number": 9, "cdate": 1573694967105, "ddate": null, "tcdate": 1573694967105, "tmdate": 1573694967105, "tddate": null, "forum": "BylaUTNtPS", "replyto": "B1lZO_ly9B", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "Updated Impression ?", "comment": "Hello,\n\nWe believe, we have addressed your concerns and clarified some of your points. Do you have an updated impression of our paper?  Should that not be the case, please do not hesitate to get in touch with us. Thanks for your consideration and time. Appreciate it. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "r1xY9hm9sB", "original": null, "number": 7, "cdate": 1573694609470, "ddate": null, "tcdate": 1573694609470, "tmdate": 1573694609470, "tddate": null, "forum": "BylaUTNtPS", "replyto": "SyxeOPO8tr", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "Updated Impression ?", "comment": "Hello, \n\nR1, We believe, we have addressed your concerns and clarified some of your points. \n\nWe hope to have changed your assessment of our work for the better; should that not be the case, please do not hesitate to get in touch with us.\n\nThanks for your time. "}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "SklPIVgOjB", "original": null, "number": 5, "cdate": 1573549135243, "ddate": null, "tcdate": 1573549135243, "tmdate": 1573549135243, "tddate": null, "forum": "BylaUTNtPS", "replyto": "SkxGMecLFB", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "R3 Response", "comment": "Thank you for your feedback, and we appreciate your comment that \u201cthis paper is well-written and it provides a very thorough empirical analysis of the proposed idea\u201d.  \n\n\u201cIt is said that the method seems to be robust to this choice [of number of activated RIMs] but this claim is made because it performs similarly for the values 5 and 6 if I recall correctly.\u201d\n\nOn the larger scale experiments we mostly used 6 RIMs and considered making 4 or 5 activated per-step.  For the smaller scale experiments, we performed a full hyperparameter sweep.  We did find that only keeping one or two activated per step tended to hurt optimization, but 3, 4, and 5 all performed well (when using a total of 6 RIMs).  We also note that using 6 RIMs and a sparsity of k_A = 4 worked well on all of the tasks that we considered, including Atari-PPO, BabyAI, Copying, Sequential MNIST, Mujoco Imitation Learning, and bouncing balls.  \n\nAlso see the top level comment to all reviewers for a new experiment that explores this more thoroughly.  \n\n\u201c2) Is there any incentive in this architecture for a module to not simply \u201cgive up\u201d? I mean, the modules are not necessarily incentivized to be used as often as possible, so could it be the case that a module learns to set its weights to zero?\u201d\n\nThis is a really interesting question.  In practice we never observed an activation pattern where the model uses the same 3 RIMs on all time steps and has 3 RIMs always dormant (where there are 6 RIMs total).  Nonetheless it\u2019s easy to see that such a solution could fit the data (since a single RIM is as powerful as an LSTM, and an LSTM is usually capable of fitting the data).  \n\nOne hypothesis is that because deactivated RIMs keep the same state, they do a better job of keeping their values and sustaining gradients over many time steps.  Thus when the model tries to use information from more distant time steps, it naturally looks towards the currently deactivated RIMs, which in turn causes them to be activated on the following steps.  \n\nThere is also an issue of the model having an incentive to use all the RIMs to complete a complex task, especially if each RIM is relatively simple.  \n\nAs a side note, the notion of RIMs \u201cgiving up\u201d was framed negatively in your question, but we could hypothetically imagine a situation where it becomes a desirable property.  For example if it were the case that some RIMs never activated once the task is fit, we could prune them out and achieve a smaller but functionally identical model, thus allowing the model\u2019s complexity to adaptively adjust to the complexity of the problem.  Note that this is way beyond the scope of this paper, but I think it might be worth considering this angle.  \n\n\u201c3) Would it make sense to present baseline results for an architecture that uses attention? It seems to me that LSTM was often the baseline of choice but RIMs have two important components: multiple LSTMs and an attention mechanism. Could the attention mechanism be explaining some of the results we are seeing?\u201d\n\nMany of our baselines do use attention.  For example the relational memory core uses attention between modules and the transformer does as well.  We found that neither matched the generalization performance of RIMs on the tasks we considered (Table 1 and 2).  \n\n\u201c important details missing with respect to the empirical setup, at least in the Atari experiments\u201d\n\nWe trained both the baselines as well as proposed method for 40M steps. We updated the paper include a new table (Table 7 in the appendix) with the average as well as standard deviations for our method as well as the baseline. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "SkeBPQlOjB", "original": null, "number": 4, "cdate": 1573548892703, "ddate": null, "tcdate": 1573548892703, "tmdate": 1573548892703, "tddate": null, "forum": "BylaUTNtPS", "replyto": "B1lZO_ly9B", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "R2 Response", "comment": "Thank you for your review.  We appreciate that you like the idea and believe that it is \u201cvery inspiring and is indeed a potentially very useful way of modeling the physical world\u201d.  \n\n\u201c1) For the selective activation of RIMs, the number of activated RIMS is a hyperparameter and needs to be pre-defined. According to your experiments, I believe you need tune this hyperparameter a little bit in order to obtain the best performance. First of all, the design of a fixed number of activated RIMs does not seem to be reasonable and is also highly dependent on your task.\u201d\n\nOur experiments show that this hyperparameter is not very task dependent.  For example, having 6 RIMs with 4 RIMs activated-per-step actually works well on all the tasks we considered.  Intuitively, because the model learns with a certain number of RIMs activated-per-step, it is able to adapt the information that it puts on RIMs to handle that level of sparsity.  \n\nAs a side note, many papers that have considered sparse attention such as Outrageously Large Neural Networks (https://arxiv.org/abs/1701.06538), Sparse Attentive Backtracking (https://arxiv.org/abs/1809.03702) and Sparse Transformers (https://openreview.net/pdf?id=Hye87grYDH) achieved robustly good results using a fixed top-k sparsity as a hyperparameter.  \n\nWe ran an experiment (see the top level comment aimed at all reviewers) where we use a much larger number of RIMs and verify that the amount of RIM sparsity is still very flexible.  \n\n\u201c2) I find it quite interesting that the top-down attention in selective RIM activation is corresponding to the states of these recurrent cells. I am wondering what if you do not select these top K activation and directly train it using the entire distribution of the soft attention output?\u201d\n\nIf I understand correctly, you\u2019re asking about removing the selective activation mechanism and updating all RIMs on all time steps?  We studied this extensively and found that on the hard generalization tasks, RIMs performs substantially worse without selective activation.  It also generally hurts results on Atari.  \n\n\u201cFor the experiments, I think they all serve the purpose of showing the advantages of RIMs quite well, except that they are relatively easy tasks\u201d\n\nOur aim has been to show that RIMs is an idea with breadth\u2014over the quite disparate domains of sequence memorization, sequence classification tasks, control tasks, robustness to distractions and prediction tasks. More broadly, RIMs is a step towards addressing an important issue in deep learning: how to build robust models given that internal state spaces are continuous, high dimensional, and often unbounded. The way RIMs do is by modularizing the dynamics model. The high level intuition is, this form of computational modularity is essential for modeling independent mechanisms in a way that generalizes, but it is not clear how this is implemented, or what learning mechanisms may lead to such distributed representations. Here, we investigate this question by using recurrent neural networks with modular interactions, and evaluate it on various different tasks. We find that this modular inductive bias has distinct advantages: (1) it learns distributed representations in subsystems with limited interactions, consistent with observations in the brain (2) it shows improved generalization performance and transferability to novel environments.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "HkestRy_iH", "original": null, "number": 3, "cdate": 1573547651416, "ddate": null, "tcdate": 1573547651416, "tmdate": 1573547651416, "tddate": null, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "New Experiment on Activation Sparsity hyperparameter (especially R2 and R3).  ", "comment": "Several reviewers expressed concern about the \u201cRIM sparsity hyperparameter\u201d which we call k_A.  In practice we\u2019ve found that it\u2019s fairly resilient across tasks and number of RIMs.  For example, using 6 RIMs and a sparsity of k_A = 4 worked well on all of the tasks that we considered, including Atari-PPO, BabyAI, Copying, Sequential MNIST, Mujoco Imitation Learning, and bouncing balls.  \n\nNonetheless we believe that understanding the role of this hyperparameter more carefully is important so we conducted a new analysis where we study the role of the RIM sparsity hyperparameter as we grow the number of RIMs.  \n\nWe analyzed this on the copying task, considering 9 RIMs, 16 RIMs, and 24 RIMs.  Using 9 RIMs, we obtained ideal performance with k_A from 3 to 6.  Using 16 RIMs, we obtained ideal performance with k_A from 4 to 12.  Using 24 RIMs, we obtained ideal performance with k_A between 6 and 20.  As percentages, this is 33% to 66% for 9 RIMs, 25% to 75% for 16 RIMs, and 25% to 83% for 24 RIMs.  \n\nk_A Sparsity / Total RIMs: Test Loss (Train Loss).  \n2/9: 0.15 (0.24)\n3/9: 0.02 (0.03)\n4/9: 0.00 (0.01)\n5/9: 0.00 (0.01)\n6/9: 0.01 (0.01)\n7/9: 0.07 (0.04)\n8/9: 0.10 (0.00)\n9/9: 0.24 (0.03)\n\n4/16: 0.02 (0.05)\n6/16: 0.00 (0.00)\n8/16: 0.00 (0.00)\n10/16: 0.00 (0.00)\n12/16: 0.00 (0.00)\n14/16: 0.26 (0.01)\n\n6/24: 0.00 (0.01)\n8/24: 0.00 (0.00)\n16/24: 0.00 (0.00)\n20/24: 0.00 (0.00)\n22/24: 0.42 (0.00)\n\nLSTM: 2.28 (0.0)\n\n----\n\nIt\u2019s worth noting here that the situation is somewhat analogous to setting the rate hyperparameter for the dropout regularizer.  p=0.5 is often quite good across a variety of tasks.  Using a very large p, such as p=0.9 may lead to underfitting and using too small p, such as p=0.1 may show a reduced benefit for generalization.  Indeed this is exactly what we observe with RIMs.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "SkeBftGWsr", "original": null, "number": 2, "cdate": 1573099789510, "ddate": null, "tcdate": 1573099789510, "tmdate": 1573099789510, "tddate": null, "forum": "BylaUTNtPS", "replyto": "rJxjgFGbsH", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "Challenges ML community is facing. (2/2)", "comment": "\u201cwhat are the challenges that the ML community is now facing\u201d\n\nCurrent machine learning methods often perform poorly when they are required to generalize beyond the training distribution, which is what is often needed in practice. It is not enough to obtain good generalization on a test set sampled from the same distribution as the training data, we would also like what has been learned in one setting to generalize well in other related distributions. These distributions may involve the same concepts that were seen previously by the learner, with the changes typically arising because of actions of agents. More generally, we would like what has been learned previously to form a rich base from which very fast adaptation to a new but related distribution can take place, i.e., obtain good transfer. \n\n\u201cThe mathematical descriptions in 2.2, 2.3 and 2.4 are very hard to follow. There is no cost function (for training) around, but there are discussions of  'gradient'. Gradient of which function?\u201d\n\nWe want to emphasize a very important point, which is that RIMs is a recurrent architecture and it is a drop-in replacement for an LSTM or GRU cell, following the exact same interface with the exact same inputs and outputs.  There is no change to the loss function which results from using RIMs.  \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "rJxjgFGbsH", "original": null, "number": 1, "cdate": 1573099762618, "ddate": null, "tcdate": 1573099762618, "tmdate": 1573099762618, "tddate": null, "forum": "BylaUTNtPS", "replyto": "SyxeOPO8tr", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment", "content": {"title": "\u201cIndependent Mechanisms and RIMs\u201d (1/2)", "comment": "Thank you for the feedback.  We appreciate your comment that the \u201cpaper stands at a high level in general\u201d.  \n\n\n\u201cIndependent Mechanisms and RIMs\u201d\n\nIn the causality literature, it is a common assumption to view any real-world distribution as a product of causal mechanisms (i.e., causal conditionals). A change in such a distribution (e.g., when moving from one setting/domain to a related one) will always be due to changes in at least one of those mechanisms. Consistent with the independence principle, we hypothesize that such changes tend to manifest themselves in a sparse or local way, i.e., they should usually not affect all factors simultaneously. In contrast, if we consider a non-causal factorization, then many terms will be affected simultaneously as we change one of the physical mechanisms responsible for a system\u2019s statistical dependencies. Such a factorization may be described as entangled, a term that has recently gained popularity in machine learning (Bengio et al., 2012; Locatello et al., 2018; Suter et al., 2018). The notion of invariant, autonomous, and independent mechanisms has appeared in various guises throughout the history of causality research. \t\nOur high-level argument is that we want to create a recurrent architecture which makes it easy for the model to capture independent mechanisms.  Thus we view RIMs as a recurrent architecture which is motivated by the notion of causal independence.  How this relates to other research in the space of recurrent architectures is discussed in related work and we\u2019ve found that none have the same kind of strong modularity captured by RIMs, although some of them share certain specific aspects (for example neural turing machines and relational memory cores both benefit from having multiple separate memory cells).  \n\nWe think that at least in the case of monolithic architectures (like normal LSTMs), there is an intuitive argument for why very difficult for them to learn fully independent mechanisms.  The simple reason for this is that fully-connected layers are used over the entire hidden state (per each time step) so to keep information perfectly-separated between the hidden states, the majority of the parameters need to be zero.  In practice, the optimization has no incentive to learn this modularity perfectly, but would instead settle for only having it hold well enough to fit the training data, which in turn leads to poor generalization to changing environments.  You can see this on the copying task, where an LSTM is able to ignore the dormant-phase of the sequence well enough to fit the training data, but it hasn\u2019t perfectly learned to keep this phase separate, so it generalizes very poorly if the length of this phase is increased at test time.  \n\n[1] Suter et al. 2018, Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness.\n[2] Bengio et. al, 2012, Representation learning: A review and new perspectives. \n[3] Locatello et al., 2018, Challenging common assumptions in the unsupervised learning of disentangled representations. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper577/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylaUTNtPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper577/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper577/Authors|ICLR.cc/2020/Conference/Paper577/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169368, "tmdate": 1576860549254, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper577/Authors", "ICLR.cc/2020/Conference/Paper577/Reviewers", "ICLR.cc/2020/Conference/Paper577/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Comment"}}}, {"id": "B1lZO_ly9B", "original": null, "number": 3, "cdate": 1571911784568, "ddate": null, "tcdate": 1571911784568, "tmdate": 1572972577933, "tddate": null, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "invitation": "ICLR.cc/2020/Conference/Paper577/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper draws inspiration from Physical world and considers an independent mechansim among recurrent modules. The authors apply the proposed RIM to several relatively simple tasks and show some advantages.\n\nIn general, I like the idea of making recurrent cells operate with nearly independent transition dynamics and interact only sparingly through the attention bottleneck. It is essentially to combine some environment prior into the model design. It makes senses to me that RIMs will work better in environments that objects and background are nearly independent and only interact with each other when collision happens. RIMs share similar to spirits with capsule networks, and its recurrent cells serve somewhat similar role to capsules. Such independent mechanism, selective activation and sparse communication is very inspiring and is indeed a potentially very useful way of modeling the physical world.\n\nFor the model itself, I appreciate its simplicity, but I also have some concerns.\n\n1) For the selective activation of RIMs, the number of activated RIMS is a hyperparameter and needs to be pre-defined. According to your experiments, I believe you need tune this hyperparameter a little bit in order to obtain the best performance. First of all, the design of a fixed number of activated RIMs does not seem to be reasonable and is also highly dependent on your task. I believe the framework will be more interesting if the model can determine this number automatically.\n\n2) I find it quite interesting that the top-down attention in selective RIM activation is corresponding to the states of these recurrent cells. I am wondering what if you do not select these top K activation and directly train it using the entire distribution of the soft attention output?\n\nFor the experiments, I think they all serve the purpose of showing the advanatges of RIMs quite well, except that thery are relatively easy task. However, it is still interesting to see that RIMs obtain significant gains over some baselines. Some of the details can be made more clear, such as loss function and evaluation metrics in every task. It is sometimes difficult to find what loss function you are using. I suggest the authors make the experiments more self-contained in the main paper, such that authors do not need frequently scroll down to the appendix and check the details."}, "signatures": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper577/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["anirudhgoyal9119@gmail.com", "alex6200@gmail.com", "sshagunsodhani@gmail.com", "jhoffmann@g.harvard.edu", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec", "bs@tuebingen.mpg.de"], "title": "Recurrent Independent Mechanisms", "authors": ["Anirudh Goyal", "Alex Lamb", "Shagun Sodhani", "Jordan Hoffmann", "Sergey Levine", "Yoshua Bengio", "Bernhard Scholkopf"], "pdf": "/pdf/e4152a9f64c5e4ecef050e8bdcd91c1f76b37756.pdf", "TL;DR": "Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.", "abstract": "Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.", "keywords": ["modular representations", "better generalization", "learning mechanisms"], "paperhash": "goyal|recurrent_independent_mechanisms", "original_pdf": "/attachment/1186d566cb6efd47d5a2f3a3c612593bec5f8015.pdf", "_bibtex": "@misc{\ngoyal2020recurrent,\ntitle={Recurrent Independent Mechanisms},\nauthor={Anirudh Goyal and Alex Lamb and Shagun Sodhani and Jordan Hoffmann and Sergey Levine and Yoshua Bengio and Bernhard Scholkopf},\nyear={2020},\nurl={https://openreview.net/forum?id=BylaUTNtPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylaUTNtPS", "replyto": "BylaUTNtPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper577/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575780511355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper577/Reviewers"], "noninvitees": [], "tcdate": 1570237750118, "tmdate": 1575780511369, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper577/-/Official_Review"}}}], "count": 18}