{"notes": [{"id": "r1e13s05YX", "original": "HyxApcj5Ym", "number": 672, "cdate": 1538087846628, "ddate": null, "tcdate": 1538087846628, "tmdate": 1548749493486, "tddate": null, "forum": "r1e13s05YX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJeFD9qrxV", "original": null, "number": 1, "cdate": 1545083488619, "ddate": null, "tcdate": 1545083488619, "tmdate": 1545354480613, "tddate": null, "forum": "r1e13s05YX", "replyto": "r1e13s05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Meta_Review", "content": {"metareview": "The paper focuses on hybrid pipelines that contain black-boxes and neural networks, making it difficult to train the neural components due to non-differentiability. As a solution, this paper proposes to replace black-box functions with neural modules that approximate them during training, so that end-to-end training can be used, but at test time use the original black box modules. The authors propose a number of variations: offline, online, and hybrid of the two, to train the intermediate auxiliary networks. The proposed model is shown to be effective on a number of synthetic datasets.\n\nThe reviewers and AC note the following potential weaknesses: (1) the reviewers found some of the experiment details to be scattered, (2) It was unclear what happens if there is a mismatch between the auxiliary network and the black box function it is approximating, especially if the function is one, like sorting, that is difficult for neural models to approximate, and (3) the text lacked description of real-world tasks for which such a hybrid pipeline would be useful.\n\nThe authors provide comments and a revision to address these concerns. They added a section that described the experiment setup to aid reproducibility, and incorporated more details in the results and related work, as suggested by the reviewers. Although these changes go a long way, some of the concerns, especially regarding the mismatch between neural and black box function, still remain.\n\nOverall, the reviewers agreed that the issues had been addressed to a sufficient degree, and the paper should be accepted.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Important problem setup and strong evaluation"}, "signatures": ["ICLR.cc/2019/Conference/Paper672/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper672/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353131441, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1e13s05YX", "replyto": "r1e13s05YX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper672/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper672/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper672/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353131441}}}, {"id": "r1xKLxl5R7", "original": null, "number": 5, "cdate": 1543270481489, "ddate": null, "tcdate": 1543270481489, "tmdate": 1543270481489, "tddate": null, "forum": "r1e13s05YX", "replyto": "BJx7vWFOCm", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "content": {"title": "Response to authors' reply", "comment": "I have read the authors' reply. I am generally happy with the revision and will keep my rating. "}, "signatures": ["ICLR.cc/2019/Conference/Paper672/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper672/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612771, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1e13s05YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper672/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper672/Authors|ICLR.cc/2019/Conference/Paper672/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612771}}}, {"id": "BJx7vWFOCm", "original": null, "number": 1, "cdate": 1543176539170, "ddate": null, "tcdate": 1543176539170, "tmdate": 1543179739978, "tddate": null, "forum": "r1e13s05YX", "replyto": "rkgznMOCnX", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "content": {"title": "Paper revised (including experiment appendix) to address comments", "comment": "Thank you for the very detailed criticism and positive review.\n\nWe have updated the paper to address your concerns in the following way:\n\n(1) We have added an appendix section with experimental details for the purpose of reproducing the results. While we cannot make a concrete promise, we will make a concerted effort to release the code.\n\n(2) We refer to both of your points (1) and \"minor\" together:\n\nWe have added more detail to Table 3 in the form of a k=4 experiment and an additional column for the estimator accuracy on the online sampling dataset. We could not execute a k=5 experiment because of resource constraints in training an estimator network for a 10^5 lookup table. The experiment results are:\nImage-Lookup k=4\nTrain: 0.69\nTest: 0.1\nInference: 0.95\nArgument extractor: 0.986\nEstimator: 0.7\n\nIn this experiment, the bbf estimator only reaches a performance of 0.7 (in other words, it learns about 70% of the values in the 10^4 lookup table), which proves enough for the argument extractor to learn the desired functionality, allowing the network to perform well at inference time with the real black-box function. This result should help address your concern in point (1). We note that it is not necessary for the estimator to learn in a way that generalizes to unseen inputs (because it is discarded after training), as long as it performs the correct mapping on the training set.\n\nIt is true that learning is dependent on the performance of the estimator. Whether the argument extractor can learn from an imperfect estimator is likely dependent on the ratio of noisy signals (from incorrect estimator decisions) to informative signals, and the ability of the interface between them to generalize from the informative examples to the noisy examples. In the case where the estimator is never correct for any decision of one of the argument extractors, that specific argument extractor will be unable to learn (in the Image-Lookup case, this would mean the estimator is incorrect for an entire dimension slice of the lookup bbf tensor).\n\n(3) Regarding the practicality of the approach towards real-world tasks like Semantic Parsing and Question Answering, it has indeed been our main motivation for this work. The synthetic Text-Lookup-Logic experiment was meant to serve as a first step in that direction. We've added a short mention of these motivations in the introduction.\n\nWe have also appended the Related Works section with your suggestions.\n\nThe new revision of the paper has been uploaded to this page.\n\nThank you again for your comments\n- Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper672/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612771, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1e13s05YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper672/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper672/Authors|ICLR.cc/2019/Conference/Paper672/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612771}}}, {"id": "rJxBpMKOC7", "original": null, "number": 4, "cdate": 1543176892653, "ddate": null, "tcdate": 1543176892653, "tmdate": 1543176892653, "tddate": null, "forum": "r1e13s05YX", "replyto": "r1e13s05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "content": {"title": "Revision change-log", "comment": "We thank all of the reviewers and we apologize for the late response.\n\nTo address comments of the reviewers, the PDF file of the paper has been updated with the following changes:\n\n- We have added an appendix that contains the implementation details of our experiments, including architecture and hyperparameters.\n- Another experiment was added to Table 3 in Section 4. It is an additional k=4 experiment for the Image-Lookup task, and its primary addition is an example of an instance where the argument extractor was able to learn its desired functionality in spite of an imperfect estimator.\n- Added citations in the Related Works section (under RL). \n\nAs well as some small typo fixes.\n\nThanks for all of the great comments\n\n- Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper672/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612771, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1e13s05YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper672/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper672/Authors|ICLR.cc/2019/Conference/Paper672/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612771}}}, {"id": "SkeZmft_07", "original": null, "number": 3, "cdate": 1543176729034, "ddate": null, "tcdate": 1543176729034, "tmdate": 1543176741112, "tddate": null, "forum": "r1e13s05YX", "replyto": "rkg1n-hLhm", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "content": {"title": "Reply to the reviewer's suggestion", "comment": "Thank you for your detailed and positive review.\n\nRegarding your last comment, if we have understood it correctly, you are suggesting to use the argument extractor when sampling to the estimator during training for better exploration of the argument space. We believe that this has potential to improve sample complexity. The hybrid training has served us well enough in our experiments, but it is a great idea for future work. One challenge is that it is not immediately clear what the initial prior should be (and how strong when updating the posterior). \n\nThank you again for your comments\n- Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper672/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612771, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1e13s05YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper672/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper672/Authors|ICLR.cc/2019/Conference/Paper672/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612771}}}, {"id": "ByxwCZF_0Q", "original": null, "number": 2, "cdate": 1543176654805, "ddate": null, "tcdate": 1543176654805, "tmdate": 1543176654805, "tddate": null, "forum": "r1e13s05YX", "replyto": "r1xRq4Zah7", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "content": {"title": "Appendix added to clarify experiment details", "comment": "Thank you for the detailed and positive review. \n\nTo answer your questions regarding specific experiment details, we have added an appendix which contains all of the relevant details. Specifically, we have indeed used hybrid training for experiments 4.2, 4.3, where the pre-training continued until either a satisfactory performance was reached, such as 90%, or performance stopped increasing (the behavior depends on the difficulty of the task and quality of the offline sampling). Experiment 4.1 uses online training (this was specified in Table 1's caption but we have now added the information to the main text).\n\nThe pre-training took a negligible amount of time in comparison to the target/online training since it relied on a sub-component of the network and loss (additionally, the arguments domain is far smaller than the input domain).\n\nThe exact loss function is also now detailed in the appendix, but in essence, we used a direct sum (i.e. weights of 1.0) of the target loss and online loss. \n\nThank you again for your comments\n- Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper672/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612771, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1e13s05YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper672/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper672/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper672/Authors|ICLR.cc/2019/Conference/Paper672/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper672/Reviewers", "ICLR.cc/2019/Conference/Paper672/Authors", "ICLR.cc/2019/Conference/Paper672/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612771}}}, {"id": "rkgznMOCnX", "original": null, "number": 3, "cdate": 1541468841965, "ddate": null, "tcdate": 1541468841965, "tmdate": 1541533787725, "tddate": null, "forum": "r1e13s05YX", "replyto": "r1e13s05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Official_Review", "content": {"title": "Interesting approach with good results on synthetic tasks", "review": "This paper presents an approach, called EstiNet, to train a hybrid models which uses both neural networks and black-box functions. The key idea is that, during training, a neural network can be used to approximate the functionality of the black-box functions, which makes the whole system end-to-end differentiable. At test time, the true black-box functions are still used. The training objective composes two parts: L_bbf, the loss for approximating the black-box function and L_target, the loss for the end-to-end goal. They tried different variations of when to train the black-box function approximator or not. It is shown to outperform the baselines like end-to-end differentiable model or NALU over 4 synthetic tasks in sample efficiency. There are some analysis about how the entropy loss and label smoothing helps with the gradient flow. \n\nThe proposed model is interesting, and is shown to be effective in the synthetic tasks. The paper is well-written and easy to follow. However, some of the experiment details are missing or scattered in the text, which might make it hard for the readers to reproduce the result. I think it helps to have the experimental details (number of examples, number of offline pretraining steps, size of the neural network, etc) organized in some tables (could be put in the appendix). \n\nTwo main concerns about how generally applicable is the proposed approach: \n\n1. It helps to show how L_target depends on L_bbf, or how good the approximation of the black-box function has to be to make the approach applicable. For example, some functions, such as sorting, are hard to approximate by neural network in a generalizable way, so in those cases, is it still possible to apply the proposed approach? \n\n2.The proposed approach can be better justified by discussing some potential real world applications. Two closely related applications I can think of are visual question answering and semantic parsing. However, it is hard to find good black-box functions for VQA and people often learn them as neural networks, and the functions in semantic parsing often need to interact with a database or knowledge graph, which is hard to approximate with a neural network. \n\nSome minor issues:\n\nTable 3 isn\u2019t very informative since k=2 and k=3 provides very similar results. It would help to show how large k needs to be for the performance to severely degrade. \n\nMissing references: \n\nThe Related Works section only reviewed some reinforcement learning work on synthetic tasks. However, with some bootstrapping, RL is also shown to achieve state-of-the-art performance on visual question answering and semantic parsing tasks (Johnson et al, 2017; Liang et al, 2018), which might be good to include here. \n\nJohnson, J., Hariharan, B., van der Maaten, L., Hoffman, J., Fei-Fei, L., Zitnick, C. L., & Girshick, R. B. (2017, May). Inferring and Executing Programs for Visual Reasoning. In ICCV (pp. 3008-3017).\nLiang, C., Norouzi, M., Berant, J., Le, Q., & Lao, N. (2018). Memory augmented policy optimization for program synthesis with generalization. arXiv preprint arXiv:1807.02322.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper672/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Official_Review", "cdate": 1542234406267, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1e13s05YX", "replyto": "r1e13s05YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper672/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335777242, "tmdate": 1552335777242, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper672/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1xRq4Zah7", "original": null, "number": 2, "cdate": 1541375125795, "ddate": null, "tcdate": 1541375125795, "tmdate": 1541533787479, "tddate": null, "forum": "r1e13s05YX", "replyto": "r1e13s05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Official_Review", "content": {"title": "Well written paper and convincing results", "review": "This paper is about training a neural network (NN) to perform regression given a dataset (x, y) *and* a black box function which we know correctly maps from some intermediate representation to y. Instead of learning a NN directly from x to y, we want to make use of this black box function and learn a mapping from x to the intermediate representation. Call this the \"argument extractor\" NN. The problem is that (i) the black box function is typically non-differentiable so we cannot learn end-to-end and (ii) we don't have labels for the intermediate representations in order to learn a NN to approximate the black box function. The authors propose to train in three different ways: (1) offline training: train an auxiliary NN that approximates the black box function based on data generated by sampling the input uniformly (or similar); then train both the auxiliary NN and the argument extractor NN together end-to-end using (x, y) data, (2)  online training: train the auxiliary NN and the argument extractor NN together, based on (x, y) data; data for training the auxiliary NN comes from the argument extractor NN during the main training, and (3) hybrid training: pre-train the auxiliary NN as in (1) and then train both NNs as in (2).\n\nExperimental results show:\n- this approach leads to better performance than regressing directly from x to y in the small data regime,\n- this approach leads to better generalization (being able to add more image numbers during test),\n- this approach learns faster than an actor-critic based RL agent,\n- this approach can be useful even if the functionality of the black-box function inherently cannot be estimated by a differentiable function (lookup table) - the resulting argument extractor NN is useful when used with the non-differentiable black box function,\n- hybrid training is the best; offline training is the worst,\n- penalizing low output entropy helps.\n\nIt wasn't quite clear to me which training procedure was used for experiments 4.1-4.3. Presumably hybrid? It would also be nice to see how much time is spent in pre-training vs main training. In figure 2, what are the update steps for EstiNet (since there are two losses + pretraining)?\n\nI found this paper to be generally well-written and results convincing.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper672/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Official_Review", "cdate": 1542234406267, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1e13s05YX", "replyto": "r1e13s05YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper672/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335777242, "tmdate": 1552335777242, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper672/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkg1n-hLhm", "original": null, "number": 1, "cdate": 1540960678528, "ddate": null, "tcdate": 1540960678528, "tmdate": 1541533787276, "tddate": null, "forum": "r1e13s05YX", "replyto": "r1e13s05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper672/Official_Review", "content": {"title": "A good idea with proper validation", "review": "The paper proposes a method to solve end-to-end learning tasks using a combination of deep networks and domain specific black-box functions. In many machine learning tasks there may be a sub-part of the task can be easily solved with a black-box function (e.g a hard coded logic).  The paper proposes to use this knowledge in order to design a deep net that mimics the black-box function. This deep net being differentiable can be utilized while training in order to perform back-propagation for the deep nets that are employed to solve the remaining parts of the task. \n\nThe paper is well written and in my opinion the experiments are solid. They show significant gains over well-designed baselines. (It should be noted that I am not super familiar with prior work in this area and may not be aware of some related baselines that can be compared with.)\n\nIn Section 3.1.2 the authors discuss offline and online methods to train the mimicking deep network of a black-box function. The offline version suffers from wasting samples on unwanted regions while the online version will have a cold-start problem. However, I believe there can be better solution than the hybrid strategy. In fact there is a clear explore/exploit trade-off  here. Therefore, one may start with a prior over the input domain of the black-box function and then as the argument extractor learns well the posterior can be updated. Then we can Thompson sample the inputs from this posterior in order to train the mimicking network.  I think such a bandit inspired approach will be interesting to try out. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper672/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.", "keywords": ["neural networks", "black box functions", "gradient descent"], "authorids": ["alon.jacovi@il.ibm.com", "guyh@il.ibm.com", "einatke@il.ibm.com", "boazc@il.ibm.com", "oferl@il.ibm.com", "gkour@ibm.com", "joberant@cs.tau.ac.il"], "authors": ["Alon Jacovi", "Guy Hadash", "Einat Kermany", "Boaz Carmeli", "Ofer Lavi", "George Kour", "Jonathan Berant"], "TL;DR": "Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training", "pdf": "/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf", "paperhash": "jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces", "_bibtex": "@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper672/Official_Review", "cdate": 1542234406267, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1e13s05YX", "replyto": "r1e13s05YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper672/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335777242, "tmdate": 1552335777242, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper672/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}