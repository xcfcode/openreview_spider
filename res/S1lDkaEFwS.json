{"notes": [{"id": "S1lDkaEFwS", "original": "S1evogvSvS", "number": 304, "cdate": 1569438942687, "ddate": null, "tcdate": 1569438942687, "tmdate": 1577168258985, "tddate": null, "forum": "S1lDkaEFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "op722O20zF", "original": null, "number": 1, "cdate": 1576798692814, "ddate": null, "tcdate": 1576798692814, "tmdate": 1576800942574, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a defense technique against query-based attacks based on randomization applied to a DNN's output layer. It further shows that for certain types of randomization, they can bound the probability of introducing errors by carefully setting distributional parameters. It has some valuable contributions; however, the rebuttal does not fully address the concerns.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729890, "tmdate": 1576800282572, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper304/-/Decision"}}}, {"id": "SyxsIAlnjr", "original": null, "number": 7, "cdate": 1573813842627, "ddate": null, "tcdate": 1573813842627, "tmdate": 1573815333581, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "SkgxviV9jB", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment", "content": {"title": "The value of \"h\" and results of suggested adaptive attacks; A question not answered yet", "comment": "Thanks for your reply!\n\nIt seems that you did not mention the value of \"h\" used in the experiments in the paper (tell me if I am wrong). According to your code, it seems that h=1e-4 in the experiments. I think you should mention this in the paper.\n\nThanks for your verification that when a larger \"h\" is used, the success rate of the black box attack drops significantly. It would be better if you could add the results with different \"h\" in the paper. According to my personal experience, a larger \"h\" such as h=1e-3 is still OK when attacking ordinary models although the performance would degrade a bit. Therefore it would be better if you present the experimental results carefully and clearly with a large \"h\" since it is likely to be a good adaptive attack.\n\nThere is one remaining question in my review that you perhaps have missed: In Figure 3a, the defense is effective even if \\sigma^2<1e-6. However, by the analysis in Section 4.2 (the formula below Line 3, Page 6), when \\sigma^2=1e-6, |E[g_i-\\gamma_i]| should be rather small, hence it should not block finite-difference based attacks. What is the reason? I guess that in theoretical analysis part, analyzing the bias is not the correct direction. Perhaps you should analyze the variance."}, "signatures": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lDkaEFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper304/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper304/Authors|ICLR.cc/2020/Conference/Paper304/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173398, "tmdate": 1576860543745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment"}}}, {"id": "H1xruv5oiS", "original": null, "number": 6, "cdate": 1573787500674, "ddate": null, "tcdate": 1573787500674, "tmdate": 1573787500674, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "BkgtLKIqjS", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment", "content": {"title": "Adversary", "comment": "To be clear, we consider an adversary that mounts a finite-difference black box attack with the goals, knowledge, and capabilities laid out in section 2. Our main result is showing the brittleness of these types of attacks specifically their susceptibility to randomization of the model outputs.\n\nThanks for your comments!"}, "signatures": ["ICLR.cc/2020/Conference/Paper304/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lDkaEFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper304/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper304/Authors|ICLR.cc/2020/Conference/Paper304/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173398, "tmdate": 1576860543745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment"}}}, {"id": "BkgtLKIqjS", "original": null, "number": 5, "cdate": 1573706064714, "ddate": null, "tcdate": 1573706064714, "tmdate": 1573706064714, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "rJgndKNqoB", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment", "content": {"title": "Threat model", "comment": "If the black-box threat model is not the intended threat model, what is? Black-box attacks attacks are only interesting from a theoretical or security perspective, and this paper is mostly written from the security perspective---in this case we must have a concrete threat model. \n\nRe: more samples --- without using more samples it is hard to tell when the defense breaks down. However, knowing the limitations of a defense is important from both a science perspective and a practical perspective.\n\nRe: applying algorithms correctly---there are always a number of caveats when applying algorithms from another author directly (e.g. making sure that the images are scaled properly, tuning critical hyperparameters). It would be good to check this over and ensure that you have searched over the parameters properly."}, "signatures": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lDkaEFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper304/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper304/Authors|ICLR.cc/2020/Conference/Paper304/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173398, "tmdate": 1576860543745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment"}}}, {"id": "rJgndKNqoB", "original": null, "number": 1, "cdate": 1573697907650, "ddate": null, "tcdate": 1573697907650, "tmdate": 1573698422115, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "HyeCSCxhYr", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment", "content": {"title": "Clarification of evaluation", "comment": "We reiterate that our proposed method is intended to highlight the weaknesses of finite difference based black box attacks to simple perturbations to the model output (See our response to Reviewer #2 for more details). Indeed, we state this explicitly in the paper in several places in the manuscript in an effort to place our work in the correct context and prevent misunderstandings. Thus, we did not consider the \u201clabel-only\u201d black box attacks referenced in the review. We don\u2019t agree with the reviewer\u2019s suspicion that these types of attacks are able to deal with the randomization proposed in our work but leave those experiments to future work. \n-\tConcerning the number of samples used by the BAND and QL attacks (and to address the reviewer\u2019s comments about the NES-based attack), we allowed these attacks to use up to a maximum of 10,000 samples which we considered sufficient. We are happy to include these details in the Attack Details section of the appendix.\n-\tThe BAND and QL performance on the undefended classifier in Table 1 was measured directly using the code provided by the authors. We could not find any flaws in their code, can the reviewer elaborate more on this?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper304/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lDkaEFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper304/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper304/Authors|ICLR.cc/2020/Conference/Paper304/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173398, "tmdate": 1576860543745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment"}}}, {"id": "SkgxviV9jB", "original": null, "number": 4, "cdate": 1573698392021, "ddate": null, "tcdate": 1573698392021, "tmdate": 1573698392021, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "SkxkhdKrqS", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment", "content": {"title": "Adaptive attacker with large step size", "comment": "Regarding the limitations wrt white box and transfer attacks, please see the first paragraph in our response to reviewer #2.\n\nThanks for the suggestion for exploring the effect of increasing \u201ch\u201d as a type of adaptive attack. We missed this dimension as the three attacks we evaluated (especially [18]) recommended that \u201ch\u201d be set very small. As suggested by the reviewer, we ran the attacks with larger \u201ch\u201d (h=1e-3) and quickly realized why \u201ch\u201d must be small. When \u201ch\u201d is large, the success rate of the black box attack drops significantly. \nThanks for pointing out the error in the probability, you are correct: the equal sign should be a <= according to the union bound.\nWhat we mean by \u201clevel of noise (\\sigma^2) can be set for each class separately\u201d is that \\sigma_i for i = 2\u2026C can be the same for each class (a spherical distribution) or different for each class based on the desired misclassification probability K as we show in section 4.1.\nThe clarification comments and suggestions are also appreciated."}, "signatures": ["ICLR.cc/2020/Conference/Paper304/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lDkaEFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper304/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper304/Authors|ICLR.cc/2020/Conference/Paper304/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173398, "tmdate": 1576860543745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment"}}}, {"id": "SJxAjqEcsS", "original": null, "number": 3, "cdate": 1573698214268, "ddate": null, "tcdate": 1573698214268, "tmdate": 1573698214268, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "SkglhWCXcB", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment", "content": {"title": "Why not transfer or white box - a rebuttal", "comment": "The reviewer brings up a valid point that our proposed defense against finite difference black box attacks do not defend against white-box or transfer-based attacks. We agree with the reviewers on this point. Following the advice of [25] and to leave no room for assumptions on the effectiveness of the proposed defense against other attacks, we clearly defined our problem setting to be defense against finite difference attacks. [25] showed in 2018 that  7 out of 9 papers that proposed defense strategies in ICLR 2018 made false claims and over-promised their effectiveness. This includes defenses in our related work section, such as [26]. We found our proposed defense against finite difference attacks to be both novel and interesting, but we did not want to over-claim the defense's effectiveness. We are careful to only claim the defense is effective against (adaptive) finite different black box attacks.\n\nIndeed we evaluated an adaptive black box attacker, Figure 3b is describing a black box attacker that adopt the same adaptive strategy as the one in Figure 2b. \nIn the black box case, we were limited by the available computational budget as increasing the number of samples in the adaptive case past 100 was very slow.\nPage by page responses\n2- successful in the sense of satisfying the two conditions mentioned in the next sentence.\n2- The authors in [18] consider the given loss function the best for their attack\n2 \u2013 yes we verified the samples produced by the attacks were not semantically unperturbed in most cases\n7- Correct, T=targeted and U=untargeted\n7- The authors in [7] show defensive distillation is ineffective against a transfer attack, we show it is also ineffective against finite-difference based attacks. We believe the distinction is important and hope the reviewer agrees.\n\n[25] A. Athalye, N. Carlini, D. Wagner. \"Obfuscated Gradients give a false sense of security: circumventing defenses to adversarial examples\" ICML 2018\n[26] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille, \u201cMitigating Adversarial Effects Through Randomization\u201d ICLR 2018\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper304/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lDkaEFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper304/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper304/Authors|ICLR.cc/2020/Conference/Paper304/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173398, "tmdate": 1576860543745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment"}}}, {"id": "Byehyc4qjH", "original": null, "number": 2, "cdate": 1573698020406, "ddate": null, "tcdate": 1573698020406, "tmdate": 1573698020406, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "rklTFuDlcS", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment", "content": {"title": "Some clarification points", "comment": "Thanks for your review, the following should address the points you raised.\n- We compare directly to the defense proposed in [26] in Table 2.\n- In the \u201cExtensions\u201d section of the Appendix, we considered noise sampled from a gaussian mixture model.\n- The analysis for targeted attacks is almost identical to untargeted attacks.\n- Our method involves a simple modification to the model under attack, and so our code changes to the attack code are very minimal.\nThanks for pointing out errors in the notation and references.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper304/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lDkaEFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper304/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper304/Authors|ICLR.cc/2020/Conference/Paper304/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173398, "tmdate": 1576860543745, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper304/Authors", "ICLR.cc/2020/Conference/Paper304/Reviewers", "ICLR.cc/2020/Conference/Paper304/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Comment"}}}, {"id": "HyeCSCxhYr", "original": null, "number": 1, "cdate": 1571716677615, "ddate": null, "tcdate": 1571716677615, "tmdate": 1572972612246, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors present a randomization defense in the black-box threat model, where bounded l_2 norm perturbations are allowed. In the presented scheme, the defender adds Gaussian noise to every coordinate of the output probability vector before returning an inference result. \n\n\nThe paper suffers from an incomplete evaluation, and therefore I cannot recommend acceptance. \n\n\nDetails:\n* Completeness of evaluation. As shown by Ilyas et al [1] and Cheng et al [2] and Brendel et al [3], black-box attacks can succeed even when only information about the label is present. Thus, I suspect that an attacker can simply run any of these attack algorithms in order to fool the model. In addition, if we use enough samples in the NES-based \u201cQuery-Limited\u201d algorithm of Ilyas et al (that is, with enough samples per step) then we should be able to perfectly mimic the white-box attack, which as shown in Figure 2 is effective. \n* Potential flaw in evaluation. BAND performs worse than QL on the undefended classifier in Table 1, which should not occur. I would check to make sure that the attacks are applied correctly here.\n* Lacking details in evaluation. I could not find how many samples are used to perform the attacks in Table 1. It is hard to evaluate the defense without knowing how many samples are used in each algorithm.\n\n\n[1] https://arxiv.org/abs/1804.08598\n[2] https://arxiv.org/abs/1807.04457\n[3] https://arxiv.org/abs/1712.04248\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575452480329, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper304/Reviewers"], "noninvitees": [], "tcdate": 1570237754071, "tmdate": 1575452480346, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Review"}}}, {"id": "rklTFuDlcS", "original": null, "number": 2, "cdate": 1572006021446, "ddate": null, "tcdate": 1572006021446, "tmdate": 1572972612214, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for defending black box (in particular, finite difference based loss) adversary attacks by randomisation of the output of the network. \n\nThe idea appears to be somewhat novel considering that majority of existing methods consider randomize inputs or the model itself. \n\nA natural question that would be particularly interesting to me is how does such defence compare against the defence by  randomizing the input and the model. There is no such comparison in the paper, which, to me, is the main weakness of the paper. \n\nThe authors consider the randomization in terms of Gaussian distribution. How would this differ if other types of distributions are considered, e.g. non-Gaussian distributions?\n\nSection 4.2 considered the finite difference gradient error, and discussed the results for untargetted attacks. What bout targetted attacks?   The presentation of this section is also not very clear, e.g. the equation on page 6. \n\nThe citation style look odd to me, often you use either something like \"[2,3]\", or something like \"Dhillon et al. (2018)\", but not \"(2,3)\". In addition, the equations should be number for the convenience of references.\n\n\"... in our code \\superscript {1}\" - the links all point to other people's code, not your codes.\n\nThe notation for the output p is different from the notation in page 2 where you used y. Try to use consistent notations. \n\nRegarding the novelty of this paper, I was based on my judgement and experience of reading a few papers, not I never published papers on adversary attacks or defence. "}, "signatures": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575452480329, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper304/Reviewers"], "noninvitees": [], "tcdate": 1570237754071, "tmdate": 1575452480346, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Review"}}}, {"id": "SkglhWCXcB", "original": null, "number": 3, "cdate": 1572229543933, "ddate": null, "tcdate": 1572229543933, "tmdate": 1572972612170, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to introduce randomness in a classifier\u2019s predictions to mitigate black-box attacks that rely on gradient estimation through finite differences. The intuition behind the defense is correct: finite differences rely on the outputs of the neural network being non-deterministic and accurate to estimate gradients near the test points being attacked.\n\nHowever, the threat model chosen in this paper is not well justified: the adversary cannot be forced to use a particular strategy. Unlike what is suggested in Section 3, estimating gradients through finite differences is not the only strategy available in the black-box threat model (this is later mentioned in Section 6). In this case, the adversary could for instance decide to adapt by instead mounting a black-box attack that relies on transferability. Because Figure 2 shows that the defense does not provide robustness in the white-box setting, this suggests that other forms of black-box attacks that either (a) rely on transferability or (b) are label-based only would still evade the model. This limitation should be addressed to understand how applicable the defense strategy is in a realistic deployment.\n\nPutting this aside, it is not clear from Figure 3 that an adaptive strategy was evaluated in the limited black-box setting that is considered here (the caption of Figure 3.b only describes a \u201cwhite-box\u201d adaptive adversary), or that the defense is effective. The attack success rates are high for many graphs and increase as the adversary averages over more runs. Moving forward, increasing further the highest number of runs would help appreciate the limitations of the approach: it is currently set to at most 100, which is low. \n\nAs far as organization is concerned, a lot of real estate is spent on background material, and few experimental results are presented to support claims made in the introduction. Addressing the above comments would probably require compressing background material a bit. \n\nPage-by-page details:\n\n1/ An attack is always adversarial by definition, \u201cadversarial attack\u201d is a tautology. \n\n2 / What do you mean by \u201csuccessful attacks\u201d?\n\n2/ What do you mean by \u201cstrongest\u201d loss?\n\n2/ Having a perturbation limited to be small does not guarantee it won\u2019t impact the semantics of the input, even in the vision domain. Have you verified that the perturbations that you chose left semantics unperturbed?\n\n2/ It is best to avoid making broad statements such as \u201cWe use the l2 perturbation penalty as this type of attack results in the strongest attacks\u201d because they are unlikely to hold across datasets and models.\n\n7/ Figures are difficult to parse (e.g., does T and U stand for targeted and untargeted?)\n\n7/ Distillation was already shown to be vulnerable to black-box attacks in [7]\n\n8/ Gradient masking was introduced in [7] prior to [25].\n\n8/ It would be good to justify the following statement (see my comment above): \u201cAlthough this is a valid attack vector for even black box models, we do not consider this type of attack in this work\u201d"}, "signatures": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575452480329, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper304/Reviewers"], "noninvitees": [], "tcdate": 1570237754071, "tmdate": 1575452480346, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Review"}}}, {"id": "SkxkhdKrqS", "original": null, "number": 4, "cdate": 1572341926738, "ddate": null, "tcdate": 1572341926738, "tmdate": 1572972612127, "tddate": null, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "invitation": "ICLR.cc/2020/Conference/Paper304/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes applying randomization to the output layer of a DNN to defend against query-based attacks based on finite difference estimates. Then some theoretical analysis is provided, showing that with perturbation of a suitable scale, the randomization layer will not affect the accuracy of the model, while causing a large estimation error of finite difference methods that prevents finite-difference based attacks. Empirical results verify that the proposed defense is still effective against adaptive attacks where the randomness is averaged.\n\nPros:\n\nThe proposed method is simple, straightforward, yet novel. Its working mechanism is easy to understand and analyze, so it should be useful against finite-difference based attacks.\n\nLimitation:\n\nThe proposed method is not useful to defense against white-box attacks and transfer-based attacks, since basically it does not change the predictive model. Some other randomization methods like [26], by contrast, change the predictive model, hence they may be useful against white-box attacks and transfer-based attacks.\n\nQuestions and suggestions:\n\nThis part is my main concern.\n\nIt seems that the experimental results are very good. For example, in Figure 3a, the defense is effective even if \\sigma^2<1e-6. However, by the analysis in Section 4.2 (the formula below Line 3, Page 6), when \\sigma^2=1e-6, |E[g_i-\\gamma_i]| should be rather small, hence it should not block finite-difference based attacks. I think more explanation is needed for the good performance in the experiments.\n\nFinite differences are extremely sensitive to small random perturbation of the function value when the spacing (step size) h is small. For example, g_i=\\frac{L(f(x+he_i))-L(f(x-he_i))}{2h}, when h is very small, f(x+he_i) and f(x-he_i) is very close, hence adding perturbation to them will change g_i a lot. To present stronger adaptive attacks to output randomization, my suggestion is that a larger h can be adopted. It will be better if the results are investigated against attacks with different values of h.\n\nA mistake:\n\nIn Section 4.1 on Page 4: \"we can express the probability that x is misclassified in the vector d(p) as: \\sum_{i=2}^C P(d(p_i)>d(p_m))\". I think this is wrong, since P(A or B happens)=P(A happens)+P(B happens) only when A and B are mutually exclusive. However, \"d(p_i)>d(p_m)\" and \"d(p_j)>d(p_m)\" are not mutually exclusive. Hence, the probability that x is misclassified in the vector should be less than or equal to that sum of probabilities.\n\nBy the way, the writing in Section 4.1 is not clear:\n\n- The overall misclassification probability is presented first, but after that K only represents the misclassification probability into a specific class. The connection between them is unclear.\n- At the beginning of Section 4.1, the distribution of \\epsilon is \\epsilon\\sim\\mathcal{N}(\\mu,\\sigma^2\\cdot\\mathbf{I}_C): a unique \\sigma is used. But after that, the variance of \\epsilon_i becomes \\sigma_i^2 instead of \\sigma^2.\n- In the second to the last line on Page 4, \"level of noise (\\sigma^2) can be set for each class separately\", but the authors did not explain how to set them, and in the experiments \\sigma^2 is set as the same scalar.\n- In Figure 1b, the line style of \"K=5.0e-3\" and \"K=1.0e-1\" in the legend is very similar. The line style of \"K=2.0e-01\" in the legend is not clear: I do not know whether it refers to \"-.-.-.\" or \"------\".\n\nTypos:\n\nSection 6, Page 8: \"unintentionlly\" => \"unintentionally\"\nSome missing spaces after punctuation:\n- Section 4.2, Page 5, \"... gradient estimate.When the ...\" => should add a space before \"When\"\n- Section 5, Page 7, \"In addition,input randomization ...\" => should add a space before \"input\""}, "signatures": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper304/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Thwarting finite difference adversarial attacks with output randomization", "authors": ["Haidar Khan", "Dan Park", "Azer Khan", "B\u00fclent Yener"], "authorids": ["haidark@gmail.com", "parkd5@gmail.com", "azerkkhan@gmail.com", "byener@gmail.com"], "keywords": ["black box adversarial attacks", "adversarial examples", "defense", "deep learning"], "TL;DR": "Black box adversarial attacks are rendered ineffective by simple randomization of neural network outputs.", "abstract": "  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the \"black box\" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. ", "pdf": "/pdf/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "code": "https://gofile.io/?c=Q7x8SX", "paperhash": "khan|thwarting_finite_difference_adversarial_attacks_with_output_randomization", "original_pdf": "/attachment/3adbb854ce84c2fe77880ee65bc31c67d524f1a9.pdf", "_bibtex": "@misc{\nkhan2020thwarting,\ntitle={Thwarting finite difference adversarial attacks with output randomization},\nauthor={Haidar Khan and Dan Park and Azer Khan and B{\\\"u}lent Yener},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lDkaEFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lDkaEFwS", "replyto": "S1lDkaEFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper304/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575452480329, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper304/Reviewers"], "noninvitees": [], "tcdate": 1570237754071, "tmdate": 1575452480346, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper304/-/Official_Review"}}}], "count": 13}