{"notes": [{"id": "VRgITLy0l2", "original": "Hb1na6pChJQ", "number": 3441, "cdate": 1601308381998, "ddate": null, "tcdate": 1601308381998, "tmdate": 1614985764767, "tddate": null, "forum": "VRgITLy0l2", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "lyl-5m9syZd", "original": null, "number": 1, "cdate": 1610040368847, "ddate": null, "tcdate": 1610040368847, "tmdate": 1610473959991, "tddate": null, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper aims to study the convergence of deep neural networks training via a control theoretic analysis. This is a very interesting approach to establish theoretical understanding of deep learning. However, there are several concerns raised by the reviewers:\n\n1.\tThe contribution of this paper is limited. The results simply follow from standard optimal control. It is not clear what new insight the paper provides.\n2.\tThere are already quite a few works on control theoretic analysis of deep learning. This paper did not do a good job on presenting its novelty and difference with existing works.\n3.\tThe experimental part is weak. It only involves small data set and very simple networks.\n\nBased on these, I am not able to recommend acceptance for the current manuscript. But the authors are encouraged to continue this research.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040368833, "tmdate": 1610473959967, "id": "ICLR.cc/2021/Conference/Paper3441/-/Decision"}}}, {"id": "OaGQC2Ge0yb", "original": null, "number": 3, "cdate": 1603809071949, "ddate": null, "tcdate": 1603809071949, "tmdate": 1606629616435, "tddate": null, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Review", "content": {"title": "Interesting connections between neural network training and control, as well as control-theoretic analysis of the neural network loss function ", "review": "This paper presents a Lyapunov based analysis of the loss function in neural network training and derives a priori upper bounds on the settling time of the training, which somewhat complements existing studies. The supervised neural network learning problem is formulated as a control problem with the weight parameters being the control input, and the learning problem as a tracking problem. Analytic formula for computing the finite-time upper bound on the settling time is provided under suitable assumptions on the input. Furthermore, the loss function is also shown robust against input perturbations.\n\nThis paper contains some interesting ideas in revealing relationships between control and neural network learning, which is a plus. Hopefully, this can further motivate exploration and application of more control-theoretic tools to understanding of neural network training. Although this paper is fairly readable, the presentation and organization can be improved. Several detailed comments are provided below. \n\ni) In control, particularly tracking problem, it is known that there is a given reference signal y(t) one wants the control plant to track. Nonetheless, here in the discussion the y^\\ast I guess is determined by the loss function, training method, data, as well as the neural network architecture altogether, right? What exactly is this y^\\ast? How is it related to e.g., the equilibrium point of the weight parameters and stationary points in the optimization context? \n\nii) The current analysis in Section 2 pertains to a single data point? How would having more data points affect the analysis and the results? In that case, what would be the y^\\ast? Or will be functions of the input? \n\niii) In the experiments, since the Lyapunov loss function and other loss functions are plotted, how does the loss function convergence correspond to the learned weights parameters? In the context learning, one is more interested in the neural network parameters that not only capture the training data but also predict well the unseen ones? So it would also be interesting to present the corresponding testing results?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075768, "tmdate": 1606915763716, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3441/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Review"}}}, {"id": "S8rjXtjwDkm", "original": null, "number": 8, "cdate": 1606222984239, "ddate": null, "tcdate": 1606222984239, "tmdate": 1606222984239, "tddate": null, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment", "content": {"title": "Manuscript and responses to reviews have been updated", "comment": "Dear Reviewers and all, \n\nWe have revised our manuscript based on all the comments provided to us. We thank the reviewers for their comments and would like to welcome them to review our updated manuscript and let us know their views/suggestions or any concerns. \n\nWith Regards,\nAuthors"}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VRgITLy0l2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3441/Authors|ICLR.cc/2021/Conference/Paper3441/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837504, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment"}}}, {"id": "PiCmxUV2HYj", "original": null, "number": 7, "cdate": 1606146627358, "ddate": null, "tcdate": 1606146627358, "tmdate": 1606146627358, "tddate": null, "forum": "VRgITLy0l2", "replyto": "v2i53SgfkX", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment", "content": {"title": "Response: Using Lyapunov function to model training is interesting, but scalability of the results is an issue.", "comment": "We have updated Table 3 with the convergence bounds and experimental convergence time on larger dataset (0.5 million images). "}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VRgITLy0l2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3441/Authors|ICLR.cc/2021/Conference/Paper3441/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837504, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment"}}}, {"id": "v2i53SgfkX", "original": null, "number": 6, "cdate": 1605752839070, "ddate": null, "tcdate": 1605752839070, "tmdate": 1605752839070, "tddate": null, "forum": "VRgITLy0l2", "replyto": "tYlEQok6A6P", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment", "content": {"title": "Response: Using Lyapunov function to model training is interesting, but scalability of the results is an issue. ", "comment": "Thank you for your time to read and review our manuscript.\nThe assumption that we have used to derive our theorems is that atleast one of the input dimensions should be greater than zero and the all input dimension values should be finite i.e. less than some scalar value \u2018a\u2019. While training any machine learning task, the inputs are usually normalized (ranges from -1 to 1 or 0 to 1), hence the assumption does not seem unreasonable for any machine learning application. This assumption holds true even in batched case, as the inputs will be normalized to some finite range. Thus, our algorithm does not depend or have any restrictions on choice of samples in a batch.\nWe include experimental results (test rmse, error plots) for larger dataset (0.5 million images) in the modified submission on Page 9. The convergence bounds and experimental convergence time will be updated in the table in a couple of days. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VRgITLy0l2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3441/Authors|ICLR.cc/2021/Conference/Paper3441/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837504, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment"}}}, {"id": "1VPaHRsWV_L", "original": null, "number": 5, "cdate": 1605752775314, "ddate": null, "tcdate": 1605752775314, "tmdate": 1605752775314, "tddate": null, "forum": "VRgITLy0l2", "replyto": "eYO6UnyXyLC", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment", "content": {"title": "Response: Review", "comment": "Thank you for your time to read and review our manuscript.\nThe main innovation in this paper is to model neural networks as a dynamical control system. Recasting the problem of supervised learning as a dynamical control problem has several benefits. For example, it becomes possible to compute a priori convergence bounds, simpler hyper-parameter optimization and a finite-time convergent weight update. The main argument of the paper is not to use a particular loss function, but to demonstrate that by treating the loss function as a Lyapunov function and modifying the weight update accordingly, the supervised learning framework can benefit from the well-known methods of dynamical control systems. As for empirical generalization of the Lyapunov loss, please see the comparison of test error plots of the proposed Lyapunov loss function and that of traditional loss functions in the revised manuscript on page 6, 7, 8 and 9.   \nThank you for your comments on the loss plots. We have modified the plots so that they are normalized by the respective maximum of the loss function (which happens to be at the initial condition). We have also included these normalized plots of test error in the revised manuscript. We believe the information conveyed by such a comparison is the rate at which the proposed weight update converges. This has direct implication to the learning problem. \nThe results hold for all activation functions which are once differentiable. We mentioned a particular activation function so as to show detailed derivation with backpropagation for the weight update equation. Looking at equation (8) of the manuscript, it is a requirement to have the activation function \u03c3 such that the partial derivative with respect to weight w exists. All such activation functions are admitted by the proposed theory. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VRgITLy0l2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3441/Authors|ICLR.cc/2021/Conference/Paper3441/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837504, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment"}}}, {"id": "MMp--YKW2lv", "original": null, "number": 4, "cdate": 1605752707262, "ddate": null, "tcdate": 1605752707262, "tmdate": 1605752707262, "tddate": null, "forum": "VRgITLy0l2", "replyto": "OaGQC2Ge0yb", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment", "content": {"title": "Response:  Interesting connections between neural network training and control, as well as control-theoretic analysis of the neural network loss function ", "comment": "Thank you for your time to read and review our manuscript.\ni) The target output is denoted by $y^{\\ast}$ for a given supervised learning task. The output of the neural network is y. The weight updates are such that y tracks $y^{\\ast}$ in finite time. In an analogy with control systems, y stands for the output of the system and $y^{\\ast}$ the commanded signal. In optimization, we try to minimize a cost / performance criteria where the weight update is obtained using equation (13). In our control theory based formulation, we have treated the cost / loss as a Lyapunov function. Then, the weight update is treated as a control signal for the plant (neural network) such that the temporal derivative of the Lyapunov function is always negative definite. Hence, the optimization is achieved via proper control synthesis.\nii) The analysis in Section 2.1 corresponds to the case of a single neuron. The motivation behind providing that analysis is to demonstrate the main concept of finite-time convergent learning. This motivates the development of a more complex multi-neuron case.  Of course, the single neuron case is not practically useful. \n Both single and multi neuron case analysis hold for multiple data points. Theoretically, both theorems admit multiple data points. Empirically, we demonstrate this for the single neuron case in section 3 Figure 2, where we trained the single neuron case with Iris dataset (80 data points in training). For multi-neuron case, we use larger datasets and show that the analysis works for multiple data points (Page 9, figure 4). In case we have misunderstood what was meant by a single datapoint (we assume it means one input, output pair in the dataset), we welcome the reviewer to clear our misunderstanding.\niii) We have included the corresponding test results in the modified submission in Figure 1, 2, and 3."}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VRgITLy0l2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3441/Authors|ICLR.cc/2021/Conference/Paper3441/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837504, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment"}}}, {"id": "jycZMY-K06E", "original": null, "number": 3, "cdate": 1605752583196, "ddate": null, "tcdate": 1605752583196, "tmdate": 1605752583196, "tddate": null, "forum": "VRgITLy0l2", "replyto": "YzWdj58SZDr", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment", "content": {"title": "Response: A priori guarantees of finite-time convergence for Deep Neural Networks", "comment": "Thank you for your time to read and review our manuscript.\n Most of the real world datasets or tasks have a defined range of input values. For example, image values range from 0 to 255. Also, when the input is provided to the network, it is usually normalized, hence the boundedness of assumption holds true for a wide range of tasks and datasets. \nWe have included graphs on test data and accuracy in the revised manuscript.  \nRegarding classification tasks, yes these results can be extended to classification tasks as well. However, it is needed to identify suitable Lyapunov functions that result in a continuous finite-time update. Certainly, more work is needed before the presented results become applicable for classification tasks. \nFrom a control theoretic perspective, a more aggressive controller will usually produce a large overshoot while tracking a command input. In the present case, the weight update is being treated as the controller. Hence, setting the gains $k_{ij}$ large will result in oversensitive training. Setting \u237a\u226a1 close to zero will also result in a highly non-Lipschitz training update. These scenarios correspond to overfitting. This can be seen, for example, in the limiting case (not covered by theory) in appendix A where \u237a=0 causes the oscillations in training update due to limitations of explicit Euler discretization. It is therefore recommended to have a judicious balance between the required convergence rates and the ensuing overfitting of the network.\n\nWe agree that the bounds are conservative in nature. This is due to the fact that the upper bound on settling time is computed using Lyapunov derivative. In future work, we plan to show how to derive less conservative bounds.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VRgITLy0l2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3441/Authors|ICLR.cc/2021/Conference/Paper3441/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837504, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment"}}}, {"id": "9knqT2qPOdY", "original": null, "number": 2, "cdate": 1605167481141, "ddate": null, "tcdate": 1605167481141, "tmdate": 1605167481141, "tddate": null, "forum": "VRgITLy0l2", "replyto": "9jmFVevRyoy", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment", "content": {"title": "Discussion on novelty and discretization", "comment": "Thank you for a detailed reading and review of our work. We appreciate your comments. \n\nAs for the title of the comment, indeed, the very idea of Lyapunov functions for continuous finite-time optimization is not new. We do not intend to claim it as our contribution. We also agree that we have not identified a new Lyapunov function. We have stated on the second page that our focus is on posing the learning problem as a control theoretic problem where existing Lyapunov theory is utilized. Perhaps the novelty claim made in Introduction is misleading and we agree to change it as follows: \u201cThe novelty lies in the fact that the weight update is cast as a finite-time control synthesis such that the loss function is proven to be a valid Lyapunov function\u201d. Thank you for this comment.\n\nHowever, we do stand by our claim that Deep Neural Networks have not been rigorously studied from a control theory perspective. In fact, the focus of the ICML 2020 paper being cited in your comment is on continuous and discontinuous differential equations and inclusions arising during optimization of cost functions. The main focus also seems to be on discretization. Theoretically, the discontinuous case in our results arises only when alpha=0 when the differential inclusion has to be considered in the sense of Filippov\u2019s definition . We would like to stress that our theorems do not cover this case. The solutions of the differential equations are understood as defined in Bhat and Bernstein (SIAM, 2000). Of course, with very small values of 0<alpha<1, it is well known that the right hand side of the differential equation becomes non-Lipschitz and numerical methods may not give a solution that matches the corresponding analytical one especially in the presence of disturbances. Hence, we do not understand the comment why our results are incorrect for the case when alpha is nonzero.     \n\nAs for the comment on superfluousness of continuous time optimization, we would like to point out a few references that help bridge the divide between the continuous and discrete-time analysis that the comment alludes to. We would also like to point out the latest advances in discretization algorithms for discontinuous cases (alpha=0). The implicit numerical schemes reported by Vincent Acary, Bernard Brogliato (\u201cImplicit Euler numerical scheme and chattering-free implementation of sliding mode systems\u201d in Systems and Control Letters, 2010) prove that the analytical and numerical solutions match after a finite number of samples at least for the case when there is no disturbance in control system. This result also extends to the multivariable case. These results relate to differential inclusions such as equation (8) in the cited ICML 2020 paper. Another recent relevant result on implicit numerical schemes is given by Brogliato et. al. (The Implicit Discretization of the Super-twisting Sliding-Mode Control Algorithm in IEEE Transactions on Automatic Control, 2020). As for the catch mentioned on explicit discretization, explicit Euler discretization results are given by Barbot et. al. (\u201cDiscrete differentiators based on sliding modes\u201d in Automatica, 2020) for a discontinuous case arising in sliding modes. This reference establishes optimal accuracy asymptotics of their continuous-time counterparts. This reference also encompasses continuous non-Lipschitz right hand sides. In the presence of these results, we do not agree that it is superfluous to apply continuous finite-time methods to the case of DNN when several discretization methods are available, including the one proven in ICML 2020 reference.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VRgITLy0l2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3441/Authors|ICLR.cc/2021/Conference/Paper3441/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837504, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Comment"}}}, {"id": "9jmFVevRyoy", "original": null, "number": 1, "cdate": 1605146341787, "ddate": null, "tcdate": 1605146341787, "tmdate": 1605146341787, "tddate": null, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Public_Comment", "content": {"title": "The results of finite-time optimization using signed first order flows and Lyapunov function costs are not novel, sorry !", "comment": "This is a nice attempt to apply results of continuous optimization in finite-time to the case of DNNs\u2019 training.\n\nHowever, the authors are invited to compare this work to the recent work Romero et al . ICLM 2020 ( https://proceedings.icml.cc/static/paper_files/icml/2020/4879-Paper.pdf) on the subject of continuous finite-time optimization. Indeed, it appears to me that the Lyapunov cost that the authors are arguing to be novel is simply the Lyapunov function used in this ICML paper (see Proof of Theorem 1 sketch ), where the function $f$ is replaced with the output of the DNN in this particular case. The optimization flow its self is very similar to the signed flow introduced in this ICML paper, referred to as q-SGF, leading to similar finite-time convergence results.\n\nBesides, it also appears to me that the theoretical analysis in this submission is incorrect due to the potential discontinuity of the optimization flow. Indeed, the authors are clearly stating that the acceleration (in continuous time) observed numerically is due the \u2018aggressive\u2019 discontinuous flow. Well, that might be true, but that discontinuity needs to be carefully studied, since the argument that the authors are using in their Lyapunov analysis is only valid for Lipschitz continuous flows. For discontinuous flows, one must use the notion of differential inclusion for example, and the associated Lyapunov theory, please refer to the supplementary material of the ICML paper cited above (can also be found in the more general version of the work, which includes the case of time varying cost functions at  https://www.merl.com/publications/docs/TR2020-088.pdf ).\n\nFinally, it seems to me that the notion of continuous time optimization is rather superfluous in the context of DNNs due to their large scale. Indeed, one cannot expect to use a stiff ODE solver to be able to solve the discontinuous flows with the very high dimensions associated with DNNs. As such this work is not suitable for such application, and a proper discretization scheme is needed for that. The catch is, it is far from proven that any explicit discretization will lead to the same finite-time convergence result. "}, "signatures": ["~Mouhacine_Benosman1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Mouhacine_Benosman1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VRgITLy0l2", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/Authors", "ICLR.cc/2021/Conference/Paper3441/Reviewers", "ICLR.cc/2021/Conference/Paper3441/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024955052, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Public_Comment"}}}, {"id": "eYO6UnyXyLC", "original": null, "number": 2, "cdate": 1603759999193, "ddate": null, "tcdate": 1603759999193, "tmdate": 1605024000191, "tddate": null, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Review", "content": {"title": "Review", "review": "This work studies the finite-time convergence for neural networks. In particular, it tries to recast the problem of training neural networks as a control problem. Supervised learning is then reformulated as a non-linear control problem with a Lyapunov based loss. The weight update is then transformed to be the control inputs. Finally, convergence results are obtained with standard theory from non-linear systems.\n\nOverall, connecting neural networks with classical control theory is an interesting direction. However, results presented in this paper seems limited, and it is not clear what contributions the current work really bring to the community.\n(1) there does not seem to be enough innovation in this paper. To me, the result simply follows from the classical control theory. The authors simply try to mimic the theory by having a candidate Lyapunov loss and continuous weight update equations. It is not clear why these are used for neural networks at the first place; rather, it seems that these are only applied for the sake of proving some technical results. For example, does the candidate Lyapunov loss actually generalize better (theoretically or empirically)? what's the property of it? Howe does it compare to traditional loss function? It is not convincing for me why someone should use it for training neural networks. It seems to be an artifact used solely for the theorem.\n(2) relatedly, the experiments focus on plotting the training loss of Lyapunov loss and l1/l2 loss. From my perspective, this is not informative. The loss function is likely not on the same scale; it is probably better to plot a \"normalized\" version so that the comparison indeed makes sense. Again, such a comparison does not reveal any interesting property about the new loss, e.g.., generalization/testing error? \n(3) Please clarity to what extent, the results hold with respect to different activation function. In Section 2.1, it is explicitly mentioned that sigmoid activation is used. In Section 2.2, the authors use the same notation \\sigma. Does the result hold for other common activation functions? If not, any comments on the difficulties?\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075768, "tmdate": 1606915763716, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3441/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Review"}}}, {"id": "YzWdj58SZDr", "original": null, "number": 4, "cdate": 1604048778713, "ddate": null, "tcdate": 1604048778713, "tmdate": 1605024000047, "tddate": null, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Review", "content": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "review": "The paper aims to make strides towards a theoretical understanding of Deep neural networks, which remains elusive to date. This paper uses a control theoretic formulation to analyze the convergence rate of deep neural networks. More specifically, a Lyapunov based analysis of the loss function is used to derive a priori upper bound on the settling time of a restricted set of fully connected neural network architectures with some assumptions on the input space. \n\nI'm interested to know, for what kind of real-world tasks or datasets is their assumption on the boundedness of the input valid?\nAlthough the proposed Lyapunov loss provides the possibility of analyzing convergence guarantees a-priori, how does this affect the performance of the underlying model on test data? \n\nThe paper provides experiments supporting their theoretical claims for MLPs on a regression task and a single neuron on a classification task. They show that their proposed Lyapunov loss converges faster than the L1 and L2 losses, and faster than the a-priori upper bound. Can a similar loss function for MLPs on classification tasks be easily derived? In other words, do these results easily extend to classification tasks?\n\nAnd what effect does the new loss have on overfitting?\n\nI'm a bit confused by the theoretical upper bound. The derived upper bounds in Table 1 are orders of magnitude higher than the actual time taken, even with the L1 and L2 losses. What does this mean? What's the use of the upper bound in this case?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075768, "tmdate": 1606915763716, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3441/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Review"}}}, {"id": "tYlEQok6A6P", "original": null, "number": 1, "cdate": 1603675835075, "ddate": null, "tcdate": 1603675835075, "tmdate": 1605023999985, "tddate": null, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "invitation": "ICLR.cc/2021/Conference/Paper3441/-/Official_Review", "content": {"title": "Using Lyapunov function to model training is interesting, but scalability of the results is an issue.", "review": "The authors in this paper make an attempt in providing finite time convergence guarantees of the training process of neural networks, using ideas from control theory.  The loss function in the training process is framed as a Lyapunov function.  The training process at each time step is seen as  assigning dynamics to the Lyapunov function over time. The convergence of which can then be analyzed using standard control theoretic techniques. \n\nFor some fixed input - output target, the idea is to come up with a weight update rule which guarantees the convergence rate with some assumptions on the inputs. This  is the novelty in the paper. The extension to multi-layer case is  an extension of the back propagation algorithm. \n\nThough the above is an interesting contribution in itself,  I am not convinced that the results for a fixed input case would generalize well to the batched input case. Which in my opinion is more general,  and has enabled the training of large scale neural networks. The authors have analyzed the robustness to perturbations  in Section 2.4. Specifically Eqn 22. Where the authors have bounds on the perturbation limits, under which it can still guarantee convergence rates. For the batched case it might need some restrictions on the choice of samples in a batch. \n\nThe next concern I have is the experiments definitely looks very insufficient. The current experiments include much smaller datasets. I would be interested to see how this technique performs on some of the larger  neural network architectures. Since convergence guarantees become more important, only when the time it takes to train a network is much longer.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3441/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3441/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A priori guarantees of finite-time convergence for Deep Neural Networks", "authorids": ["~Anushree_Rankawat1", "~Mansi_Rankawat1", "harshal.oza@sot.pdpu.ac.in"], "authors": ["Anushree Rankawat", "Mansi Rankawat", "Harshal B. Oza"], "keywords": [], "abstract": "In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rankawat|a_priori_guarantees_of_finitetime_convergence_for_deep_neural_networks", "supplementary_material": "/attachment/e1893e93ceef09cc35352ac08cd2bdb4973a250b.zip", "pdf": "/pdf/4781058fdde8bdc83bf67fb9a420d558f2b8e991.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b8nJ5iUubT", "_bibtex": "@misc{\nrankawat2021a,\ntitle={A priori guarantees of finite-time convergence for Deep Neural Networks},\nauthor={Anushree Rankawat and Mansi Rankawat and Harshal B. Oza},\nyear={2021},\nurl={https://openreview.net/forum?id=VRgITLy0l2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VRgITLy0l2", "replyto": "VRgITLy0l2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3441/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075768, "tmdate": 1606915763716, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3441/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3441/-/Official_Review"}}}], "count": 14}