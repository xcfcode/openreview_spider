{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396370187, "tcdate": 1486396370187, "number": 1, "id": "BkqAifI_l", "invitation": "ICLR.cc/2017/conference/-/paper123/acceptance", "forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.\n \n Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc). \n \n Paper would be strengthened by a better exploration of the problem."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396370669, "id": "ICLR.cc/2017/conference/-/paper123/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396370669}}}, {"tddate": null, "tmdate": 1481935968271, "tcdate": 1481935968271, "number": 3, "id": "Sy_P2-fNx", "invitation": "ICLR.cc/2017/conference/-/paper123/official/review", "forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "signatures": ["ICLR.cc/2017/conference/paper123/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper123/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "The findings of applying sparsity in the backward gradients for training LSTMs is interesting. \n\nBut the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. \n\nAlso actual justification of the gains in terms of speed and efficiency would make the paper much stronger.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512691008, "id": "ICLR.cc/2017/conference/-/paper123/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper123/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper123/AnonReviewer2", "ICLR.cc/2017/conference/paper123/AnonReviewer3", "ICLR.cc/2017/conference/paper123/AnonReviewer1"], "reply": {"forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512691008}}}, {"tddate": null, "tmdate": 1481934834626, "tcdate": 1481934834626, "number": 2, "id": "SJjgObfNe", "invitation": "ICLR.cc/2017/conference/-/paper123/official/review", "forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "signatures": ["ICLR.cc/2017/conference/paper123/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper123/AnonReviewer3"], "content": {"title": "Detailed analysis/implementation needed", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.\n\nMinor note:\nThe LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.\n\nThe paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.\n\nWhile the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?\n\nAt present this is an interesting technical report and I would like to see more detailed results in the future.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512691008, "id": "ICLR.cc/2017/conference/-/paper123/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper123/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper123/AnonReviewer2", "ICLR.cc/2017/conference/paper123/AnonReviewer3", "ICLR.cc/2017/conference/paper123/AnonReviewer1"], "reply": {"forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512691008}}}, {"tddate": null, "tmdate": 1481868815192, "tcdate": 1481868815192, "number": 1, "id": "H1wz8bbEl", "invitation": "ICLR.cc/2017/conference/-/paper123/official/review", "forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "signatures": ["ICLR.cc/2017/conference/paper123/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper123/AnonReviewer2"], "content": {"title": "Review: Training Long Short-Term Memory with Sparsified Stochastic Gradient Descent", "rating": "4: Ok but not good enough - rejection", "review": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d, https://arxiv.org/abs/1609.07061\n\nOtt et al, \u201cRecurrent Neural Networks With Limited Numerical Precision\u201d, https://arxiv.org/abs/1608.06902 \n\nLow-precision arithmetic for recurrent networks promises to improve both training and inference efficiency. How does the proposed sparsification method compare with low-precision arithmetic? Are the ideas complementary?\n\nEXPERIMENTS\nThe main experimental result of the paper (Section 4.1) is that training LSTM language models with sparse gradients does not affect convergence or final performance (Figure 5). This result is promising, but I do not think that this single experiment is enough to prove the utility of the proposed method.\n\nI also have some problems with this experiment. Plotting validation loss for character-level language modeling is not a standard way to report results; it is much more typical to report bits-per-character or perplexity on a held-out test set. These experiments also lack sufficient details for replication. What optimization algorithm, learning rate, and regularization were used? How were these hyperparameters chosen? Is the \u201ctruncated Wikipedia dataset\u201d used for training the standard text8 dataset? In addition, the experiments do not compare with existing published results on this dataset.\n\nIn the OpenReview discussion, the authors remarked that the \u201cThe final validation loss for the sparsified model is [...] almost the same as the baseline.\u201d Comparing validation loss at the end of training is not the proper way to compare models. From Figure 5, it is clear that all models achieve minimal validation loss after around 10k iterations, after which the validation losses increase, suggesting that the models have slightly overfit the training data by the end of training.\n\nIn Section 4.2 the authors claim to obtain similar experimental results with other network architectures, on other datasets (tiny-shakespeare and War and Peace), and for other tasks (image captioning and machine translation). However, the details and results of these experiments are not included in the paper, making it difficult to assess the utility of the proposed method and the significance of the results.\n\nENERGY EFFICIENCY AND TRAINING SPEED\nOne of the main claims of the paper is that sparse gradients can be exploited in hardware to reduce the training speed and improve the energy efficiency of recurrent network training, but these benefits are neither quantified nor demonstrated experimentally. Even without actually implementing custom hardware, would it be possible to estimate the expected improvements in efficiency through simulation or other means? Such results would significantly strengthen the paper.\n\nGRADIENT TRUNCATION AS REGULARIZATION\nIn Figure 5 all models appear to reach a minimum validation loss at around 10k iterations and then overfit; at this point the Low model achieves even lower loss than the baseline. This is an interesting experimental result, but it is not discussed in the paper. Perhaps a low truncation threshold acts as a weak regularizer to prevent overfitting? Is this a general phenomenon of training recurrent networks with sparse gradients, or is it just a quirk of this particular experiment? This idea deserves more investigation, and could strengthen the paper.\n\nSUMMARY\nThe core idea of the paper (thresholding gradients to induce sparsity and improve efficiency of RNN training) is interesting and practically useful, if a bit incremental. Nevertheless with thorough and deliberate experiments quantifying the tradeoffs between task performance, training speed, and energy efficiency across a variety of tasks and datasets, this simple idea could be the core of a strong paper.\n\nUnfortunately, as written the paper provides neither theoretical arguments nor convincing experimental results to justify the proposed method, and as such I do not believe the paper is ready for publication in its current form.\n\nPROS\n- The proposed method is simple, and seems to be a promising direction for improving the speed of training recurrent networks.\n\nCONS\n- No discussion of prior work on low-precision recurrent networks\n- Experimental results are not sufficient to validate the method\n- Many experimental details are missing\n- Results of key experiments (image captioning and machine translation) are missing\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512691008, "id": "ICLR.cc/2017/conference/-/paper123/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper123/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper123/AnonReviewer2", "ICLR.cc/2017/conference/paper123/AnonReviewer3", "ICLR.cc/2017/conference/paper123/AnonReviewer1"], "reply": {"forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512691008}}}, {"tddate": null, "tmdate": 1480794294268, "tcdate": 1480794294261, "number": 2, "id": "ry0hlixXe", "invitation": "ICLR.cc/2017/conference/-/paper123/public/comment", "forum": "HJWzXsKxx", "replyto": "SkwEAIkXg", "signatures": ["~Maohua_Zhu1"], "readers": ["everyone"], "writers": ["~Maohua_Zhu1"], "content": {"title": "Speedup expected to show in future work", "comment": "Thanks for the questions!\n\n1) In this paper we only present the sparsifying method, which induces a considerable amount of sparsity in the LSTM training process. High sparsity has been demonstrated crucial to improve the training speed and energy efficiency of hardware accelerators, as said in the paper. We are now developing a hardware design to exploit the sparsity and will deliver it in our future work.\n\n2) We have tested the final models trained by our proposed sparsified SGD and they behave almost the same as the baseline. For example, we fed the character-based language model with input strings \"though\", \"peac\", \"stron\", \"tha\" and both the sparsified model and the baseline model output \"thought\", \"peace\", \"strong\" and \"that\". The sparsified model is trained with a threshold of 1e-7, which exhibits more than 70% sparsity during the training. The final validation loss for the sparsified model is 1.47013, which is almost the same as the baseline (1.47021). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287718337, "id": "ICLR.cc/2017/conference/-/paper123/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJWzXsKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper123/reviewers", "ICLR.cc/2017/conference/paper123/areachairs"], "cdate": 1485287718337}}}, {"tddate": null, "tmdate": 1480792489627, "tcdate": 1480792489622, "number": 1, "id": "HJzntqgQx", "invitation": "ICLR.cc/2017/conference/-/paper123/public/comment", "forum": "HJWzXsKxx", "replyto": "SJSoFXJml", "signatures": ["~Maohua_Zhu1"], "readers": ["everyone"], "writers": ["~Maohua_Zhu1"], "content": {"title": "Data for other experiments", "comment": "Thanks for your question!\n\nThe experimental results for sensitivity tests and other applications show the same conclusion as the data in the paper, as we say in the paper.\nFor example, language models with 2, 3, 6, and 9 LSTM layers (128 LSTM cells each) exhibit 9.92%, 10.62%, 27.96% and 16.09% sparsity with standard SGD algorithm. By applying a threshold of 1e-6, we can achieve 82.84%, 84.62%, 83.09% and 99.92% sparsity respectively, without performance loss. If we increase the threshold further to 1e-5, the performance loss will be unacceptable. \nSimilarly, a 3-layer RNN language model with 128, 256 and 512 cells each layer exhibit 84.62%, 91.19% and 96.76% sparsity given a threshold of 1e-6 without performance loss. \nOur sparsifying method is also insensitive to the sequence length of training data. \n\nFor other applications, for example, neuraltalk2 (https://github.com/karpathy/neuraltalk2), we can achieve 71.5% sparsity by a threshold of 1e-6. And by increasing the threshold to 5e-6, the sparsity will be more than 80%. However, when we apply 1e-5 as threshold, the validation loss will be significantly higher than the baseline. \nIn seq2seq (https://www.tensorflow.org/versions/r0.12/tutorials/seq2seq/index.html), the sparsity in this encoder-decoder model is considerably high even in the baseline (~40%). We can achieve a sparsity more than 80% by applying a threshold of 1e-5 without performance loss. \n\nAll these data points are supportive to our sparsified SGD method. Due to the recommended 8-page limit, we saved them for more space for discussion.\nIf the data points are interesting to you, please feel free to contact me (maohuazhu@ece.ucsb.edu) for more details about the experiments :)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287718337, "id": "ICLR.cc/2017/conference/-/paper123/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJWzXsKxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper123/reviewers", "ICLR.cc/2017/conference/paper123/areachairs"], "cdate": 1485287718337}}}, {"tddate": null, "tmdate": 1480711727360, "tcdate": 1480711727355, "number": 2, "id": "SkwEAIkXg", "invitation": "ICLR.cc/2017/conference/-/paper123/pre-review/question", "forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "signatures": ["ICLR.cc/2017/conference/paper123/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper123/AnonReviewer1"], "content": {"title": "training speed comparisons and final model performance", "question": "1) Any results to show the sparsified SGD does speedup the training? \n\n2) How does the final model perform on testing sets, not just the losses? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959450197, "id": "ICLR.cc/2017/conference/-/paper123/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper123/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper123/AnonReviewer3", "ICLR.cc/2017/conference/paper123/AnonReviewer1"], "reply": {"forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959450197}}}, {"tddate": null, "tmdate": 1480698313458, "tcdate": 1480698268849, "number": 1, "id": "SJSoFXJml", "invitation": "ICLR.cc/2017/conference/-/paper123/pre-review/question", "forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "signatures": ["ICLR.cc/2017/conference/paper123/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper123/AnonReviewer3"], "content": {"title": "Results of Section 4.2", "question": "Why are detailed results from the sensitivity, captioning and translation experiments not provided?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959450197, "id": "ICLR.cc/2017/conference/-/paper123/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper123/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper123/AnonReviewer3", "ICLR.cc/2017/conference/paper123/AnonReviewer1"], "reply": {"forum": "HJWzXsKxx", "replyto": "HJWzXsKxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper123/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959450197}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478377947360, "tcdate": 1478238985523, "number": 123, "id": "HJWzXsKxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJWzXsKxx", "signatures": ["~Maohua_Zhu1"], "readers": ["everyone"], "content": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "pdf": "/pdf/a26baac5dcca7219fca1435ab9342312c2db22be.pdf", "TL;DR": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "paperhash": "zhu|training_long_shortterm_memory_with_sparsified_stochastic_gradient_descent", "keywords": ["Optimization", "Deep learning"], "conflicts": ["nvidia.com", "ucsb.edu"], "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}