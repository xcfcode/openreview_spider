{"notes": [{"id": "sFDJNhwz7S", "original": "A5lfd7KRpa", "number": 2021, "cdate": 1601308222691, "ddate": null, "tcdate": 1601308222691, "tmdate": 1614985667781, "tddate": null, "forum": "sFDJNhwz7S", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zhTLfWu6ftU", "original": null, "number": 1, "cdate": 1610040491007, "ddate": null, "tcdate": 1610040491007, "tmdate": 1610474096883, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Thanks for your submission to ICLR.\n\nThis paper presents an extension to Deep Hashing Networks that utilizes angular similarity, and show improved results using the proposed method.  The reviewers were somewhat mixed on this paper, with two of three reviewers on the negative side.  Some reviewers appreciated that the paper was easy to follow and well written, though one reviewer felt that the paper's writing and presentation could improve.  A big concern about the paper expressed by multiple reviewers was that the paper was incremental, in that the main architectural difference seemed to be a change in loss function over existing work.  Unfortunately, the reviewers were fairly unresponsive to attempts to get them to respond to the rebuttals offered by the authors.  \n\nUltimately, I took a look at the paper and found it to be borderline.  I do think the contribution is a bit limited, particularly as it is in an area which has seen many papers over the years (and thus has a high bar for new work).  However, with some additional work this paper could definitely be acceptable.  I think it could use an additional round of editing and review, and I'd encourage the authors to submit this paper to another venue."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040490991, "tmdate": 1610474096867, "id": "ICLR.cc/2021/Conference/Paper2021/-/Decision"}}}, {"id": "RIcmxqgcP9n", "original": null, "number": 10, "cdate": 1605642568750, "ddate": null, "tcdate": 1605642568750, "tmdate": 1605642568750, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "tvQA_5UB8v6", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment", "content": {"title": "Updates on theoretical justification", "comment": "Please see sections 3.3 and A.2 for theoretical justifications for angular similarity."}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "sFDJNhwz7S", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2021/Authors|ICLR.cc/2021/Conference/Paper2021/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853134, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment"}}}, {"id": "V521Tvd8wxu", "original": null, "number": 9, "cdate": 1605642432048, "ddate": null, "tcdate": 1605642432048, "tmdate": 1605642432048, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "f-OHYZXPKO4", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment", "content": {"title": "Study on Hard Negatives", "comment": "Please see Section A.1 in appendix for a study on the impact of hard negatives.  We note that DHN model improves in the scenario when hard negatives are removed."}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "sFDJNhwz7S", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2021/Authors|ICLR.cc/2021/Conference/Paper2021/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853134, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment"}}}, {"id": "rwz177xDRfb", "original": null, "number": 8, "cdate": 1605641689741, "ddate": null, "tcdate": 1605641689741, "tmdate": 1605641689741, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "vMvqFO74ypT", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment", "content": {"title": "Updates as requested", "comment": "Please see Sections 3.3, A.2 for updates on the theoretical justifications of the model, and Section A.1 for the ablation study and impact of hard negatives."}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "sFDJNhwz7S", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2021/Authors|ICLR.cc/2021/Conference/Paper2021/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853134, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment"}}}, {"id": "6dwcsY1a2-", "original": null, "number": 7, "cdate": 1605641455363, "ddate": null, "tcdate": 1605641455363, "tmdate": 1605641455363, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "uyJIEIlys1", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment", "content": {"title": "Update with Ablation Study", "comment": "We have updated the paper to include a hyperparameter sweep which is modifying the weight of hard negatives, rather than all negatives.  We see that DHN performance improves considerably when hard negatives are removed.  We use this parameter sweep for an ablation study, showing the performance of each model under no hard negatives, K=1, and \\lambda=0, respectively.  Please see Section A.1 in the appendix for details."}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "sFDJNhwz7S", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2021/Authors|ICLR.cc/2021/Conference/Paper2021/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853134, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment"}}}, {"id": "A7w0XKRsgpU", "original": null, "number": 6, "cdate": 1605163972068, "ddate": null, "tcdate": 1605163972068, "tmdate": 1605163972068, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "tvQA_5UB8v6", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment", "content": {"title": "Rebuttal to R4: We believe \"representations for exact collision only retrieval\" problem hasn't been solved before.  Solutions here open doors to many novel applications, making for high impact potential", "comment": "Thank you for your review and your time.\n\nWe vehemently disagree that the contribution is incremental.  There are three aspects to the contributions of our work\n1) Potential/Novelty\nTo our knowledge the \"exact hashing representations problem\" has not been seriously studied before, nor promising results shown. This has many potential applications and thus we belive high impact.  These applications are not simply limited to replacing ANN datastructures with hashing in IR systems, although that is one such application.\n 1a) This methodology can be be used as a nonparametric clustering/factorization method which\ncan help practitioners reason about large scale data -- discrete clusters/representations are generally more interpretable to humans.  The size of each cluster is determined by the data -- giving another advantage in applications for data understanding.\n 1b) Exact hashing representations have potential to be used as intermediate representations in larger models for solving ANN problems without the need of intermediate searches.  For example, a multi-task scenario where hashes are learned based on a similarity label in one task and other targets are learned conditioned on these hashes in the other tasks.  The summarization achieved by the hashes enables simpler layers downstream.\n1c) Retrieval algorithms storing only hashes are considerably more memory efficient (1 bit vs 1 double is a factor of 64 savings of memory). Thus exact hashing representations can help in scaling applications for both large scale retrieval systems, or can allow retrieval systems to run more easily on lightweight devices.\n2) Algorithmic Contributions\nWhile each of these is fairly simple, each contribution complements the others and the end result is high quality semantic hashes\n2a) Angular Similarity - We believe section 3.3 demonstrates angular similarity is a natural choice for hashing applications and perhaps better than existing choices\n2b) Concentration Param (K)- All methods perform better when introducing this parameter\n2c) Hard Negative Samples - Negative mining is common practice, but we found it to be very important for the success of exact hashing methods.  We provide a simple strategy for negative mining which works well in both synthetic and OSCAR datasets.\n2d) Multiple-Hash model - For data where items may not fall naturally into single categories, multiple hashes as in the OSCAR expt can be used for \"factorization\" type models\n3) Problem Space Contributions\n3a) Large number of groups - Prior work has focused small number of semantic groups for evaluation.  We needed to include the K param for baselines for them to perform well in this regime\n3b) Noisy Label - Prior work has focused on data with supervised labels, used to create a block diagonal matrix as dataset.  It's not clear similarity learning is needed at all in clean-label case (why not use supervised learning, eg)\n\n\nWe fixed many of the typos/etc. For the BCE typo, the sign of the first and second term of the BCE didn't match (they should both be negative as BCE is a Negative LL)\n\nCan you be more specific when you say word2hashes is not novel?  Do you mean exact hashing representations of words?  We are not aware of any work using exact hashing retrieval (ie **only exact collisions used in retrieval**) for words.  If you know of some relevant work, would you please share it?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "sFDJNhwz7S", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2021/Authors|ICLR.cc/2021/Conference/Paper2021/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853134, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment"}}}, {"id": "vCPNc9ZuHat", "original": null, "number": 5, "cdate": 1605163414562, "ddate": null, "tcdate": 1605163414562, "tmdate": 1605163414562, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "f-OHYZXPKO4", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment", "content": {"title": "Rebuttal for R2: Additional plot in Fig 1 to show fundamental difference of DHN/logistic based similarity, expanded grid search", "comment": "Thank you for your review and your time.\n\nModels trained using Angular similarity have a stronger inductive bias (see Fig 1) and thus can more easily handle noisy data. Appropriate negative sampling is also important for handling label noise, see Appendix (Fig 4) which may give some intuition on why our method of negative sampling can help to distinguish strongly connected groups from weakly coupled groups.\n\nHR metric on the test set for the OSCAR experiment (Table 3) does do sorting by dense embedding.  In addition, Figure 3 in main text and Figures 8 and 9 in Appendix show that there is additional structure for the dense embeddings associated with a single hash.  The remaining metrics for the \"exact hashing experiments\" do not do any ranking with retrieved results.\n\nCould you please elaborate on what you mean that comparison isn't fair?\n\n\nQ1: Expt 2 (Images) is using Hamming Ranking evaluation, on clean label, small # of groups tasks.  This explains why DHN can perform well here.  Metrics in other experiments are not typos.  The new plot in Fig 1 shows that it saturates easily (meaning loss is minimized too easily) resulting in poor performance for exact hashing retrieval case\nQ2: This comparison can be see in Fig 2, 3rd panel -- we increased the size of the plots in the rebuttal revision\nQ3: For Expt 2 we have computed pvalues according to the popular Iman Daveport test (results where significant for all datasets)\nQ4: Our apologies, the number of clusters column had an error in the computation (we didn't filter non-singleton hashes for all methods.)  We've updated with the correct values (which are all similar except DHN.)  Please note we are also reporting Precision which penalizes models producing overly large clusters.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "sFDJNhwz7S", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2021/Authors|ICLR.cc/2021/Conference/Paper2021/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853134, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment"}}}, {"id": "BkwA-r3Zq7G", "original": null, "number": 4, "cdate": 1605162936502, "ddate": null, "tcdate": 1605162936502, "tmdate": 1605162936502, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "vMvqFO74ypT", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment", "content": {"title": "Rebuttal for R5 - Added  theoretical analysis of Angular similarity and it's strong connection to hashing", "comment": "Thank you for your review and your time.\n\nWe've added a section (3.3) demonstrating connections between various similarities and probabilistic statements about hashing collisions.  In addition, we show a connection between angular distance/SimHash and hamming distance.  While these results do not definitively show that angular similarity will necessarily outperform other methods, we believe they do show it is a natural choice when training models for hashing based representations, with better analytical properties than existing choices.\nWe have also added an additional plot in Figure 1 showing that Angular distance does indeed induce a sharper topology to baseline distances.  \nThis figure also shows the logistic similarity.  As can be seen, the inverse temperature parameter for the logistic similarity would need to be near 10^-6 to achieve the same behavior as angular similarity near the saturation point, however as the range of cosine similarity is bounded between -1 and 1, the saturation point would not be achievable with such a temperature.  For an inner product based input, embeddings would need large norms (10^3 for each) in order to reach the saturation point -- which may be feasible but would require significant information to be contained in the norm of the embeddings.\n\nWe vehemently disagree that the contribution is incremental.  There are three aspects to the contributions of our work\n1) Potential/Novelty\nTo our knowledge the \"exact hashing representations problem\" has not been seriously studied before, nor promising results shown. This has many potential applications and thus we belive high impact.  These applications are not simply limited to replacing ANN datastructures with hashing in IR systems, although that is one such application.\n 1a) This methodology can be be used as a nonparametric clustering/factorization method which\ncan help practitioners reason about large scale data -- discrete clusters/representations are generally more interpretable to humans.  The size of each cluster is determined by the data -- giving another advantage in applications for data understanding.\n 1b) Exact hashing representations have potential to be used as intermediate representations in larger models for solving ANN problems without the need of intermediate searches.  For example, a multi-task scenario where hashes are learned based on a similarity label in one task and other targets are learned conditioned on these hashes in the other tasks.  The summarization achieved by the hashes enables simpler layers downstream.\n1c) Retrieval algorithms storing only hashes are considerably more memory efficient (1 bit vs 1 double is a factor of 64 savings of memory). Thus exact hashing representations can help in scaling applications for both large scale retrieval systems, or can allow retrieval systems to run more easily on lightweight devices.\n2) Algorithmic Contributions\nWhile each of these is fairly simple, each contribution complements the others and the end result is high quality semantic hashes\n2a) Angular Similarity - We believe section 3.3 demonstrates angular similarity is a natural choice for hashing applications and perhaps better than existing choices\n2b) Concentration Param (K) - All methods perform better when introducing this parameter\n2c) Hard Negative Samples - Negative mining is common practice, but we found it to be very important for the success of exact hashing methods.  We provide a simple strategy for negative mining which works well in both synthetic and OSCAR datasets.\n2d) Multiple-Hash model - For data where items may not fall naturally into single categories, multiple hashes as in the OSCAR expt can be used for \"factorization\" type models\n3) Problem Space Contributions\n3a) Large number of groups - Prior work has focused small number of semantic groups for evaluation.  We needed to include the K param for baselines for them to perform well in this regime\n3b) Noisy Label - Prior work has focused on data with supervised labels, used to create a block diagonal matrix as dataset.  It's not clear similarity learning is needed at all in clean-label case (why not use supervised learning, eg)\n\nPrecision and Recall are defined on the elements that collide together for the 1st and 3rd experiments (this is what we mean by exact hashing retrieval,) for the 2nd experiment we use Hamming ranking to give consistent metrics with regard to prior work.\n\nSome ablations may already be viewed in the tuning plots (specifically \"no quantization loss\" corresponds to \\lambda=0, \"no K param\" corresponds to K=1) -- we included these so that such ablation questions and dependencies on hyperameters can be easily checked.\n\nWe are running an additional tuning experiment where we modify the loss to only change the weight of hard negatives, rather than all negatives. This enables us to more clearly show the impact of hard negatives.  This is a rather expensive grid search; it will take several days."}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "sFDJNhwz7S", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2021/Authors|ICLR.cc/2021/Conference/Paper2021/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853134, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment"}}}, {"id": "uyJIEIlys1", "original": null, "number": 3, "cdate": 1605162258967, "ddate": null, "tcdate": 1605162258967, "tmdate": 1605162258967, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment", "content": {"title": "Paper Updates", "comment": "We have made an update to the paper with several revisions.  We have also included the \"word2hashes\" model hashes as a parquet file in the supplementary material.  Unfortunately the dense versions will not fit in the 100MB file limit\n\nMajor changes:\nAdded section 3.3 - theoretical analysis of Angular similarity as it relates to hashing\nAdditional plot in Fig 1 showing angular similarity induces the sharpest topology among alternatives and thus introduces the strongest preference for near-binary embeddings\nIncreased the range of the OSCAR hyperparameter search (baselines improve but do not surpass LSE model)"}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "sFDJNhwz7S", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2021/Authors|ICLR.cc/2021/Conference/Paper2021/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853134, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Comment"}}}, {"id": "tvQA_5UB8v6", "original": null, "number": 1, "cdate": 1602747064438, "ddate": null, "tcdate": 1602747064438, "tmdate": 1605024305958, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Review", "content": {"title": "This paper proposes an angular-based metric which tries to pull similar objects together and dissimilar ones apart. Three different experiments, i.e., a toy example, an image retrieval task, and a co-occurrence modeling task, demonstrate the benefits of the learned embeddings.", "review": "+++Pros.  \n-----The idea learning embeddings via angular similarity is interesting and important for exact hashing retrievals; and the design of experiments on different domains to validate the proposed method is worth encouraging.\n\n+++Cons.  \n-----This paper just devised the angular similarity, whose contribution is limited for ICLR.  \n-----There are some technical minors or typos, such as the binary cross entropy loss in Eq.~(5) (should be \u201c+\\beta(1-y_{ij})log(\u2026\u2026)\u201d).  \n-----There are several grammar minors, such as \u201c\u2026utilizes cosine similarity to as the crossing layer between the two halves\u2026\u201d in the beginning of Section 3, \u201cWe measure precision, recall and F1-score on with the data generating factions as the target.\u201d in Section 4.1 above Table 2; and some confusing expressions, such as \u201cin Table 4.1.\u201d (there is no Table 4.1.).  \n-----Better replace beta with \\beta, lambda with \\lambda, and keep them identical in texts and figures. And better organize figures with subtitles.  \n-----At the end of each equation marked (1), \u2026, (10), there should be appropriate punctuations, such as commas.  \n-----The authors concluded that \u201cThe learned representations show superior performance in the exact hashing retrieval setting\u201d. But the results in Table 2 does not support it.  \n-----Besides, the authors mentioned that \u2026 a \u201cword2hashes\u201d model which is novel to the best of the authors\u2019 knowledge \u2026, and I do not agree with such statements.  \n-----Actually, I think Figures 4-8 and Table 5 should be regarded as the main texts; and if so, this paper exceeds the page limit.  \n\n+++Conclusions.  \n-----Based on the above pros and cons, I think this paper is interesting, but the contribution is limited; thus, I would make a REJECT recommendation.  \n\n+++Suggestions.  \n-----Need careful writing and presentation.  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105712, "tmdate": 1606915797723, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2021/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Review"}}}, {"id": "f-OHYZXPKO4", "original": null, "number": 2, "cdate": 1603896939904, "ddate": null, "tcdate": 1603896939904, "tmdate": 1605024305886, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Review", "content": {"title": "The authors propose an approach for learning embeddings such that approximate nearest neighbor search becomes an exact hashing problem.  The experiments show that their approach is promising. ", "review": "########\nPros\n########\n\n- The paper is well written and easy to read\n\n- The authors have explained the background, motivation and prior work quite comprehensively\n\n- The idea of learning embeddings such that nearest neighbors have the same hash is interesting. \nBecause of this property, nearest neighbor search becomes a simple hash lookup. \n\n\n\n########\nCons\n########\n\n- The authors claim, \"We extend semantic hashing methods to problems with **substantial label noise**\", but there doesn't appear to be any specific modeling to handle label noise in Section 3.\n\n- In practice, for IR, can this method support a ranking of the items based on query similarity? For example, if for query q, documents A, B, C are relevant in that order (most relevant to least). It appears that this method would all assign the same hash to A, B, C and therefore, retrieve them, but not in any particular order?\n\n- Experiments don't appear to be necessarily a fair comparison that can show the merits of this approach conclusively.\n\n\n########################\nComments / queries\n########################\n\nQ1 Section 4.1,  Table 1 shows very low numbers for DHN. Is that a typo, given that DHN performs quite well in all the other experiments. \n\nQ2. Section 4.1, Table 1: Can you also provide the numbers for LSE without false negatives for fair comparison with the baselines?\n\nQ3. Section 4.2, Table 2: It appears that LSE is able to perform well and beats the baselines in many cases. Can you provide the statistical significance p values?\n\nQ4. Section 4.3, Table 3: LSE has a substantially lesser number of clusters, making it difficult to isolate the merits of the method vs. outcome that is a result of just lesser clusters. E.g. Recall can be better just by returning a larger cluster, which is probably the case if the number of clusters are lesser. Can you provide a fairer comparison, for example, by lowering the number of clusters of the other methods or some other way?\n\n\n########\nTypos\n########\n\nSection 4.2: \"100 images per category it test query set\"\n\nSection 4.2: \"Results are shown in Table 4.1.\" --> Table 2?\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105712, "tmdate": 1606915797723, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2021/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Review"}}}, {"id": "vMvqFO74ypT", "original": null, "number": 3, "cdate": 1604983561820, "ddate": null, "tcdate": 1604983561820, "tmdate": 1605024305807, "tddate": null, "forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "invitation": "ICLR.cc/2021/Conference/Paper2021/-/Official_Review", "content": {"title": "Theory needs to be justified more and some questions on the experiments", "review": "The authors consider the problem of learning a hash function such that semantically similar elements have high collision probability.  They modify the approach Deep Hashing Networks (Zhu et al., 2016) with a new loss function. Rather than use a sigmoid based loss function, the authors argue that a loss function based on angular similarity and SimHash would be better. Specifically, they use the probability of SimHash collisions as a loss function. They then experimentally verify their method on synthetic data from a Stochastic Block Model distribution, image data (CIFAR-10 and ImageNet), and text data (OSCAR). They show improvements over related methods.\n\nOverall, I found this paper to be incremental compared to previous work, such as (Zhu et al., 2016). The theoretical contributions are fairly weak. Why is the relation to SimHash useful? The authors need to better justify their choice of loss function. Additionally, how does the method compare to using a sigmoid loss but changing the temperature parameter so that the loss function doesn't saturate as quickly?\n\nThe authors do improve over related methods in the experiments. However it is not clear if this is due to the choice of loss function or the use of negative mining. The authors should also improve the clarity of how their metrics are defined. Is precision, recall simply based on the elements that collide together?\n\nTo summarize, I think this paper needs to better justify their use of loss function in theory and also perform ablation tests in the experiments before it can be accepted.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2021/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2021/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantic Hashing with Locality Sensitive Embeddings", "authorids": ["~Levi_Boyles1", "~Aniket_Anand_Deshmukh1", "~Urun_Dogan1", "rkoduru@microsoft.com", "cdx@microsoft.com", "~Eren_Manavoglu1"], "authors": ["Levi Boyles", "Aniket Anand Deshmukh", "Urun Dogan", "Rajesh Koduru", "Charles Denis", "Eren Manavoglu"], "keywords": ["Semantic Hashing", "Approximate Nearest Neighbor"], "abstract": "Semantic hashing methods have been explored for learning transformations into binary vector spaces. These learned binary representations may then be used in hashing based retrieval methods, typically by retrieving all neighboring elements in the Hamming ball with radius 1 or 2.  Prior studies focus on tasks with a few dozen to a few hundred semantic categories at most, and it is not currently well known how these methods scale to domains with richer semantic structure.  In this study, we focus on learning embeddings for the use in exact hashing retrieval, where Approximate Nearest Neighbor search comprises of a simple table lookup. We propose similarity learning methods in which the optimized similarity is the angular similarity (the probability of collision under SimHash.)  We demonstrate the benefits of these embeddings on a variety of domains, including a coocurrence modelling task on a large scale text corpus; a rich structure of which cannot be handled by a few hundred semantic groups.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "boyles|semantic_hashing_with_locality_sensitive_embeddings", "one-sentence_summary": "We extend semantic hashing methods to problems with substantial observation noise and to the exact hashing retrieval case; applied to large scale text the method discovers hash clusters for words that are meaningful and outperform baselines.", "pdf": "/pdf/1a9d94dc3a4cf76bea31a4889f4e0e0703bde7a1.pdf", "supplementary_material": "/attachment/369eda6cce825c0b383ece14ae1e7d1fce2e6f49.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g-GOdZap0N", "_bibtex": "@misc{\nboyles2021semantic,\ntitle={Semantic Hashing with Locality Sensitive Embeddings},\nauthor={Levi Boyles and Aniket Anand Deshmukh and Urun Dogan and Rajesh Koduru and Charles Denis and Eren Manavoglu},\nyear={2021},\nurl={https://openreview.net/forum?id=sFDJNhwz7S}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "sFDJNhwz7S", "replyto": "sFDJNhwz7S", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2021/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105712, "tmdate": 1606915797723, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2021/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2021/-/Official_Review"}}}], "count": 13}