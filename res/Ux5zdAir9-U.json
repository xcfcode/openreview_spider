{"notes": [{"id": "Ux5zdAir9-U", "original": "HNXMnKYDaYc", "number": 2236, "cdate": 1601308246282, "ddate": null, "tcdate": 1601308246282, "tmdate": 1614985720793, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "g_T8W3JF-cs", "original": null, "number": 1, "cdate": 1610040420742, "ddate": null, "tcdate": 1610040420742, "tmdate": 1610474019449, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers point out several important issues to be addressed, including comparing to other methods that can address the \"combinatorial generalization\" problems studied (one reviewer points out the crucial difference from \"compositional generalization\" studied before), addressing the gap between the proposed dataset (simple and has the value of diagnosing/model debug/research algorithm development) and real datasets/problem settings.   \n\nAs such the AC recommends Reject and encourages the authors to take the constructive feedback to improve. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040420728, "tmdate": 1610474019432, "id": "ICLR.cc/2021/Conference/Paper2236/-/Decision"}}}, {"id": "wh0Pa6G5PcK", "original": null, "number": 4, "cdate": 1603991247719, "ddate": null, "tcdate": 1603991247719, "tmdate": 1606956476304, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Review", "content": {"title": "An interesting dataset but efficacy not demonstrated", "review": "I liked the idea of GraphLog; it seems like an interesting and potentially very useful dataset. The \"results\" seemed little to do with the dataset (apart from being enabled by the dataset), but were comparing some models that were neither the current state-of-the-art or particularly novel. I could imagine a paper that does a comprehensive reevaluation the state-of-the-art algorithms (unchanged!) using the new data set and then reports on which ones work well for different tasks. Such reproducibility in science is important. However, this is not was done.\n\nI would like to see more discussion about why doing well on the benchmark would translate into doing well on real-world problems. I would hypothesize that the real-world does not consist of a few simple rules chained together, but the underlying dynamics is much more complicated, and agents are trying to concoct a story to get some signal out of the noise. All of your tests assume there is a clean signal -- constructed by your rules (perhaps corrupted by random noise(?)).  You just assume that this will form a good testbed, but it isn't obvious to me.\n\nThe rules all seem overly simplistic. I was expecting your rule generator to generate more complex rules from simpler ones. But it doesn't; they all seem to be just the same complexity.  Here are some things I'm wondering:   By sharing subset of rules, the worlds are structurally similar? Are the entities shared in any meaningful way? I think they are just meaningless names, and so there is no commonalities between the worlds. I think they share relations. Is there any notion that a relation learned in one world are like the relation with the same name in other worlds? You are not learning about relations using diverse populations, or are you?\n\nMinor comments\n\nIn RGCN, $\\times_i$ what is $i$?\n\nFigure 3, I thought was weird. If we divide the datasets into easy medium and hard, don't we always have a nice straight lines when plotted with difficulty? I think it is just trying to say that the E-GAT models work better across difficulties.\n\nIn figure 5, I can't see how/why \"The colors of the bars depict how similar two distributions are\"\n\n\n\nAfter rebuttal:\n\n\nComment:\nI am suspicious of your argument on \"complex due to multiple resolution pathways\".\n\nSurely there is always a (unique) canonical deviation. For example, one could imagine extending Prolog to allow for any literal to be resolved; not just the left one. However that just adds extra complexity. Because we have to resolve all literals, we might as well do in a left-to-right order. Surely in your case, we can always do the leftmost (for example) one. Or the rules can be defined so that we can resolve them left to right.\n\nOne counter to my argument might be (parent, ancestor) -> ancestor parent -> ancestor.\n\nHere we cannot go left-to-right, but need to go right to left. We do not need to search over orderings.\n\nThis issue is faced daily by Prolog programmers; we choose whichever one of (using your notation): (parent, ancestor) -> ancestor (ancestor, parent) -> ancestor works with your engine. I don't see why they get around it without problem and you claim it is not possible. The rules may depend on the order used, but what is the problem with that if it drastically reduces the search space? You don't have to search for all proofs; just one.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100958, "tmdate": 1606915778371, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2236/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Review"}}}, {"id": "QZ3oEXNdXgE", "original": null, "number": 1, "cdate": 1603734243880, "ddate": null, "tcdate": 1603734243880, "tmdate": 1606749952503, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Review", "content": {"title": "Official Blind Review #4", "review": "The authors propose a benchmark for evaluating to which extend GNNs are able to reason in terms of logical/symbolic rules. The results show that standard GNNs are mostly lacking this capability (what is not really surprising). The paper is fairly understandable and the idea is interesting. The design of the benchmark makes sense to me. Since the writing could be more clear/thorough and I have issues with related work/systems that are not considered (details below), I however do not vote for acceptance.\n\n(+) The benchmark consists of various datasets, interesting settings (e.g., multi-task training and continual learning), and proposes a small extension of GAT which improves performance.\n\n(-) Logic Part: \n- There's a mix of first-order logic (FOL) terminology (constants, relations) and Semantic Web terminology (entities, relations). I suggest to use either constant and predicate or entity and relation. In FOL, a relation (the notion the paper uses for predicates) is the interpretation of a predicate.\n-  Rule (1) is in the most typical format considered in rule learning and called a \"chain rule\". You should mention this at least besides dyadic definite datalog, since the latter seems to be not fully covered. Also, other parts of the paper mention unary rules, which are not covered by (1).\n- I do not think assumption (2) is realistic and wonder in how far it helps with the evaluation.\n\n(-) Related Work: The paper mentions several related works, especially benchmarks. However, I do not understand why the authors completely ignore the entire field of rule learning/ILP. While this area is not solely based on deep learning approaches, the research and insights (esp. how to evaluate logical generalization) are relevant. There are even existing rule learning approaches based on deep learning. The paper compares to benchmarks considering image/text data but does not even mention rule-based benchmarks such as the ILP competition or Rudas (see below links). I would not support the paper's claim as it is written: \"GraphLog is the only dataset specifically designed to test logical generalization capabilities on graph data, whereas previous works have largely focused on the image and text modalities.\" The similarities and differences to existing benchmarks (I agree that there are some, e.g., support for continual learning) need to be pointed out.\n- http://ilp16.doc.ic.ac.uk/competition\n- https://github.com/IBM/RuDaS\n\n(-) Experiments: While it is ok to generally focus on GNNs, the evaluation should consider established rule-learning approaches to set the context (e.g., the recently published Open Graph Benchmark also considers standard baselines), in particular, given that the GNNs perform so disappointing. There are competitive rule learning systems such as AMIE and also deep-learning based systems which could serve as baselines. \n\n----------------------------------------------\nSmaller Comments:\n- p.2 What is the difference between generalization and adaptation?\n- I think it might be good to define the notion of \"compositional generalization\" more clearly.\n- Table. 3 caption: I think the definition of rule similarity is important enough to come in the main paper.\n- p.17 ?? Fig ref\n- A.2 \"we ensure that all generated rules are sound and consistent with respect to R.\" - why should a rule not be sound or consistent w.r.t R? \n\n----------------------------------------------\nUpdate after Rebuttal: I have read the other reviews and authors' responses but do no change my scores.\n\nI still share the impression of Reviewer 3 that several choices in the design of the benchmark seem to be very restricted and arbitrary (e.g., the single possible derivation sequence).\n\nRelated work in terms of rule learning has been incorporated partly now, but there is no proper comparison -- especially in terms of the dimensions mentioned in the previous item. Also, there have been proposals similar to the E-GAT model the authors introduce which are completely ignored in the paper (e.g., MEMORY-BASED GRAPH NETWORKS, ICLR 2020; Graph Neural Networks for Social Recommendation, WWW 2019).", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100958, "tmdate": 1606915778371, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2236/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Review"}}}, {"id": "z9uv7vPVWl8", "original": null, "number": 14, "cdate": 1606260862385, "ddate": null, "tcdate": 1606260862385, "tmdate": 1606260862385, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "Paper updated with addressing reviewer concerns", "comment": "We have updated the paper following the concerns from reviewers. The majority of the changes were done to address the following:\n\n- Related Works - incorporated feedback from R2, R1, and R4\n  - Defined scope of compositional generalization\n  - Added a detailed comparison with CLUTRR.\n  - Added a section on Synthetic Graph Generation.\n- Terminologies - incorporated feedback from R3, R1, and R4\n  - Clarified the equation to being a formulation of chain-rule\n  - Added section on Controlling similarity in the main paper\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "S6MADpY0Ca8", "original": null, "number": 13, "cdate": 1606186480268, "ddate": null, "tcdate": 1606186480268, "tmdate": 1606186480268, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "uaUgnaU7OEm", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "Revision updated", "comment": "Many thanks for your feedback, we have updated the draft with the suggested feedback, which includes changes to the Related Works, clearing the scope of generalization, and incorporating changes in terminologies related to chain-rule."}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "uaUgnaU7OEm", "original": null, "number": 12, "cdate": 1606149714355, "ddate": null, "tcdate": 1606149714355, "tmdate": 1606149714355, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "jB6PLYteZM", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "Response to author updates", "comment": "I have read the other reviews and the responses of the authors.\n\nI disagree with the authors statement about deliberately mixing terminology. If you use notions from two overlapping vocabularies (e.g., logical \"constant\" and Semantic Web \"relation\") which differ in meaning, it is confusing for readers who are not familiar with both logic (e.g., relation=interpretation of a logical predicate) and Semantic Web terminology (e.g., relation=a logical predicate).\n\nThe authors mention that they plan to update the draft, but I cannot see a revised version, and several reviewers pointed out parts that need to be revised (e.g., about compositional generalization, related work)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "wxxppSbdcMP", "original": null, "number": 11, "cdate": 1605915085871, "ddate": null, "tcdate": 1605915085871, "tmdate": 1605915085871, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "6sZ7pMioezd", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "Your argument on complexity", "comment": "I am suspicious of your argument on \"complex due to multiple resolution pathways\".\n\nSurely there is always a (unique) canonical deviation. For example, one could imagine extending Prolog to allow for any literal to be resolved; not just the left one.  However that just adds extra complexity. Because we have to resolve all literals, we might as well do in a left-to-right order.  Surely in your case, we can always do the leftmost (for example) one. Or the rules can be defined so that we can resolve them left to right. \n\nOne counter to my argument might be\n(parent, ancestor) -> ancestor\nparent -> ancestor.\n\nHere we cannot go left-to-right, but need to go right to left.  We do not need to search over orderings.\n\nThis issue is faced daily by Prolog programmers; we choose whichever one of (using your notation):\n(parent, ancestor) -> ancestor\n(ancestor, parent) -> ancestor\nworks with your engine. I don't see why they get around it without problem and you claim it is not possible. The rules may depend on the order used, but what is the problem with that if it drastically reduces the search space?  You don't have to search for all proofs; just one."}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "pUu8rEGIaqG", "original": null, "number": 10, "cdate": 1605742281519, "ddate": null, "tcdate": 1605742281519, "tmdate": 1605742299990, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "N9Qe3ujrrp4", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "Response to reviewer queries", "comment": "We thank the reviewer for their prompt response and the useful follow-up questions:\n\n1. Regarding your question about chain queries, we clarify that Graphlog does provide non-chain queries (as a result of subgraph extraction setup). We direct the reviewer to section A2 in the appendix for further details.\n\n2. Further, we use only the chain graph to estimate the lower bound on the complexity of a query graph. The model still has access to the non-chain queries generated by the underlying rules.\n\n3. Due to the stochastic process of generation, cases such as 2i, 3i, pi, and ip are prevalent in abundance in GraphLog.\n\n4. Additionally, GraphLog allows controlling this lower bound on complexity in generation by either increasing the length of the chain graph, or by increasing the number of relations considered, or by increasing the number of rules considered, which dramatically increases the complexity of the task. None of this is explored in query2box. \n----\n\nGAT has been proven to be superior in performance to RGCN in many works in the literature [1,2], so we do not understand why \"biases\" in GraphLog would be any more favorable to EGAT.  One potential reason is the attention mechanism could enable or allow the model to ignore or include certain reasoning pathways in the query graph, whereas for RGCN similar capacity is not built inductively in the model.\n\n[1] https://arxiv.org/abs/1906.01195\n\n[2] https://arxiv.org/abs/1908.06177"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "N9Qe3ujrrp4", "original": null, "number": 9, "cdate": 1605722584457, "ddate": null, "tcdate": 1605722584457, "tmdate": 1605722584457, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "SayKprlp89V", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "More clarity needed", "comment": "Regarding your reasons for focussing on chain rules: All three reasons would also pertain to the kind of queries answered by approaches such as query2box and betaE. That is, these works are also based upon binary predicates, claim to be interpretable, and describe various queries of increasing complexity. However they do not necessarily stick to chain rules. As examples, see queries 2i, 3i, pi, ip etc. queries in Fig 4 in the query2box paper (by Ren et al in ICLR'20). So I remain a bit unclear on your focus on chain rules.\n\nAbout the comparison between the various approaches considered in the paper: Do you think its possible that your results favoring E-GAT over RGCNs is in part due to the biases in GraphLog? Why or why not?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "jB6PLYteZM", "original": null, "number": 8, "cdate": 1605568667860, "ddate": null, "tcdate": 1605568667860, "tmdate": 1605568667860, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "QZ3oEXNdXgE", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "We thank the reviewer for their constructive comments! Please read our rebuttal below.", "comment": "We thank the reviewer for mentioning the points about terminology centered around Logic. The mix in terminology is deliberate as we try to cater to both logic and graph communities - semantic web terminologies are more geared towards a reader from the graph community as opposed to someone having prior exposure to the logic background. We hope our terminology helps to be both general as well as easy to understand for the core graph community.\n\n---\n\nWe thank the reviewer for the related works, we will update these works in the paper and will modify our claim to include the previous works on logical reasoning. Specifically, both RuDas and ILP competition are built for rule learning in the scope of prolog/ILP and do not specifically cater to graph neural networks. We will mention these as alternative datasets that practitioners can possibly use for investigating logical understanding in general.  \n\n---\n\nWe thank the reviewer for making these interesting observations about our experiments. Due to the scope of our paper to evaluate the generalization capabilities of Graph Neural Networks, we chose not to include baselines that are not of the same family of algorithms. The question here is not whether GNNs are the best models for learning this task. In fact, we agree with the reviewer that GNNs are probably not designed to perform well on such tasks. The question more is how graphs perform this task and what are the strengths and weaknesses of the existing GNNs.\n\n---\n\nRegarding smaller comments:\n\n- By generalization we expect the model to perform well on held out patterns of the same rule set. By adaptation, we mean the ability of the model to learn a new distribution on new sets of rules.\n- We will define the scope of compositional generalization more clearly in the draft.\n- We make sure that the rule generation process generates sound and consistent rules, as the generation process is stochastic and without proper checks and balances the generated rules can quickly go inconsistent across different worlds. \n- We will move the definition of rule similarity earlier in the main paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "SayKprlp89V", "original": null, "number": 7, "cdate": 1605568421447, "ddate": null, "tcdate": 1605568421447, "tmdate": 1605568421447, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "a3n5hP_ZZg2", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "We thank the reviewer for the constructive comments! Please read our rebuttal below.", "comment": "We thank the reviewer for raising the point about the bias of chain rules. We imbibe the bias of chain rule due to three reasons: \n\n1. It allows us to build logically sound resolutions of a given relation by using the facts (edges of the chain) using dyadic Horn Clauses, and \n2. It allows us to make interpretable model inferences and \n3. It allows us to control the complexity of the task. \n\nThis chain query graph is in no way simple for a model to predict and the test performance varies between 30% to 80%. . To give a concrete example, suppose a chain rule is of the form $r_1(a,b) \\land r_2(b,c) \\land r_3(c,d) \\land r_2(d,e) \\rightarrow r_5(a,e)$. This chain rule can be decomposed in many different ways for a learner if they have access to the underlying rules (i.e, decomposing $r_1(a,b) \\land r_2(b,c)$ to say $r_x(a,c)$ and so on, please refer to the concrete example posted in response to Reviewer 1 for an idea). Additionally, each graph also consists of an entire subgraph built using the same rules encompassing this query path (which we term as \u201cresolution path\u201d). Thus, this allows the GNN to either determine the relation following dangling paths as the reviewer suggested or \u201cif\u201d the GNN behaves like a recursive learner they can opt for the chain rule path we create, which we use to control the complexity of the task. \n\n---\n\nRegarding \u201cthe authors claim that E-GAT outperforms RGCNs\u201d, we highlight that our focus is not on explicitly ranking GNNs but to understand the strengths and weaknesses of the existing models.\n\n---\n\nWe thank the reviewer for bringing up related work on scale-free random graph generation, and we will add it to our draft.\n\n---\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "swgY5mPW__9", "original": null, "number": 6, "cdate": 1605568234932, "ddate": null, "tcdate": 1605568234932, "tmdate": 1605568234932, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "QvLIOScP5Q", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "We thank the reviewer for their constructive feedback and for highlighting the strengths of our work. Please read our rebuttal below.", "comment": "We thank the reviewer for their question on compositional generalization. We agree with them that the scope of compositional generalization is broad. In this work, we focused on a specific aspect of generalization which involves generalizing to new combinations of existing rules. Since GraphLog is built on top of FOL rules, each graph is constructed by a combination of such rules. By compositional generalization we inspect the ability of a Graph Neural Network to be able to solve unseen combinations of existing rules, thus generalizing through the composition of known concepts. We will make this point clear in the draft.\n\n---\n\nRegarding comparison with CLUTTR [1], we think CLUTTR is a useful precursor to our work on GraphLog and there are some important differences between the two works:\n\n- CLUTRR consists of a single rule world which in turn contains only 15 rules. GraphLog extends the concept to a more general framework where a user can define how many rules can occur in a single world, as well as define multiple such worlds.\n- GraphLog allows building multiple worlds consisting of either the same, overlapping, or distinct set of rules - which allows practitioners to test multi-task and continual learning scenarios in minute detail by controlling the distribution shift. No such scope is present in CLUTRR, and hence CLUTRR is not suitable for performing Multitask/Continual Learning. \n- CLUTRR is built on a static set of relations (22 family relations) while GraphLog can contain any number of such relations since it's a flexible generator along with a dataset.\n \nRegarding \u201cdifference is really (just) in being able to tune the degree of shared structure between multiple datasets\u201d, we highlight that enabling control over the degree of shared structures is not a trivial exercise and needs rigorous empirical evaluation to ensure that changing the degree of shared structures actually leads to meaningful change in the model\u2019s performance.\n\nWe thank the reviewer for their suggestion and would update the related work to clearly outline the differences between CLUTTR and our work. \n\n[1] Sinha, Koustuv, et al. \"Clutrr: A diagnostic benchmark for inductive reasoning from text.\" arXiv preprint arXiv:1908.06177 (2019)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "ZpnGxhJw-5g", "original": null, "number": 5, "cdate": 1605567833766, "ddate": null, "tcdate": 1605567833766, "tmdate": 1605567833766, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "6sZ7pMioezd", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "Rebuttal continued [2/2]", "comment": "[2/2]\n\n\u201c$i$\u201d denotes one of the modes (dimensions) of the tensor. In the paper, we will update the notation to use \u201c$j$\u201d (and describe what this is) so that the reader does not confuse this with the relation index.\n\n--- \n\nWhile the reviewer is absolutely correct that if we divide the datasets into easy medium and hard, we will have a nice line, do note that the difficult splits are not handed over to use by someone. When we generate the datasets, we can control the complexity to only some extent. We work with the intuition that certain design choices will make the dataset harder etc. However, we need to perform experiments to ensure that our intuition is valid and the resulting datasets are indeed hard. Regarding plotting different models, we believe it is important to be able to make a general claim that dataset X is easier than dataset Y. In absence of multiple models, we can not conclude if data X is easier for model A or easier in general.\n\n---\n\nThank you for the question about the color bars. We note that the statement: \u201cThe colors of the bars depict how similar two distributions are\u201d is not an inference from the results. We use different colors for the bars when making the plots, to show how similar they are. The point we want to highlight is, more similar are the datasets, the better is the few shot generalization performance."}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "6sZ7pMioezd", "original": null, "number": 4, "cdate": 1605567665108, "ddate": null, "tcdate": 1605567665108, "tmdate": 1605567665108, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "wh0Pa6G5PcK", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment", "content": {"title": "Thanks to the reviewer for constructive comments! Please read our rebuttal below [1/2]", "comment": "[1/2]\n\n---\n\nWe thank the reviewer for their constructive feedback and we are glad that they find GraphLog \u201cinteresting\u201d and \u201cpotentially very useful\u201d. The reviewer raises an important question that our baselines are neither SOTA nor novel. We highlight that most of the SOTA GNN models are not designed for relational data. Even for models like GAT, we used the EGAT variants proposed in the literature by [1]. Additionally, we are working on adding the results with the GIN model. Regarding models not being novel, our objective was not to develop a new model but highlight the strengths and weaknesses of the existing models. We respectfully disagree with the reviewer that \u201cThe results seemed little to do with the dataset\u201d. Our \u201cdataset\u201d provides a mechanism to control data distribution shift, thus enables quantifying the performance of the different GNN models on the task of logical relation reasoning, in three setups: (i) Single Task Learning, (ii) Multi-Task Training, and (iii) Continual Learning. To marginalize the contribution of the proposed \u201cdataset\u201d by saying that they just enable the results is similar to marginalizing the contribution of a proposed \u201cmodel\u201d by saying that they just enable the results.\n\n[1] Sinha, Koustuv, et al. \"Clutrr: A diagnostic benchmark for inductive reasoning from text.\" arXiv preprint arXiv:1908.06177 (2019).\n\n-----\n\nThe reviewer raises an interesting point and we agree that we do not provide any evidence of why doing well on the benchmark would translate to doing well on the real-world tasks. However, the benchmark enables evaluating the models for compositionality, which can not be easily tested for real-world datasets. The point of the benchmark is not to be an indicator of success on real-world tasks, but to provide a diagnostic setup for properties that are not easy to evaluate in existing real-world datasets and that are known to be important for human-level abstract reasoning and planning.\n\n----- \n\nWe argue the underlying dynamics of GraphLog is very complex, and would require a GNN to perform compositional reasoning. To give a concrete example, a noise-free graph in our setup can be as follows: \n\n$a \\xrightarrow[]{r_1} b \\xrightarrow[]{r_2} c \\xrightarrow[]{r_3} d \\xrightarrow[]{r_2} e$ ,\n\nwhere, a logical resolution of this graph could yield the relation $r_5$ : $r_1(a,b) \\land r_2(b,c) \\land r_3(c,d) \\land r_2(d,e) \\rightarrow r_5(a,e)$\n\nwhere $r_5$ is the target relation to predict between the nodes $a$ and $e$, which is not available to the graph. (Note that our graphs are rarely noise-free, which we elaborate below). Say we have the following underlying rules:\n\n1. ${(r_1, r_2) \\rightarrow r_3}$,\n2. ${(r_3, r_2) \\rightarrow r_4}$, \n3. ${(r_3, r_4) \\rightarrow r_5}$, \n4. ${(r_2, r_3) \\rightarrow r_6}$, \n5. ${(r_6, r_2) \\rightarrow r_4}$, \n6. ${(r_1, r_4) \\rightarrow r_5}$\n\n\nIf a learner has access to the true underlying rules then there can exist two different ways to resolve the same query:\n\nExecution 1:\n1. $r_1(a,b) \\land r_2(b,c) \\land r_3(c,d) \\land r_2(d,e)$ \u2192 use 1\n2. $r_3(a,c) \\land r_3(c,d) \\land r_2(d,e)$ \u2192 use 2\n3. $r_3(a,c) \\land r_4(c, e)$ \u2192 use 3\n4. $r_5(a, e)$\n\nExecution 2:\n1. $r_1(a,b) \\land r_2(b,c) \\land r_3(c,d) \\land r_2(d,e)$  \u2192 use 4\n2. $r_1(a,b) \\land r_6(b,d) \\land r_2(d,e)$ \u2192 use 5\n3. $r_1(a,b) \\land r_4(b, e)$ \u2192 use 6\n4. $r_5(a,e)$\n\n- Now even for a rule-based learner having access to the underlying rules, the task is complex due to multiple resolution pathways, and each having many degenerate solutions by rule application. For a GNN, this becomes even more complex due to the absence of rules. Still, a GNN can just potentially memorize all possible edge combinations for relations. To counter that, we focus on inductive reasoning, where a GNN is shown a novel chain path during testing, forcing the model to reason in a compositional manner.\n- Furthermore, we add a subgraph around the resolution chain which is built using the same rules. This subgraph while not potentially useful for the logical resolution of the task can aid or inhibit the reasoning process of GNN. We take extra care of removing any shorter length path than the query path, removing the probability of the model to memorize shortcut connections.\n- As we show above with a trivial example, even with dyadic Horn clauses (or rules) it is possible to generate complex reasoning chains that vary in complexity due to the length of the chain as well as the number of ways the learner can resolve a chain. Thus, we respectfully disagree that the problem formulation is \u201coverly simplistic\u201d or have the \u201csame complexity\u201d.\n\nWe sample the rules procedurally, thus the complexity of the graphs generated across different worlds varies drastically based on the closure of the rules generated. We treat entities as a cloze task form, where across graphs there is no sharing of entity representation.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ux5zdAir9-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2236/Authors|ICLR.cc/2021/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850734, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Comment"}}}, {"id": "a3n5hP_ZZg2", "original": null, "number": 2, "cdate": 1603864903167, "ddate": null, "tcdate": 1603864903167, "tmdate": 1605024257424, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Review", "content": {"title": "Proposes a rule-based synthetic graph generator; Biases in the generation process lead to doubts about the paper's conclusions", "review": "The authors propose a synthetic graph generator to evaluate graph neural networks. The generation process starts with defining rules, subset of rules are used to define a world, each world is then used to sample a graph. Test queries are generated by picking a pair of vertices u, v and generating a path connecting them via the rules. There are some biases in the generation process. For instance, the rules are exclusively open path or chain rules. And as noted above, the test queries mostly stick to a path (the authors allow some variations by adding nodes to vertices already on the path but the path seems to form the backbone of the test query). The remainder of the paper takes a few well known graph neural networks and evaluates them on data generated using GraphLog. Based on these results, the authors claim that E-GAT outperforms RGCNs.\n\nThis reviewer acknowledges the need for a benchmark for knowledge base completion. However, GraphLog seems to have one too many biases baked into it for it to form a definitive benchmark. The first bias is the adherence to open path or chain rules. I don't understand the need for this. I can think of two different property nodes p1, p2 hanging off a vertex u leading to an edge with another vertex v. Can this be captured by a chain rule? In section 3, the paper states \"Path-based Horn clauses of this form ... encompass the types of logical rules learned by state-of-the-art rule induction systems\" and then goes on to cite \\partial ILP and NeuralLP. \\partial ILP captures more than just chain rules, it tries to capture recursive logic programs, and NeuralLP is based on TensorLog which is much more general than this. If you are going to cite related work then might as well cite it properly. Another bias baked into GraphLog is how it generates its test queries which is mostly a path between two vertices. I found myself wondering whether these biases are what's causing RGCNs to underperform on GraphLog generated data. Such doubts lead me to believe there's some gap in GraphLog that needs bridging. Please don't get me wrong, I'm sure GraphLog will still be used to evaluate KBC approaches but I remain unconvinced that it is general enough to warrant a full conference research paper. \n\nWriting quality wise, the presentation is clear enough with details delegated to the appendices. Related work about graph neural networks and knowledge base completion seems to be well covered. One aspect of related work that seems missing is previously proposed synthetic graph generators. For instance, did the authors try to look for generative approaches for scale-free random graphs? Are these not of interest to the KBC and graph neural networks communities for some reason?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100958, "tmdate": 1606915778371, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2236/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Review"}}}, {"id": "QvLIOScP5Q", "original": null, "number": 3, "cdate": 1603923725482, "ddate": null, "tcdate": 1603923725482, "tmdate": 1605024257361, "tddate": null, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "invitation": "ICLR.cc/2021/Conference/Paper2236/-/Official_Review", "content": {"title": "A refinement of synthetic knowledge graph benchmarks", "review": "This work proposes a method for generating synthetic datasets for testing path-based (knowledge) graph completion. Until recently, there were not many good benchmarks for evaluating reasoning with learned rules or learned knowledge, but there has been a fair amount of work on developing benchmarks for this lately. The synthetic datasets generated here are distinguished by the ability to produce datasets that share a controllable amount of rules. This permits the benchmarks to be used to evaluate multitask learning, robustness to distribution shift, etc. As an illustration of this, the paper includes experiments with a variety of baseline methods showing how (a) the generalization ability of various methods grows and then declines as the number of tasks is increased and (b) fine-tuning on diverse tasks improves accuracy. They also show that in a continual learning setting, the baseline methods all exhibit catastrophic forgetting.\n\nThere is value in being able to control the \"relatedness\" of these synthetic tasks in a principled way (the experiments are a good illustration of how this may be used), so I am leaning towards acceptance. My hesitation is that it's a bit on the incremental side, and seems oversold in a few places, as follows:\n\nThe work suggests that the proposed benchmarks examine the \"compositional generalization\" abilities of models, but it is not clear to me how this is so. The relations are drawn from a fixed set of types. It's true that the rules may examine new combinations of relations, but the end result is just that one of the existing relations is inferred to hold between the start and end of the path. The fact that those rules hold for all bindings is a kind of generalization, but I would not call it \"compositional.\" Compositional generalization would mean something more like learning how to map a grammatical structure to the logical form correctly, irrespective of what was in the constituent phrases.\n\nAlso, the work says \"GraphLog is the only dataset specifically designed to test logical generalization capabilities on graph data.\" That seems a little bit of a stretch. CLUTTR seems to feature a very similar set of synthetic benchmarks, the only difference is that CLUTTR *additionally* produces some text, but the abstract focus of the task was an essentially similar knowledge graph completion task. Placing this work side-by-side with CLUTTR, the difference is really (just) in being able to tune the degree of shared structure between multiple datasets. That's valuable, I agree. But this one difference is only discussed by a table featuring four X's for each of the variants of the task setup that this feature enables. The lack of a proper discussion of related work obscures the actual extent of the contribution.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2236/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2236/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks", "authorids": ["~Koustuv_Sinha1", "~Shagun_Sodhani1", "~Joelle_Pineau1", "~William_L._Hamilton1"], "authors": ["Koustuv Sinha", "Shagun Sodhani", "Joelle Pineau", "William L. Hamilton"], "keywords": ["graph neural networks", "dataset", "benchmark", "logic"], "abstract": "Relational inductive biases have a key role in building learning agents that can generalize and reason in a compositional manner. While relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand their effectiveness to adapt to new tasks. In this work, we study the logical generalization capabilities of GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. \nGraphLog consists of relation prediction tasks on 57 distinct procedurally generated logical worlds. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task (with pretraining), and continual learning. Unlike previous benchmarks, GraphLog enables us to precisely control the logical relationship between the different worlds by controlling the underlying first-order logic rules. We find that models' ability to generalize and adapt strongly correlates to the availability of diverse sets of logical rules during multi-task training. We also find the severe catastrophic forgetting effect in continual learning scenarios, and GraphLog provides a precise mechanism to control the distribution shift. Overall, our results highlight new challenges for the design of GNN models, opening up an exciting area of research in generalization using graph-structured data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|graphlog_a_benchmark_for_measuring_logical_generalization_in_graph_neural_networks", "supplementary_material": "/attachment/e68e27fd1ee318564547c20600849b6d835a0fb4.zip", "pdf": "/pdf/8e08a66555cca2c5187d83292e6e394e24798dc4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UH57cSN1P2", "_bibtex": "@misc{\nsinha2021graphlog,\ntitle={GraphLog: A Benchmark for Measuring Logical Generalization in Graph Neural Networks},\nauthor={Koustuv Sinha and Shagun Sodhani and Joelle Pineau and William L. Hamilton},\nyear={2021},\nurl={https://openreview.net/forum?id=Ux5zdAir9-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ux5zdAir9-U", "replyto": "Ux5zdAir9-U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100958, "tmdate": 1606915778371, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2236/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2236/-/Official_Review"}}}], "count": 17}