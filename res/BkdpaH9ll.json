{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396474129, "tcdate": 1486396474129, "number": 1, "id": "B1GBnzU_g", "invitation": "ICLR.cc/2017/conference/-/paper272/acceptance", "forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper investigates several ways of conditioning an image captioning model on image attributes. While the results are good, the novelty is too limited for the paper to be accepted."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396474634, "id": "ICLR.cc/2017/conference/-/paper272/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396474634}}}, {"tddate": null, "tmdate": 1484954273915, "tcdate": 1484954273915, "number": 2, "id": "HJqiqzxPe", "invitation": "ICLR.cc/2017/conference/-/paper272/official/comment", "forum": "BkdpaH9ll", "replyto": "SyPyawULg", "signatures": ["ICLR.cc/2017/conference/paper272/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper272/AnonReviewer3"], "content": {"title": "Additional experiments strengthen the paper but novelty still limited.", "comment": "The additional results and experiments strengthen the paper as an experimental evaluation paper.\nI appreciate the human evaluation and the additional qualitative results.\n\nHowever, I remain with my assessment and in line with the other reviewers that the technical novelty is not especially big.\n\nMy final score is thus somewhere between 6 and 7."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287645768, "id": "ICLR.cc/2017/conference/-/paper272/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkdpaH9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper272/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper272/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper272/reviewers", "ICLR.cc/2017/conference/paper272/areachairs"], "cdate": 1485287645768}}}, {"tddate": null, "tmdate": 1484319967669, "tcdate": 1484319967669, "number": 7, "id": "SyPyawULg", "invitation": "ICLR.cc/2017/conference/-/paper272/public/comment", "forum": "BkdpaH9ll", "replyto": "rkxp2qWVe", "signatures": ["~Ting_Yao1"], "readers": ["everyone"], "writers": ["~Ting_Yao1"], "content": {"title": "Re: ICLR 2017 conference paper272 AnonReviewer3", "comment": "We would like to thank all the reviewers for your great efforts on our submission. We have tried our best to comply with your valuable recommendations and have made corresponding revisions in the revised manuscript. We feel that the revisions have enhanced the readability of the paper. \n\nR3.1 Significance and Novelty\n\nThe main idea of this paper is to explore how to incorporate high-level image attributes for boosting image captioning. We do believe that this is the right way to leverage more knowledge for building richer representations and description models. In particular, the utilization of attributes has a great potential to be an elegant solution of generating open-vocabulary sentences, making image captioning system really practical and helpful for robotic vision or visually impaired people.\n\nThough the techniques of attribute detector training by multiple instance learning (Fang et al., 2015) or CNN plus RNN captioning models (e.g., Donahue et al., 2015) are studied before, how to integrate the attributes into CNN plus RNN captioning framework is still a problem not yet fully understood in the literature. Therefore, the main contribution of our work is the proposal of a general image captioning framework with attributes, which is later regarded as an extended direction to the basic captioning model in (Liu et al., 2016)*. More importantly, we devise five variants of attribute augmented architectures, empirically verify the merit of each and provide a comprehensive comparison in between. We do think our work takes a further step forward to enhance image captioning and could have a direct impact of indicating a new future direction of vision and language research.\n   \nWe have added more explanations about the significance and novelty of our work throughout the paper in our revision. \n\n*: Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama and Kevin Murphy. Optimization of image description metrics using policy gradient methods. arXiv preprint arXiv: 1612.00370v2, 2016. \n\nR3.2 While the exact way how attributes are used is different to prior work, the presented variants are not especially exiting\n\nThanks. In order to examine the effectiveness of our architectures irrespective of the learnt attributes influence, we conducted an additional experiment by inputting ground-truth attributes in our architectures. In this case, the performances of LSTM-A3 achieve B@1: 95.7%, B@2: 82.5%, B@3: 68.5%, B@4: 55.9%, METEOR: 34.1%, ROUGE-L: 67.3% and CIDEr-D: 150.5%, which could be regarded as the upper bound of employing attributes in our framework and lead to large performance gain against LSTM-A3. The results, on one hand, indicate the advantage and great potential of our proposed architecture for boosting image captioning with attributes, and on the other, suggest that more efforts are further required towards mining and representing attributes more effectively. We have updated Table 1 and added these explanations in our revision. \n\nR3.3 SPICE\n\nThanks. We have added SPICE as another evaluation metric and reported the SPICE performances in Table 1. The performance trends of SPICE are similar with that of METEOR. \n\nR3.4 Human judgment\n\nThanks for your valuable suggestions. We conducted an additional human study to compare our LSTM-A3 against three approaches, i.e., CaptionBot, LRCN and LSTM. A total number of 12 evaluators (6 females and 6 males) from different education backgrounds, including computer science (4), business (2), linguistics (2) and engineering (4), are invited and a subset of 1,000 images is randomly selected from testing set for the subjective evaluation. The evaluation process is as follows. All the evaluators are organized into two groups. We show the first group all the four sentences generated by each approach plus the five human-annotated sentences and ask them the question: Do the systems produce captions resembling human-generated sentences? In contrast, we show the second group once only one sentence generated by different approach or human annotation and they are asked: Can you determine whether the given sentence has been generated by a system or by a human being? From evaluators' responses, we calculate two metrics: 1) M1: percentage of captions that are evaluated as better or equal to human caption; 2) M2: percentage of captions that pass the Turing Test. Table 3, in our revision, lists the result of the user study. Overall, our LSTM-A3 is clearly the winner for all two criteria. In particular, the percentage achieves 62.8% and 72.2% in terms of M1 and M2, respectively, making the absolute improvement over the best competitor CaptionBot by 4.6% and 5.9%. We have added the human judgment in our revision. \n\nR3.5 Qualitative analysis between LSTM-A1 to A5\n\nThanks for this point. We have added one figure (Figure 3 in our revision) to showcase the sentence generated by our proposed five variants. In general, the sentences generated by LSTM-A3 and LSTM-A5 are very comparable and more accurate than those by LSTM-A1, LSTM-A2 and LSTM-A4. For instance, LSTM-A3 and LSTM-A5 produce the sentence of \"a bunch of stuffed animals hanging from a ceiling,\" which describes the first image very precisely and finely.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287645974, "id": "ICLR.cc/2017/conference/-/paper272/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkdpaH9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper272/reviewers", "ICLR.cc/2017/conference/paper272/areachairs"], "cdate": 1485287645974}}}, {"tddate": null, "tmdate": 1484319771265, "tcdate": 1484319771265, "number": 6, "id": "rkQQnPILl", "invitation": "ICLR.cc/2017/conference/-/paper272/public/comment", "forum": "BkdpaH9ll", "replyto": "SkyGipWEx", "signatures": ["~Ting_Yao1"], "readers": ["everyone"], "writers": ["~Ting_Yao1"], "content": {"title": "Re: ICLR 2017 conference paper272 AnonReviewer2", "comment": "We would like to thank all the reviewers for your great efforts on our submission. We have tried our best to comply with your valuable recommendations and have made corresponding revisions in the revised manuscript. We feel that the revisions have enhanced the readability of the paper. \n\nR2.1 Significance and Novelty\n\nThe main idea of this paper is to explore how to incorporate high-level image attributes for boosting image captioning. We do believe that this is the right way to leverage more knowledge for building richer representations and description models. In particular, the utilization of attributes has a great potential to be an elegant solution of generating open-vocabulary sentences, making image captioning system really practical and helpful for robotic vision or visually impaired people.\n \nThough the techniques of attribute detector training by multiple instance learning (Fang et al., 2015) or CNN plus RNN captioning models (e.g., Donahue et al., 2015) are studied before, how to integrate the attributes into CNN plus RNN captioning framework is still a problem not yet fully understood in the literature. Therefore, the main contribution of our work is the proposal of a general image captioning framework with attributes, which is later regarded as an extended direction to the basic captioning model in (Liu et al., 2016)*. More importantly, we devise five variants of attribute augmented architectures, empirically verify the merit of each and provide a comprehensive comparison in between. We do think our work takes a further step forward to enhance image captioning and could have a direct impact of indicating a new future direction of vision and language research.\n  \nWe have added more explanations about the significance and novelty of our work throughout the paper in our revision. \n\n*: Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama and Kevin Murphy. Optimization of image description metrics using policy gradient methods. arXiv preprint arXiv: 1612.00370v2, 2016. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287645974, "id": "ICLR.cc/2017/conference/-/paper272/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkdpaH9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper272/reviewers", "ICLR.cc/2017/conference/paper272/areachairs"], "cdate": 1485287645974}}}, {"tddate": null, "tmdate": 1484319685326, "tcdate": 1484319685326, "number": 5, "id": "rJa6iPIUg", "invitation": "ICLR.cc/2017/conference/-/paper272/public/comment", "forum": "BkdpaH9ll", "replyto": "B1_UZWz4g", "signatures": ["~Ting_Yao1"], "readers": ["everyone"], "writers": ["~Ting_Yao1"], "content": {"title": "Re: ICLR 2017 conference paper272 AnonReviewer1", "comment": "We would like to thank all the reviewers for your great efforts on our submission. We have tried our best to comply with your valuable recommendations and have made corresponding revisions in the revised manuscript. We feel that the revisions have enhanced the readability of the paper. \n\nR1.1 Significance and Novelty\n\nThe main idea of this paper is to explore how to incorporate high-level image attributes for boosting image captioning. We do believe that this is the right way to leverage more knowledge for building richer representations and description models. In particular, the utilization of attributes has a great potential to be an elegant solution of generating open-vocabulary sentences, making image captioning system really practical and helpful for robotic vision or visually impaired people.\n \nThough the techniques of attribute detector training by multiple instance learning (Fang et al., 2015) or CNN plus RNN captioning models (e.g., Donahue et al., 2015) are studied before, how to integrate the attributes into CNN plus RNN captioning framework is still a problem not yet fully understood in the literature. Therefore, the main contribution of our work is the proposal of a general image captioning framework with attributes, which is later regarded as an extended direction to the basic captioning model in (Liu et al., 2016)*. More importantly, we devise five variants of attribute augmented architectures, empirically verify the merit of each and provide a comprehensive comparison in between. We do think our work takes a further step forward to enhance image captioning and could have a direct impact of indicating a new future direction of vision and language research.\n  \nWe have added more explanations about the significance and novelty of our work throughout the paper in our revision. \n\n*: Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama and Kevin Murphy. Optimization of image description metrics using policy gradient methods. arXiv preprint arXiv: 1612.00370v2, 2016. \n\nR1.2 Image features\n\nThanks for pointing this out. Yes, the performances of different approaches reported in Table 1 are based on different image representations. Specifically, Oxford VGG architecture is utilized as image feature extractor in the methods of Hard-Attention & Soft-Attention (Xu et al., 2015) and Sentence-Condition (Zhou et al., 2016), while GoogleNet is exploited in NIC & LSTM (Vinyals et al., 2015), LRCN (Donahue et al., 2015), ATT (You et al., 2016) and our LSTM-A. In view that the GoogleNet and Oxford VGG features are comparable, we compare directly with results in Table 1. Considering the reviewer\u2019s comments, we will explicitly indicate the image features exploited in different approaches.\n \nIn addition, we utilize ResNet-152 as image feature extractor in our submission to the online testing server and report the results in Table 2. We have rephrased the statement to better describe the image features in our revision.\n\nR1.3 Experiments with ground-truth attributes\n\nThanks for your valuable suggestions. We conducted an additional experiment by inputting ground-truth attributes in our LSTM-A3 architecture. In this case, the performances achieve B@1: 95.7%, B@2: 82.5%, B@3: 68.5%, B@4: 55.9%, METEOR: 34.1%, ROUGE-L: 67.3% and CIDEr-D: 150.5%, which could be regarded as the upper bound of employing attributes in our framework and lead to large performance gain against LSTM-A3. Such an upper bound enables us to obtain more insights on the factor accounting for the success of the current attribute augmented architecture and also provides guidance to future research in this direction. More specifically, the results, on one hand, indicate the advantage and great potential of leveraging attributes for boosting image captioning, and on the other, suggest that more efforts are further required towards mining and representing attributes more effectively. We have updated Table 1 and added these explanations in our revision. \n\nR1.4 Visualization of prediction changes with respect to the additional attribute inputs\n\nThanks. We have added two image examples to clarify the prediction changes with respect to the additional attribute inputs in Figure 2. Take the first image as an example, the predicted subject is \"a cake\" in LSTM model. By additionally incorporating the detected attributes, e.g., \"candles\" and \"birthday,\" the output subject in the sentence by our LSTM-A changes into \"a birthday cake with candles,\" demonstrating the advantage of the auxiliary attribute inputs. \n\nR1.5 Human judgment\n\nThanks for your valuable suggestions. We conducted an additional human study to compare our LSTM-A3 against three approaches, i.e., CaptionBot, LRCN and LSTM. A total number of 12 evaluators (6 females and 6 males) from different education backgrounds, including computer science (4), business (2), linguistics (2) and engineering (4), are invited and a subset of 1,000 images is randomly selected from testing set for the subjective evaluation. The evaluation process is as follows. All the evaluators are organized into two groups. We show the first group all the four sentences generated by each approach plus the five human-annotated sentences and ask them the question: Do the systems produce captions resembling human-generated sentences? In contrast, we show the second group once only one sentence generated by different approach or human annotation and they are asked: Can you determine whether the given sentence has been generated by a system or by a human being? From evaluators\u2019 responses, we calculate two metrics: 1) M1: percentage of captions that are evaluated as better or equal to human caption; 2) M2: percentage of captions that pass the Turing Test. Table 3, in our revision, lists the result of the user study. Overall, our LSTM-A3 is clearly the winner for all two criteria. In particular, the percentage achieves 62.8% and 72.2% in terms of M1 and M2, respectively, making the absolute improvement over the best competitor CaptionBot by 4.6% and 5.9%. We have added the human judgment in our revision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287645974, "id": "ICLR.cc/2017/conference/-/paper272/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkdpaH9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper272/reviewers", "ICLR.cc/2017/conference/paper272/areachairs"], "cdate": 1485287645974}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484319092034, "tcdate": 1478282687594, "number": 272, "id": "BkdpaH9ll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkdpaH9ll", "signatures": ["~Ting_Yao1"], "readers": ["everyone"], "content": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481933136203, "tcdate": 1481933136203, "number": 3, "id": "B1_UZWz4g", "invitation": "ICLR.cc/2017/conference/-/paper272/official/review", "forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "signatures": ["ICLR.cc/2017/conference/paper272/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper272/AnonReviewer1"], "content": {"title": "Review: Boosting Image Captioning with Attributes", "rating": "4: Ok but not good enough - rejection", "review": "CONTRIBUTIONS\nThis paper extends end-to-end CNN+RNN image captioning model with auxiliary attribute predictions. It proposes five variants of network architectures, which take the attribute/image features in alternating orders or at every timestamp. The attributes (i.e., 1,000 most common words on COCO) are obtained by attribute classifiers trained by a multiple instance learning approach. The experiment results indicated that having these attributes as input improves captioning performance on standard metrics, including BLEU, METEOR, ROUGE-L, CIDEr-D. \n\nNOVELTY + SIGNIFICANCE\nAll five variants of the network architectures, shown in Fig. 1, have followed a standard seq-to-seq LSTM model. The differences between these variants come from two aspects: 1. the order of image/attribute inputs; 2. whether to input attribute/image features at each time step. No architectural changes have been added to the proposed model over a standard seq-to-seq model. The proposed approach achieves decent performance improvement over previous work, but the technical novelty of this work is significantly limited by existing work that used similar ideas. In particular, Fang et al. 2015 and You et al. 2016 have both used attributes for image captioning. This work has used the same multiple instance learning procedure as Fang et al. 2015 to train visual detectors for common words and used detector outputs as conditional inputs to a language model. In addition, the idea of using image feature as input to every RNN timestamp has been widely explored, for instance, in Donahue et al. 2015. The authors did not offer a clear explanation about the technical contribution of this work over these existing approaches.\n\nCLARITY\nFirst, it is not clear to me which image features have been used by the baseline methods. As the baselines may rely on different image representations, the experiments would not offer a completely fair comparison. For example, the results of the attention-based models in Table 1 are directly copied from Xu et al., 2015, which were reported with Oxford VGG features, instead of GoogLeNet used by LSTM-A. Even if the baselines do not use the same types of features, it should at least be explicitly mentioned. Besides, the fact that the results of Table 1 and Table 2 are reported with different features (GoogLeNet v.s. ResNet) are not described clearly in the paper.\n\n\u201cWe select 1,000 most common words on COCO\u2026\u201d How would this approach guarantee that the selected attributes have clear semantic meanings, as many words among the top 1000 would be stop words, abstract nouns, non-visual verbs, etc.? It would be interesting to perform some quantitative analysis to see whether these 1000-dimensional attribute vectors actually carry semantic meanings, or merely capture biases of word distributions from the ground-truth captions. One possible way to justify the importance of semantic attributes is to experiment with ground-truth attributes or attribute classifiers trained on annotated sources such as COCO-Attribute (Patterson et al. 2016) or Visual Genome (Krishna et al. 2016).\n\nEXPERIMENTS\nIt would be interesting to analyze how the behavior of the seq-to-seq model changes with respect to the additional attribute inputs. My hypothesis is that the model\u2019s word choices will shift towards words with high scores. This experiments will help us understand how the model can take advantage of the auxiliary attribute inputs.\n\nSince the attributes are selected as the most frequent words from COCO, it is likely for the model to overfit the metrics, such as BLEU, that rely on word matching. On the other hand, it has been shown that these automatic caption evaluation metrics do not necessarily correlate with human judgment. Therefore, I think it is necessary to conduct a human study to convince the readers the quality improvement of the proposed model is not caused by overfitting to metrics.\n\nSUMMARY\nThis paper demonstrates that image captioning can be improved by having attributes as auxiliary inputs. However, the model has minor novelty given existing work that has explored similar ideas. Besides, more analysis is necessary to demonstrate the semantic meanings of attributes. A human study is recommended to justify the actual performance improvement. Given these points to be improved, I would recommend rejecting this paper in its current form.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512641744, "id": "ICLR.cc/2017/conference/-/paper272/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper272/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper272/AnonReviewer3", "ICLR.cc/2017/conference/paper272/AnonReviewer2", "ICLR.cc/2017/conference/paper272/AnonReviewer1"], "reply": {"forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512641744}}}, {"tddate": null, "tmdate": 1481919238648, "tcdate": 1481919238648, "number": 2, "id": "SkyGipWEx", "invitation": "ICLR.cc/2017/conference/-/paper272/official/review", "forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "signatures": ["ICLR.cc/2017/conference/paper272/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper272/AnonReviewer2"], "content": {"title": "good results, very limited novelty", "rating": "5: Marginally below acceptance threshold", "review": "\nThe authors take a standard image captioning system and, in addition to the image, also condition the caption on a fixed vector of attributes. These attributes are the top 1,000 most common words trained with MIL as seen in Fang et al. 2015. The authors investigate 5 ways of plugging in the attributes vector for each image. More or less all of them turn out to work comparably (24 - 25.2 METEOR), but also a good amount better than a standard image captioning model (25.2 > 23.1 METEOR). At the time this approach was state of the art on the MS COCO leaderboard.\n\nI am conflicted judging these kinds of application-heavy papers. It is clear that the technical execution is done relatively well, but there is little to take away or learn. I find it interesting and slightly surprising that you can boost image captioning results by appending these automatically-extracted attributes, but on the topic of how many people would find this relevant or how much additional work it can inspire I am not so sure.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512641744, "id": "ICLR.cc/2017/conference/-/paper272/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper272/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper272/AnonReviewer3", "ICLR.cc/2017/conference/paper272/AnonReviewer2", "ICLR.cc/2017/conference/paper272/AnonReviewer1"], "reply": {"forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512641744}}}, {"tddate": null, "tmdate": 1481907383908, "tcdate": 1481907383908, "number": 1, "id": "rkxp2qWVe", "invitation": "ICLR.cc/2017/conference/-/paper272/official/review", "forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "signatures": ["ICLR.cc/2017/conference/paper272/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper272/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper evaluates different variants to include attributes for caption generation. Attributes are automatically learned from descriptions as in [Fang et al., 2015].  \n\n\nStrength:\n1.\tThe paper discusses and evaluates different variants to incorporate attribute and image representation in a recurrent network.\n2.\tThe paper reports improvements over state-of-the-art on the large-scale MS COCO image caption dataset.\n\nWeaknesses:\n1.\tThe technical novelty of the paper is limited; it is mainly a comparison of different variants to include attributes.\n2.\tWhile the exact way how attributes are used is different to prior work, the presented variants are not especially exiting.\n3.\tThe reported metrics are known to not always correlate very well with human judgments.\n3.1.\tIt would be good to additionally also report the SPICE (Anderson et al ECCV 2016) metric, which has shown good correlation w.r.t. human judgments.\n3.2.\tIn contrast to the arguments of the authors during the discussion period, I believe a human evaluation is feasible at least on a subset of the test set. In my experience, most authors will share their generated sentences if they did not publish them. Also, a comparison between the different variants should be possible.\n4.\tQualitative results:\n4.1.\tHow do the different variants A1 to A5 compare? Is there a difference in the sentence between the different approaches?\n\n\nOther (minor/discussion points)\n-\tPage 8: \u201cis benefited\u201d -> benefits\n\n\nSummary:\nWhile the paper presents a valuable experimental evaluation with convincing results, the contribution w.r.t. to novelty and approach is limited.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512641744, "id": "ICLR.cc/2017/conference/-/paper272/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper272/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper272/AnonReviewer3", "ICLR.cc/2017/conference/paper272/AnonReviewer2", "ICLR.cc/2017/conference/paper272/AnonReviewer1"], "reply": {"forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512641744}}}, {"tddate": null, "tmdate": 1480943440024, "tcdate": 1480943440016, "number": 3, "id": "rkOLwymme", "invitation": "ICLR.cc/2017/conference/-/paper272/public/comment", "forum": "BkdpaH9ll", "replyto": "SyH9ikeme", "signatures": ["~Ting_Yao1"], "readers": ["everyone"], "writers": ["~Ting_Yao1"], "content": {"title": "re: Clarifications", "comment": "Dear Reviewer,\n\nThanks for your comments. \n\n1. Thanks for suggesting human evaluation. At this time, it is difficult to carry out human evaluation on all the compared approaches, since there is only reported performance but no public model/code available for some of them (e.g., [You et al., 2016]). Furthermore, manually annotating and comparing the sentences generated by different approaches also requires immense human labor and thus it is hard to be scaled up. Considering this comment, we will conduct additional significance test to verify that our performance boost against different baselines is not by chance. We will include this in our revision.\n\n2. Yes, that is exactly what we did. In Table 1, to ensure the performance of these methods comparable, we learn image representations by GoogleNet. In contrast, image representations are learnt by ResNet-152 in Table 2. Following the settings in [Fang et al., 2015], attribute representations are learnt by VGG-16 reported in both Table 1&2.\n\n3. Thanks for pointing this out. The performances reported in current Table 2 were officially published in COCO leaderboard when we submitted this paper and our run by training the attribute detectors using ResNet-152 was an internal evaluation on online COCO server at that time. Considering this comment, we will additionally include the performances of this run (c5 [B@1: 75.1%, B@2: 58.8%, B@3: 44.9%, B@4: 34.3%, METEOR: 26.6%, ROUGE-L: 55.2%, CIDEr-D: 104.9%], c40 [B@1: 92.6%, B@2: 85.1%, B@3: 75.1%, B@4: 64.6%, METEOR: 36.1%, ROUGE-L: 70.9%, CIDEr-D: 105.3%]).\n\n4. We indeed experimented with feeding both image and attributes at each time step and that makes a performance drop compared to LSTM-A5. This is similar to the observation that LSTM-A4 yields inferior performances to LSTM-A3. The results basically indicate that the noise in the image can be explicitly accumulated in learning LSTM and leads to overfitting more easily. We will add some discussions to clarify this.\n\n\nThanks,\nTing"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287645974, "id": "ICLR.cc/2017/conference/-/paper272/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkdpaH9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper272/reviewers", "ICLR.cc/2017/conference/paper272/areachairs"], "cdate": 1485287645974}}}, {"tddate": null, "tmdate": 1480747917053, "tcdate": 1480747917049, "number": 2, "id": "SyH9ikeme", "invitation": "ICLR.cc/2017/conference/-/paper272/pre-review/question", "forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "signatures": ["ICLR.cc/2017/conference/paper272/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper272/AnonReviewer3"], "content": {"title": "Clarifications", "question": "Hi,\n\n1. I would like to know if you consider doing a human evaluation as the reported metrics are known to not necessarily align with human evolutions?\n\n2. Is it correct that Table 1 does not use ResNet (for your approach), but Table 2 does partially?\n\n3. You write\n\"In addition, when training the attribute detectors by ResNet-152, our CIDEr-D scores on c5 and c40\ntesting sets will be further boosted up to 104.9% and 105.3%, respectively.\" \nWhy don't you report this in Table 2? What are the other evaluation scores for this variant?\n\n4. Did you consider feeding both, image and attributes at each time step?\n\n\nThanks for the clarifications in advance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959368039, "id": "ICLR.cc/2017/conference/-/paper272/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper272/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper272/AnonReviewer2", "ICLR.cc/2017/conference/paper272/AnonReviewer3"], "reply": {"forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959368039}}}, {"tddate": null, "tmdate": 1480691801775, "tcdate": 1480691759546, "number": 2, "id": "HJPNlf1me", "invitation": "ICLR.cc/2017/conference/-/paper272/public/comment", "forum": "BkdpaH9ll", "replyto": "ryBbvnAGx", "signatures": ["~Ting_Yao1"], "readers": ["everyone"], "writers": ["~Ting_Yao1"], "content": {"title": "re: questions", "comment": "Dear Reviewer,\n\nThanks for your comments. \n\nYes, no additional supervision is used in our approach. We determine the vocabulary of attributes by using the 1000 most common words in the training captions of COCO dataset and train the attribute detectors by using Multiple Instance Learning (MIL) in [Fang et al., 2015]. \n\nNo. Following the settings in [Fang et al., 2015], we didn\u2019t filter out any stop words. We will add more details to clarify this in our revision. \n\nThanks for the suggestions regarding including a short discussion of MIL for attribute learning and shortening the related work section. The suggestions are much appreciated and changes will be made accordingly. \n\nThanks,\nTing"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287645974, "id": "ICLR.cc/2017/conference/-/paper272/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkdpaH9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper272/reviewers", "ICLR.cc/2017/conference/paper272/areachairs"], "cdate": 1485287645974}}}, {"tddate": null, "tmdate": 1480668924803, "tcdate": 1480668924798, "number": 1, "id": "ryBbvnAGx", "invitation": "ICLR.cc/2017/conference/-/paper272/pre-review/question", "forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "signatures": ["ICLR.cc/2017/conference/paper272/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper272/AnonReviewer2"], "content": {"title": "questions", "question": "Can the authors explicitly clarify that no additional supervision was used in the approach? My understanding is that the top words are used as labels and pretrain a set of classifiers, output of which is passed into the LSTM at encoding time. Also, did the authors filter stop words from the most common words list? I imagine a lot of them are stop words such as \"and\", \"a\", and so on. Given that these attributes from Fang et al. are so critical and seem to make so much difference here, it might make sense to copy a short discussion of the approach into this submission. If extra space is needed the related work section is a little too verbose.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959368039, "id": "ICLR.cc/2017/conference/-/paper272/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper272/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper272/AnonReviewer2", "ICLR.cc/2017/conference/paper272/AnonReviewer3"], "reply": {"forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959368039}}}, {"tddate": null, "tmdate": 1478384471749, "tcdate": 1478384471742, "number": 1, "id": "SylPjCseg", "invitation": "ICLR.cc/2017/conference/-/paper272/public/comment", "forum": "BkdpaH9ll", "replyto": "BkdpaH9ll", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "missing citation", "comment": "The results are great. A suggestion: I think you should also cite the paper \"Review Networks for Caption Generation\" (NIPS 2016) in Table 2."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Image Captioning with Attributes", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \\citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "pdf": "/pdf/5aa369dcc37861dea4b93aa119580e9a94613028.pdf", "TL;DR": "Boosting Image Captioning with Attributes", "paperhash": "yao|boosting_image_captioning_with_attributes", "conflicts": ["microsoft.com"], "keywords": ["Computer vision", "Applications"], "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "authorids": ["tiyao@microsoft.com", "v-yipan@microsoft.com", "v-yehl@microsoft.com", "v-zhqiu@microsoft.com", "tmei@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287645974, "id": "ICLR.cc/2017/conference/-/paper272/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkdpaH9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper272/reviewers", "ICLR.cc/2017/conference/paper272/areachairs"], "cdate": 1485287645974}}}], "count": 14}