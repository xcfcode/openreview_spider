{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363672020000, "tcdate": 1363672020000, "number": 2, "id": "JNpPfPeAkDJqK", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "PRuOK_LY_WPIq", "replyto": "PRuOK_LY_WPIq", "signatures": ["simon bolivar"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "It has already been mentioned above, but I checked the longer version of the document posted at http://www.cc.gatech.edu/~lebanon/papers/lee_icml_2013.pdf\r\nand there really is not enough discussion of the huge previous literature on locally low rank representations, going back at least as far back as \r\nhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1671801\r\n\r\nand continuing with *many* recent works which represent data precisely as a weighted combination of local low rank matrices, for example, any of the papers on subspace clustering (especially if the model is restarted several times), or anchor graph embeddings of W. Liu et al, or the Locally Linear Coding of Yang et. al.  \r\n\r\nThis does not even begin to  touch the many manifold learning papers which explicitly model data via locally linear structures (which are necessarily locally low rank) and glue these together to get a parameterization of the data."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Approximation under Local Low-Rank Assumption", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks.", "pdf": "https://arxiv.org/abs/1301.3192", "paperhash": "lee|matrix_approximation_under_local_lowrank_assumption", "keywords": [], "conflicts": [], "authors": ["Joonseok Lee", "Seungyeon Kim", "Guy Lebanon", "Yoram Singer"], "authorids": ["joonseok2010@gmail.com", "sylund@gmail.com", "lebanon@cc.gatech.edu", "singer@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363319940000, "tcdate": 1363319940000, "number": 1, "id": "CkupCgw-sY1o7", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "PRuOK_LY_WPIq", "replyto": "PRuOK_LY_WPIq", "signatures": ["Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We appreciate for both of your reviews and questions.\r\n\r\n- Kernel width: we validated the kernel width experimentally. Specifically, we examined the following kernel types: Gaussian, triangular, and Epanchnikov kernels. We also experimented with the kernel width (0.6, 0.7, 0.8). We found that sufficiently large width (>= 0.8) performs well, probably due to the fact that the similarity between users or items is typically small.\r\n\r\n- Distance measure: as the second reviewer mentioned, we first factorize M using standard incomplete SVD. We then compute the distance d based on the arc-cosine similarity between the rows of factor matrices U and V. We found that this approach performs better than defining the distance based on the rows and columns of the original rating matrix. This choice deems better probably due to the fact that many pairs of users (or item pairs) have almost no shared items because of the high sparsity of the rating matrix.\r\n\r\n- Variance observed due to the sampling of anchor points: We omitted the discussion of this issue due to space constraints (3 pages).\r\n\r\n- Effect of Nadaraya-Watson smoothing: we observed that the prediction quality is almost the same for anchor points and non-anchor points. In other words, the approximation due to the Nadaraya-Watson procedure does not seem to be the limiting factor.\r\n\r\n- Hoelder continuity: this assumption is actually essential in our model since we smooth locally with respect to the distance d. We performed large deviation analysis which is also omitted due to the lack of space.\r\n\r\n- We omitted references due to the strict page limit. We will add references to the most relevant work. Moreover, the long version of the submission includes substantial discussion and references of related work.\r\n\r\nPlease feel free to reply us if you have further questions.\r\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Approximation under Local Low-Rank Assumption", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks.", "pdf": "https://arxiv.org/abs/1301.3192", "paperhash": "lee|matrix_approximation_under_local_lowrank_assumption", "keywords": [], "conflicts": [], "authors": ["Joonseok Lee", "Seungyeon Kim", "Guy Lebanon", "Yoram Singer"], "authorids": ["joonseok2010@gmail.com", "sylund@gmail.com", "lebanon@cc.gatech.edu", "singer@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362191520000, "tcdate": 1362191520000, "number": 4, "id": "9QsSQSzMpW9Ac", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "PRuOK_LY_WPIq", "replyto": "PRuOK_LY_WPIq", "signatures": ["anonymous reviewer 76ef"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Matrix Approximation under Local Low-Rank Assumption", "review": "Approximation and completion of sparse matrices is a common task. As popularized by the Netflix prize, there are many possible approaches, and combinations of different styles of approach can lead to better predictions than individual methods. In this work, local prediction and low-rank factorization are combined as one coherent method.\r\n\r\nThis is a short paper, with an interesting idea, and some compelling results. It has the appealing property that one can almost guess what is going to come from the abstract. My key question while reading was how locality was going to be defined: one of the goals of low-rank learning is finding a space in which to represent entities. The paper uses a simple distance measure to local support points. I'm not sure if a value is missing from one row and not another whether it is ignored, or counted as zero. I wonder if an approach that finds a low-rank or factor model fit and uses that to define distances for local modelling might work better. Potentially one could iterate, after fitting the model, get improved distances and refit.\r\n\r\nI find the large improvement over the Netflix prize winners surprising given the large effort invested over three years to get that result. Is one relatively simple method really sufficient to blow that away? I think open code and scrutiny would be required to be sure. (Honest mistakes are not unprecedented: http://arxiv.org/abs/1301.6659v2 ) It will be a great result if correct.\r\n\r\nI found the complete lack of references distracting. There is clearly related work in this area. Some of it is even mentioned, just with no formal citations. This is a workshop submission for light touch review, but citations seem like a basic requirement for any scientific document.\r\n\r\nPros: neat idea, quick, to-the-point presentation.\r\nCons: I'm suspicious of the results, and would like to see a reference section."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Approximation under Local Low-Rank Assumption", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks.", "pdf": "https://arxiv.org/abs/1301.3192", "paperhash": "lee|matrix_approximation_under_local_lowrank_assumption", "keywords": [], "conflicts": [], "authors": ["Joonseok Lee", "Seungyeon Kim", "Guy Lebanon", "Yoram Singer"], "authorids": ["joonseok2010@gmail.com", "sylund@gmail.com", "lebanon@cc.gatech.edu", "singer@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362123600000, "tcdate": 1362123600000, "number": 3, "id": "4eqD-9JEKn4Ea", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "PRuOK_LY_WPIq", "replyto": "PRuOK_LY_WPIq", "signatures": ["anonymous reviewer 4b7c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Matrix Approximation under Local Low-Rank Assumption", "review": "Matrix Approximation under Local Low-Rank Assumption\r\n\r\nPaper summary\r\n\r\nThis paper deals with low-rank matrix approximation/completion. To reconstruct a matrix element M_{i,j}, the proposed method performs a weighted low rank matrix approximation which considers a similarity metric between matrix coordinates. More precisely, the weighting scheme emphasizes reconstruction errors close to the element {i,j} to reconstruct. As a computational speedup, the authors perform the low rank approximation only on a small set of coordinates and approximate the reconstruction for any coordinate through kernel estimation.\r\n\r\nReview Summary\r\n\r\nThe core idea of the paper is interesting and could be helpful in many practical applications of low rank decomposition. The paper reads well and is technically correct. On the negative side, I feel that the availability of a meaningful similarity metric between coordinates should be discussed. The experimental section could be greatly improved. There is no reference to related work at all.\r\n\r\nReview \r\n\r\nIn many application of matrix completion, the low rank decomposition algorithm is here to circumvent the fact that no meaningful similarity metric between coordinate pairs is available. For instance, if such a metric is available in a collaborative filtering scenario, one would simply take the input (customer, item) fetch its neighbors and average its ratings. Your algorithm presuppose the availability of such a metric, could you discuss this core aspect of your proposal in the paper?\r\n\r\nFollowing on this idea, would you consider as baseline performing the Nadaraya-Watson kernel regression on the matrix itself and reports the result in your experimental section. This would be meaningful to quantify how much comes from the low rank smoothing and how much comes simply from the quality of the similarity metric.\r\n\r\nStill in the experimental section, \r\n- would you consider validating the kernel width?\r\n- discuss the influence of the L2 regularizer which is not even introduced in the previous sections\r\n- define clearly the d you use. To me d(s,s') compare two coordinate pairs and I do not know how to relate it to the arccos you are using, i.e. what are x,y?\r\n- could you measure the variance observed due to the sampling of anchor points and could you report whether the reconstruction error is greater further from anchor points?\r\n- how does Nadaraya-Watson smoothing compare with respect to solving the low rank problem for each point?\r\n\r\nReferences:\r\n- you should at least refer to weighted low rank matrix approximation (Srebro & Jaakkola, ICML-03). It would be good to refer to prior work on expanding low rank matric approximation, given how fertile this field has been in the Netflix prize days (Maximum Margin Matrix Factorizations, RBM for matrix completion...).\r\n\r\nDetails along the text\r\n\r\n- In Eq. 1, to unify notation, you could use the projection Pi here as well\r\n- Hoelder continuity: I do not understand how it relate to the smoothing kernel approach defined below. I believe this sentence could be removed."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Approximation under Local Low-Rank Assumption", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks.", "pdf": "https://arxiv.org/abs/1301.3192", "paperhash": "lee|matrix_approximation_under_local_lowrank_assumption", "keywords": [], "conflicts": [], "authors": ["Joonseok Lee", "Seungyeon Kim", "Guy Lebanon", "Yoram Singer"], "authorids": ["joonseok2010@gmail.com", "sylund@gmail.com", "lebanon@cc.gatech.edu", "singer@google.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358317800000, "tcdate": 1358317800000, "number": 15, "id": "PRuOK_LY_WPIq", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "PRuOK_LY_WPIq", "signatures": ["joonseok2010@gmail.com"], "readers": ["everyone"], "content": {"title": "Matrix Approximation under Local Low-Rank Assumption", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks.", "pdf": "https://arxiv.org/abs/1301.3192", "paperhash": "lee|matrix_approximation_under_local_lowrank_assumption", "keywords": [], "conflicts": [], "authors": ["Joonseok Lee", "Seungyeon Kim", "Guy Lebanon", "Yoram Singer"], "authorids": ["joonseok2010@gmail.com", "sylund@gmail.com", "lebanon@cc.gatech.edu", "singer@google.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}