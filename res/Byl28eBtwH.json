{"notes": [{"id": "Byl28eBtwH", "original": "Bkx0uqxFDS", "number": 2337, "cdate": 1569439827537, "ddate": null, "tcdate": 1569439827537, "tmdate": 1577168280119, "tddate": null, "forum": "Byl28eBtwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yljblues@whu.edu.cn", "ly.wd@whu.edu.cn", "haijian.zhang@whu.edu.cn", "liuzhou@whu.edu.cn"], "title": "Learning Cluster Structured Sparsity by Reweighting", "authors": ["Yulun Jiang", "Lei Yu", "Haijian Zhang", "Zhou Liu"], "pdf": "/pdf/3bb43c33c1bab83811763506a2b81008d95f501e.pdf", "abstract": "Recently, the paradigm of unfolding iterative algorithms into finite-length feed-forward neural networks has achieved a great success in the area of sparse recovery. Benefit from available training data, the learned networks have achieved state-of-the-art performance in respect of both speed and accuracy. However, the structure behind sparsity, imposing constraint on the support of sparse signals, is often an essential prior knowledge but seldom considered in the existing networks. In this paper, we aim at bridging this gap. Specifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1) algorithm, we propose to learn the cluster structured sparsity (CSS) by rewegihting adaptively. In particular, we first unfold the Reweighted Iterative Shrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture termed as RW-LISTA. Then instead of the element-wise reweighting, the global and local reweighting manner are proposed for the cluster structured sparse learning. Numerical experiments further show the superiority of our algorithm against both classical algorithms and learning-based networks on different tasks. ", "keywords": ["Sparse Recovery", "Sparse Representation", "Structured Sparsity"], "paperhash": "jiang|learning_cluster_structured_sparsity_by_reweighting", "original_pdf": "/attachment/0183b43b7234e991c848669e874ba1d01248347a.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Cluster Structured Sparsity by Reweighting},\nauthor={Yulun Jiang and Lei Yu and Haijian Zhang and Zhou Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl28eBtwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "oNJhPl8LlN", "original": null, "number": 1, "cdate": 1576798746526, "ddate": null, "tcdate": 1576798746526, "tmdate": 1576800889578, "tddate": null, "forum": "Byl28eBtwH", "replyto": "Byl28eBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2337/-/Decision", "content": {"decision": "Reject", "comment": "The paper is proposed a rejection based on majority reviews.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yljblues@whu.edu.cn", "ly.wd@whu.edu.cn", "haijian.zhang@whu.edu.cn", "liuzhou@whu.edu.cn"], "title": "Learning Cluster Structured Sparsity by Reweighting", "authors": ["Yulun Jiang", "Lei Yu", "Haijian Zhang", "Zhou Liu"], "pdf": "/pdf/3bb43c33c1bab83811763506a2b81008d95f501e.pdf", "abstract": "Recently, the paradigm of unfolding iterative algorithms into finite-length feed-forward neural networks has achieved a great success in the area of sparse recovery. Benefit from available training data, the learned networks have achieved state-of-the-art performance in respect of both speed and accuracy. However, the structure behind sparsity, imposing constraint on the support of sparse signals, is often an essential prior knowledge but seldom considered in the existing networks. In this paper, we aim at bridging this gap. Specifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1) algorithm, we propose to learn the cluster structured sparsity (CSS) by rewegihting adaptively. In particular, we first unfold the Reweighted Iterative Shrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture termed as RW-LISTA. Then instead of the element-wise reweighting, the global and local reweighting manner are proposed for the cluster structured sparse learning. Numerical experiments further show the superiority of our algorithm against both classical algorithms and learning-based networks on different tasks. ", "keywords": ["Sparse Recovery", "Sparse Representation", "Structured Sparsity"], "paperhash": "jiang|learning_cluster_structured_sparsity_by_reweighting", "original_pdf": "/attachment/0183b43b7234e991c848669e874ba1d01248347a.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Cluster Structured Sparsity by Reweighting},\nauthor={Yulun Jiang and Lei Yu and Haijian Zhang and Zhou Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl28eBtwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byl28eBtwH", "replyto": "Byl28eBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707001, "tmdate": 1576800255143, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2337/-/Decision"}}}, {"id": "HygwxY8niB", "original": null, "number": 3, "cdate": 1573837038808, "ddate": null, "tcdate": 1573837038808, "tmdate": 1573837271773, "tddate": null, "forum": "Byl28eBtwH", "replyto": "SyxMwM1B5B", "invitation": "ICLR.cc/2020/Conference/Paper2337/-/Official_Comment", "content": {"title": "Thanks for you constructional review", "comment": "Q1: \"Motivation for CSS is that there is structure in the recovered signal \u2014 however no comparison of the recovered structure is made. While it is true, that is the signal is perfectly recovered, it would follow the structure from this data was obtained, however no such guarantees can be made for non-zero errors. \"\n\nA1: This paper is discussing cluster structured sparsity (CSS) recovery, and the main goal is to utilize the structure priori knowledge to help the recovery of the signal. So we focus more about the signal itself and only meaured our algorithm by NMSE. However,  to make this paper more convicing, we've tested support error rate (SER) of LISTA and RW-LISTA. Specifially, let T(x) denote the support of x and n denote its length, we define SER as $\\frac{[T(x)\\cup T(x^*) - T(x)\\cap T(x^*)]}{n}$. On synthesized data, the comparison of LISTA and RW-LISTA is:\n| SNR            |  15dB    |  20dB     |  25dB    |   30dB   |  35dB   | 40dB    |    \n| LISTA         |  0.2327  |  0.2234  |  0.2323  |  0.2466 | 0.2504  | 0.2436  | \n| RW-LISTA  |  0.0282  |  0.0175  |  0.0119  |  0.0076 | 0.0059 | 0.0059  |\n  \nQ2: \"Why is LISTA not compared under all varialtions in section 3.2?\"\nA2: Thanks for your kind notification. Privously we doesn't consider LISTA in section 3.2 because it has been compared in section 3.1. Now we have include LISTA in section 3.3. Please refer to Figure 6 of the paper.\n\nQ3: \"I would also like to see what happens whens a pure learning based approach (such as denoting auto-encoders) is used to recover the signal. Do they perform worse than Rw-LISTA? \"\nA3: Thanks for you valuable opinion. There've been some work using pure learning based approaches for compresive sensing. For example, Mousavi et al. (2015) has applied a three layer stacked denoising autoencoder (SDA) for compressive sensing. However, in the original paper Sigmoid is used as the activation of each layer, which is not suitable for recovery of signal with gaussian distribution. Based on our simulation, SDA performs quiet bad on synthesized data with its nonzero coefficients following stardard gaussian distribution , but behaves a little better on MNIST dataset. The result on MNIST dataset is:\n| SNR            |       5 dB      |    10dB         |   20dB       | \n| SDA            |    - 8.08dB   |  -8.56dB      |  - 8.88dB   | \n| LISTA         |    -12.51dB  |  -15.75dB    |   -23.53dB |  \n| RW-LISTA  |   -14.05dB   | -17.17dB     |  -24.23dB  |       \nStill, the performance of SDA is not as good as deep architectures unfolded by iterative algorithm (LISTA).\n\nQ4:\"'considered as unsupervised approaches since all the paramters are fixed instead of learning from data.' \u2014 this is incorrect\".\nA4: Thanks for your kind remind. We found the use of supervised vs. unsupervised misleading. So we have corrected it and instead use learnable vs. unlearnable to denote the distinction between classical SR solvers and recent deep learning based solvers."}, "signatures": ["ICLR.cc/2020/Conference/Paper2337/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yljblues@whu.edu.cn", "ly.wd@whu.edu.cn", "haijian.zhang@whu.edu.cn", "liuzhou@whu.edu.cn"], "title": "Learning Cluster Structured Sparsity by Reweighting", "authors": ["Yulun Jiang", "Lei Yu", "Haijian Zhang", "Zhou Liu"], "pdf": "/pdf/3bb43c33c1bab83811763506a2b81008d95f501e.pdf", "abstract": "Recently, the paradigm of unfolding iterative algorithms into finite-length feed-forward neural networks has achieved a great success in the area of sparse recovery. Benefit from available training data, the learned networks have achieved state-of-the-art performance in respect of both speed and accuracy. However, the structure behind sparsity, imposing constraint on the support of sparse signals, is often an essential prior knowledge but seldom considered in the existing networks. In this paper, we aim at bridging this gap. Specifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1) algorithm, we propose to learn the cluster structured sparsity (CSS) by rewegihting adaptively. In particular, we first unfold the Reweighted Iterative Shrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture termed as RW-LISTA. Then instead of the element-wise reweighting, the global and local reweighting manner are proposed for the cluster structured sparse learning. Numerical experiments further show the superiority of our algorithm against both classical algorithms and learning-based networks on different tasks. ", "keywords": ["Sparse Recovery", "Sparse Representation", "Structured Sparsity"], "paperhash": "jiang|learning_cluster_structured_sparsity_by_reweighting", "original_pdf": "/attachment/0183b43b7234e991c848669e874ba1d01248347a.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Cluster Structured Sparsity by Reweighting},\nauthor={Yulun Jiang and Lei Yu and Haijian Zhang and Zhou Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl28eBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl28eBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference/Paper2337/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2337/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2337/Reviewers", "ICLR.cc/2020/Conference/Paper2337/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2337/Authors|ICLR.cc/2020/Conference/Paper2337/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142866, "tmdate": 1576860535411, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference/Paper2337/Reviewers", "ICLR.cc/2020/Conference/Paper2337/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2337/-/Official_Comment"}}}, {"id": "rkgDrRBhoB", "original": null, "number": 2, "cdate": 1573834303496, "ddate": null, "tcdate": 1573834303496, "tmdate": 1573834303496, "tddate": null, "forum": "Byl28eBtwH", "replyto": "HyxE0iR-cS", "invitation": "ICLR.cc/2020/Conference/Paper2337/-/Official_Comment", "content": {"title": "Thank you for you careful review and kind notification", "comment": "Q1: \"The paper is slightly hard to read due to many typos, and hard-to-read sentence.\"\nA1: Thanks for your careful reading and kind notification of the paper. We'll fix these typos and meticulously proofread  the paper. \n\nQ2: \"I am not sure how it shows that the sparse representation is learning the underlying structure?\"  / \"Also, I think a more intuitive explanation of how the reweighting helps preserve structure is needed.\"\nA2:  On the one hand, the experiments show that RW-LISTA significantly outcomes LISTA and classical CSS solvers in clustered structuer sparse recovery, revealing that the proposed algorithm has successfully (learned to) utilize the structure priori for better recovery. On the other hand, as shown in Figure 2, the learned reweighting block favors cluster pattern of the signal while depressing isolated coefficients.\n\nQ3: \"I did not really see any substantial improvements in performance as compared to say LISTA\" /\n\"Perhaps some improved experiments in real datasets.\"\nA3: Thanks for the constructional idea. In different noist conditions, the performance of LISTA against RW-LISTA on MNIST dataset is (measured by NMSE):\n| SNR            |       5 dB      |  10dB   \t      |   20dB       |\n| LISTA         |    -12.51dB  |   -15.75dB   |   -23.53dB |\n| RW-LISTA  |   -14.05dB   | -17.17dB     |  -24.23dB  |\nIt shows that the superority of RW-LISTA against LISTA is more clear under lower SNR conditions, so we've changed the SNR in section 3.3 to 5 dB."}, "signatures": ["ICLR.cc/2020/Conference/Paper2337/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yljblues@whu.edu.cn", "ly.wd@whu.edu.cn", "haijian.zhang@whu.edu.cn", "liuzhou@whu.edu.cn"], "title": "Learning Cluster Structured Sparsity by Reweighting", "authors": ["Yulun Jiang", "Lei Yu", "Haijian Zhang", "Zhou Liu"], "pdf": "/pdf/3bb43c33c1bab83811763506a2b81008d95f501e.pdf", "abstract": "Recently, the paradigm of unfolding iterative algorithms into finite-length feed-forward neural networks has achieved a great success in the area of sparse recovery. Benefit from available training data, the learned networks have achieved state-of-the-art performance in respect of both speed and accuracy. However, the structure behind sparsity, imposing constraint on the support of sparse signals, is often an essential prior knowledge but seldom considered in the existing networks. In this paper, we aim at bridging this gap. Specifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1) algorithm, we propose to learn the cluster structured sparsity (CSS) by rewegihting adaptively. In particular, we first unfold the Reweighted Iterative Shrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture termed as RW-LISTA. Then instead of the element-wise reweighting, the global and local reweighting manner are proposed for the cluster structured sparse learning. Numerical experiments further show the superiority of our algorithm against both classical algorithms and learning-based networks on different tasks. ", "keywords": ["Sparse Recovery", "Sparse Representation", "Structured Sparsity"], "paperhash": "jiang|learning_cluster_structured_sparsity_by_reweighting", "original_pdf": "/attachment/0183b43b7234e991c848669e874ba1d01248347a.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Cluster Structured Sparsity by Reweighting},\nauthor={Yulun Jiang and Lei Yu and Haijian Zhang and Zhou Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl28eBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl28eBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference/Paper2337/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2337/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2337/Reviewers", "ICLR.cc/2020/Conference/Paper2337/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2337/Authors|ICLR.cc/2020/Conference/Paper2337/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142866, "tmdate": 1576860535411, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference/Paper2337/Reviewers", "ICLR.cc/2020/Conference/Paper2337/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2337/-/Official_Comment"}}}, {"id": "SylaFTH2sH", "original": null, "number": 1, "cdate": 1573834117142, "ddate": null, "tcdate": 1573834117142, "tmdate": 1573834117142, "tddate": null, "forum": "Byl28eBtwH", "replyto": "rylJ1Qhx9H", "invitation": "ICLR.cc/2020/Conference/Paper2337/-/Official_Comment", "content": {"title": "Thanks for your valuable and professional opinion", "comment": "Q1: \"The proposed approach in too incremental.\" / \"The proposed approach is a straightforward combination of LISTA and RwISTA and the section of global/local dependence regarding cluster-sparse structures is unsurprising.\"\n\nA1: This paper aims at addressing the problem of clustered structure sparse recovery. The main contribution of this paper is the insight of learning (to recover) cluster structured sparsity by reweighting.  Note that traditional reweighting algorithm (candes et al, 2008) was not designed for structured spasity. We expand the reweighting mechanism to structured problems by the power of deep learning. Specifically a reweighting block is proposed to introduce local and global dependencies of the signal's coefficients. And existing algorithms of RwISTA and LISTA are chosen for the incarnation of our idea. However, it should not be limited to RwISTA and LISTA, other algorithms (such as AMP) are also applicable. \n\nMoreover, detailed analysis and experiments is also an important contribution of our work. In section 3 we conduct exhaustive simulations by comparsion to both classical and deep learning based algorithms in different settings. The result of experiments verifies the effectiveness and superioriy of our algorithm especially in noisy cases.\n\nQ2:\"Even though signals may exhibit cluster-sparsity, the size of such cluster might vary widely and it is therefore questionable if such patterns can be best captured via connections in the proposed reweighing blocks. Indeed for some blocks wider or lower neighborhoods might be needed to capture various radii of dependence. \"\n\nA2: Even though the block size could vary widely, each coefficient is most statistially related to its very nearing neighborhoods. The larger the distances between two coefficients are, the less the dependence is. Considering the variation of block size, a reasonable receptive field of the reweighting block suits best for recovery. This idea is verified by figure 4(b) and (c): in figure 4(b) there is only 5 to 6 elements excitated in each row of the learned adjacency matrix, while in figure 4(c) a reweighting block consists of two 1*3 convolution layers achieves best performance, which is able to couple each coefficient with its neighboring 4 coefficients.\n\nIndeed, some previous works also showed that a relatively reasonable field of connection is enough to handle the variation of block sizes. For example, in PCSBL (Fang et al, 2014) and CluSS (Yu et al, 2015), clustered pattern is captured only via the connection of 2 neighboring coefficients, i.e., $x_i$ is only coupled with $x_{i-1}$ and $x_{i+1}$. \n\nQ3: \"The use of unsupervised vs supervised is misleading.\"\nA3: Thanks for your kind notification. We've revised it and instead use learnable vs. unlearnable to refer the distinction between classical SR solvers and recent deep learning based solvers.\n\nQ4: \"A wider variety of block structure with more or less variability in block size etc should be considered. In addition it would be important to compare against vanilla Rw-ISTA, as one of the classical CSS solvers.\"\nA4: Thanks for you suggestion, we have considered the variety of block structure by changing sparsity and block number of the signal in section 3.3. Please refer to Figure 6(a) and Figure 6(c). Also, we've made comparison with vallina Rw-ISTA Figure (4).\n\nQ5: Ideally we are looking to improve in less favorable conditions of low SNR.\nA5: We've revised the experiments in section 3.3 and set SNR to 5dB, where the difference of LISTA and RW-LISTA is more distincted.\n\nQ6: As an alternative to adopting RwISTA it might be pertinent to compare against a counterpart using fused lasso penalty (Tibshirani et al 2005).\nA6: To our best knowledge, fused lasso is proposed to encourage both sparsity and smoothness of recovered signal. However, in our setting the magnitude of signal's coefficients could vary so we think it's not proper to compare with fused lasso.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2337/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yljblues@whu.edu.cn", "ly.wd@whu.edu.cn", "haijian.zhang@whu.edu.cn", "liuzhou@whu.edu.cn"], "title": "Learning Cluster Structured Sparsity by Reweighting", "authors": ["Yulun Jiang", "Lei Yu", "Haijian Zhang", "Zhou Liu"], "pdf": "/pdf/3bb43c33c1bab83811763506a2b81008d95f501e.pdf", "abstract": "Recently, the paradigm of unfolding iterative algorithms into finite-length feed-forward neural networks has achieved a great success in the area of sparse recovery. Benefit from available training data, the learned networks have achieved state-of-the-art performance in respect of both speed and accuracy. However, the structure behind sparsity, imposing constraint on the support of sparse signals, is often an essential prior knowledge but seldom considered in the existing networks. In this paper, we aim at bridging this gap. Specifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1) algorithm, we propose to learn the cluster structured sparsity (CSS) by rewegihting adaptively. In particular, we first unfold the Reweighted Iterative Shrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture termed as RW-LISTA. Then instead of the element-wise reweighting, the global and local reweighting manner are proposed for the cluster structured sparse learning. Numerical experiments further show the superiority of our algorithm against both classical algorithms and learning-based networks on different tasks. ", "keywords": ["Sparse Recovery", "Sparse Representation", "Structured Sparsity"], "paperhash": "jiang|learning_cluster_structured_sparsity_by_reweighting", "original_pdf": "/attachment/0183b43b7234e991c848669e874ba1d01248347a.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Cluster Structured Sparsity by Reweighting},\nauthor={Yulun Jiang and Lei Yu and Haijian Zhang and Zhou Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl28eBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl28eBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference/Paper2337/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2337/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2337/Reviewers", "ICLR.cc/2020/Conference/Paper2337/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2337/Authors|ICLR.cc/2020/Conference/Paper2337/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142866, "tmdate": 1576860535411, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2337/Authors", "ICLR.cc/2020/Conference/Paper2337/Reviewers", "ICLR.cc/2020/Conference/Paper2337/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2337/-/Official_Comment"}}}, {"id": "rylJ1Qhx9H", "original": null, "number": 1, "cdate": 1572025047417, "ddate": null, "tcdate": 1572025047417, "tmdate": 1572972351800, "tddate": null, "forum": "Byl28eBtwH", "replyto": "Byl28eBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2337/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper combines deep learning and compressed sensing. Specifically the RW-LISTA algorithm is proposed for cluster-structured sparse recovery, building upon two existing methods: the Reweighted Iterative Shrinkage Algorithm and the LISTA algorithm. The reweighing process is employed to infer the dependencies between coefficients and encourage cluster structure. Strategies for local and global dependence are presented. The approach is evaluated on synthetic and real datasets.\n\nThe paper considers and important topic. However the proposed approach is too incremental and the empirical evaluation could also be improved. In addition the presentation needs more work.  Specifically:\n- the use of unsupervised vs supervised is misleading. Traditional CS approaches are in fact supervised as they map to a regression problem when both input sensing matrix and response vector are available. The distinction has more to do with the ability (or lack of ability) to learn representations.\n- the proposed approach is a straightforward combination of LISTA and RwISTA and the section of global/local dependence regarding cluster-sparse structures is unsurprising.\n- Even though signals may exhibit cluster-sparsity, the size of such cluster might vary widely and it is therefore questionable if such patterns can be best captured via connections in the proposed reweighing blocks. Indeed for some blocks wider or lower neighborhoods might be needed to capture various radii of dependence.  \n- As an alternative to adopting RwISTA it might be pertinent to compare against a counterpart using fused lasso penalty (Tibshirani et al 2005).\n- Experiments are limited: a wider variety of block structure with more or less variability in block size etc should be considered. In addition it would be important to compare against vanilla  Rw-ISTA, as one of the classical CSS solvers.\n- It is somewhat disappointing that the proposed approach should be better the higher the SNR, where other approaches can do well enough. Ideally we are looking to improve in less favorable conditions of low SNR.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2337/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2337/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yljblues@whu.edu.cn", "ly.wd@whu.edu.cn", "haijian.zhang@whu.edu.cn", "liuzhou@whu.edu.cn"], "title": "Learning Cluster Structured Sparsity by Reweighting", "authors": ["Yulun Jiang", "Lei Yu", "Haijian Zhang", "Zhou Liu"], "pdf": "/pdf/3bb43c33c1bab83811763506a2b81008d95f501e.pdf", "abstract": "Recently, the paradigm of unfolding iterative algorithms into finite-length feed-forward neural networks has achieved a great success in the area of sparse recovery. Benefit from available training data, the learned networks have achieved state-of-the-art performance in respect of both speed and accuracy. However, the structure behind sparsity, imposing constraint on the support of sparse signals, is often an essential prior knowledge but seldom considered in the existing networks. In this paper, we aim at bridging this gap. Specifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1) algorithm, we propose to learn the cluster structured sparsity (CSS) by rewegihting adaptively. In particular, we first unfold the Reweighted Iterative Shrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture termed as RW-LISTA. Then instead of the element-wise reweighting, the global and local reweighting manner are proposed for the cluster structured sparse learning. Numerical experiments further show the superiority of our algorithm against both classical algorithms and learning-based networks on different tasks. ", "keywords": ["Sparse Recovery", "Sparse Representation", "Structured Sparsity"], "paperhash": "jiang|learning_cluster_structured_sparsity_by_reweighting", "original_pdf": "/attachment/0183b43b7234e991c848669e874ba1d01248347a.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Cluster Structured Sparsity by Reweighting},\nauthor={Yulun Jiang and Lei Yu and Haijian Zhang and Zhou Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl28eBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl28eBtwH", "replyto": "Byl28eBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575635940609, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2337/Reviewers"], "noninvitees": [], "tcdate": 1570237724274, "tmdate": 1575635940621, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2337/-/Official_Review"}}}, {"id": "HyxE0iR-cS", "original": null, "number": 2, "cdate": 1572101068037, "ddate": null, "tcdate": 1572101068037, "tmdate": 1572972351751, "tddate": null, "forum": "Byl28eBtwH", "replyto": "Byl28eBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2337/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "An approach is proposed to learn sparse representations while preserving some structures in the data.\n\nThe idea seems quite nice where we want to learn the structure that induces sparsity instead of simply sparse representations. The idea is to extend a recent algorithm RwISTA by adding reweighting block. The reweighing block changes weights to encode whether coefficients in the model learned by the network are similar or dissimilar to each other. To build the reweighing block convolutional layers are used.\n\nThe paper is slightly hard to read due to many typos, and hard-to-read sentences. Also, I think a more intuitive explanation of how the reweighting helps preserve structure is needed. Right now it is very difficult to understand (except maybe for experts working on similar problems?) Regarding the experiments, most of them are run with synthetic cases. It seems like the approach is compared with several recent approaches though showing good results. On the MNIST data, results are shown where the images are recovered from the sparse representation. I did not really see any substantial improvements in performance as compared to say LISTA. Maybe I am misunderstanding what is being evaluated in Figure 7. Also I am not sure how it shows that the sparse representation is learning the underlying structure? Maybe some re-writing is needed to make this clearer.\n\nI think the paper is interesting but needs some polishing to make it easier to read and perhaps some improved experiments in real datasets.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2337/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2337/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yljblues@whu.edu.cn", "ly.wd@whu.edu.cn", "haijian.zhang@whu.edu.cn", "liuzhou@whu.edu.cn"], "title": "Learning Cluster Structured Sparsity by Reweighting", "authors": ["Yulun Jiang", "Lei Yu", "Haijian Zhang", "Zhou Liu"], "pdf": "/pdf/3bb43c33c1bab83811763506a2b81008d95f501e.pdf", "abstract": "Recently, the paradigm of unfolding iterative algorithms into finite-length feed-forward neural networks has achieved a great success in the area of sparse recovery. Benefit from available training data, the learned networks have achieved state-of-the-art performance in respect of both speed and accuracy. However, the structure behind sparsity, imposing constraint on the support of sparse signals, is often an essential prior knowledge but seldom considered in the existing networks. In this paper, we aim at bridging this gap. Specifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1) algorithm, we propose to learn the cluster structured sparsity (CSS) by rewegihting adaptively. In particular, we first unfold the Reweighted Iterative Shrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture termed as RW-LISTA. Then instead of the element-wise reweighting, the global and local reweighting manner are proposed for the cluster structured sparse learning. Numerical experiments further show the superiority of our algorithm against both classical algorithms and learning-based networks on different tasks. ", "keywords": ["Sparse Recovery", "Sparse Representation", "Structured Sparsity"], "paperhash": "jiang|learning_cluster_structured_sparsity_by_reweighting", "original_pdf": "/attachment/0183b43b7234e991c848669e874ba1d01248347a.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Cluster Structured Sparsity by Reweighting},\nauthor={Yulun Jiang and Lei Yu and Haijian Zhang and Zhou Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl28eBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl28eBtwH", "replyto": "Byl28eBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575635940609, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2337/Reviewers"], "noninvitees": [], "tcdate": 1570237724274, "tmdate": 1575635940621, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2337/-/Official_Review"}}}, {"id": "SyxMwM1B5B", "original": null, "number": 3, "cdate": 1572299354272, "ddate": null, "tcdate": 1572299354272, "tmdate": 1572972351702, "tddate": null, "forum": "Byl28eBtwH", "replyto": "Byl28eBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2337/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Paper extends LISTA by introducing learned re-weighting for the problem of sparse signal recovery. Paper combines the insights from RW-ISTA (a re-weighted iterative algorithm with fixed parameters and LISTA, a learned iterative algorithm without re-weighting. \n\nResults show that: \n(a) On synthetic data, performance of Rw-LISTA is superior to many variants of LISTA for various SNRs\n(b) On synthetic data, Rw-LIST is better than classical approaches when SNR, Sparsity and other factors are varied. Why is LISTA not compared under all these different variations? \n(c) On MNIST, Rw-LISTA is much better than non-learned approaches, but seems very close to LISTA. \n\nIt strikes me that authors make all evaluations based on NMSE. However, motivation for CSS is that there is structure in the recovered signal \u2014 however no comparison of the recovered structure is made. While it is true, that is the signal is perfectly recovered, it would follow the structure from this data was obtained, however no such guarantees can be made for non-zero errors. \n\nI would also like to see what happens whens a pure learning based approach (such as denoting auto-encoders) is used to recover the signal. Do they perform worse than Rw-LISTA? \n\nAt present, I think the paper doesnot meet the standard of ICLR submissions. However, if authors address my concerns, I am happy to change the rating. \n\nMinor Comments:\n\u201cconsidered as unsupervised approaches since all the paramters are fixed instead of learning from data.\u201d \u2014 this is incorrect. A significant human intuition went into design of these systems. While one can argue that even NNet architecture design requires human intuition, the trend is towards using less domain-specific architectures. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2337/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2337/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yljblues@whu.edu.cn", "ly.wd@whu.edu.cn", "haijian.zhang@whu.edu.cn", "liuzhou@whu.edu.cn"], "title": "Learning Cluster Structured Sparsity by Reweighting", "authors": ["Yulun Jiang", "Lei Yu", "Haijian Zhang", "Zhou Liu"], "pdf": "/pdf/3bb43c33c1bab83811763506a2b81008d95f501e.pdf", "abstract": "Recently, the paradigm of unfolding iterative algorithms into finite-length feed-forward neural networks has achieved a great success in the area of sparse recovery. Benefit from available training data, the learned networks have achieved state-of-the-art performance in respect of both speed and accuracy. However, the structure behind sparsity, imposing constraint on the support of sparse signals, is often an essential prior knowledge but seldom considered in the existing networks. In this paper, we aim at bridging this gap. Specifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1) algorithm, we propose to learn the cluster structured sparsity (CSS) by rewegihting adaptively. In particular, we first unfold the Reweighted Iterative Shrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture termed as RW-LISTA. Then instead of the element-wise reweighting, the global and local reweighting manner are proposed for the cluster structured sparse learning. Numerical experiments further show the superiority of our algorithm against both classical algorithms and learning-based networks on different tasks. ", "keywords": ["Sparse Recovery", "Sparse Representation", "Structured Sparsity"], "paperhash": "jiang|learning_cluster_structured_sparsity_by_reweighting", "original_pdf": "/attachment/0183b43b7234e991c848669e874ba1d01248347a.pdf", "_bibtex": "@misc{\njiang2020learning,\ntitle={Learning Cluster Structured Sparsity by Reweighting},\nauthor={Yulun Jiang and Lei Yu and Haijian Zhang and Zhou Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl28eBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl28eBtwH", "replyto": "Byl28eBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2337/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575635940609, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2337/Reviewers"], "noninvitees": [], "tcdate": 1570237724274, "tmdate": 1575635940621, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2337/-/Official_Review"}}}], "count": 8}