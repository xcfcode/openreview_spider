{"notes": [{"id": "BklSwn4tDH", "original": "SJxWzY-WXH", "number": 3, "cdate": 1569438813108, "ddate": null, "tcdate": 1569438813108, "tmdate": 1577168285185, "tddate": null, "forum": "BklSwn4tDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Twvf8XCqaF", "original": null, "number": 1, "cdate": 1576798684552, "ddate": null, "tcdate": 1576798684552, "tmdate": 1576800950221, "tddate": null, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Decision", "content": {"decision": "Reject", "comment": "This paper focuses on avoiding overfitting in the presence of noisy labels. The authors develop a two phase method called pre-stopping based on a combination of early stopping and a maximal safe set. The reviewers raised some concern about illustrating maximal safe set for all data sets and suggest comparisons with more baselines. The reviewers also indicated that the paper is missing key relevant publications. In the response the authors have done a rather through job of addressing the reviewers comments. I thank them for this. However, given the limited time some of the reviewers comments regarding adding new baselines could not be addressed. As a result I can not recommend acceptance because I think this is key to making a proper assessment. That said, I think this is an interesting with good potential if it can outperform other baselines and would recommend that the authors revise and resubmit in a future venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718897, "tmdate": 1576800269445, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper3/-/Decision"}}}, {"id": "Bkx69x5qoS", "original": null, "number": 11, "cdate": 1573720213496, "ddate": null, "tcdate": 1573720213496, "tmdate": 1573720213496, "tddate": null, "forum": "BklSwn4tDH", "replyto": "r1xfP7FKFS", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (2/2)", "comment": "We were very happy to have the opportunity to reflect your insightful comments. During the remaining rebuttal period, we are willing to reflect your additional comments if you have. Thank you again for your valuable comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "BklzkXwLiH", "original": null, "number": 7, "cdate": 1573446362313, "ddate": null, "tcdate": 1573446362313, "tmdate": 1573720179415, "tddate": null, "forum": "BklSwn4tDH", "replyto": "r1xfP7FKFS", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (1/2)", "comment": "Thank you for raising your insightful and detailed comments. We have revised our paper to address your concerns; please see the modified parts marked as \u201cR2\u201d in teal color and marked as \u201cRA\u201d in magenta color. \n\nBelow is the summary of our response to your concerns:\n\nQ2-1. \"Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks\" is not cited.\nA2-1. The suggested paper provides a theoretical analysis of early stopping and makes the claim that the early stopped network is fairly robust to the label noise compared with the fully trained one. However, please note that this paper does not provide any idea to exploit the early stopped network for \"sample selection\". In contrast, Prestopping adopts early stopping to derive a seed for confident samples called the maximal safe set. Thus, our novelty lies in the \u201cmerger\u201d of early stopping and learning from the maximal safe set. This merger is our unique contribution. We have highlighted this contribution in Section 2.\n\nQ2-2. It would be necessary to illustrate the maximal safe set for all the data sets. Also, it would be essential to compare the proposed method with small loss-based methods.\nA2-2. Thank you for your careful comments. We have added the results in Figure 4 (Section 3.2.2). The label precision and recall were very high in all datasets. Furthermore, we have added the performance evaluation of selecting clean samples using the small-loss trick (Co-teaching) and the maximal safe set (Prestopping). The selection accuracy of Prestopping was much higher (by up to 18.2pp) than that of Co-teaching (See Section 5.3 for details).\n\nQ2-3. The authors are suggested to compare the proposed method with more baselines.\nA2-3. Thank you for your comments. We could not add more baselines during the rebuttal period. However, many baselines exploiting the transition matrix (e.g., S-model[1], F-correction[2]) have been reported to perform poorly than Co-teaching [3] we included, and we confirm that Prestopping outperforms more recent baselines according to the accuracy reported in the literature. For example, for CIFAR-10 and 30%-pair noise, the test accuracy of Prestopping was 90.8%, while the test accuracy of ITLM[4] was 88.2%; For CIFAR-10 and 40%-symmetric noise, the test accuracy of Prestopping was 88.7%, while the test accuracy of D2L[5] was 83.4%. We will definitely add more baselines to the camera-ready version, if accepted. \n\n[1] Training Deep Neural Networks using A Noise Adaptation Layer (ICLR\u201917)\n[2] Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach (CVPR\u201917)\n[3] Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (NIPS\u201918)\n[4] Learning with Bad Training Data via Iterative Trimmed Loss Minimization (ICML\u201919)\n[5] Dimensionality-Driven Learning with Noisy Labels (ICML\u201918)\n\nQ2-4. It is unclear from the paper that if the baselines have used the clean validation sets.\nA2-4. For fair comparison, all the baselines used the clean validation sets to select the best number of epochs based on the validation accuracy. The best test error reported in our paper was evaluated using the network at the time of the lowest validation error. We have clarified this issue in the updated paper. (See Section 5 for details).\n\nQ2-5. Clothing1M is a more challenging dataset with real-world label noise.\nA2-5. We have conducted additional experiments for Clothing1M. Prestopping significantly outperformed the existing methods by 0.7\u20145.8pp. Please see Appendix B.3 of the updated paper.\n\nQ2-6. The authors are suggested to make it clear why noise rates are sometimes available for use.\nA2-6. The noise rate is typically not known in real-world scenarios. Thus, the validation heuristic is preferred to the noise-rate heuristic. However, users may figure out the noise rate by manually investigating a small, random sample of a data set. It is also available to use the estimated noise rate or noise transition matrix based on the recent work[1, 2, 6].\n[6] Using Trusted Data to Train Deep Networks on Label Corrupted by Severe Noise (NIPS'18)\n\nIf you want to know the other changes in the updated paper, see the comment \"Summary of the overall revision\" at the top.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "S1g68fP8ir", "original": null, "number": 6, "cdate": 1573446229015, "ddate": null, "tcdate": 1573446229015, "tmdate": 1573720163541, "tddate": null, "forum": "BklSwn4tDH", "replyto": "r1gL1Hq6Yr", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (1/2)", "comment": "Thank you for raising your insightful and detailed comments. We have revised our paper to address your concerns; please see the modified parts marked as \u201cR1\u201d in brown color and marked as \u201cRA\u201d in magenta color. \n\nBelow is the summary of our response to your concerns:\n\nQ1-1: The reviewer believes the paper has missed several very relevant papers that provide very similar ideas.\nA1-1: We fully understand your concern. However, please note that, in our work, early stopping is adopted only to derive a \"seed\u201d for the maximal safe set. Our novelty lies in the \"merger\u201d of early stopping and learning from the maximal safe set. On the other hand, the two suggest papers [2, 3] provide only empirical or theoretical evidence that the early stopped network is fairly robust to label noise compared with the fully trained one. \n\nAs for the rest work [1], although they used early stopping in their experiment, they did not answer the following challenging question: when is the best point for the early stop? They simply stopped the network at a certain point and then used small-loss samples as clean samples. In our work, we thoroughly explore the best stop point to obtain not only quantitatively sufficient but also qualitatively less noisy \"seed\u201d for the clean samples (i.e., maximal safe set). The advantage of our criterion for the safe set is proven by the fact we exploit almost all true-labeled samples in \"both\u201d types of noise (See Figure 4). Furthermore, we have added the performance evaluation of selecting clean samples using the small-loss trick and the maximal safe set. The selection accuracy using the maximal safe set was much higher (by up to 18.2pp) than that using the small-loss trick. Please see Section 5.3 for details (This section has been added at the request of Reviewer 2).\n\nRobust training of DNNs with label noise is a very active research topic. Thus, many studies are being conducted \"in parallel\" along this direction. Thus, it seems possible that well-established techniques (e.g., early stopping) could be shared in these parallel studies. Nevertheless, the above-mentioned merger is our unique contribution. We have clarified this contribution to our updated paper (See Section 2 for details).\n\nBesides, Prestopping has achieved the \"best accuracy\" in popular benchmark data sets, as far as we know. We strongly believe this high accuracy is the most important contribution. Specifically, Prestopping is shown to outperform Co-teaching+(ICML\u201919), an improvement of Co-teaching. Although we could not directly compare Prestopping with ITLM[1], for CIFAR-10 and 30%-pair noise, the test accuracy of Prestopping was 90.8%, while the test accuracy of ITLM[1] was 88.2%.\n\nIf you want to know the other changes in the updated paper, see the comment \"Summary of the overall revision\" at the top.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "HJl6Ilc5sS", "original": null, "number": 10, "cdate": 1573720148988, "ddate": null, "tcdate": 1573720148988, "tmdate": 1573720148988, "tddate": null, "forum": "BklSwn4tDH", "replyto": "r1gL1Hq6Yr", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (2/2)", "comment": "We were very happy to have the opportunity to reflect your insightful comments. During the remaining rebuttal period, we are willing to reflect your additional comments if you have. Thank you again for your valuable comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "HyxY4gccjr", "original": null, "number": 9, "cdate": 1573720113425, "ddate": null, "tcdate": 1573720113425, "tmdate": 1573720113425, "tddate": null, "forum": "BklSwn4tDH", "replyto": "Syem3k3LqH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (2/2)", "comment": "We were very happy to have the opportunity to reflect your insightful comments. During the remaining rebuttal period, we are willing to reflect your additional comments if you have. Thank you again for your valuable comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "rJxwlfDLjS", "original": null, "number": 5, "cdate": 1573446127149, "ddate": null, "tcdate": 1573446127149, "tmdate": 1573720083636, "tddate": null, "forum": "BklSwn4tDH", "replyto": "Syem3k3LqH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (1/2)", "comment": "Thank you for raising your insightful and detailed comments. We have revised our paper to address your concerns; please see the modified parts marked as \u201cRA\u201d in magenta color. \n\nBelow is the summary of our response to your concerns:\n\nQ4-1. The authors should include related work about early-stopping on label noise training.\nA4-1. Thank you for suggesting useful references. The suggested papers provide empirical or theoretical evidence that the early stopped network is fairly robust to label noise compared with the fully trained one. However, these papers do not provide any idea to exploit the early stopped network for \"sample selection\". In contrast, Prestopping adopts early stopping to derive a seed for clean samples called the maximal safe set. Also, Prestopping is clearly different from recent studies that select the clean samples based on the small-loss[1-5], in considering that memorized samples are regarded as the maximal safe set. Thus, our novelty lies in the \u201cmerger\u201d of early stopping and learning from the maximal safe set. This is our unique contribution. We have highlighted this contribution by including the suggested papers (See Section 2 for details).\n\n[1] Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (NIPS\u201918)\n[2] MentorNet: Learning Data-driven Curriculum for Very Deep Neural Networks on Corrupted Labels (ICML'18)\n[3] Understanding Generalization of Deep Neural Networks Trained with Noisy Labels (ICML'19)\n[4] Learning with Bad Training Data via Iterative Trimmed Loss Minimization (ICML'19)\n[5] How does Disagreement Help Generalization against Label Corruption? (ICML'19)\n\nQ4-2. The reviewer is slightly worried about novelty of the ideas.\nA4-2. We understand your concern. However, as mentioned earlier, our novelty lies in selecting highly confident samples initially derived from the early stopped network. Owing to the advantage of the merger, Prestopping has achieved much higher test accuracy than state-of-the-art methods, and Prestopping is the best performer, as far as we know. Furthermore, the accuracy of selecting true-labeled samples via Prestopping was much higher (by up to 18.2pp) than that via the small-loss trick. Please see Section 5.3 for details (This section has been added at the request of Reviewer 2). We hope that this significant contribution is acknowledged. \n\nIf you want to know the other changes in the updated paper, see the comment \"Summary of the overall revision\" at the top."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "Hyx_Ze55iH", "original": null, "number": 8, "cdate": 1573720064212, "ddate": null, "tcdate": 1573720064212, "tmdate": 1573720064212, "tddate": null, "forum": "BklSwn4tDH", "replyto": "Byx-7fCU5S", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (2/2) ", "comment": "We were very happy to have the opportunity to reflect your insightful comments. During the remaining rebuttal period, we are willing to reflect your additional comments if you have. Thank you again for your valuable comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "B1gIN-DLir", "original": null, "number": 4, "cdate": 1573445933789, "ddate": null, "tcdate": 1573445933789, "tmdate": 1573719881462, "tddate": null, "forum": "BklSwn4tDH", "replyto": "Byx-7fCU5S", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (1/2)", "comment": "Thank you for raising your insightful and detailed comments. We have revised our paper to address your concerns; please see the modified parts marked as \u201cR3\u201d in violet color and marked as \u201cRA\u201d in magenta color. \n\nBelow is the summary of our response to your concerns:\n\nQ3-1. The criterion to construct a maximal safe set is conventional.\nA3-1. We understand your point. The overall framework is similar to self-training or co-training, as you mentioned. Nevertheless, the detail of deciding safe samples is obviously different. The advantage of our criterion is proven by the fact we exploit almost all true-labeled samples in \"both\u201d types of noise (See Figure 4). Then, this advantage leads to much higher test accuracy (See Q3-3 & Q3-4).\n\nQ3-2. The pipeline (learning the network - selecting a subset of the training samples - re-learning the network) has been used in other methods.\nA3-2. Yes, you are right. The existing methods which belong to the \"sample selection\" category in Section 2 have the same pipeline, including the most recent approach Co-teaching+ (ICML\u201919). In this sense, we believe that the novelty can be found in how to select clean samples.\n\nSpecifically, for sample selection, Iterative[1] used the local outlier factor algorithm and D2L[2] used the local intrinsic dimensionality. Also, there are many studies used the small-loss trick such as Co-teaching (ICML'18), Co-teaching+ (ICML'19), ITLM (ICML'19), and INCV (ICML'19). Compared with these methods, our novelty lies in selecting highly safe samples called the maximal safe set initially derived from the early stopped network. We have clarified the difference between sample selection methods by including the recent work we missed (See Section 2 for details).\n\nQ3-3. More baselines need to be compared.\nA3-3. Thank you for your comments. We could not add the two methods[1,2] during the rebuttal period, but we could indirectly compare Prestopping with Iterative[1] and D2L[2] because the two papers have the common setting. Specifically, Prestopping outperforms Co-teaching and Co-teaching+, but Iterative[1] has been reported to perform poorly than them. In addition, according to the test accuracy in the literature, the test accuracy of Prestopping was 88.7% for CIFAR-10 with 40%-symmetric noise, while the test accuracy of D2L[2] was 83.4%. We will definitely add more baselines to the camera-ready version, if accepted.\n\nQ3-4. ImageNet or even a subset of ImageNet needs to be used for experiments.\nA3-4. We have included Tiny-ImageNet in the updated experiments. Prestopping maintained dominance also in Tiny-ImageNet by 1.7\u201413.9pp. Please see Appendix B.2 of the updated paper.\nBesides, ANIMAL-10N and Food-101N datasets contain 64x64 images, and a new challenging real-world dataset called Clothing1M have been added in Appendix B.3. \n\nQ3-5. The contribution of this paper is very incremental.\nA3-5. We understand your concern. However, as mentioned earlier, owing to the advantage of our maximal safe set, Prestopping has achieved much higher test accuracy than the state-of-the-art methods, and Prestopping is the best performer, as far as we know. Furthermore, the accuracy of selecting true-labeled samples via Prestopping was much higher (by up to 18.2pp) than that via the small-loss trick. Please see Section 5.3 for details (This section has been added at the request of Reviewer 2). We hope that this significant contribution is acknowledged. Thank you for your insightful comments.\n\nIf you want to know the other changes in the updated paper, see the comment \"Summary of the overall revision\" at the top.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "rkxHUxPIor", "original": null, "number": 3, "cdate": 1573445709209, "ddate": null, "tcdate": 1573445709209, "tmdate": 1573447867705, "tddate": null, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"title": "Summary of the overall revision", "comment": "Dear reviewers, we would like to thank for your insightful and detailed comments.\nWe have responded individually to each reviewer and below is the summary of the overall revision of the updated paper.\n\n1.\t*** Contribution of the paper (Reviewer 1, 2, 3, 4) ***\nMost reviewers agreed that the paper is well written, the idea is clearly presented, and the experiments also seem convincing. However, because we missed many relevant papers, all reviewers pointed out that the novelty of the paper is unclear. \n\nAs pointed out by the reviewers, few recent studies[1-3] provide empirical or theoretical evidence that the early stopped network is fairly robust to label noise compared with the fully trained one. However, they do not provide any idea to exploit early stopping for \"sample selection\", which is one of the common directions for handling label noise. In contrast, our work adopts early stopping to derive a \"seed\u201d for the safe samples called the maximal safe set and explores the best stop point to obtain not only quantitatively sufficient but also qualitatively less noisy \"seed\u201d for the maximal safe set. \n\n[1] Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks (arXiv'19)\n[2] Understanding Generalization of Deep Neural Networks Trained with Noisy Labels (ICML'19)\n[3] Using Pre-training can Improve Model Robustness and Uncertainty (ICML\u201919)\n\nCompared with the work [2, 4-8] belonging to \"sample selection\u201d category, our novelty lies in how to select clean samples. Specifically, for sample selection, Iterative[4] used the local outlier factor algorithm and D2L[5] used the local intrinsic dimensionality. Also, INCV[2], Co-teaching[6], Co-teaching+[7], and ITLM[8] used the small-loss trick. Please note, in our paper, Prestopping uses the maximal safe set derived from the memorized samples at the (estimated) best stop point. Definitely, the criterion of the maximal safe set is different. \n\n[4] Iterative Learning with Open-set Noisy Labels, (CVPR\u201918)\n[5] Dimensionality-Driven Learning with Noisy Labels (ICML\u201918)\n[6] Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (NIPS\u201918)\n[7] How does Disagreement Help Generalization against Label Corruption? (ICML'19)\n[8] Learning with Bad Training Data via Iterative Trimmed Loss Minimization (ICML\u201919)\n\nRobust training of DNNs with label noise is a very active research topic. Thus, many studies are being conducted \"in parallel\" along this direction. Thus, it seems possible that well-established techniques (e.g., early stopping) could be shared in these parallel studies. Nevertheless, the above-mentioned \"merger\" of early stopping and learning from the maximal safe set is our unique contribution. We have clarified this contribution to our updated paper (See Section 2 for details).\n\n2.\tEvaluation on more datasets such as Tiny-ImageNet and Clothing1M (Reviewer 2, 3)\n\nWe have conducted additional experiments for Tiny-ImageNet and Clothing1M. Prestopping maintained its dominance also in both datasets. In Tiny-ImageNet, Prestopping outperforms the existing methods by 1.7\u201413.9pp. In Clothing1M, Prestopping outperforms the existing methods by 0.7\u20145.8pp. Please see Appendix B.2 and B.3 of the updated paper.\n\n3.\tComparison with more baselines (Reviewer 2, 3)\n\nWe could not add more baselines during the rebuttal period. However, many baselines (e.g., Iterative[4], S-model[9], F-correction[10]) have been reported to perform poorly than Co-teaching[6] we included, and we confirm that Prestopping outperforms more recent baselines according to the accuracy reported in the literature. For example, for CIFAR-10 and 30%-pair noise, the test accuracy of Prestopping was 90.8%, while the test accuracy of ITLM[8] was 88.2%; For CIFAR-10 and 40%-symmetric noise, the test accuracy of Prestopping was 88.7%, while the test accuracy of D2L[5] was 83.4%. We will definitely add more baselines to the camera-ready version, if accepted. \n\n[9] Training Deep Neural Networks using A Noise Adaptation Layer (ICLR\u201917)\n[10] Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach (CVPR\u201917)\n\n4.\tComparison of sample selection using the small-loss trick and the maximal safe set (Reviewer 2)\n\nWe have added the performance evaluation of selecting clean samples using the small-loss trick (Co-teaching) and the maximal safe set (Prestopping). The selection accuracy of Prestopping was much higher (by up to 18.2pp) than that of Co-teaching (See Section 5.3 for details).\n\n5.\tFair comparison: did the baselines use the clean validation set? (Reviewer 2)\n\nFor fair comparison, all the baselines used the clean validation sets to select the best number of epochs based on the validation accuracy. The best test error reported in our paper was evaluated using the network at the time of the lowest validation error. We have clarified this issue in the updated paper. (See Section 5 for details).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "r1xfP7FKFS", "original": null, "number": 1, "cdate": 1571554137876, "ddate": null, "tcdate": 1571554137876, "tmdate": 1572972650955, "tddate": null, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes to study how early stopping in optimization helps find confident examples. Overall, the paper is well-organized and easy to read. Although there is some parallel study regarding the theoretical aspect of how early stopping help finds confident examples (i.e., Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks, which has unfortunately not been cited), the paper focuses on the empirical perspective. A thorough empirical study illustrating how early stop works would interest the label noise community.\n\nThe authors claim that early stopping is efficient to find a maximal safe set. I think it would be necessary to illustrate the maximal safe set for all the datasets. The authors only did this for one case of CIFAR-100, which is not convincing. The small loss based learning has shown the confident examples extracted. It would be essential to compare the proposed method with those methods.\n\nThe authors are suggested to compare the proposed method with more baselines. There are lots of algorithms exploiting the transition matrix and with statistically consistent estimators. The authors have ignored all those methods. \n\nIt is unclear from the paper that if the baselines have used the clean validation sets. For fair comparison, those clean data should be used in the training procedures of the baselines.\n\nClothing1M is a more challenging dataset with real-world label noise. The dataset also includes some clean data for validation use.  The authors should verify the effectiveness of the proposed method on this dataset.\n\nThe authors are suggested to make it clear why noise rates are sometimes available for use."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575663008046, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper3/Reviewers"], "noninvitees": [], "tcdate": 1570237758611, "tmdate": 1575663008060, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Review"}}}, {"id": "r1gL1Hq6Yr", "original": null, "number": 2, "cdate": 1571820766497, "ddate": null, "tcdate": 1571820766497, "tmdate": 1572972650918, "tddate": null, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a two-phase training method for learning with label noise. \n\nOn the positive side, this paper focuses on the idea of prestopping and proposes several relevant definitions to formalize their idea and come up with a heuristic algorithm. \n\nHowever, I believe the paper has missed several very relevant papers that provides very similar ideas. Both [2] & [3] provide theoretical analysis to why early stopping matters in learning with label noise for DNNs. Before these two papers, [1] also observed that the learning trajectories for clean and noisy samples are different in label noise problem, and they used early stopping in their experiments to address this issue. Given these existing literatures, the contribution of this paper should be considered more properly. \n\n[1] Learning with Bad Training Data via Iterative Trimmed Loss Minimization, Yanyao Shen, Sujay Sanghavi, ICML 2019.\n[2] Hu, Wei, Zhiyuan Li, and Dingli Yu. \"Understanding Generalization of Deep Neural Networks Trained with Noisy Labels.\"\u00a0arXiv preprint arXiv:1905.11368\u00a0(2019).\n[3] Li, Mingchen, Mahdi Soltanolkotabi, and Samet Oymak. \"Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks.\"\u00a0arXiv preprint arXiv:1903.11680\u00a0(2019).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper3/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575663008046, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper3/Reviewers"], "noninvitees": [], "tcdate": 1570237758611, "tmdate": 1575663008060, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Review"}}}, {"id": "Syem3k3LqH", "original": null, "number": 3, "cdate": 1572417451383, "ddate": null, "tcdate": 1572417451383, "tmdate": 1572972650872, "tddate": null, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a training approach on label noise datasets and outperforms state-of-art methods. It defines the samples whose average probability on assigned label in recent q iterations is largest among all labels as memorized samples, in the sense of the network memorize these samples. Then authors proposed two stage method which firstly early-stops at minimum validation error (or $\\tau$ memorized rate), and then trains on maximal safe set that gathers memorized samples. The experiments compared several state-of-art approaches and showed that the proposed method benefits from early-stopping and safe set. Authors also showed that the prestopping idea can also be used to improve other approaches.\n\nPros:\n\nThe proposed method achieves better performance than state-of-art methods.\n\nAuthors have good experiments which evaluate on multiple datasets and algorithms.\n\nAuthors also investigate the relation between model complexity and performance of co-teaching+\n\nCons:\n\nMany recent papers indicate the \u201cerror-prone period\u201d, authors should include related works about early-stopping on label noise training.\nhttps://arxiv.org/pdf/1901.09960.pdf fig1\nhttps://arxiv.org/pdf/1903.11680.pdf fig5\nhttps://arxiv.org/pdf/1906.05392.pdf fig3\n\nAlthough the method achieves good performance, since the idea is a bit straightforward especially after exploring above papers, I am slightly worried about novelty of the ideas.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper3/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575663008046, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper3/Reviewers"], "noninvitees": [], "tcdate": 1570237758611, "tmdate": 1575663008060, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Review"}}}, {"id": "Byx-7fCU5S", "original": null, "number": 4, "cdate": 1572426265401, "ddate": null, "tcdate": 1572426265401, "tmdate": 1572972650826, "tddate": null, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a training strategy for robustness against label noise. The training strategy is simple and straightforward. The neural network will first be trained on the entire dataset with all the noisy labels. After obtaining the network with lowest validation error, the network will be used to make a prediciton on the original training set and select a subset of it to construct a maximal safe set. Finally, the network will be findtuned on this maximal safe set. The training strategy is very similar to tradictional  self-training in semi-superivsed learning and co-training for domain adaptation ([Co-training for domain adaptation, NIPS 2011]), except that the proposed prestopping only iterate the procedure once.\n\nThe paper discusses two important questions for the method: (1) when to early stop the training; (2) how to constuct a maximal safe set. The authors' responses to these questions are very natual but less interesting. Using the lowest validation error to early stop the training could be suboptimal, since the small validation set can not fully capture the data distribution and could make the network empirically overfit to this validation set. The criterion to contruct a maxial safe set is also conventional, and is similar to what a number of papers are doing, for example,\n[1] Co-training for domain adaptation, NIPS 2011\n[2] Self-ensembling for visual domain adaptation, ICLR 2018\n[3] A dirt-t approach to unsupervised domain adaptation, ICLR 2018\n[4] Iterative learning with open-set noisy labels, CVPR 2018\n\nIn experiments, the results are not very surprising. There are some baselines that adopt a similar (iterative) pipeline (learning the network - selecting a subset of the training samples - re-learning the network):\n[1] Iterative Learning with Open-set Noisy Labels, CVPR 2018\n[2] Dimensionality-Driven Learning with Noisy Labels, ICML 2018\n[3] Symmetric Cross Entropy for Robust Learning with Noisy Labels, ICCV 2019\nThe authors can consider to compare to some of these baselines, especially [1] and [2]. The difference between the paper and [1,2] is basically the criterion to construct the maximal safe subset.\n\nBesides, I suggest the authors to conduct large-scale experiments on ImageNet or even a subset of ImageNet, since the difficulty of detecting label noise is much higher when the resolution of images become bigger. CIFAR-10 and CIFAR-100 only contain 32x32 images, which is far less challenging.\n\nOverall, I think the paper is well written, the idea is clearly presented, and the experiments also seem convinceing. However, the contribution of this paper is very incremental."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575663008046, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper3/Reviewers"], "noninvitees": [], "tcdate": 1570237758611, "tmdate": 1575663008060, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Review"}}}, {"id": "HklU67zxKr", "original": null, "number": 2, "cdate": 1570935742204, "ddate": null, "tcdate": 1570935742204, "tmdate": 1570936543279, "tddate": null, "forum": "BklSwn4tDH", "replyto": "SkeYl6CktH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"comment": "Thanks a lot for your interest and comments on the two recent work we missed. We will cover both papers during the rebuttal period.\n\nWe think that the first work you mentioned will give many theoretical intuitions to improve our two heuristics for the question \"(Q1) When is the best point to early stop the training process?\". Actually, we were looking for a theoretical paper related to this question for our future research. Besides, please note that the early stopping is adopted to derive a \"maximal safe set\" that enables noise-free training during the remaining learning period. The significant performance improvement of our method is achieved by a merger of \"early stopping\" and \"learning form the maximal safe set\". \n\nAlso, the second work, one another direction to use \"quick convergence by pre-training\", is really interesting!\nFollowing the paper, the potential of the weakly pre-trained model seems to be great for a wide range of tasks such as adversarial perturbations and label corruption. Thus, it is likely that our method \"Prestopping\" can be further enhanced by taking advantage of a weakly pre-trained model for publicly available data (e.g., ImageNet). \n", "title": "Thanks for your comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}, {"id": "SkeYl6CktH", "original": null, "number": 1, "cdate": 1570921712905, "ddate": null, "tcdate": 1570921712905, "tmdate": 1570921712905, "tddate": null, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Public_Comment", "content": {"comment": "Hi,\n\nThe theoretical work _Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks_ seems quite related and should probably be mentioned. https://arxiv.org/pdf/1903.11680.pdf\n\nThe following empirical work uses quick convergence in order to combat label noise memorization.\n_Using Pre-Training Can Improve Model Robustness and Uncertainty_ https://arxiv.org/pdf/1901.09960.pdf (ICML 2019)\nInstead of \"pre-stopping\", pre-training allows quick convergence which lets one side-step the noise memorization phase. This work is also worth mentioning. (See \"By training for longer, the network eventually begins to model and memorize label noise\" and Section 3.2)", "title": "Related Work"}, "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Dan_Hendrycks1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504215312, "tmdate": 1576860567011, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Public_Comment"}}}, {"id": "r1g23-JswH", "original": null, "number": 1, "cdate": 1569546676061, "ddate": null, "tcdate": 1569546676061, "tmdate": 1569546676061, "tddate": null, "forum": "BklSwn4tDH", "replyto": "BklSwn4tDH", "invitation": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment", "content": {"comment": "We apology to the readers for the typo in Eq. (6).\n- $\\{x \\in \\mathcal{R}_{t} \\cap \\mathcal{S}_{t_{end}}\\}$ -> $\\{x \\in \\mathcal{R}_{t} \\cup \\mathcal{S}_{t_{end}}\\}$ ", "title": "Typo in Eq. (6)."}, "signatures": ["ICLR.cc/2020/Conference/Paper3/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prestopping: How Does Early Stopping Help Generalization Against Label Noise?", "authors": ["Hwanjun Song", "Minseok Kim", "Dongmin Park", "Jae-Gil Lee"], "authorids": ["songhwanjun@kaist.ac.kr", "minseokkim@kaist.ac.kr", "dongminpark@kaist.ac.kr", "jaegil@kaist.ac.kr"], "keywords": ["noisy label", "label noise", "robustness", "deep learning", "early stopping"], "TL;DR": "We propose a novel two-phase training approach based on \"early stopping\" for robust training on noisy labels.", "abstract": "Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by \"early stopping\" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a \"maximal safe set,\" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4\u20138.2 percent points under existence of real-world noise.", "pdf": "/pdf/b63a270838957c2a83ffb9eff74ea52d5d24710b.pdf", "code": "https://bit.ly/2l3g9Jx", "paperhash": "song|prestopping_how_does_early_stopping_help_generalization_against_label_noise", "original_pdf": "/attachment/a9559cf14e4b4999eafb9b81bbd3214bbb622613.pdf", "_bibtex": "@misc{\nsong2020prestopping,\ntitle={Prestopping: How Does Early Stopping Help Generalization Against Label Noise?},\nauthor={Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=BklSwn4tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklSwn4tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper3/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper3/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper3/Authors|ICLR.cc/2020/Conference/Paper3/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177843, "tmdate": 1576860533376, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper3/Authors", "ICLR.cc/2020/Conference/Paper3/Reviewers", "ICLR.cc/2020/Conference/Paper3/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper3/-/Official_Comment"}}}], "count": 18}