{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730156911, "tcdate": 1509138956347, "number": 1140, "cdate": 1518730156900, "id": "HJNGGmZ0Z", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HJNGGmZ0Z", "original": "B1FgG7WCb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260084732, "tcdate": 1517249896494, "number": 593, "cdate": 1517249896481, "id": "r1lOSyTSz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "Paper reviewed by three experts who have provided detailed feedback. All three recommend rejection, and this AC sees no reason to overrule their recommendation. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1516876525434, "tcdate": 1516875839081, "number": 12, "cdate": 1516875839081, "id": "S1wHgNwHz", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "SyZwTXVrz", "signatures": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "content": {"title": "On Final Review (part 1)", "comment": "We appreciate and thank the reviewer for going through our rebuttal and revised manuscript and for the comments. However, we disagree with many of the points raised in the above review.\n\n> - \"End-to-end IC models are remarkably capable of separating structure from noisy input representations, as demonstrated by pseudo-random vectors\": This statement is factually incorrect and its empirical implications are not surprising. The pseduo random vectors (pg 5) are not noisy at all! First, they are deterministic mapping from image space to a vector space (this is the definition of a feature extractor). Secondly, and more importantly, they are generated using Gold (or ground-truth) object counts. By the very definition of this, they are NOT noisy! Your conclusion that such a representation works well for image captioning is not surprising. To any system using image features, all it really cares about is how good the mapping from image to vector (feature) space is. Your mapping is defined using ground truth counts. This claim is repeated throughout the paper - abstract, introduction and conclusion. In all three repititions, the authors claim incorrectly that the representation is `noisy'.\n\n\nWe claim pseudo-random vectors as being noisy representations as they are not `just\u2019 one to one mapping from the image to vector feature space but they are actually a composition of object vectors (where objects are represented by random vectors). The composition specifically involves addition and the information about the number of occurrences of the objects --specifically: multiplication of random vectors per object by the number of object occurrences and then addition of vectors across multiple objects. The resultant composition is `noisy\u2019 -- this can be seen both in Figure 1, where the initial representations are shown to not form any clusters, as well as in Figure 2(d), where the initial representations again form no clusters. We kindly refer the reviewer to the full tSNE plot for the initial representations of pseudo-random vectors here: https://github.com/anonymousiclr/HJNGGmZ0Z/blob/master/tsne_initial_pseudorandom_4000.png. Further, the conclusion doesn\u2019t change even if we use predicted counts. \n\nDespite the review strongly stating it to be factually incorrect, we stand by our conclusions. We have repeated our claims of the representations in the projected space. We observe that the resultant framework has, to some extent, captured the compositional operation, despite the initial representation being difficult to decipher.\n\n\n> - \"A sparse, low-dimensional bags-of-objects representation can be used as a tool to investigate the contribution of images in IC; we demonstrated that such a vector is sufficient for generating good image captions\": Like I had mentioned in my review earlier - bag-of-objects based representations have been shown to be sufficient for generating good image captions (Fang et al., 2015).\n\nFirstly, Fang et al 2015 does not use neural end-to-end image captioning. Secondly, they used a high 1000-dimensional \"bag of surface-level-text-labels taken directly from captions\" space, while we show a *low* barely 80-dimensional *object category* space (that is objects occuring in the images) performs as well. We disagree with the reviewer that Fang et al have the same conclusions as ours.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "tmdate": 1516876288428, "tcdate": 1516875803196, "number": 11, "cdate": 1516875803196, "id": "ryXXgEPHG", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "SyZwTXVrz", "signatures": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "content": {"title": "On final review (part 2) ", "comment": "> - \"End-to-end IC models repeatedly generate the same captions by matching images in the joint visual-textual space and \u2018retrieving\u2019 a caption in the learned joint space\": Again, a similar claim was empirically shown by Devlin et al., by using a nearest neighbor technique for image captioning. They also showed that such a simple technique outperformed end-to-end captioning systems.\n\nDevlin et al. only show that a `nearest neighbour method\u2019 works as well as end-to-end captioning systems. We show that end-to-end IC *is* sort of a nearest neighbour retrieval in the joint space. These are two completely different conclusions. We again respectfully disagree with the reviewer that the two are same or similar. \n\n> - \"End-to-end IC models rely on test sets with a similar distribution as the training set for generating good captions\": Please look at Sec 4.3.3 of \"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge\" where transfer experiments are shown across datasets, showing that similarity between train and test distributions is important from transfer.\n\nWe are aware of this and have explicitly mentioned Vinyals et al 2016 in Sec 4.4 of the our paper. We have looked carefully again at Vinyals et al 2016 - they only mention that the ``BLEU scores degrade by 10 points\u2019\u2019 and merely suggest that this *could* be because ``more differences in vocabulary and a larger mismatch\u2019\u2019. In our paper, we demonstrated that what they suggested is not exactly true. We show that there are only 8.6% out of vocabulary words in Flickr30k (around 8% vocabulary mismatch is true in the MSCOCO train v/s dev set). So there is more to the performance drop than just vocabulary mismatch. And thus, we further show that it is also because the `object\u2019 distribution (from the image side) in MSCOCO is almost identical in train and test, compared to the larger differences between MSCOCO train vs. Flickr30k test. That is, the `types\u2019 of images are maintained in MSCOCO evaluation sets, while they vary in Flickr30k test set. To our knowledge this is still a novel and an important claim. \n\n\n> There are also a few issues with the experimental setup.\n> - Table 1 shows nearly constant numbers across B-4, M, and S.\n\nWe do not understand why constant metric scores can indicate that it\u2019s an \u201cissue with our experimental setup\u201d. In fact, we think the constant scores further strengthen our claim that the metrics do not capture what exactly happens as shown by several other papers that we cite.  \n\n\n> - Table 1 shows that using Pool5 features for ResNet-152 performs better than softmax for ResNet-152. Now Figure 1 shows us that softmax ResNet-152 is better at discriminating between images based on object groups rather than Pool5 ResNet-152. A similar negative correlation exists between Table 1 and Figure 1 for the pairs (softmax ResNet-152, pseudo-random). The reason for introducing Figure 1 as stated in the paper is \"If the representation is informative for IC, then the representations should ideally semantically related images together, and in turn allow for relevant captions to be generated.\" This statement is clearly falsified by the pairs (and many more such exist within your results).\n\nWe state \u2018If the representation is informative for IC, then the representations should ideally semantically related images together, and in turn allow for relevant captions to be generated\u2019 as our hypothesis and we follow it up in the next section. Figure 1 merely shows how well the initial representations cluster with cosine distance as the metric. We don\u2019t make any conclusions using Figure 1. \n\nAs our work mainly the deals with the evaluation of representational contributions we consider ICLR the best venue to disseminate our findings.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "tmdate": 1516678489502, "tcdate": 1516678489502, "number": 10, "cdate": 1516678489502, "id": "SyZwTXVrz", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1140/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/AnonReviewer2"], "content": {"title": "Final Review - Rejection", "comment": "After reading the other reviews, the discussion and the revised paper, I am not convinced of the contributions of the paper (even if I were to ignore the weakness in the experimental setup, as I explain later).\nLet's focus on the conclusion section of the paper (page 11) to see what the authors claim.\n\n- \"End-to-end IC models are remarkably capable of separating structure from noisy input representations, as demonstrated by pseudo-random vectors\": This statement is factually incorrect and its empirical implications are not surprising. The pseduo random vectors (pg 5) are not noisy at all! First, they are deterministic mapping from image space to a vector space (this is the definition of a feature extractor). Secondly, and more importantly, they are generated using Gold (or ground-truth) object counts. By the very definition of this, they are NOT noisy! Your conclusion that such a representation works well for image captioning is not surprising. To any system using image features, all it really cares about is how good the mapping from image to vector (feature) space is. Your mapping is defined using ground truth counts. This claim is repeated throughout the paper - abstract, introduction and conclusion. In all three repititions, the authors claim incorrectly that the representation is `noisy'.\n- \"A sparse, low-dimensional bags-of-objects representation can be used as a tool to investigate the contribution of images in IC; we demonstrated that such a vector is sufficient for generating good image captions\": Like I had mentioned in my review earlier - bag-of-objects based representations have been shown to be sufficient for generating good image captions (Fang et al., 2015).\n- \"End-to-end IC models repeatedly generate the same captions by matching images in the joint visual-textual space and \u2018retrieving\u2019 a caption in the learned joint space\": Again, a similar claim was empirically shown by Devlin et al., by using a nearest neighbor technique for image captioning. They also showed that such a simple technique outperformed end-to-end captioning systems.\n- \"End-to-end IC models rely on test sets with a similar distribution as the training set for generating good captions\": Please look at Sec 4.3.3 of \"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge\" where transfer experiments are shown across datasets, showing that similarity between train and test distributions is important from transfer.\n\nThere are also a few issues with the experimental setup.\n- Table 1 shows nearly constant numbers across B-4, M, and S.\n- Table 1 shows that using Pool5 features for ResNet-152 performs better than softmax for ResNet-152. Now Figure 1 shows us that softmax ResNet-152 is better at discriminating between images based on object groups rather than Pool5 ResNet-152. A similar negative correlation exists between Table 1 and Figure 1 for the pairs (softmax ResNet-152, pseudo-random). The reason for introducing Figure 1 as stated in the paper is \"If the representation is informative for IC, then the representations should ideally semantically related images together, and in turn allow for relevant captions to be generated.\" This statement is clearly falsified by the pairs (and many more such exist within your results).\n\nI do not think this paper is ready for publication and stick with \"needs work\"."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642389189, "tcdate": 1511391224608, "number": 1, "cdate": 1511391224608, "id": "BybWlKXgz", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Review", "forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1140/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Needs work", "rating": "4: Ok but not good enough - rejection", "review": "This paper analyzes the effect of image features on image captioning. The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on. The MSCOCO captioning and Flickr30K datasets are used for evaluation.\n\nIntroduction\n- The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later.\n- The introduction also states that the authors will propose ways to improve image captioning. This is never discussed.\n\nCaptioning Model and Table 1\n- The authors use greedy (argmax) decoding which is known to result in repetitive captions. In fact, Vinyals et al. note this very point in their paper. I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting. In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning - Table 1 shows meteor scores within 0.19 - 0.22 for all methods.\n- Another effect (possibly due to greedy decoding + choice of model), is that the numbers in Table 1 are rather low compared to the COCO leaderboard. The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22. Similar trend holds for other metrics like BLEU-4.\n- The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding.\n\nExperimental Setup and Training Details\n- How was the model optimized? No training details are provided. Did you use dropout? Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152? What is the variance in the numbers for Table 1?\n\nMain claim of the paper\nDevlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly. Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier.\n\nMetrics for evaluation\n- Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions. The COCO leaderboard also uses this metric as one of its evaluation metrics. If the authors are evaluating on the test set and reporting numbers, then it is odd that they `skipped' reporting SPICE numbers.\n\nChoice of Datasets\n- If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important. Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO. These datasets should show whether using more general features (YOLO-9k) helps.\nThe authors should evaluate on these datasets to make their findings stronger and more valuable.\n\nMinor comments\n- Figure 1 is hard to read on paper. Please improve it.\n- Figure 2 is hard to read even on screen. It is really interesting, so improving the quality of this figure will really help.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642389084, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1140/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1140/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1140/AnonReviewer1"], "reply": {"forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642389084}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642389148, "tcdate": 1512087594283, "number": 2, "cdate": 1512087594283, "id": "r1fNl7Cgz", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Review", "forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1140/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Not clear contributions, lack of comparisons with other methods and weak results.", "rating": "4: Ok but not good enough - rejection", "review": "The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect.\n\nThe paper lacks novelty, just reports some results without proper analysis or insights.\n\nMain weakness of the paper:\n - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results)\n - According to \"SPICE: Semantic Propositional Image Caption Evaluation\" current metrics used in image captioning don't correlate with human judgement.\n- Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016). Therefore correlation of the image features with the captions is weaker that it could be.\n- The experiments reported in Table1 are way below state-of-the-art results, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results\n - To provide a fair comparison authors, should compare their results with other paper results.\n - Tables 2 and 3 are missing the original baselines.\nThe evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference.\n- Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3. \n- The claim \"The bag of objects model clusters these group the best\" is not supported by any evidence or metric.\n\nOne interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task.\n\n\nTypos:\n - synsest-level -> synsets-level", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642389084, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1140/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1140/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1140/AnonReviewer1"], "reply": {"forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642389084}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642389104, "tcdate": 1512184433607, "number": 3, "cdate": 1512184433607, "id": "By5_q5y-z", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Review", "forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1140/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Clear rejection.", "rating": "4: Ok but not good enough - rejection", "review": "This paper is an experimental paper. It investigates what sort of image representations are good for image captioning systems. \n\nOverall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings.\n\nThe main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact. Unfortunately, I do not see any novel concept in the experimental setup.\n\nI recomend this paper for a workshop presentation.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642389084, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1140/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1140/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1140/AnonReviewer1"], "reply": {"forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642389084}}}, {"tddate": null, "ddate": null, "tmdate": 1513973079926, "tcdate": 1513973079926, "number": 8, "cdate": 1513973079926, "id": "H1xPSyofM", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "content": {"title": "Updates", "comment": "We have updated the paper with these salient changes: \n\n* re-written introduction\n* updated results with SPICE\n* updated sections 4.3, 4.4 and 4.5 with more support to claims \n* re-written conclusion "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "tmdate": 1513934137551, "tcdate": 1513933218185, "number": 7, "cdate": 1513933218185, "id": "rkqoKB9fz", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "BybWlKXgz", "signatures": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "content": {"title": "on specific comments", "comment": "> Main claim of the paper Devlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly. Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier. Metrics for evaluation\n\nOnce again, we use the object representations as a tool for our investigation. Our aim is not to improve on the task.\n\n>  - Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions. The COCO leaderboard also uses this metric as one of its evaluation metrics. If the authors are evaluating on the test set and reporting numbel rs, then it is odd that they `skipped' reporting SPICE numbers.\n\nWe have answered this before. We note however that our observations are also consistent with the numbers on the SPICE metric. \n\n>  Choice of Datasets - If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important. Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO. These datasets should show whether using more general features (YOLO-9k) helps. The authors should evaluate on these datasets to make their findings stronger and more valuable.\n\nSIND represents a very different type of data where sentences compose a narrative. Different kinds of models are needed and these are evaluated using different metrics. Visual Genome, on the other hand, is a subset of MSCOCO with different kind of annotations (object specific captions). We are interested in investigating the CNN-LSTM model in this paper, and while it may be applied to a different domain of the same task (e.g. image captioning on Flickr30k), it is not clear how this can be applied directly to a different set of tasks.\n\n\n> Minor comments - Figure 1 is hard to read on paper. Please improve it. - Figure 2 is hard to read even on screen. It is really interesting, so improving the quality of this figure will really help.\n\nWe have enlarged the Figure 1.\n\nWe initially planned to add the full, high-resolution versions of Figure 2 in the appendix. Unfortunately each t-SNE visualisation was around 18MB -- which will increase the file size to over 100MB if we were to add all images (3 pairs before-after projection). We have added an anonymised external link in the updated version of the paper. The images can now be found here: https://github.com/anonymousiclr/HJNGGmZ0Z\n "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "tmdate": 1513933192607, "tcdate": 1513933192607, "number": 6, "cdate": 1513933192607, "id": "HybctS5zf", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "BybWlKXgz", "signatures": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "content": {"title": "on specific comments", "comment": "> - The introduction to the paper could be made clearer\n\nWe have updated the introduction to make it clearer.\n\n> the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later.\n\nIn our analysis we observed that in all cases, i.e., using any type of representation, there is only a small subset (20-30%) of the captions that are unique. This was mentioned in section 4.5 of our original submission of the paper. We have further clarified this section in the updated version.\n\n\n> The introduction also states that the authors will propose ways to improve image captioning. This is never discussed.\n\nWe do not promise to do that, but rather state that findings could help improve image captioning systems.\n\n>  Captioning Model and Table 1 - The authors use greedy (argmax) decoding which is known to result in repetitive captions. In fact, Vinyals et al. note this very point in their paper. I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting.\n>  In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning\n\nThis was purposefully done for determinism. We wanted to understand the best 'choice of words' by the model given a particular representation. \n\n\n> The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22.  Similar trend holds for other metrics like BLEU-4.\n\nOur model should be compared with the Neuraltalk model as it has the same settings. Other similar models (like Vinyals et al 2015) use ensembles and other engineering tricks that we are not interested in. \n\n> - The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding. Experimental Setup and Training Details - How was the model optimized? No training details are provided. Did you use dropout? Were hyperparameters fixed for training across different feature sizes of VGG19 and ResNet-152? What is the variance in the numbers for Table 1?\n\nOur settings are: \nLSTM with 128 dimensional word embeddings and 256 dimensional hidden representations\nDropout over LSTM of 0.8\nAdam for optimization. \nLearning rate = 4e-4\nWe\u2019ll add the variance figures to an improved version of the paper.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "tmdate": 1513932789759, "tcdate": 1513932789759, "number": 5, "cdate": 1513932789759, "id": "SkCgOrcGG", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "r1fNl7Cgz", "signatures": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "content": {"title": "on specific comments", "comment": "> The paper lacks novelty, just reports some results without proper analysis or insights.\n> Main weakness of the paper:\n> - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results)\n\nWe stress that our evaluations are with respect to the model proposed by Karpathy et al 2015. Our goal is not to 'beat or break' systems but to understand the 'whys' and 'hows'.\n\n>  - According to \"SPICE: Semantic Propositional Image Caption Evaluation\" current metrics used in image captioning don't correlate with human judgement.\n\nWe are not claiming explicitly that any of the metrics has good correlation with human judgements. As we mentioned before our focus on CIDEr is because a) the official evaluation script from MSCOCO  contains only CIDEr, Meteor, BLEU and ROUGE, b) CIDEr is a metric that was officially developed for the task of image captioning, c) CIDEr is the official metric for MSCOCO, d) papers by Liu et al 2017, Kilickaya et al. 2017 and Vedantam et al, 2015 (with the human correlation experiments over Flickr8k dataset) still state the importance of CIDEr as a metric for image captioning. We further note that we observe a similar trend as we found in CIDEr, so all our observations are still valid. \n\n\n>  - Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016). Therefore correlation of the image features with the captions is weaker that it could be.\n\nWhile it is true that fine-tuning could have been helpful to bump performance, our paper deals with an exploration of representational properties. Vinyals et al. 2016 has shown that fine-tuning gives only a minor 1-point improvement for BLEU. This is also using an ensemble of models. We again state that our experiments are about understanding image captioning models.\n\n> - To provide a fair comparison, authors should compare their results with other paper results. - Tables 2 and 3 are missing the original baselines.\n\nWe will add the results from the comparable papers, even though our focus is not comparisons or to show performance improvements over other models. However, we do not understand what the reviewer means by \u201coriginal baselines\u201d. Could you please clarify?\n\n> The evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference.\n\nPlease see answer above regarding metrics. In addition, our focus is not to improve the performance of the system, but to interpret the 'how' and 'why' of the system. To this end, we have made significant progress.\n\n>  - Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3.\n\nWe have not chosen Vinyals et al. 2016 since it uses ensembles and other clever engineering tricks. This would make it hard to answer the questions we ask in this paper -- namely, the contribution of image representation. Our results are comparable to those in Karpathy et al, 2015. We will add this into the table.\n\n> - The claim \"The bag of objects model clusters these group the best\" is not supported by any evidence or metric.\n\nWe believe that the reviewer has misunderstood the sentence. This sentence explains the observations in Figure 1 (more specifically Figure 1a). The figure shows that the bag of objects representation forms better clusters. It shows the cosine distances between each group for the bag of objects representation. We see from the figure that the bag of objects representations clusters these groups best. For example, the average image representation of \u201cdog\u201d correlates with images containing \u201cdog\u201d as a pair like \u201cdog+person\u201d and \u201cdog+toilet\u201d. We are aware that this is true for our given example, however we expect this to extrapolate over other examples in the dataset. \n\n\n\n> One interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task.\n\nWe will do it as a future work, even though this does not allow us to answer our questions posed in this paper.  \n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "tmdate": 1513932533461, "tcdate": 1513932500810, "number": 4, "cdate": 1513932500810, "id": "Hk60UB5GM", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "By5_q5y-z", "signatures": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "content": {"title": "on comments", "comment": "> Overall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings.\n> The main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact.\n\nOur claim is in the novel 'insights' into end-to-end model of image captioning models. Our empirical evaluations with multiple representations, visualizations and out of domain experiments reveal new and important insights that should be of interest to the community.\n\nWe kindly ask clarification from the reviewer regarding what is meant by 'novelty in experimental methodology'. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "tmdate": 1513932409643, "tcdate": 1513932409643, "number": 3, "cdate": 1513932409643, "id": "rJGFUBqfM", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1140/Authors"], "content": {"title": "Rebuttal", "comment": "We thank the reviewers for the comments. \n\nOur submission is based on the simple end-to-end model as proposed by Karpathy et al 2015. We use this model because its simplicity makes it easier to focus on the image component. We are interested in the interpretability of the image-captioning system rather than the performance on the task. In addition, more advanced models can be considered similar variants of Karpathy et al 2015. We do not claim novelty with respect to the captioning model. Instead, our submission presents novel insights into the image captioning task which we are confident that it should be of interest to the community. Also as our submission involves work on understanding the representational contributions, we consider our work highly relevant to the conference on learning representations. Our main contributions are:\n\n1) We show that the image-conditioned language model implicitly learns and exploits a joint image representation and language semantic space instead of actually understanding images (sections 4.2, 4.3, 4.4). \n\n2) Our experiments with factorized and compressed image embeddings (section 4.1) reveals that the models do not benefit from the full representational space. We observe that the performance of the model trained with a 2048 dimensional representation is nearly identical to the performance of the model trained with a compressed 80-dimensional representation virtually resulting in \u2018no information loss\u2019. \n\n3) The experiments with pseudorandom representations (section 3.2) reveal that the end-to-end models learn to separate structure from noisy representations in the framework and exploit it to produce near ideal performance, i.e., the performance with structured representations versus the performance with noisy representations is similar. \n\n\nThe reviewers also raised concern regarding the absence of SPICE as a metric for evaluation. We focus on CIDEr because: a) the metrics in the official evaluation script from MSCOCO contains support for only CIDEr, Meteor, BLEU and ROUGE; b) CIDEr is a metric that was officially developed for the task of image captioning, and is supposed to be the official metric for MSCOCO; c) papers by Liu et al 2017, Kilickaya et al. 2017 and Vedantam et al, 2015 (with the human correlation experiments over Flickr8k dataset) still state the importance of CIDEr as a metric for image captioning. However, we will provide the results on SPICE in the revised version. We also note that a similar trend is observed with SPICE. \n\n* Liu et al. (ICCV 2017) Improved Image Captioning via Policy Gradient Optimization of SPIDEr\n* Kilickaya et al. (EACL 2017) Re-evaluating Automatic Metrics for Image Captioning\n* Vedantam et al. (CVPR 2015) CIDEr: Consensus-based Image Description Evaluation\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723825, "id": "ICLR.cc/2018/Conference/-/Paper1140/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1140/Authors|ICLR.cc/2018/Conference/Paper1140/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1140/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1140/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723825}}}, {"tddate": null, "ddate": null, "tmdate": 1513398895534, "tcdate": 1513398895534, "number": 1, "cdate": 1513398895534, "id": "S1P_G7Mfz", "invitation": "ICLR.cc/2018/Conference/-/Paper1140/Public_Comment", "forum": "HJNGGmZ0Z", "replyto": "HJNGGmZ0Z", "signatures": ["~Abhisek_Konar1"], "readers": ["everyone"], "writers": ["~Abhisek_Konar1"], "content": {"title": "Report on reproducibility of the paper", "comment": "In this report, the \ufb01ndings of this paper submitted to the ICLR 2018 Conference were attempted to be replicated. In the process of replication, two major components were identi\ufb01ed. The \ufb01rst breakdown included building the baseline model. The second subsection contained the core of the research, which was to answer the key questions and address which image transformations affected the accuracy of neural image captioning systems. Following the steps outlined in the paper as closely as possible, we were able to build a very similar baseline model, and perform three of the \ufb01ve image transformations that were speci\ufb01ed. \nThe base line model used in the paper is a combination of the approaches of Karpathy [1] and Vinyals [2]. We were able to closely replicate that model by breaking down it into 3 subsets, a combination of an image model and a language model with a CNN used as an encoder of the images, and an LSTM for the language model as mentioned in the paper. \nFor the image transformations, we were able to successfully reproduce three out of the \ufb01ve: penultimate layer extraction, class prediction vector, and object-class word embeddings. For the penultimate layer extraction, we implemented the pretrained VGG19 and ResNet152 models. The VGG19 uses very small convolutional \ufb01lters and uses very deep weight layers of up to 19. The ResNet152 model, as implemented by He et al.[9], uses 8 times deeper nets than VGG19. Both the models were implemented via the Keras distribution with a TensorFlow backend. \nThe class prediction vector transformation involved investigating more complex image representations, where the vector elements are now estimated posterior probabilities of the possible object categories. To obtain these posterior distribution vectors, the pre-trained network ResNet152 was again used to retrieve a 1000 dimensional posterior vector. \nThe last transformation we were able to replicate was the object-class word embeddings. This procedure is carried out over the entire 1000 dimensional output of the Softmax layer of pre-trained model ResNet152 where all the procured word2vec representations are \ufb01nally averaged. This averaged vector acts as the image representation for the image model. \nThe evaluation metric used for the score calculation nltk based corpus BLEU introduced by Papineni et al. [12]. Using a beam size of 1, as done by the authors, a steady rise was observed in the corpus BLEU score for all three representations. Penultimate layer and softmax implementations outperformed the word2vec image representation which had BLEU scores ranging between 0.7540 and 0.4646 from BLEU-2 to BLEU4. For both penultimate and softmax image representations, ResNet152 performed better than VGG19 with BLEU scores ranging from 0.5598 to 0.9216 for softmax and 0.5889 to 0.8937 for penultimate with BLEU varying from 4 to 1. It was only marginally better than VGG19\u2019s BLEU 4-1 scores ranging between 0.5346 and 0.9158 for softmax and 0.5962 and 0.8524 for penultimate. \nOne caveat of this report was that it was not feasible to train the model on the MSCOCO dataset as the paper. This was due to computational restrictions, as training a model on the Flickr8K dataset, which is much smaller than the MSCOCO dataset, took a K80 equipped server approximately 2 days for a small batch size. Due to the inability to use the MSCOCO, we experienced two drawbacks during the replication; The \ufb01rst included hindering our ability to implement the 4th and 5th image transformations, and the second was fact that we were not able to reproduce an exact copy of the works presented by the authors. Although we used a different dataset, we still noticed similar trends in the ones obtained by the tests carried out in the MSCOCO dataset. For example, both our tests and the original authors\u2019 test both had the ResNet152 pre-trained network slightly outperforming the VGG19 network in the different image transformations.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What is image captioning made of?", "abstract": "We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn \u2018distributional similarity\u2019 in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the \u2018image\u2019 side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and \u2018retrieving\u2019 a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n", "pdf": "/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf", "TL;DR": "This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.", "paperhash": "madhyastha|what_is_image_captioning_made_of", "_bibtex": "@misc{\nmadhyastha2018what,\ntitle={What is image captioning made of?},\nauthor={Pranava Madhyastha and Josiah Wang and Lucia Specia},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNGGmZ0Z},\n}", "keywords": ["image captioning", "representation learning", "interpretability", "rnn", "multimodal", "vision to language"], "authors": ["Pranava Madhyastha", "Josiah Wang", "Lucia Specia"], "authorids": ["p.madhyastha@sheffield.ac.uk", "j.k.wang@sheffield.ac.uk", "l.specia@sheffield.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791673624, "id": "ICLR.cc/2018/Conference/-/Paper1140/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HJNGGmZ0Z", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper1140/Authors", "ICLR.cc/2018/Conference/Paper1140/Reviewers", "ICLR.cc/2018/Conference/Paper1140/Area_Chair"], "cdate": 1512791673624}}}], "count": 15}