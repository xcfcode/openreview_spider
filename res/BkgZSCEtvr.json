{"notes": [{"id": "BkgZSCEtvr", "original": "SJgMRd8dwH", "number": 1101, "cdate": 1569439289103, "ddate": null, "tcdate": 1569439289103, "tmdate": 1577168276445, "tddate": null, "forum": "BkgZSCEtvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aa6gblT3Nl", "original": null, "number": 1, "cdate": 1576798714467, "ddate": null, "tcdate": 1576798714467, "tmdate": 1576800922033, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Decision", "content": {"decision": "Reject", "comment": "Novelty of the proposed model is low. Experimental results are weak.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713459, "tmdate": 1576800263081, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Decision"}}}, {"id": "ryeE5v8joS", "original": null, "number": 4, "cdate": 1573771147931, "ddate": null, "tcdate": 1573771147931, "tmdate": 1573771147931, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "Byxc74-0Yr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We would like to thank Reviewer2 for the constructive comments regarding the experiments.\n\n*Discussion about CGF and GNF. \nGNF and CGF are fundamentally two different flow based models for graph-structured data. Both these models have their own advantages as well as challenges and it is valuable that these co-exist in the field. We highlight key differences between these models as follows.\n- Theoretical backbone: CGF is heavily entangled with Neural Ordinary Differential Equations \u2014 requires solving initial value problem and has continuous time property. GNF, on the other hand, is purely based on change of variable formula. They shall be treated differently.\n- Application: GNF and CGF can both be used for generating static graph structured data. In addition, since CGF is capable of modeling continuous time dynamics of graph-structured variables, the CGF can also be potentially applied to graph data having regular and irregular time dynamics.\n\n*Discussion about GraphNVP\nWe agree that GraphNVP is a relevant related work in this field. However, we believe GraphNVP is based on normalizing flow based models which are different from CGF which is based on neural ordinary differential equations (ODEs). We have cited and discussed this work in the updated paper.\n\n*Comparison with concurrent work\nWe would like to clarify that GNF is a concurrent work developed in parallel and not a previous work. We only use their numbers on graph generation experiments for better reference and don\u2019t use it as a standard baseline. We are happy to discuss the differences between the models (in the following part), but we are not claiming full contributions over GNF. \n\n\n*Choices of f-hat and g: \nWe apologize for the confusion. f_hat (x_i,x_j) is given by f_u (x_i) + f_b(x_i,x_j) for all j in the set of neighbors of node in the graph. To clarify the implementation details, we chose two types of functions for f_u and f_b as convolutional networks for puzzle graph generation and linear neural networks for scene graph based layout generation and graph generation. The function g is the mean of all the outputs f_hat used to aggregate these message functions. \n\n*CGF for continuous time dynamics for graphs (reasons): \nThank you for the comment.\n- Continuous normalizing flow is the state-of-the-art flow-based model which yield a powerful tool for modeling graph distributions\n- The freedom on incorporating any function types as message functions. This allows the model to be general and makes it applicable to various types of graph neural networks in graph generation setting.\n- There is potential to use our model for continuous time graph data, especially on irregular timestamps. It\u2019s a promising direction for future work. \n\n*Discrete vs. continuous experiments:\n- First of all, the functions in continuous dynamics cannot be used in discrete cases, since the discrete flow has limitation on using specific function types (reversible functions with easy to compute Jacobian determinant)\n- Secondly, for the standard discrete normalizing flow and continuous dynamic based flow, this experiments are already presented in FFJORD paper which compares the capacity of popular discrete models and the proposed continuous dynamics based flow. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1101/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1101/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1101/Authors|ICLR.cc/2020/Conference/Paper1101/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161241, "tmdate": 1576860536967, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Official_Comment"}}}, {"id": "H1xDXdR7sS", "original": null, "number": 2, "cdate": 1573279775261, "ddate": null, "tcdate": 1573279775261, "tmdate": 1573329261397, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "BkgReQ9aKB", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We would like to thank the reviewer for the thorough and valuable feedback for the manuscript. We updated the manuscript and the changes are marked by red color. We first list our contributions and then provide clarifications on the specific concerns listed in the reviewer\u2019s comments.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\u2014\u2014\u2014-Our contributions\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nWe propose Continuous Graph Flow (CGF) - a continuous time flow-based model for graph structured data. In particular,\n- CGF integrates neural ODE based model and message passing (in graph neural networks) to model distributions of complex graph-structured data. Our work is among the first ones in this direction.\n- CGF learns flexible representations to handle variable data dimensions. In addition, given that CGF is based on NeuralODE, it inherits various advantages of NeuralODE such as reversibility, unconstrained functions in model design and exact likelihood computation. \n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\u2014\u2014--Response to questions\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014----\n\n*Discussion about CGF and GNF. \n\nWe would like to clarify that GNF and CGF are fundamentally two different flow based models for graph-structured data. Both these models have their own advantages as well as challenges and it is valuable that these co-exist in the field.\n- Theoretical backbone: CGF is heavily entangled with Neural Ordinary Differential Equations \u2014 requires solving initial value problem and has continuous time property. GNF, on the other hand, is purely based on change of variable formula. They shall be treated differently.\n- Further improvements in models: Given that CGF and GNF rely on two different theoretical backbones, advances in the fields of Normalizing flows and Neural ODEs can potentially result in more powerful graph flow models on two independent directions. We appreciate the reviewer for pointing out the recent work on Flow++ and Neural spline flows which study improved techniques to enforce invertibility in functions. On the other hand, CGF has flexible functions in the model design. Moreover, the recent work on ordinary differential equations, such as Augmented Neural ODE [1] and Neural Jump SDE [2] can be applied to CGF or follow-up works for improving their performance, but potentially are less useful for normalizing flow-based models.\n\n*Comparison with concurrent work\nWe would like to clarify that GNF is a concurrent work developed in parallel and not a previous work.  We agree with the reviewer that both these models integrate GNN and flow methods, which could be considered as key contributions of these two models. We are happy to discuss the differences between the models, but we are not claiming full contributions over GNF. \n\n*Detailed questions\nQ1. GNF uses different input data\n- The full model proposed by Liu et al(2019) consists of an autoencoder and normalizing flow with graphs as a novel and powerful architecture for handling graph generation task. We are comparing with their full model. We apologize for the confusion and we have clarified that in the table. \n- This model in the paper uses coupling layers which requires the input data to have dimension higher than one for splitting. In contrast, our model CGF can handle both one-dimensional and higher dimensional data. \n- Moreover, while the representations are not the same, the task remains the same (i.e., generating diverse graphs) and we compute the metrics on the output of the model irrespective of the model being used to represent the value of the input graph variables.\n\nQ2. Dual graph\nThank you for pointing out. We changed the term dual graph to line graph and describe the transformation performed on the graph as edges being switched to nodes and nodes to edges. We\u2019ve made these changes in the paper.\n\nQ3. Baselines for table 2 and table 3\nThe baselines are all state-of-the-art structured and unstructured latent space baseline models \u2014 all published in the timeline 2016-2019. Our method is effective because it integrates powerful flow models for graph-structured data, and eliminate the assumptions over latent distributions (Gaussian, etc.).\n\n[1] Augmented Neural ODEs, Emilien Dupont \u00b7 Arnaud Doucet \u00b7 Yee Whye Teh, NeurIPS 2019\n[2] Neural Jump Stochastic Differential Equations, Junteng Jia \u00b7 Austin Benson, NeurIPS 2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1101/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1101/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1101/Authors|ICLR.cc/2020/Conference/Paper1101/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161241, "tmdate": 1576860536967, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Official_Comment"}}}, {"id": "H1xMx907jH", "original": null, "number": 3, "cdate": 1573280233867, "ddate": null, "tcdate": 1573280233867, "tmdate": 1573329170768, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "SkgZi5y0Yr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We would like to thank the reviewer for valuable comments and suggestions on our manuscript. We updated the manuscript and changes are marked by red color.\n\nQ1.1 Motivation of dual Graph.\nClarification on the dual graph: We would like to clarify that the term dual graph that we previously used is confusing. We have corrected the term \u201cdual graph\u201d to \u201cline graph\u201d. This line graph is obtained by transforming the original graph with edges being switched to nodes and nodes to edges.\n\nQ1.2 The experiment on Graph Generation is not clear to me. What is the motivation of using the dual of the graph? \nCGF operates over graph nodes (variables). The task of graph generation requires modeling of edges as variables (e.g. generating the adjacency matrix). Therefore, we transform the input graph G=(V, E) to line graph G\u2019=(V\u2019, E\u2019) and use CGF to directly model nodes in the line graph G\u2019. V\u2019 corresponds to E in the original graph G and E\u2019 is determined by the nodes V in the original graph G.\n\nQ1.3 What is the input to the full-connected layers? Is the input dimension N^2?\nThe input to the CGF is the node variables V\u2019 from the line graph. It takes value either 0 or 1, representing non-existence or existence. The input dimension to the model is N^2 (N is number of nodes ).The fully connected layers are message functions and used for local message passing between variables.\n\nQ2. How does the proposed method scale with the graph size? \nOur CGF model scales with number of dimensions similar to FFJORD or NeuralODE. We would like to mention that although the ODE equations involving independent variables linearly scale with number of nodes. However, in the case of the graph-structured variables, they are solved collectively instead of solving them independently. Relying on graph message passing will lead to better modeling of each node and potentially lessen the computation time.  \nThank you for the suggestion. We analyzed the number of function evaluation w.r.t graph size on graph generation problem. The plots are updated in Appendix under A5. The plots show that the number of function evaluations (steps to solve ode) doesn\u2019t linearly increase with the increment in number of nodes.\n\nQ3. Experiments on larger graphs\nOur model is generating entire graphs with a single feed-forward pass. Since our focus is on expressiveness of graph generative models, we show the potential of our model to generate graphs on small-scale graph datasets (Community-small and Ego-small). These are the established benchmark datasets for evaluating single feed-forward pass methods as done in [1,2]. \nOur model could potentially be extended to larger graphs by employing autoregressive (i.e.,generating K nodes at a time) or multiscale schemes.\n\n\n[1] GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders, Martin Simonovsky, Nikos Komodakis, arXiv, 2018\n[2] Graph Normalizing Flow, Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, Kevin Swersky, arXiv, 2018"}, "signatures": ["ICLR.cc/2020/Conference/Paper1101/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1101/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1101/Authors|ICLR.cc/2020/Conference/Paper1101/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161241, "tmdate": 1576860536967, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Official_Comment"}}}, {"id": "SylErbrzoS", "original": null, "number": 7, "cdate": 1573175611651, "ddate": null, "tcdate": 1573175611651, "tmdate": 1573175611651, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "B1xZgPNbsH", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment", "content": {"title": "Also dual graph is not the correct term to use here", "comment": "In graph theory dual graph has a different meaning.\nThe correct term to use here is Line graph (or edge-to-vertex dual graph) [Gross and Yellen 2006]."}, "signatures": ["~Mr_Reviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Mr_Reviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199575, "tmdate": 1576860570725, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment"}}}, {"id": "ByeLVtNWiS", "original": null, "number": 6, "cdate": 1573108014407, "ddate": null, "tcdate": 1573108014407, "tmdate": 1573108014407, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment", "content": {"title": "My third question regarding the performance of the proposed method on the puzzle generation task", "comment": "There are huge discrepancy between patches in each generated puzzle in Figure 3. It seems as if patches are generated independently of each other without passing any information among them. \nI was hoping to see an ablation study on the graph structure. For example, what happens if none of the nodes (patches) in the graph are connected? Or in the opposite case, what happens if all the nodes are connected?\nIf the method actually works, I would expect to see more similarity between patches in the generated puzzles when there are connections between nodes and less similarity when there's no connection.\nAlso, what would be the NLL in those cases? Is Continuous Graph Flow actually capable of performing message passing?\n\nSuch ablation study with both qualitative and quantitative results are necessary.\n\nAlso, I wonder if a simple generative model (e.g. VAE, Flow, etc) that is trained on each patch and conditioned on the index of each patch ( top-left: 1, top-middle: 2, etc) would produce better results (NLL) than this method or not. (P(x_i | i) where x_i is a patch and i is the index of that patch. Then log-likelihood of a generated puzzle would be \\Sum_{i=1}^9 P(x_i | i) ).\n\n\n"}, "signatures": ["~Mr_Reviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Mr_Reviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199575, "tmdate": 1576860570725, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment"}}}, {"id": "B1xZgPNbsH", "original": null, "number": 5, "cdate": 1573107432840, "ddate": null, "tcdate": 1573107432840, "tmdate": 1573107432840, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment", "content": {"title": "My second question regarding the vague experimental section", "comment": "What does DEGREE, CLUSTERING, and ORBIT mean in the graph generation section?\nThese words (degree, clustering, and orbit) are only mentioned in Table 1 and there're no explanations of these in the text. I believe this task (graph generation) and the criteria for comparison deserve a short description, even if it is following the same experimental setup as GNF.\n\nWhy lower Orbit is better? Why lower Clustering is better? etc."}, "signatures": ["~Mr_Reviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Mr_Reviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199575, "tmdate": 1576860570725, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment"}}}, {"id": "H1luoS4bsS", "original": null, "number": 4, "cdate": 1573107103682, "ddate": null, "tcdate": 1573107103682, "tmdate": 1573107103682, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "Hkgp-7dstr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment", "content": {"title": "Thank you for the response -- Clarifying my concern", "comment": "Here's the first question:\n\nIn one set of experiments the following baselines are chosen to be compared against the proposed method\n- GraphVAE\n- DeepGMG\n- GraphRNN\n- GNF\n\nIn another set of experiments these baselines are chosen to be compared against the proposed method:\n- BiLSTM + VAE \n- StructuredVAE\n- Graphite\n- VMP-SIN\n- GAE\n- NRI\n\nThe problem is that there's no intersection between these set of baselines. Why? \nDoes this mean that the proposed method could only beat some of the baselines in each task but not all? So only those with lower performance than the proposed method are included in the tables?\nIn any case, this kind of experimental setup seems so weird to me, except there's a very particular reason for this choice of baselines.\n"}, "signatures": ["~Mr_Reviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Mr_Reviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199575, "tmdate": 1576860570725, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment"}}}, {"id": "BkgReQ9aKB", "original": null, "number": 1, "cdate": 1571820277909, "ddate": null, "tcdate": 1571820277909, "tmdate": 1572972512789, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes continuous graph flow (CGF), a flow-based generative model for graphs.  Based on the idea to build continuous-time flows with neural ODEs, the node features are transformed into Gaussian random variables via message passing. The log-likelihood can be approximated stochastically.\n\nI find it hard to assess the novelty of this work because 1) the algorithm looks like a trivial application of continuous-time normalizing flow to graph data using message passing algorithm, and 2) the concurrent work graph normalizing flow (GNF, Liu et al., 2019).  I failed to find any algorithmic innovation more than a mere application of continuous-time flow to graph data. The very idea of a flow-based model for graphs using the message passing algorithm may be considered as a contribution, but this is also blurred because of the concurrent work GNF. \n\nThe experiments look interesting, but there are some minor concerns. For the unsupervised graph generation task, CGF is shown to perform better than GNF, but I think the comparison here may not be fair because the results for GNF seem to be obtained from the paper uses a different way to construct initial node features to be fed into message passing. Specifically, according to the GNF paper, they inject an i.i.d. Gaussian noise as initial node features, but this work uses different schemes based on the dual graph (btw I think the term dual graph may be misleading. There already exists a common term called dual-graph with different definition). So I think to compare the expressive power of flows, it would be fair to start from a common scheme to build initial node features. GNF should also be compared to CGF for the other experiments. Seems like the baselines presented in Table 2 and Table 3 are quite dumb baselines not well suited for the tasks considered here. \n\nIn page 2, the authors stated that \"GNF involves partitioning of data dimensions into two halves and employs coupling layers to couple them back. This leads to several constraints on function forms and model architectures...\". I think GNF can be improved using more advanced transformation other than affine coupling, such as recently proposed mixture logistic CDF [1] or neural spline flow [2].  Continuous-time flow may still have an advantage in memory usage, but at least in terms of expressive power, I think it is not clear whether CGF is particularly better.\n\n[1] Ho et al., Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design, 2019\n[2] Durkan et al., Neural Spline Flows, 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper1101/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1101/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832786691, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1101/Reviewers"], "noninvitees": [], "tcdate": 1570237742362, "tmdate": 1575832786703, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Official_Review"}}}, {"id": "SkgZi5y0Yr", "original": null, "number": 2, "cdate": 1571842713115, "ddate": null, "tcdate": 1571842713115, "tmdate": 1572972512746, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents a method to do Continuous Normalizing Flow for graphs by defining the density evolution via ODEs over shared variables. Continous message passing scheme introduced can leverage the free-form model architectures which allow flexibility in handling variable data dimensions. The results of three different generation tasks indicate improved performance over the baseline methods. \n\nNovelty: The basic methodological contribution of the paper seems somewhat limited. The main idea of the paper follows directly from NeuralODE (Chen et al., 2018) and FFJord (Grathwohl et al., 2019). Nevertheless, I do agree that adapting the NeuralODEs and continuous flows for unstructured data is challenging and deserves its own analysis. However, I find the experiment section to be not very clear (See specific comments below).  \n\nRecommendation: Weak Reject\n\nConcerns: \n\n- The experiment on Graph Generation is not clear to me. What is the motivation of using the dual of the graph? What is the input to the full-connected layers? Is the input dimension N^2 (the number of edges)? Please clarify. It would be helpful if the implementation details in Appendix A.1 can be formalized in the main paper since this is the main contribution of the paper. Overall the experiment section in the main paper lacks sufficient details to comment on the efficacy of the method.\n       \n- NeuralODEs involve iterative numerical solvers which can be very slow. It seems that the proposed method involves simultaneously solving ODEs that linearly scale with the number of nodes (Equation 11)? I think there should be some analysis of how the proposed method scales with the graph size. I am not sure if NeuralODE would perform well when the number of nodes is large.\n\n- Compared to other baseline methods (see You et al. 2018), the datasets used for analysis are smaller versions of the original datasets. There are other datasets that can be used for experiments on graph generation:  See Grid, Protein "}, "signatures": ["ICLR.cc/2020/Conference/Paper1101/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1101/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832786691, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1101/Reviewers"], "noninvitees": [], "tcdate": 1570237742362, "tmdate": 1575832786703, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Official_Review"}}}, {"id": "Byxc74-0Yr", "original": null, "number": 3, "cdate": 1571849250060, "ddate": null, "tcdate": 1571849250060, "tmdate": 1572972512703, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new variant of invertible flow-based model for graph structured data. \nSpecifically, the authors proposed a continuous normalizing flow model for graph generation, first in the graph generation literature. \nThe authors claim that the free-form model architecture of neural ODE formulation of continuous flow is advantageous against standard discrete flow models. Experimental results show that the proposed model is superior to recent models in image puzzling and layout generation datasets. \n\nOverall, manuscript is well organized. \nThe combination of continuous flows and graph structured data is new in the literature (as far as I know). Proposed formulation seems natural and reasonable. I find no fatal flows in the formulation. In experiments, the proposed model achieved good scores against recent GNN works. \n\nConcerning the existing invertible flow-based models for graph structured data, Madhawa\u2019s work is one of the first attempts in the literature. I think the paper below should be refereed appropriately:  \nMadhawa+, \u201cGraph NVP\u201d, arXiv: 1905.11600, 2019. \n\nThe way of incorporating relational structure into flows are very similar to the GraphNVP and Graph Normalizing Flow: using neighboring nodes\u2019 hidden vectors as parameters (or input to parameter inference networks). \nIn addition, I found no special tricks or theoretical considerations to achieve the continuous flow for graphs. Based on these points, I think technical contribution of this paper is somewhat limited. \n\nI cannot find information about the specific chosen forms of f-hat and g in Eq.(10) within the manuscript. Are the choices of f-hat and g are crucial for performance? It is preferable if the authors can present any experimental validations concerning this issue. \n\nMy main concerns are in the experimental section. \n\nI\u2019m not fully convinced in the necessity of the continuous normalizing flows for the experimental tasks. None of the experimental tasks have `````'' intrinsic continuous time dynamics over graph-structured data (Sec. 2)''. Then, what is the rationale to adopt Continuous graph flow for these tasks? \n\nOne reason to adopt continuous flows is that the ODE formulation allows users to choose free-form model architecture, yielding more complicated mappings to capture delicate variable distributions. \nI expect some assessments are made concerning this issue. My suggestion is (i) to test the discretized model of the proposed Continuous Graph Flow and see how the discretization deteriorate the performance, and (ii) to test several choices of f-hat and g (Eq.10) to show the necessity of ODE formulation, accommodating free-form model architecture.  \n\nIn the current manuscript, Graph Normalizing Flow (GNF) is the closest competitor. However, GNF is not tested in the puzzle and the scene graph experiments. Why is that?\n\nI\u2019d like to hear opinions of the authors concerning these issues, and hope some discussions are included in the manuscript. \n\n\nSummary\n+ continuous normalizing flow is first applied to graph structure data\n+ manuscript is well organized\n+- natural and reasonable formulation. But at the same time, technological advancement is limited. \n- Less convinced to adopt continuous flow for graph-structured data without no intrinsic continuous dynamics. \n- Necessity or advantages of ``continuous\u2019\u2019 flows are not well assessed in the experiments. Please consider some additional assessments suggested in the review comment. \n- GNF, the closest competitor, is omitted in the 2d and the 3rd experiments. No explanations about this. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1101/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1101/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575832786691, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1101/Reviewers"], "noninvitees": [], "tcdate": 1570237742362, "tmdate": 1575832786703, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Official_Review"}}}, {"id": "Hkgp-7dstr", "original": null, "number": 1, "cdate": 1571681029216, "ddate": null, "tcdate": 1571681029216, "tmdate": 1571681029216, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "rylhIt7wKS", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Official_Comment", "content": {"title": "Thank you for your interest and comments", "comment": "We thank the Anonymous Reader for the interest in our paper. However, we don't quite understand the comment especially on experiments since there are no specific concerns listed. We are happy to provide more information but we are not sure which parts are unclear.\n\nRegarding comparisons, to clarify, Graph Normalizing Flows by Liu et al(2019) is a concurrent work (as mentioned in Intro) rather than a standard baseline. We take the quantitative results for graph generation directly from their paper for reference only and follow the same evaluation protocol to conduct experiments for our proposed model."}, "signatures": ["ICLR.cc/2020/Conference/Paper1101/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1101/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1101/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1101/Authors|ICLR.cc/2020/Conference/Paper1101/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161241, "tmdate": 1576860536967, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Official_Comment"}}}, {"id": "rylhIt7wKS", "original": null, "number": 3, "cdate": 1571400020066, "ddate": null, "tcdate": 1571400020066, "tmdate": 1571400020066, "tddate": null, "forum": "BkgZSCEtvr", "replyto": "BkgZSCEtvr", "invitation": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment", "content": {"comment": "Since on the theoretical side there is only incremental contribution, it's necessary to have a stronger experimental results. Thus it's necessary to see the superiority of this method in a correct set of experiments. I have read through the experimental section at least 3 times and still many of the things don't make sense to me.\n\nOne important question is, why the authors compared their method with different set of baselines in each task?\nI think the GraphNormalizingFlow (Liu et al., 2019) is an important baseline and should be included in all the experiments. It is not clear why this method is not included in the puzzle/layout generation tasks. ", "title": "Incremental contribution and a very very vague experimental section"}, "signatures": ["~Anonymous_Reader2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anonymous_Reader2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Graph Flow", "authors": ["Zhiwei Deng", "Megha Nawhal", "Lili Meng", "Greg Mori"], "authorids": ["zhiweid@princeton.edu", "mnawhal@sfu.ca", "lilimeng1103@gmail.com", "mori@cs.sfu.ca"], "keywords": ["graph flow", "normalizing flow", "continuous message passing", "reversible graph neural networks"], "TL;DR": "Graph generative models based on generalization of message passing to continuous time using ordinary differential equations ", "abstract": "In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to  state-of-the-art models.", "pdf": "/pdf/340192db2a28ff7029a2bae975c0e386912d7888.pdf", "paperhash": "deng|continuous_graph_flow", "original_pdf": "/attachment/a749a6e3130acf2dd20b5c9848320d62cc37865d.pdf", "_bibtex": "@misc{\ndeng2020continuous,\ntitle={Continuous Graph Flow},\nauthor={Zhiwei Deng and Megha Nawhal and Lili Meng and Greg Mori},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgZSCEtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgZSCEtvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199575, "tmdate": 1576860570725, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1101/Authors", "ICLR.cc/2020/Conference/Paper1101/Reviewers", "ICLR.cc/2020/Conference/Paper1101/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1101/-/Public_Comment"}}}], "count": 14}