{"notes": [{"id": "UBdM-t9zXe", "original": null, "number": 18, "cdate": 1584270224354, "ddate": null, "tcdate": 1584270224354, "tmdate": 1584270224354, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "content": {"title": "I have add multiple Solutions to fix", "comment": "Here is a link to the solution: https://magicvibes.co/fix-0x00000002-error/"}, "signatures": ["~Joseph_Diaz1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Joseph_Diaz1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"id": "jdjgR8GgRz", "original": null, "number": 17, "cdate": 1577691044056, "ddate": null, "tcdate": 1577691044056, "tmdate": 1577691044056, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "content": {"title": "It,s Really Hepful", "comment": "I was looking for router error code 488"}, "signatures": ["~Router_Error_Code1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Router_Error_Code1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"id": "rklvv9e9Lr", "original": null, "number": 16, "cdate": 1568438879207, "ddate": null, "tcdate": 1568438879207, "tmdate": 1568438879207, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "HyxlvBUU_N", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "content": {"title": "developer", "comment": "Can you guys help me to fix https://www.errorcodeshelp.com/0xc0000005/\nlet me know if someone can help me here."}, "signatures": ["~Jon_Snow2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jon_Snow2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"id": "S1eUFCdWjV", "original": null, "number": 14, "cdate": 1556348542199, "ddate": null, "tcdate": 1556348542199, "tmdate": 1556348589731, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "r1xuv3ZdVQ", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "content": {"title": "A related paper.", "comment": "Hi, there is a paper \"AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods\" which solves the non-convergence of Adam via using temporally decorrelated g_t and shares the spirit with your comment. See https://openreview.net/forum?id=HkgTkhRcKQ for more details."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"id": "HyxlvBUU_N", "original": null, "number": 13, "cdate": 1553519959615, "ddate": null, "tcdate": 1553519959615, "tmdate": 1553519959615, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "SJRh-9lef", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "content": {"title": "developer", "comment": "This is something which I am looking for and thanks for sharing the information with us. https://errorcode0x.com/error-code-0x80070002/  helped me to get the hyperparameters.\n"}, "signatures": ["~joe_denly1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~joe_denly1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"id": "SJgoJfM_4Q", "original": null, "number": 12, "cdate": 1532662242732, "ddate": null, "tcdate": 1532662242732, "tmdate": 1532662242732, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "r1xuv3ZdVQ", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "content": {"title": "oh, wait...", "comment": "sorry.. on reviewing how adam works again, I see that the first moment, as well as the 2nd moment, are moving averages, so changing t to t-1 wouldn't change the fact that you have the same samples appear in both the numerator and denominator of the parameter-change equation delta-params = moving-average-mean / sqrt(moving-average-variance + epsilon).\nTo fix it, I guess you could either use a fix like you are proposing, involving taking the average over more samples, or you could take steps to make sure that the samples appearing in the numerator and denominator of the equation were independent, e.g. using some odd-even formulation to separate the two moving averages into two bins and crossing them over so that bin A's mean was divided by bin B's variance  and vice versa.\nTo me the easiest demonstration of the problem is a parameter whose derivative at equilibrium is small and negative for almost all samples, and large and positive on very rare samples.  Because you divide by that large number whenever you encounter those rare samples, the expected parameter change is negative in a situation where it should be zero.\n"}, "signatures": ["~Daniel_Povey1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Daniel_Povey1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"id": "r1xuv3ZdVQ", "original": null, "number": 11, "cdate": 1532660832383, "ddate": null, "tcdate": 1532660832383, "tmdate": 1532660832383, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "content": {"title": "Using previous t's 2nd moment estimate", "comment": "Guys,\nI know it's been a while since this conversation was active, but I just noticed this paper and thread, and wanted to comment.\n\nTo me the obvious problem with Adam that would prevent convergence is that it uses the 2nd moment estimate from the *current* t, instead of the *previous* t.   That introduces a nonlinearity in the dependence on the currently selected sample, which I think would tend to be fatal for most proofs.\n\nYour approach seems to be to try to remove the dependence on the current sample by making the history very long."}, "signatures": ["~Daniel_Povey1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Daniel_Povey1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"id": "r1loSZC6km", "original": null, "number": 11, "cdate": 1527796034722, "ddate": null, "tcdate": 1527796034722, "tmdate": 1527796034722, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "rkfPomJKG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "content": {"title": "beta_{1t} is upper bounded", "comment": "Thanks for your interest in the paper. This is obtained by applying Cauchy-Schwarz inequality and using the fact that 1 - \\beta_{1t} is bounded by 1.\n"}, "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"id": "BkgiQbRp1X", "original": null, "number": 10, "cdate": 1527796002610, "ddate": null, "tcdate": 1527796002610, "tmdate": 1527796002610, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "B1PDZUChG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "content": {"title": "TL;DR : Its with the algorithm", "comment": "Dear Jeremy,\n\nThank you for your interest in the paper.\n\nTo answer your question \"Is the problem with Adam ....?\" : Our paper shows that the algorithm defined in the Adam paper  (https://arxiv.org/pdf/1412.6980.pdf, Algorithm 1)  (including one with decreasing step size alpha) has convergence issues. Specifically, for any setting of the Adam parameters (beta_1, beta_2, minibatch size, epsilon, etc) there is a convex optimization setting where Adam will not converge to the optimal solution, even if decreasing learning rates are used. This is in contrast to algorithms like SGD which, with decreasing learning rates, is guaranteed to converge. \n\nAs to how to fix the convergence issues, there are quite a few ways and our paper mentions a couple of them. Using a high epsilon as mentioned in the paper (please refer to the discussion after Theorem 3 in our paper), or using increasing schedule for epsilon is another fix. Also, as also studied in https://arxiv.org/pdf/1507.02030.pdf and probably other papers, increasing minibatch can also help in the stochastic setting. But it is important to understand the practical implications of any of these fixes because they involve adding more hyper-parameters, schedules and may be more resources. We proposed two solutions to tackle the aforementioned issues, including one which does not have any additional parameters (AMSGrad).\n\nOf course, Adam (Algorithm 1) in https://arxiv.org/pdf/1412.6980.pdf roughly subsumes SGD by using a large value of epsilon and obviously one should simply use SGD instead of Adam with very large (and increasing with iterations) epsilon if the former works well. But several question remain: whether simply using SGD is better or are there other values (or schedules) of Adam that make it work better (and furthermore, is it dimensionality dependent?). Or is there a modification of the algorithm that can tackle these issues with fewer hyper-parameters? While designing any of these algorithms, it is important to not add too many additional hyper-parameters because tuning them can be very difficult in practice."}, "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"id": "Sygtwl0T1X", "original": null, "number": 9, "cdate": 1527795809256, "ddate": null, "tcdate": 1527795809256, "tmdate": 1527795809256, "tddate": null, "forum": "ryQu7f-RZ", "replyto": "ryxs8e-pf", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "content": {"title": "Problem setting in the paper", "comment": "Dear Martin,\n\nThanks for your interest in our paper. We think there is a misunderstanding regarding the online setting. Please refer to Section 2 of the paper. Also, please refer to http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf and references therein for a more detailed description of the setting. Our results also apply to the stochastic setting (see Theorem 3 in the paper).\n"}, "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"tddate": null, "ddate": null, "tmdate": 1524870098088, "tcdate": 1524856471854, "number": 10, "cdate": 1524856471854, "id": "ryxs8e-pf", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["~Martin_Georg_Wei\u00df1"], "readers": ["everyone"], "writers": ["~Martin_Georg_Wei\u00df1"], "content": {"title": "Are the counter examples valid?", "comment": "Your change in the algorithm using the maximum is a great progress. Two of my master students also noticed that there are several errors in the proof of Kingma and Ba. They corrected some constants and parts of the proof, but the main points seem beyond repair. Because both master theses contain confidential material from industrial applications only some small part has been published, see https://www.researchgate.net/publication/324808725_An_improvement_of_the_convergence_proof_of_the_ADAM-Optimizer\n \nHowever the counter examples in your paper do not seem valid to me: The task to be solved is not clearly defined in your paper, neither in Kingma and Ba. The task should be to minimize the expectation of some loss function L(x,z) like a the squared error between network output and training sample, with x the parameters (network weights) and z the sample from the same probability distribution for all t. So the the loss function usually is the same for all t, and t appears in f_t(x) = L(x,z_t) only via the sample z_t. The random variables do not know what time t it is, only the algorithm does. So your counterexample cannot decide what t mod 3 is. If your assumption is that the probability changes periodically with t that should be stated. The same reasoning holds for minimizing a deterministic function f(x), then dependence on t is only via f(x_t) as well.\n \nYou might argue: One can define a (probably very complicated and spiky) function g(x) not depending on t which gives the same function values for all x_t generated by the counterexample. But then it is not clear whether such a  function exists: If e.g. t1 mod 3 = 1and t2 mod 3 = 2 holds, but at the same time x_t1 = x_t2, one cannot assign a unique function value for g(x_t1) .\nThe implication is: The Adam convergence proof is wrong, but the counter examples are not relevant. So there might still be hope for a convergence proof for Adam, maybe under additional assumptions, but without the maximum.\n \nCould you please comment on that.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"tddate": null, "ddate": null, "tmdate": 1524685672934, "tcdate": 1524683102547, "number": 9, "cdate": 1524683102547, "id": "B1PDZUChG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["~Jeremy_Bernstein1"], "readers": ["everyone"], "writers": ["~Jeremy_Bernstein1"], "content": {"title": "Is the problem with Adam, or with the theoretical framework used to analyse it?", "comment": "Dear authors,\n\nThank you for the valuable contribution in pointing out issues in the original Adam convergence proof, and noticing that bimodal noise distributions can obstruct convergence.\n\nIn the paper you suggest modifying the Adam algorithm to solve the problem. I want to add two much simpler fixes that do not involve modifying Adam at all:\n\n1. A cheap trick: set epsilon large.\n----for large enough epsilon, Adam becomes exactly SGD.\n----the convergence of SGD is well-established.\n----how large should epsilon be? Well if you don't know in advance, you can use a growing schedule.\n\n2. A more subtle fix: use a larger minibatch size.\n----your construction relies on bimodal gradient noise\n----but by the central limit theorem, even for modest mini-batch sizes the stochastic gradient noise will become nice and Gaussian\n----this issue is studied in greater depth in our paper [1]\n----we establish convergence for Adam with beta1=beta2=epsilon=0, otherwise known as signSGD\n\nThese observations are important because I suspect that AMSgrad, similar to Adagrad, will not scale up to deeper networks, whereas Adam, as we all know, does.\n\nOn a meta-level, the problem you raise is an issue with the analytical framework, rather than with the Adam algorithm itself. In particular:\n----the analytical framework provides no knob to tune the mini-batch size\n----in your Theorem 6 you assume epsilon is chosen before seeing the optimisation problem. But in real life we can choose our hyperparameters after seeing the optimisation problem.\n\nWith regard to this comment in the abstract:\n>> \"In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings).\"\n\nMy claim is that some practitioners have problems with Adam not because it has fundamental convergence issues but because it has too many hyperparameters for them to tune carefully, especially when they do not know what each hyperparameter is actually controlling. For any problem, a properly tuned-Adam will converge at least as well as SGD and signSGD **since it contains these algorithms as special cases**. The practical takeaway: the practitioner's dream of having magic hyperparameters that work for every problem under the sun is exactly that, a dream :)\n\n[1] signSGD: compressed optimisation for non-convex problems (https://arxiv.org/abs/1802.04434)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"tddate": null, "ddate": null, "tmdate": 1522117108921, "tcdate": 1509135211401, "number": 807, "cdate": 1518730164448, "id": "ryQu7f-RZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "ryQu7f-RZ", "original": "HJbOQG-0Z", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "nonreaders": [], "details": {"replyCount": 29, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "tmdate": 1520564114278, "tcdate": 1520544602298, "number": 8, "cdate": 1520544602298, "id": "rkfPomJKG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "one question on lemma 2 to prove theorem 4", "comment": "Hello authors, it is a great work. However, I have a question on the first equation (the second inequality sign) of the proof of lemma2. This equation said:\n\n\\sum_{t=1}^T \\alpha_t || \\hat V_t^{-1/4}  m_t ||^2 \\leq  \\sum_{t=1}^{T-1} \\alpha_t || \\hat V_t^{-1/4}  m_t ||^2 + \\alpha \\sum_{i=1} ( numerator / denominator )\n\nMy question is on this numerator (m_{T,i}^2) which you applied the update m_t = \\beta_{1t} m_{t-1} + (1-\\beta_{1t}) g_t. Since in your proof beta_{1t} is non-increasing (changes) over time, I wonder if the numerator in your equation is correct. It looks that the coefficient (1-\\beta_{1j}) before g_{j,i} is ignored. If \\beta_{1j} is a constant \\beta, then the derived bound is correct since (1-beta)^2 \\leq 1. However, the effect of varying \\beta_{1j} makes the derivation involved. Could you please elaborate on why the term (1-\\beta_{1j}) can be ignored, or how you derive the numerator in the paper?\n\nThanks,"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260101470, "tcdate": 1517249179814, "number": 1, "cdate": 1517249179794, "id": "rJ4iGyTrz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper analyzes a problem with the convergence of Adam, and presents a solution. It identifies an error in the convergence proof of Adam (which also applies to related methods such as RMSProp) and gives a simple example where it fails to converge. The paper then repairs the algorithm in a way that guarantees convergence without introducing much computational or memory overhead. There ought to be a lot of interest in this paper: Adam is a widely used algorithm, but sometimes underperforms SGD on certain problems, and this could be part of the explanation. The fix is both principled and practical. Overall, this is a strong paper, and I recommend acceptance.\n", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642514585, "tcdate": 1511476851766, "number": 1, "cdate": 1511476851766, "id": "HkhdRaVlG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Review", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["ICLR.cc/2018/Conference/Paper807/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Review", "rating": "9: Top 15% of accepted papers, strong accept", "review": "The paper presents three contributions: 1) it shows that the proof of convergence Adam is wrong; 2) it presents adversarial and stochastic examples on which Adam converges to the worst possible solution (i.e. there is no hope to just fix Adam's proof); 3) it proposes a variant of Adam called AMSGrad that fixes the problems in the original proof and seems to have good empirical properties.\n\nThe contribution of this paper is very relevant to ICLR and, as far as I know, novel.\nThe result is clearly very important for the deep learning community.\nI also checked most of the proofs and they look correct to me: The arguments are quite standard, even if the proofs are very long.\n\nOne note on the generality of the results: the papers states that some of the results could apply to RMSProp too. However, it has been proved that RMSProp with a certain settings of its parameters is nothing else than AdaGrad (see Section 4 in  Mukkamala and Hein, ICML'17). Hence, at least for a certain setting of its parameters, RMSProp will converge. Of course, the proof in the ICML paper could be wrong, I did not check that...\n\nA general note on the learning rate: The fact that most of these algorithms are used with a fixed learning rate while the analysis assume a decaying learning rate should hint to the fact that we are not using the right analysis. Indeed, all these variants of AdaGrad did not really improve the AdaGrad's regret bound. In this view, none of these algorithms contributed in any meaningful way to our understanding of the optimization of deep networks *nor* they advanced in any way the state-of-the-art for optimizing convex Lipschitz functions.\nOn the other hand, analysis of SGD-like algorithms with constant step sizes are known. See, for example, Zhang, ICML'04 where linear convergence is proved in a neighbourhood of the optimal solution for strongly convex problems.\nSo, even if I understand this is not the main objective of this paper, it would be nice to see a discussion on this point and the limitations of regret analysis to analyse SGD algorithms.\n\nOverall, I strongly suggest to accept this paper.\n\n\nSuggestions/minor things:\n- To facilitate the reader, I would state from the beginning what are the common settings of beta_1 and beta_2 in Adam. This makes easier to see that, for example, the condition of Theorem 2 is verified.\n- \\hat{v}_{0} is undefined in Algorithm 2.\n- The graphs in figure 2 would gain in readability if the setting of each one of them would be added as their titles.\n- McMahan and Streeter (2010) is missing the title. (Also, kudos for citing both the independent works on AdaGrad)\n- page 11, last equation, 2C-4=2C-4. Same on page 13.\n- Lemma 4 contains x_1,x_2,z_1, and z_2: are x_1 and z_1 the same? also x_2 and z_2?", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642514486, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/AnonReviewer1", "ICLR.cc/2018/Conference/Paper807/AnonReviewer3", "ICLR.cc/2018/Conference/Paper807/AnonReviewer4"], "reply": {"forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642514486}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642514544, "tcdate": 1511792785639, "number": 2, "cdate": 1511792785639, "id": "H15qgiFgf", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Review", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["ICLR.cc/2018/Conference/Paper807/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Noteworthy paper; some (fixable) issues with technical presentation", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This work identifies a mistake in the existing proof of convergence of\nAdam, which is among the most popular optimization methods in deep\nlearning. Moreover, it gives a simple 1-dimensional counterexample with\nlinear losses on which Adam does not converge. The same issue also\naffects RMSprop, which may be viewed as a special case of Adam without\nmomentum. The problem with Adam is that the \"learning rate\" matrices\nV_t^{1/2}/alpha_t are not monotonically decreasing. A new method, called\nAMSGrad is therefore proposed, which modifies Adam by forcing these\nmatrices to be decreasing. It is then shown that AMSGrad does satisfy\nessentially the same convergence bound as the one previously claimed for\nAdam. Experiments and simulations are provided that support the\ntheoretical analysis.\n\nApart from some issues with the technical presentation (see below), the\npaper is well-written.\n\nGiven the popularity of Adam, I consider this paper to make a very\ninteresting observation. I further believe all issues with the technical\npresentation can be readily addressed.\n\n\n\nIssues with Technical Presentation:\n\n- All theorems should explicitly state the conditions they require\n  instead of referring to \"all the conditions in (Kingma & Ba, 2015)\".\n- Theorem 2 is a repetition of Theorem 1 (except for additional\n  conditions).\n- The proof of Theorem 3 assumes there are no projections, so this\n  should be stated as part of its conditions. (The claim in footnote 2\n  that they can be handled seems highly plausible, but you should be up\n  front about the limitations of your results.)\n- The regret bound Theorem 4 establishes convergence of the optimization\n  method, so it plays the role of a sanity check. However, it is\n  strictly worse than the regret bound O(sqrt{T}) for online gradient\n  descent [Zinkevich,2003], so it cannot explain why the proposed\n  AMSgrad method might be adaptive. (The method may indeed be adaptive\n  in some sense; I am just saying the *bound* does not express that.\n  This is also not a criticism of the current paper; the same remark\n  also applies to the previously claimed regret bound for Adam.)\n- The discussion following Corollary 1 suggests that sum_i\n  hat{v}_{T,i}^{1/2} might be much smaller than d G_infty. This is true,\n  but we should always expect it to be at least a constant, because\n  hat{v}_{t,i} is monotonically increasing by definition of the\n  algorithm, so the bound does not get better than O(sqrt(T)).\n  It is also suggested that sum_i ||g_{1:T,i}|| = sqrt{sum_{t=1}^T\n  g_{t,i}^2} might be much smaller than dG_infty, but this is very\n  unlikely, because this term will typically grow like O(sqrt{T}),\n  unless the data are extremely sparse, so we should at least expect\n  some dependence on T.\n- In the proof of Theorem 1, the initial point is taken to be x_1 = 1,\n  which is perfectly fine, but it is not \"without loss of generality\",\n  as claimed. This should be stated in the statement of the Theorem.\n- The proof of Theorem 6 in appendix B only covers epsilon=1. If it is\n  \"easy to show\" that the same construction also works for other\n  epsilon, as claimed, then please provide the proof for general\n  epsilon.\n\n\nOther remarks:\n\n- Theoretically, nonconvergence of Adam seems a severe problem. Can you\n  speculate on why this issue has not prevented its widespread adoption?\n  Which factors might mitigate the issue in practice?\n- Please define g_t \\circ g_t and g_{1:T,i}\n- I would recommend sticking with standard linear algebra notation for\n  the sqrt and the inverse of a matrix and simply using A^{-1} and\n  A^{1/2} instead of 1/A and sqrt{A}.\n- In theorems 1,2,3, I would recommend stating the dimension (d=1) of\n  your counterexamples, which makes them very nice!\n\nMinor issues:\n\n- Check accent on Nicol\\`o Cesa-Bianchi in bibliography.\n- Near the end of the proof of Theorem 6: I believe you mean Adam\n  suffers a \"regret\" instead of a \"loss\" of at least 2C-4.\n  Also 2C-4=2C-4 is trivial in the second but last display.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642514486, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/AnonReviewer1", "ICLR.cc/2018/Conference/Paper807/AnonReviewer3", "ICLR.cc/2018/Conference/Paper807/AnonReviewer4"], "reply": {"forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642514486}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642514505, "tcdate": 1513253799596, "number": 3, "cdate": 1513253799596, "id": "Hyl2iJgGG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Review", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["ICLR.cc/2018/Conference/Paper807/AnonReviewer4"], "readers": ["everyone"], "content": {"title": "Theoretical (maybe practical) issues with ADAM, and an way to solve them", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper examines the very popular and useful ADAM optimization algorithm, and locates a mistake in its proof of convergence (for convex problems). Not only that, the authors also show a specific toy convex problem on which ADAM fails to converge. Once the problem was identified to be the decrease in v_t (and increase in learning rate), they modified the algorithm to solve that problem. They then show the modified algorithm does indeed converge and show some experimental results comparing it to ADAM.\n\nThe paper is well written, interesting  and very important given the popularity of ADAM. \n\nRemarks:\n- The fact that your algorithm cannot increase the learning rate seems like a possible problem in practice. A large gradient at the first steps due to bad initialization can slow the rest of training. The experimental part is limited, as you state \"preliminary\", which is a unfortunate for a work with possibly an important practical implication. Considering how easy it is to run experiments with standard networks using open-source software, this can easily improve the paper. That being said, I understand that the focus of this work is theoretical and well deserves to be accepted based on the theoretical work.\n\n- On page 14 the fourth inequality not is clear to me.\n\n- On page 6 you talk about an alternative algorithm using smoothed gradients which you do not mention anywhere else and this isn't that clear (more then one way to smooth). A simple pseudo-code in the appendix would be welcome.\n\nMinor remarks:\n- After the proof of theorem 1 you jump to the proof of theorem 6 (which isn't in the paper) and then continue with theorem 2. It is a bit confusing.\n- Page 16 at the bottom v_t= ... sum beta^{t-1-i}g_i should be g_i^2\n- Page 19 second line, you switch between j&t and it is confusing. Better notation would help.\n- The cifarnet uses LRN layer that isn't used anymore.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642514486, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/AnonReviewer1", "ICLR.cc/2018/Conference/Paper807/AnonReviewer3", "ICLR.cc/2018/Conference/Paper807/AnonReviewer4"], "reply": {"forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642514486}}}, {"tddate": null, "ddate": null, "tmdate": 1513368971247, "tcdate": 1513368971247, "number": 7, "cdate": 1513368971247, "id": "BJQcTsbzf", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "forum": "ryQu7f-RZ", "replyto": "HkhdRaVlG", "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "content": {"title": "Thanks for the very helpful feedback", "comment": "We thank the reviewer for very helpful and constructive feedback. \n\nAbout Mukkamala and Hein 2017 [MH17]: Thanks for pointing this paper. As the anonymous reviewer rightly points out, the [MH17] does not look at the standard version of RMSProp but rather a modification and thus, there is no contradiction with our paper. We will make this point clear in the final version of the paper.\n\nRegarding note about learning rate: While it is true that none of these new rates improve upon Adagrad rates, in fact, in the worst case one cannot improve the regret of standard online gradient descent in general convex setting. Adagrad improves this in the special case of sparse gradients (see for instance, Section 1.3 of Duchi et al. 2011). However, these algorithms, which are designed for specific convex settings, appear to perform reasonably well in the nonconvex settings too (especially in deep networks). Exponential moving average (EMA) variants seem to further improve the performance in the (dense) nonconvex setting. Understanding the cause for good performance in nonconvex settings is an interesting open problem. Our aim was to take an initial step to develop more principled EMA approaches. We will add a description in the final version of the paper.\n\nLemma 4: Thanks for pointing it out and sorry for the confusion. Indeed, x1 = z1 and x2 = z2. We have corrected this typo.\n\nWe have also revised the paper to address the minor typos mentioned in the review."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"tddate": null, "ddate": null, "tmdate": 1513368843254, "tcdate": 1513368843254, "number": 6, "cdate": 1513368843254, "id": "HJXG6sWzG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "forum": "ryQu7f-RZ", "replyto": "H15qgiFgf", "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "content": {"title": "Thanks for the very helpful review", "comment": "We deeply appreciate the reviewer for a thorough and constructive feedback. \n\n- Theorem 2 & 3 are much more involved and hence the aim of Theorem 1 was to provide a simplified counter-example for a restrictive setting, thereby providing the key ideas of the paper.\n- We will emphasize your point about projections in the final version of the paper.\n- We agree that the role of Theorem 4 right now is to provide a sanity check. Indeed, it is not possible to improve upon the of online gradient descent in the worst case convex settings. Algorithms such as Adagrad exploit structure in the problem such as sparsity to provide improved regret bounds. Theorem 4 provides some adaptivity to sparsity of gradients (but note that these are upper bounds and it is not clear if they are tight). Adaptive methods seem to perform well in few non-sparse and nonconvex settings too. It remains open to understand it in the nonconvex settings of our interest. \n- Indeed, there is a typo; we expect ||g{1:T,i}|| to grow like sqrt(T). The main benefit in adaptive methods comes in terms of sparsity (and dimension dependence). For example see Section 1.3 in Duchi et al. 2011). We have revised the paper to incorporate these changes.\n- We can indeed assume that x_1 = 1 (without loss of generality) because for any choice of initial point, we can always translate the function so that x_1 = 1 is the initial point in the new coordinate system. We will add a discussion about this in the final version of the paper.\n- The last part of Theorem 6 explains the reduction with respect to general epsilon. We will further highlight this in the final version of the paper.\n\nOther remarks:\n\nRegarding widespread adoption of Adam: It is possible that in certain applications the issues we raised in this work are not that severe (although they can still lead to degradation in generalization performance). On the contrary, there exist a large number of real-world applications, for instance training models with large output spaces, which suffer from the issues we have highlighted and non-convergence has been observed to occur more frequently. Often, this non-convergence is attributed to nonconvexity but our paper shows one of the causes that applies even to convex settings. \nAs stated in the paper, using a problem specific large beta2 seems to help in some applications. Researchers have developed many tricks (such as gradient clipping) which might also play a role in mitigating these issues. We propose two different approaches to fix this issue and it will be interesting to investigate these approaches in various applications.\n\nWe have addressed all other minor concerns directly in the revision of the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"tddate": null, "ddate": null, "tmdate": 1513368661437, "tcdate": 1513368661437, "number": 5, "cdate": 1513368661437, "id": "H16UnjZMM", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "forum": "ryQu7f-RZ", "replyto": "Sy5rDQu-z", "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "content": {"title": "Thanks for the feedback", "comment": "Thanks David, for your interest in this paper and helpful comments (and pointers). We have addressed your concerns regarding typos in the latest revision of the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"tddate": null, "ddate": null, "tmdate": 1513368582829, "tcdate": 1513368582829, "number": 4, "cdate": 1513368582829, "id": "ryA-no-zz", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "forum": "ryQu7f-RZ", "replyto": "SJXpTMFbf", "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "content": {"title": "Thanks for the feedback", "comment": "Thanks for your interest in our paper and for your feedback. We believe that beta1 is not an issue for convergence of Adam (although our theoretical analysis assumes a decreasing beta1). For example, in stochastic convex optimization, momentum based methods have been shown to converge even for constant beta1. That said, it is indeed interesting to develop better understanding of the effect of momentum in convergence of these algorithms (especially in the nonconvex setting).\n\nAs the paper shows, for any constant beta2, there exists a counter-example for non-convergence of Adam (both in online as well as stochastic setting, Theorem 2 & Theorem 3). Using a large beta2 can partially mitigate this issue in practice but it is not clear how high beta2 should be and it is indeed an interesting research question. Our paper proposes a couple of approaches (AMSGrad & AdamNC) for addressing these issues. AMSGrad allows us to use a fixed beta2 by changing the structure of the algorithm (and also allows us to use a much slow decaying learning rate than Adagrad). AdamNC looks at an approach where beta2 changes with t, ultimately converging to 1, hopefully allowing us to retain the benefits of Adam but at the same time circumventing its non-convergence.\n\nThe aim of synthetic experiments was to demonstrate the effect of non-convergence. We can modify it to demonstrate similar problem for any constant beta2.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"tddate": null, "ddate": null, "tmdate": 1513368437473, "tcdate": 1513368437473, "number": 3, "cdate": 1513368437473, "id": "HJTujoWGG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "forum": "ryQu7f-RZ", "replyto": "SkjC2Ni-z", "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "content": {"title": "Thanks for the feedback", "comment": "(1) Thanks for the interest in our paper and looking into the analysis carefully. We believe there is a misunderstanding regarding the proof. The third inequality follows from the lower bound v_{t+i-1} \\ge (1-\\beta)\\beta^{i-1}_2 C^2. The fourth inequality actually follows from the upper bound on v_{t+i-1}  (which implicitly uses \\beta^{i'-1}_2 C^2 \\le 1). We revised the paper to provide the detailed derivation, including specifying precise constants that were previously omitted.\n\n(2) Actually, an easy observation from our analysis is that we can bound the regret of AMSGrad by O(G_infty sqrt(T)) as well. This can be easily seen from the proof of Lemma 2 where in the analysis the term \\sum_{t=1}^T |g_{t,i}/\\sqrt{t} can be bounded by O(G_infty sqrt(T)) instead of O(\\sqrt{\\log(T) ||g_{1:T}||_2). Thus, the regret of AMSGrad is upper bounded by minimum of O(G_infty sqrt(T)) and the bound presented in Theorem 4, and thus the worst case dependence on T is \\sqrt{T} rather than \\sqrt{T \\log(T)}. We will make this point in the final version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"tddate": null, "ddate": null, "tmdate": 1513368323734, "tcdate": 1513368323734, "number": 2, "cdate": 1513368323734, "id": "ByhZijZfG", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "forum": "ryQu7f-RZ", "replyto": "Hyl2iJgGG", "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "content": {"title": "Thanks for the review", "comment": "We thank the reviewer for the helpful and supportive feedback. The focus of the paper is to provide a principled understanding for the exponential moving average (EMA) adaptive optimization methods, which are now used as building blocks of many modern deep learning applications. The counter-example for non-convergence we show is very natural and is observed to arise in extremely sparse real-world problems (e.g., pertaining to problems with large output spaces). We provided two general directions to address the convergence issues in these algorithms (by either changing the structure of the algorithm or by gradually increasing beta2 as algorithm proceeds). We have provided preliminary experiments on a few commonly used networks & datasets but we do agree that a thorough empirical study will be very useful and is part of our future plan. \n\n- Fourth inequality on Page 14: We revised the paper to explain it further.\n- We will be happy to elaborate our comment about smoothed gradients in the final version of the paper.\n- We also addressed other minor suggestions.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"tddate": null, "ddate": null, "tmdate": 1512946899536, "tcdate": 1512946899536, "number": 5, "cdate": 1512946899536, "id": "SkjC2Ni-z", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Some questions about Theorem 2 and Corollary 1", "comment": "Dear authors,\n\nIt's a very good paper, but I have some questions as follows:\n\n(1) In the last paragraph on Page 14, it says the fourth inequality is from $\\beta^{i'-1}_2 C^2 \\le 1$, but I couldn't go through from the third inequality to the fourth inequality on Page 14. It seems that you applied the lower bound of $v_{t+i-1}$ (i.e. $v_{t+i-1} \\ge (1-\\beta)\\beta^{i-1}_2 C^2$ which is not desired) instead of its upper bound (which is truly required)? \n\n(2) In Corollary 1, from my understanding, the L2 norm of $g_{1:T,1}$ should be upper bounded by $\\sqrt(T)G_{\\inf}$, so the regret be $O(\\sqrt(T \\logT))$ instead of $O(\\sqrt(T))$ as stated in the remark of Corollary 1. \n\nCorrect me if I'm wrong. Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"tddate": null, "ddate": null, "tmdate": 1512809038490, "tcdate": 1512807866952, "number": 4, "cdate": 1512807866952, "id": "SJXpTMFbf", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Does similar analysis apply to the first moment of gradient? How about using a large beta1 and beta2?", "comment": "Thanks for the inspiring paper. The observations are interesting and important!\n\nIt is easy to capture that exponential moving average might not able to capture the long-term memory of the gradients. \n\nThe paper is mainly focused on the beta2 that involving the averaging of second moment. It makes me wonder whether the beta1 on the averaging of the first moment gradient also suffer the similar problem. \n\nIt seems a direct solution would be using a large beta1 and beta2. (Always keep the maximum of the entire history seems is not the best solution and an average over a recent history might be a better alternative.) \n\nI did not carefully check the detail of the paper. But generally, one would have a similar concern I think. Could you explain the benefits of the proposed algorithm? \n\nThe synthetic experiments seem to use a relatively insufficient large beta2 regarding the large gradient gap, which makes it not able to capture the necessary long-term dependency. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"tddate": null, "ddate": null, "tmdate": 1512744989199, "tcdate": 1512744989199, "number": 3, "cdate": 1512744989199, "id": "rkBQ_QuWf", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "forum": "ryQu7f-RZ", "replyto": "HkhdRaVlG", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "about your comment on RMSProp for some choice of parameters", "comment": "The RMSProp used in Section 4 in Mukkamala and Hein, ICML'17 is not the standard RMSProp but a modification in which the parameter used for computing the geometrical averages of the gradient entries squared changes with time. So there is no contradiction with this paper, that shows counterexamples for the standard algorithm in which that parameter is constant. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"tddate": null, "ddate": null, "tmdate": 1512744770410, "tcdate": 1512744770410, "number": 2, "cdate": 1512744770410, "id": "Sy5rDQu-z", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["~David_Mart\u00ednez1"], "readers": ["everyone"], "writers": ["~David_Mart\u00ednez1"], "content": {"title": "Nice paper", "comment": "Congratulations for this paper, I really enjoyed it. It is a well written paper that contains an exhaustive set of counterexamples. I had also noticed that the proof of Adam was wrong and included it in my Master Thesis  (https://damaru2.github.io/convergence_analysis_hypergradient_descent/dissertation_hypergradients.pdf Section 2.4) and I enjoyed reading through the paper and finding that indeed it was not just that the proof was wrong but that the method does not converge in general, not even in the stochastic case.\n\nI noticed some typos / minor things that seem that need to be fixed:\n\n+ In the penultimate line of page 16 there is this equality v_{t-1} = .... g_i. This g_i should be squared.\n\n+ In the following line, there is another square missing in a C, it should be (1-\\beta_{t-1}_2)(C^2 p + (1-p)) and there is a pair of parenthesis missing in the next term, it should be  (1-\\beta_2^{t-1})((1+\\delta)C-\\delta)\n\n+ The fact that in Theorems 2 and 3 \\beta_2 is allowed to be 1 is confusing, since the method is not well defined if \\beta_2 is 1 (and you don't use an \\epsilon in the denominator. If you use an \\epsilon then with \\beta_1 = 0 the method is equivalent to SGD so it converges for a choice of alpha). In particular, in the proof of theorem 3 \\sqrt{1-\\beta_2}  appears in some denominators and so does \\sqrt{\\beta_2} but there is no comment about what happens when this quantities are 0. There should be a quick comment on this or the \\beta_2 \\leq 1 should be removed from the theorems.\n\nBest wishes\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}, {"tddate": null, "ddate": null, "tmdate": 1511199158165, "tcdate": 1511199158165, "number": 1, "cdate": 1511199158165, "id": "SJRh-9lef", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "forum": "ryQu7f-RZ", "replyto": "Bye7sLhkM", "signatures": ["ICLR.cc/2018/Conference/Paper807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper807/Authors"], "content": {"title": "Re: Hyperparameters for experiments", "comment": "We thank you for your interest in our paper and for pointing out this missing detail. We use a decrease step size of alpha/sqrt(t) (as suggested by our theoretical analysis) for the stochastic optimization experiment. The use of decreasing step size leads to a more stable convergence to the optimal solution (especially in scenarios where the variance is reasonably high). We did not use epsilon in this particular experiment since the gradients are reasonably large (in other words, using a small epsilon like 1e-8 should produce more or less identical results). We will add these details in the next revision of our paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825727409, "id": "ICLR.cc/2018/Conference/-/Paper807/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper807/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper807/Authors|ICLR.cc/2018/Conference/Paper807/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper807/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper807/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper807/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825727409}}}, {"tddate": null, "ddate": null, "tmdate": 1510923041241, "tcdate": 1510923041241, "number": 1, "cdate": 1510923041241, "id": "Bye7sLhkM", "invitation": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "forum": "ryQu7f-RZ", "replyto": "ryQu7f-RZ", "signatures": ["~Mario_Ynocente_Castro1"], "readers": ["everyone"], "writers": ["~Mario_Ynocente_Castro1"], "content": {"title": "Hyperparameters for experiments", "comment": "Hello,\n\nI tried implementing AMSGrad (here: https://colab.research.google.com/notebook#fileId=1xXFAuHM2Ae-OmF5M8Cn9ypGCa_HHBgfG) for the experiment on the stochastic optimization setting and obtain that x_t approaches -1 faster that on the paper but convergence seems less stable, so I was wondering about the specific values for other hyperparameters like the learning rate and epsilon which weren't mentioned, in my case I chose a learning of 1e-3 and an epsilon of 1e-8 which seems to be the standard value on most frameworks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Convergence of Adam and Beyond", "abstract": " Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.", "pdf": "/pdf/36eafed31d51fbcef56081bc57d0af00b556a13c.pdf", "TL;DR": "We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ", "paperhash": "reddi|on_the_convergence_of_adam_and_beyond", "_bibtex": "@inproceedings{\nj.2018on,\ntitle={On the Convergence of Adam and Beyond},\nauthor={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryQu7f-RZ},\n}", "keywords": ["optimization", "deep learning", "adam", "rmsprop"], "authors": ["Sashank J. Reddi", "Satyen Kale", "Sanjiv Kumar"], "authorids": ["sashank@google.com", "satyenkale@google.com", "sanjivk@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791678112, "id": "ICLR.cc/2018/Conference/-/Paper807/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "ryQu7f-RZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper807/Authors", "ICLR.cc/2018/Conference/Paper807/Reviewers", "ICLR.cc/2018/Conference/Paper807/Area_Chair"], "cdate": 1512791678112}}}], "count": 30}