{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028632248, "tcdate": 1490028632248, "number": 1, "id": "rklv_Y6sl", "invitation": "ICLR.cc/2017/workshop/-/paper149/acceptance", "forum": "S1OS0krtg", "replyto": "S1OS0krtg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge distillation using unlabeled mismatched images", "abstract": "Current approaches for Knowledge Distillation (KD)  either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance.  Our examples include use of various datasets for stimulating MNIST and CIFAR teachers. \n", "pdf": "/pdf/1ab94819c657deebb96fee5f9acfd455881629d1.pdf", "TL;DR": "Distilling knowledge from neural networks under the assumption that the training data is not available.", "paperhash": "kulkarni|knowledge_distillation_using_unlabeled_mismatched_images", "conflicts": ["tcs.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Mandar Kulkarni", "Kalpesh Patil", "Shirish Karande"], "authorids": ["mandar.kulkarni3@tcs.com", "kalpeshpatil@iitb.ac.in", "shirish.karande@tcs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028632790, "id": "ICLR.cc/2017/workshop/-/paper149/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1OS0krtg", "replyto": "S1OS0krtg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028632790}}}, {"tddate": null, "tmdate": 1489936964012, "tcdate": 1489936964012, "number": 3, "id": "Hy3SzQnsl", "invitation": "ICLR.cc/2017/workshop/-/paper149/public/comment", "forum": "S1OS0krtg", "replyto": "H13xj5Goe", "signatures": ["~Mandar_Kulkarni1"], "readers": ["everyone"], "writers": ["~Mandar_Kulkarni1"], "content": {"title": "Request for feedback", "comment": "Your feedback was very useful in highlighting the right direction for the work.  Would be grateful if you can comment on the updated version which focuses on how different datasets have varied effectiveness for stimulus. Any additional comments would help guide our future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge distillation using unlabeled mismatched images", "abstract": "Current approaches for Knowledge Distillation (KD)  either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance.  Our examples include use of various datasets for stimulating MNIST and CIFAR teachers. \n", "pdf": "/pdf/1ab94819c657deebb96fee5f9acfd455881629d1.pdf", "TL;DR": "Distilling knowledge from neural networks under the assumption that the training data is not available.", "paperhash": "kulkarni|knowledge_distillation_using_unlabeled_mismatched_images", "conflicts": ["tcs.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Mandar Kulkarni", "Kalpesh Patil", "Shirish Karande"], "authorids": ["mandar.kulkarni3@tcs.com", "kalpeshpatil@iitb.ac.in", "shirish.karande@tcs.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367744137, "tcdate": 1487367744137, "id": "ICLR.cc/2017/workshop/-/paper149/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper149/reviewers"], "reply": {"forum": "S1OS0krtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367744137}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489312570348, "tcdate": 1487367743604, "number": 149, "id": "S1OS0krtg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1OS0krtg", "signatures": ["~Mandar_Kulkarni1"], "readers": ["everyone"], "content": {"title": "Knowledge distillation using unlabeled mismatched images", "abstract": "Current approaches for Knowledge Distillation (KD)  either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance.  Our examples include use of various datasets for stimulating MNIST and CIFAR teachers. \n", "pdf": "/pdf/1ab94819c657deebb96fee5f9acfd455881629d1.pdf", "TL;DR": "Distilling knowledge from neural networks under the assumption that the training data is not available.", "paperhash": "kulkarni|knowledge_distillation_using_unlabeled_mismatched_images", "conflicts": ["tcs.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Mandar Kulkarni", "Kalpesh Patil", "Shirish Karande"], "authorids": ["mandar.kulkarni3@tcs.com", "kalpeshpatil@iitb.ac.in", "shirish.karande@tcs.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489312499610, "tcdate": 1489312499610, "number": 2, "id": "H13xj5Goe", "invitation": "ICLR.cc/2017/workshop/-/paper149/public/comment", "forum": "S1OS0krtg", "replyto": "B1d71Uyog", "signatures": ["~Mandar_Kulkarni1"], "readers": ["everyone"], "writers": ["~Mandar_Kulkarni1"], "content": {"title": "Additional experiments", "comment": "Thank you for the review and comments.\n\nMain contribution of our work is to demonstrate the effectiveness of 'mismatched' unlabelled stimulus for distillation when training data is not available. In addition, we study the effect of complexity of the stimulus on the generalisation performance of the student network. Also, we validate the use of unlabelled stimulus for data augmentation.\n\nFor MNIST teacher, we have already shown results using CIFAR, STL and Shape stimulus. For CIFAR teacher, we now add additional experiments using mismatched stimulus such as MNIST, Shape, SVHN and Texture.\n\nThe details of experiments based on reviewer's questions and corresponding results are summarised below. \n\n1. Given that you don't have access to the training data, what is your stopping criteria when distilling knowledge from the teacher to the student model? \n\nOur objective function is to minimise the cross entropy loss between soft targets of the teacher and the student on the unlabelled stimulus. We terminate the iterations when the cross entropy loss cease to change. To visualise possibility of overfitting (if any), we plotted the cross entropy loss and the test accuracy for too cases: MNIST teacher using 1k CIFAR stimulus and CIFAR teacher using 1k Texture stimulus. The plots are added in the Appendix under the subsection 'Plot of cross entropy loss and test accuracy.'        \nNote that, the test accuracy and the cross entropy loss settles down with more iterations. Even with small training size (1k), overfitting is not observed. This could be because of soft labels used in the optimisation.\n\n2. Don't you think that CIFAR and Tiny ImageNet are both very similar as they both contain natural images? Did you try to leverage MNIST for distilling knowledge in the CIFAR experiment.\n\nAs mentioned in the reply to the first reviewer's comments, we have tried using MNIST stimulus for CIFAR teacher, but it did not work well. \nTo quantify the result, we used various stimulus such as MNIST, Shape ,Street View House Numbers (SVHN), Texture dataset (https://www.robots.ox.ac.uk/~vgg/data/dtd/). The description of the Shape dataset is provided in the the reply to the first reviewer's comments. \nNote that, none of these dataset has any overlap with the CIFAR data.\nAlso, these datasets are of varied 'complexity' (variations). Visually, the order of complexity is MNIST < Shape < SVHN < Texture  < TinyImagenet. The experimental results are added in the revised paper.\nThe result re-iterate the fact that the Complexity of the stimulus plays an important role in better student generalisation.  For the similar result with MNIST teacher, please refer to our first reply.\nWe explored one of the quantification approach for complexity.  The results are given the Appendix under the subsection 'Quantification of complexity'.\n\n3. Unlabelled stimulus for data augmentation\nThough we have considered a scenario of zero training data, in the real application, the mismatched stimulus can also be used for data augmentation in the cases where a small training set is available. We have performed experiments with CIFAR teacher using 5k labelled samples from CIFAR dataset and augmenting it with 5k unlabelled samples from different stimulus. Results of the experiment are tabulated below. \n\n                             Only CIFAR           + Noise       + SVHN      +MNIST       +Shape       +Texture    + TinyImagenet   \n-------------------------------------------------------------------------------------------------------------------------------------------\nCIFAR Test acc            0.5477               0.582             0.586        0.594          0.593          0.632               0.634                   \n\nNote that augmenting CIFAR training set with 5k Texture samples provided significant improvement in the test acc.\nFor the similar results with MNIST teacher, please refer to our first reply.\n\nNovelty:\nAs pointed out by the reviewer, [1] has shown results for CIFAR teacher with the subset of 80M unlabeled Tiny Imagenet dataset. \nIn our work, we use stimulus from 120k TinyImagenet labelled dataset obtained from the following links\n\nhttp://cs231n.stanford.edu/tiny-imagenet-100-A.zip \nhttp://cs231n.stanford.edu/tiny-imagenet-100-B.zip\n\nThough CIFAR and 120k TinyImagenet are both natural image datasets, we observe that there is no significant overlap between their classes. \nAdditionally, we used mismatched datasets such as MNIST, Shape, SVHN and Texture dataset as the stimulus for distillation.\n\nExperimental results demonstrate that complexity of stimulus is the key factor for a good performance. \nAlso, unlabelled stimulus can be effective for data augmentation as well.\nTo best of our knowledge, these trends are not studied in the literature yet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge distillation using unlabeled mismatched images", "abstract": "Current approaches for Knowledge Distillation (KD)  either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance.  Our examples include use of various datasets for stimulating MNIST and CIFAR teachers. \n", "pdf": "/pdf/1ab94819c657deebb96fee5f9acfd455881629d1.pdf", "TL;DR": "Distilling knowledge from neural networks under the assumption that the training data is not available.", "paperhash": "kulkarni|knowledge_distillation_using_unlabeled_mismatched_images", "conflicts": ["tcs.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Mandar Kulkarni", "Kalpesh Patil", "Shirish Karande"], "authorids": ["mandar.kulkarni3@tcs.com", "kalpeshpatil@iitb.ac.in", "shirish.karande@tcs.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367744137, "tcdate": 1487367744137, "id": "ICLR.cc/2017/workshop/-/paper149/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper149/reviewers"], "reply": {"forum": "S1OS0krtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367744137}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489160492386, "tcdate": 1489096480189, "number": 2, "id": "B1d71Uyog", "invitation": "ICLR.cc/2017/workshop/-/paper149/official/review", "forum": "S1OS0krtg", "replyto": "S1OS0krtg", "signatures": ["ICLR.cc/2017/workshop/paper149/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper149/AnonReviewer1"], "content": {"title": "Novelty and Significance?", "rating": "3: Clear rejection", "review": "This paper tackles semi-supervised learning leveraging Knowledge Distillation.  In particular, they investigate the setting where a teacher has been pretrained but the training data are not available anymore. \n\nThe authors show that distilling knowledge using samples from another distribution/dataset achieves reasonable results and outperforms knowledge distillation with noisy inputs.\n\n\n- Questions:\nGiven that you don't have access to the training data, what is your stopping criteria when distilling knowledge from the teacher to the student model?\nDon't you think that CIFAR and Tiny ImageNet are both very similar as they both contain natural images? Did you try to leverage MNIST for distilling knowledge in the CIFAR experiment.\n\n\n- Novelty:\nThe novelty is somewhat incremental.   [1]  already tried to distill knowledge from 80 million tiny image datasets for solving the CIFAR task. Although, they do use the CIFAR training set as well.\n\n- Clarity:\nThe paper clarity could be improved overall (citation format looks strange, some typos, what is W_s in equation 1). However, the experimental part is rather clear.\n\n- Significance:\nI would be nice to add in the paper a compelling example where you have a pretrained model, but don't have access to the data to train a student model.  I don't really see how the approach developed would be useful in real application.\n\nIn summary:\nPros:\n- Rather clear experiment section \n- Clearly shows the benefits of distilling knowledge from 'true data' rather noisy inputs.\n\nCons:\n- low novelty\n- low significance\n\n\n[1]  Do network really need to be deep?, Ba, Jimmy and Caruana, Rich, NIPS 2014", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge distillation using unlabeled mismatched images", "abstract": "Current approaches for Knowledge Distillation (KD)  either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance.  Our examples include use of various datasets for stimulating MNIST and CIFAR teachers. \n", "pdf": "/pdf/1ab94819c657deebb96fee5f9acfd455881629d1.pdf", "TL;DR": "Distilling knowledge from neural networks under the assumption that the training data is not available.", "paperhash": "kulkarni|knowledge_distillation_using_unlabeled_mismatched_images", "conflicts": ["tcs.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Mandar Kulkarni", "Kalpesh Patil", "Shirish Karande"], "authorids": ["mandar.kulkarni3@tcs.com", "kalpeshpatil@iitb.ac.in", "shirish.karande@tcs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489096481036, "id": "ICLR.cc/2017/workshop/-/paper149/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper149/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper149/AnonReviewer2", "ICLR.cc/2017/workshop/paper149/AnonReviewer1"], "reply": {"forum": "S1OS0krtg", "replyto": "S1OS0krtg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper149/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489096481036}}}, {"tddate": null, "nonreaders": null, "tmdate": 1488901950858, "tcdate": 1488139368881, "number": 1, "id": "SyZOEnl9l", "invitation": "ICLR.cc/2017/workshop/-/paper149/official/review", "forum": "S1OS0krtg", "replyto": "S1OS0krtg", "signatures": ["ICLR.cc/2017/workshop/paper149/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper149/AnonReviewer2"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "In this paper, the authors study knowledge distillation in a scenario where the data on which the teacher is trained is no longer available. They propose to use other stimulus (in the form of other datasets or noise) to train the student model, and evaluate the performance on the original dataset. \n\nThe main finding seems to be that using natural images as stimulus is better than using random noise. This is not very surprising. \n\nFrom a theoretical standpoint, the fact that random stimulus allows for knowledge transfer is interesting. But this was already covered by Papamakarios (2015). More practically, I cannot see how this kind of technique would be useful in real situations (i.e. I can envision a scenario in which we no longer have the data on which the teacher was trained, but why would we ever be interested in training a student model to perform well on that data?)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge distillation using unlabeled mismatched images", "abstract": "Current approaches for Knowledge Distillation (KD)  either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance.  Our examples include use of various datasets for stimulating MNIST and CIFAR teachers. \n", "pdf": "/pdf/1ab94819c657deebb96fee5f9acfd455881629d1.pdf", "TL;DR": "Distilling knowledge from neural networks under the assumption that the training data is not available.", "paperhash": "kulkarni|knowledge_distillation_using_unlabeled_mismatched_images", "conflicts": ["tcs.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Mandar Kulkarni", "Kalpesh Patil", "Shirish Karande"], "authorids": ["mandar.kulkarni3@tcs.com", "kalpeshpatil@iitb.ac.in", "shirish.karande@tcs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489096481036, "id": "ICLR.cc/2017/workshop/-/paper149/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper149/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper149/AnonReviewer2", "ICLR.cc/2017/workshop/paper149/AnonReviewer1"], "reply": {"forum": "S1OS0krtg", "replyto": "S1OS0krtg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper149/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489096481036}}}, {"tddate": null, "tmdate": 1488901939447, "tcdate": 1488901939447, "number": 1, "id": "rkoNDI39e", "invitation": "ICLR.cc/2017/workshop/-/paper149/official/comment", "forum": "S1OS0krtg", "replyto": "H17og7s5x", "signatures": ["ICLR.cc/2017/workshop/paper149/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper149/AnonReviewer2"], "content": {"title": "thank you", "comment": "Thank you for the detailed response. I have revised my score upward given the response.\nI would additionally request that a thorough revision be applied to the paper to take care of typos.\n\nThanks!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge distillation using unlabeled mismatched images", "abstract": "Current approaches for Knowledge Distillation (KD)  either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance.  Our examples include use of various datasets for stimulating MNIST and CIFAR teachers. \n", "pdf": "/pdf/1ab94819c657deebb96fee5f9acfd455881629d1.pdf", "TL;DR": "Distilling knowledge from neural networks under the assumption that the training data is not available.", "paperhash": "kulkarni|knowledge_distillation_using_unlabeled_mismatched_images", "conflicts": ["tcs.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Mandar Kulkarni", "Kalpesh Patil", "Shirish Karande"], "authorids": ["mandar.kulkarni3@tcs.com", "kalpeshpatil@iitb.ac.in", "shirish.karande@tcs.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367744124, "tcdate": 1487367744124, "id": "ICLR.cc/2017/workshop/-/paper149/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "S1OS0krtg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper149/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper149/reviewers", "ICLR.cc/2017/workshop/paper149/areachairs"], "cdate": 1487367744124}}}, {"tddate": null, "tmdate": 1488822426620, "tcdate": 1488822426620, "number": 1, "id": "H17og7s5x", "invitation": "ICLR.cc/2017/workshop/-/paper149/public/comment", "forum": "S1OS0krtg", "replyto": "SyZOEnl9l", "signatures": ["~Mandar_Kulkarni1"], "readers": ["everyone"], "writers": ["~Mandar_Kulkarni1"], "content": {"title": "Additional Experiments", "comment": "Thank you for the review and comments.\n\nAs per the comments, we performed additional experiments to investigate the effect of complexity of the stimulus and the use of stimulus for data augmentation. The experiments and results are summarized below.  \n\n1. Effect of complexity of unlabeled stimulus:\n\nThough, we have directly shown results using natural image datasets, we suspect that the 'complexity' of the stimulus plays an important role. To validate this, we used a simple shape dataset as a stimulus with MNIST teacher. The dataset was obtained from the following link.\n\nhttp://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/BabyAIImageAndQuestionDatasets\n\nIt was previously used for demonstrating an effectiveness of Curriculum learning (Bengio et. al, ICML 2009).\nThe dataset consist of 10k examples of simple shape images. Due to small variability, the dataset is more simple than CIFAR or STL.\nThe resultant plot has been added in the Fig. 1 (a)&(b). \nNote that, though the dataset performs better than noise, it performs inferior to CIFAR and STL. \nFurther, we used MNIST data as the stimulus for CIFAR teacher and it did not work well.\nWe believe that the complexity of the stimulus could be a key factor for good generalisation performance. \n  \n2. Use of stimulus for data augmentation\n\nUnavailability of training data is a practical scenario when there are strict rules about data crossing digital boundaries, and the time duration for which data can be stored, further there exist circumstances where even the weights used for network may have to remain private. \nFollowing paper talks about the privacy issues related with the training data.\nhttps://openreview.net/pdf?id=HkwoSDPgg\nWhen one has to distill the knowledge in a network to match various practical constraints: (commodity hardware, large capacity to absorb newer data, or create some for specialist/generalist, and/or privacy), in the absence of training data there is a genuine problem. Indeed there has been work on using random noise, however, the performance of random noise does not come close to the teacher, and for data even slightly more complicated the error floor is hit rather early.\n\nThough, we considered an extreme scenario of availability of no training data, an unlabelled stimulus can be effective for data augmentation as well.\nTo validate this, we performed an experiment where we use 500 samples from MNIST training set along with 3k unlabelled stimulus from various datasets.\nWe perform this experiment with and without using training labels. While using training labels along with the unlabelled stimulus, uniform prior is assumed for the labels of the stimulus.\n \nResults are tabulated below.\n\nUsing training labels         With 500 training samples          MNIST + CIFAR(3k)         MNIST + shape(3k)      MNIST + noise(3k)  \n-------------------------------------------------------------------------------------------------------------------------------------------------------\n          YES                               0.955                                     0.972                             0.973                        0.956\n-------------------------------------------------------------------------------------------------------------------------------------------------------\n          No                                 0.95                                      0.972                               -                              -\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n\nWe note that, augmenting the small training set with unlabelled stimulus (CIFAR and shape) provides a fair improvement in the test accuracy (approx. 2%).  \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge distillation using unlabeled mismatched images", "abstract": "Current approaches for Knowledge Distillation (KD)  either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance.  Our examples include use of various datasets for stimulating MNIST and CIFAR teachers. \n", "pdf": "/pdf/1ab94819c657deebb96fee5f9acfd455881629d1.pdf", "TL;DR": "Distilling knowledge from neural networks under the assumption that the training data is not available.", "paperhash": "kulkarni|knowledge_distillation_using_unlabeled_mismatched_images", "conflicts": ["tcs.com"], "keywords": ["Deep learning", "Transfer Learning"], "authors": ["Mandar Kulkarni", "Kalpesh Patil", "Shirish Karande"], "authorids": ["mandar.kulkarni3@tcs.com", "kalpeshpatil@iitb.ac.in", "shirish.karande@tcs.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367744137, "tcdate": 1487367744137, "id": "ICLR.cc/2017/workshop/-/paper149/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper149/reviewers"], "reply": {"forum": "S1OS0krtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367744137}}}], "count": 8}