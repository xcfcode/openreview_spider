{"notes": [{"id": "H1ebc0VYvH", "original": "SyejytudPH", "number": 1273, "cdate": 1569439369401, "ddate": null, "tcdate": 1569439369401, "tmdate": 1577168267971, "tddate": null, "forum": "H1ebc0VYvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "authors": ["Siyang Wang", "Justin Lazarow", "Kwonjoon Lee", "Zhuowen Tu"], "authorids": ["siw030@ucsd.edu", "jlazarow@ucsd.edu", "kwl042@ucsd.edu", "ztu@ucsd.edu"], "keywords": [], "TL;DR": "LoopGAN extends cycle length in CycleGAN to enable unaligned sequential transformation for more than two time steps.", "abstract": "We tackle the problem of modeling sequential visual phenomena. Given examples of a phenomena that can be divided into discrete time steps, we aim to take an input from any such time and realize this input at all other time steps in the sequence. Furthermore, we aim to do this \\textit{without} ground-truth aligned sequences --- avoiding the difficulties needed for gathering aligned data. This generalizes the unpaired image-to-image problem from generating pairs to generating sequences. We extend cycle consistency to \\textit{loop consistency} and alleviate difficulties associated with learning in the resulting long chains of computation. We show competitive results compared to existing image-to-image techniques when modeling several different data sets including the Earth's seasons and aging of human faces.", "pdf": "/pdf/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "paperhash": "wang|unaligned_imagetosequence_transformation_with_loop_consistency", "original_pdf": "/attachment/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "_bibtex": "@misc{\nwang2020unaligned,\ntitle={Unaligned Image-to-Sequence Transformation with Loop Consistency},\nauthor={Siyang Wang and Justin Lazarow and Kwonjoon Lee and Zhuowen Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ebc0VYvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Z-hWwUcg", "original": null, "number": 1, "cdate": 1576798719061, "ddate": null, "tcdate": 1576798719061, "tmdate": 1576800917479, "tddate": null, "forum": "H1ebc0VYvH", "replyto": "H1ebc0VYvH", "invitation": "ICLR.cc/2020/Conference/Paper1273/-/Decision", "content": {"decision": "Reject", "comment": "The main concern raised by the reviewers is limited experimental work, and there is no rebuttal.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "authors": ["Siyang Wang", "Justin Lazarow", "Kwonjoon Lee", "Zhuowen Tu"], "authorids": ["siw030@ucsd.edu", "jlazarow@ucsd.edu", "kwl042@ucsd.edu", "ztu@ucsd.edu"], "keywords": [], "TL;DR": "LoopGAN extends cycle length in CycleGAN to enable unaligned sequential transformation for more than two time steps.", "abstract": "We tackle the problem of modeling sequential visual phenomena. Given examples of a phenomena that can be divided into discrete time steps, we aim to take an input from any such time and realize this input at all other time steps in the sequence. Furthermore, we aim to do this \\textit{without} ground-truth aligned sequences --- avoiding the difficulties needed for gathering aligned data. This generalizes the unpaired image-to-image problem from generating pairs to generating sequences. We extend cycle consistency to \\textit{loop consistency} and alleviate difficulties associated with learning in the resulting long chains of computation. We show competitive results compared to existing image-to-image techniques when modeling several different data sets including the Earth's seasons and aging of human faces.", "pdf": "/pdf/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "paperhash": "wang|unaligned_imagetosequence_transformation_with_loop_consistency", "original_pdf": "/attachment/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "_bibtex": "@misc{\nwang2020unaligned,\ntitle={Unaligned Image-to-Sequence Transformation with Loop Consistency},\nauthor={Siyang Wang and Justin Lazarow and Kwonjoon Lee and Zhuowen Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ebc0VYvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1ebc0VYvH", "replyto": "H1ebc0VYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727213, "tmdate": 1576800279439, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1273/-/Decision"}}}, {"id": "Skl5hr_6FS", "original": null, "number": 1, "cdate": 1571812786256, "ddate": null, "tcdate": 1571812786256, "tmdate": 1572972490347, "tddate": null, "forum": "H1ebc0VYvH", "replyto": "H1ebc0VYvH", "invitation": "ICLR.cc/2020/Conference/Paper1273/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces the LoopGan, which aims to enforce consistency in a sequential set of images but without aligned pairs.  The prototypical example used in the paper is a network to transform seasonal images, where Summer -> Fall -> Winter -> Spring -> Summer -> etc.\n\nI suggest rejecting the paper.\n\nWhile the idea has some merit, it is an interesting premise to train a recurrent generator to create images in multiple domains, I feel as though the experiments are quite lacking.  The primary evidence the method works is Figure 3, for which we see only 4 examples of the method on two tasks.  Looking at the results they don't look all that great and the proposed method is hard to identify as the best. \n\nThe idea doesn't strike me as particular innovative, feeling like a natural extension of the prior work listed.\n\nTable 1 shows the results of a user study with 20 users.  With only 20 users rating 4 examples of each method we get very little power to resolve the best method, and the papers proposed method is selected in aggregate only 26% of the time out of 6 methods, this is hardly a clear winner.  \n\nFigure 4 showing the imputed ages for the generated images doesn't seem that strong of evidence the proposed method is that much better either.  The histograms are difficult to compare by eye, perhaps the paper should report computed estimates of the kl divergence between the two?  \n\nThe ablation studies of section 6 are out of place.  The paper does not compare against the incremental removal of its' proposed loss, it simply reports results of alternative architectures.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1273/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1273/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "authors": ["Siyang Wang", "Justin Lazarow", "Kwonjoon Lee", "Zhuowen Tu"], "authorids": ["siw030@ucsd.edu", "jlazarow@ucsd.edu", "kwl042@ucsd.edu", "ztu@ucsd.edu"], "keywords": [], "TL;DR": "LoopGAN extends cycle length in CycleGAN to enable unaligned sequential transformation for more than two time steps.", "abstract": "We tackle the problem of modeling sequential visual phenomena. Given examples of a phenomena that can be divided into discrete time steps, we aim to take an input from any such time and realize this input at all other time steps in the sequence. Furthermore, we aim to do this \\textit{without} ground-truth aligned sequences --- avoiding the difficulties needed for gathering aligned data. This generalizes the unpaired image-to-image problem from generating pairs to generating sequences. We extend cycle consistency to \\textit{loop consistency} and alleviate difficulties associated with learning in the resulting long chains of computation. We show competitive results compared to existing image-to-image techniques when modeling several different data sets including the Earth's seasons and aging of human faces.", "pdf": "/pdf/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "paperhash": "wang|unaligned_imagetosequence_transformation_with_loop_consistency", "original_pdf": "/attachment/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "_bibtex": "@misc{\nwang2020unaligned,\ntitle={Unaligned Image-to-Sequence Transformation with Loop Consistency},\nauthor={Siyang Wang and Justin Lazarow and Kwonjoon Lee and Zhuowen Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ebc0VYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ebc0VYvH", "replyto": "H1ebc0VYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1273/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1273/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910817427, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1273/Reviewers"], "noninvitees": [], "tcdate": 1570237739779, "tmdate": 1575910817442, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1273/-/Official_Review"}}}, {"id": "BJlpRXDAtB", "original": null, "number": 2, "cdate": 1571873749089, "ddate": null, "tcdate": 1571873749089, "tmdate": 1572972490299, "tddate": null, "forum": "H1ebc0VYvH", "replyto": "H1ebc0VYvH", "invitation": "ICLR.cc/2020/Conference/Paper1273/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to extend CycleGAN by replacing a bidirectional cycle with a \"loop\" or a sequence of transformations across gradually changing image domains (sequential transformation). This is achieved with a simple modification to CycleGAN training where the training involves additional steps to create a loop. A number of experimental results are shown on specific sequential datasets, comparing to a number of other baselines. On the chosen datasets and tasks, the proposed architecture provides good results over the baselines. However, I would have liked to see ablation studies showing that they achieve the same results as CycleGAN when training on 2 domains. The paper's technical novelty is limited to this simple loop extension and generator sharing. As such, while the experimental results are reasonable, I find this lack of any technical novelty a negative factor. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1273/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1273/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "authors": ["Siyang Wang", "Justin Lazarow", "Kwonjoon Lee", "Zhuowen Tu"], "authorids": ["siw030@ucsd.edu", "jlazarow@ucsd.edu", "kwl042@ucsd.edu", "ztu@ucsd.edu"], "keywords": [], "TL;DR": "LoopGAN extends cycle length in CycleGAN to enable unaligned sequential transformation for more than two time steps.", "abstract": "We tackle the problem of modeling sequential visual phenomena. Given examples of a phenomena that can be divided into discrete time steps, we aim to take an input from any such time and realize this input at all other time steps in the sequence. Furthermore, we aim to do this \\textit{without} ground-truth aligned sequences --- avoiding the difficulties needed for gathering aligned data. This generalizes the unpaired image-to-image problem from generating pairs to generating sequences. We extend cycle consistency to \\textit{loop consistency} and alleviate difficulties associated with learning in the resulting long chains of computation. We show competitive results compared to existing image-to-image techniques when modeling several different data sets including the Earth's seasons and aging of human faces.", "pdf": "/pdf/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "paperhash": "wang|unaligned_imagetosequence_transformation_with_loop_consistency", "original_pdf": "/attachment/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "_bibtex": "@misc{\nwang2020unaligned,\ntitle={Unaligned Image-to-Sequence Transformation with Loop Consistency},\nauthor={Siyang Wang and Justin Lazarow and Kwonjoon Lee and Zhuowen Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ebc0VYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ebc0VYvH", "replyto": "H1ebc0VYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1273/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1273/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910817427, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1273/Reviewers"], "noninvitees": [], "tcdate": 1570237739779, "tmdate": 1575910817442, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1273/-/Official_Review"}}}, {"id": "SyxKW99S5B", "original": null, "number": 3, "cdate": 1572346369508, "ddate": null, "tcdate": 1572346369508, "tmdate": 1572972490252, "tddate": null, "forum": "H1ebc0VYvH", "replyto": "H1ebc0VYvH", "invitation": "ICLR.cc/2020/Conference/Paper1273/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes loop consistency on the base of cycle consistency and alleviate difficulties associated with learning in the resulting long chains of computation. Like CycleGAN, the method proposed in this paper does not require data to be specifically matched.\n\nThis paper still has the following problems\uff1a\n1\uff09For the time series images, the reason for processing with a single generator is not clearly stated. In CycleGAN, the two transformations use two different generators, respectively, and use the cycle consistency loss to force the results to converge. In the examples presented in this paper, such as Face Aging, it isn't a process that can be reversed. This paper does not explain why the use of a single generator has been effective.\n2\uff09To be more convincing, this article needs to be tested on more baselines. The indicators provided in this article are not objective enough.\n3\uff09This article also does not give the training computational complexity and testing time cost of the proposed method.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1273/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1273/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "authors": ["Siyang Wang", "Justin Lazarow", "Kwonjoon Lee", "Zhuowen Tu"], "authorids": ["siw030@ucsd.edu", "jlazarow@ucsd.edu", "kwl042@ucsd.edu", "ztu@ucsd.edu"], "keywords": [], "TL;DR": "LoopGAN extends cycle length in CycleGAN to enable unaligned sequential transformation for more than two time steps.", "abstract": "We tackle the problem of modeling sequential visual phenomena. Given examples of a phenomena that can be divided into discrete time steps, we aim to take an input from any such time and realize this input at all other time steps in the sequence. Furthermore, we aim to do this \\textit{without} ground-truth aligned sequences --- avoiding the difficulties needed for gathering aligned data. This generalizes the unpaired image-to-image problem from generating pairs to generating sequences. We extend cycle consistency to \\textit{loop consistency} and alleviate difficulties associated with learning in the resulting long chains of computation. We show competitive results compared to existing image-to-image techniques when modeling several different data sets including the Earth's seasons and aging of human faces.", "pdf": "/pdf/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "paperhash": "wang|unaligned_imagetosequence_transformation_with_loop_consistency", "original_pdf": "/attachment/fb85101445593cfec1d43633ba3d178869ce4e38.pdf", "_bibtex": "@misc{\nwang2020unaligned,\ntitle={Unaligned Image-to-Sequence Transformation with Loop Consistency},\nauthor={Siyang Wang and Justin Lazarow and Kwonjoon Lee and Zhuowen Tu},\nyear={2020},\nurl={https://openreview.net/forum?id=H1ebc0VYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1ebc0VYvH", "replyto": "H1ebc0VYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1273/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1273/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910817427, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1273/Reviewers"], "noninvitees": [], "tcdate": 1570237739779, "tmdate": 1575910817442, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1273/-/Official_Review"}}}], "count": 5}