{"notes": [{"id": "ryesZANKPB", "original": "r1gvp84dPS", "number": 977, "cdate": 1569439234647, "ddate": null, "tcdate": 1569439234647, "tmdate": 1577168218162, "tddate": null, "forum": "ryesZANKPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "yoFT62NzZl", "original": null, "number": 1, "cdate": 1576798711288, "ddate": null, "tcdate": 1576798711288, "tmdate": 1576800925065, "tddate": null, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Decision", "content": {"decision": "Reject", "comment": "Despite the new ideas in this paper, reviewers feel that it needs to be revised for clarification, and that experimental results are not convincing.  I have down-weighted the criticisms of Reviewer 2 because I agree with the authors' rebuttal.  However, there is still not enough support among the remaining reviews to justify acceptance. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721959, "tmdate": 1576800273150, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper977/-/Decision"}}}, {"id": "Byl8qqOOjS", "original": null, "number": 5, "cdate": 1573583501733, "ddate": null, "tcdate": 1573583501733, "tmdate": 1573583501733, "tddate": null, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment", "content": {"title": "Additional Feedback/Comments", "comment": "Dear reviewers,  we would be very glad for and really appreciate any additional feedback/comments from the reviewers after our rebuttal to further improve the paper during the discussion phase. "}, "signatures": ["ICLR.cc/2020/Conference/Paper977/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryesZANKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper977/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper977/Authors|ICLR.cc/2020/Conference/Paper977/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163189, "tmdate": 1576860559938, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment"}}}, {"id": "SJxRWsz4jS", "original": null, "number": 4, "cdate": 1573296902148, "ddate": null, "tcdate": 1573296902148, "tmdate": 1573296902148, "tddate": null, "forum": "ryesZANKPB", "replyto": "BkgmKoYJ9B", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment", "content": {"title": "Re: Official Blind Review #3 ", "comment": "We would like to thank the reviewer for the suggestions and are happy that the reviewer found the work interesting and possibly impactful for future work in this field. We will do our best to address the remarks about the layout and the related work section during this rebuttal to increase the space for result display.\n\nWe would be grateful for any further comments/criticism that could help us improve the paper during the rebuttal phase.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper977/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryesZANKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper977/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper977/Authors|ICLR.cc/2020/Conference/Paper977/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163189, "tmdate": 1576860559938, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment"}}}, {"id": "HJl2A5MEiS", "original": null, "number": 3, "cdate": 1573296852397, "ddate": null, "tcdate": 1573296852397, "tmdate": 1573296852397, "tddate": null, "forum": "ryesZANKPB", "replyto": "ByeTlXM6Fr", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment", "content": {"title": "Re: Official Blind Review #2 ", "comment": "We respectfully disagree with the reviewer about our work being an extension to MAML. We tried to make this clearer in the related work section and would like to ask the reviewer to review our updated draft. The reviewer correctly points out that we present a bi-level optimization framework, as MAML does. However MAML presents a bi-level optimization framework to optimize model parameters given a known loss function. Our work is concerned with learning the parameters of a loss function that is then used to learn model parameters. Note, that most MAML-variants also optimize model parameters and not loss function parameters. Related work such as [1] takes MAML a step further, and learns an additional \u201cadaptation loss\u201d that is trained to adapt MAML-pretrained parameters more effectively at meta-test time. Note that in this work this adaptation loss is constrained to the architectures of their MAML pre-trained models and absolutely requires the expert demonstrations to be trained.\n\nWe on the other hand present a general-purpose framework to learn loss landscape of regression/classification/RL settings, such that new models/policies, of varying architectures, can be learned from scratch without expert demonstrations (but can utilize them when available).\n\nWe argue that learning loss function parameters is more general than meta-learning model parameter initializations (as MAML does) and have shown evidence for that in our paper:\n\n1) in the supervised learning experiments we learn the loss function from data drawn from one task, and show generalization performance to a number of unseen tasks \n2) Our learned loss functions are not tied to the model architectures, thus we can use our loss functions to learn models of different model-architectures (never seen during meta-train), as we show in our RL experiment section\n\nThus we argue that our approach is rather orthogonal to MAML.\n\nFurther a concern of the reviewer is the lack of comparison to the existing meta-learning algorithms, such as MAML and PEARL. We do not believe that such a comparison would be very meaningful due to the very different nature of MAML/PEARL vs our meta-learning framework. First, in our work, we are targeting to test the hypothesis about capability to learn non-handcrafted losses facilitating optimization. We learn the loss purely from data and train a separate neural network for this. Learning a loss is in contrast to learning a good initialization of the parameters (MAML) or a good embedding space (PEARL) as we are learning a loss function that can succeed from any initialization of an optimizee, which is significantly different from learning a single set of parameters or a suitable representation since loss functions can be used across optimizations for different tasks and policy architectures (which we show in our experiments).\n\nMore concretely, comparisons between MAML style approaches and ML3 would have to be rather artificial for the following reasons:\n\n1) ML3 does not depend on having multiple tasks splits at meta-train time, in fact in the supervised learning experiments we learn the loss function from data drawn from one task, and show generalization performance to a number of unseen tasks (showing the power of learning a loss function instead of model parameter initialization). \n\n2) MAML-style approaches work on the premise of wanting to adapt pre-trained model parameters to a \u201cnew\u201d task in very few (K-shot) steps at meta-test time. We, on the other hand, are not concerned with adapting pre-trained model parameters at meta-test time, we are studying whether our learned loss functions can optimize a model/policy from scratch at meta-test time, and can do so more effectively than a hand-designed loss.\n\n3) Finally, we wanted to test the hypothesis that loss learning can retain knowledge of extra information available during meta-train time, like exploration incentives or demonstrations. This information might not be available during test time because it is expensive to acquire. The results show that the learned loss function, incorporates the extra information and makes it available to the optimizee, during test time, in the form of a shaped loss landscape. \n\nWe would like to ask the reviewer if, in their opinion, it would be beneficial to other readers to move a comparison to MAML to the appendix, or even add a section to the main paper making this distinction very clear to avoid confusion in the future.\n\nWe also updated Figure 5 to show the benefit of adding additional information during meta train time and would ask the reviewer to look at out updated results. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper977/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryesZANKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper977/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper977/Authors|ICLR.cc/2020/Conference/Paper977/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163189, "tmdate": 1576860559938, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment"}}}, {"id": "rkx8L9fNiB", "original": null, "number": 2, "cdate": 1573296718118, "ddate": null, "tcdate": 1573296718118, "tmdate": 1573296718118, "tddate": null, "forum": "ryesZANKPB", "replyto": "Bkel5wCaKr", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment", "content": {"title": "Re: Official Blind Review #1 ", "comment": "We would like to thank reviewer 1 for the important feedback and insightful suggestions. We are glad that our efforts on learning a loss function purely from data in a fully differentiable fashion has been appreciated. In the following we would like to answer the reviewers questions.\n\nQ1: \n\n- We agree that visualizing the loss landscape is a powerful analysis tool, however unfortunately, as the number of parameters in the policy/model increases, the visualization becomes very difficult since we have to plot the loss landscape as a function of the policy/model parameters. Our goal with (Fig. 5, previously Fig.6) was to show the reader how different the loss function can look, even for low-dimensional settings.. \n\n- You are right, so far the usefulness of extra information is shown only implicitly in Figure 5 - eg optimizing the meta-learned loss function should lead to a solution faster/more robustly. We have added a comparison of learning the sine function parameters with meta-losses that were learned with/without the extra information. We also updated the Mountain Car RL experiment to include a comparison to training a policy with the meta loss that was not trained with extra information. We can see that the learned-loss that was given extra information at meta-train time, optimizes policies more effectively at meta-test time, even though the extra information is not provided at meta-test time. We hope this comparison clarifies the usefulness of extra information during meta train time.  \n\n-Generally speaking, adding extra information during meta-train time is helpful when the extra information is expensive to get, for example intermediate goals or demonstrations for a robotic task. Being able to learn a loss function, that incorporates these demonstrations and can be used for other tasks, where it is not necessary to collect these expensive demonstrations, is for example a benefit of adding additional information during meta train time. \n\nQ2:  we believe that meta learning a loss function is a novel and important research direction to explore within our community. Our main contribution is the gradient-based framework for learning general-purpose loss functions. To summarize advantages (contributions) of our framework: \ni) learned loss allows leveraging of extra information during meta-train time that helps shape the loss function to learn models/policy more effectively at meta-test time. \nii) it allows faster (more sample efficient) learning of new tasks during meta-test time (in both supervised and RL settings) (as compared to using hand-designed losses)\niii) it allows change of the architecture without the need of retraining meta-optimized part (i.e. meta-loss) that can not be achieved with other approaches, such as MAML and RL^2.\n\nIn order to make the contribution clearer we updated the related work section and we would like to ask the reviewer to look at our updated paper draft. \n\nQ3: We apologize if the explanation of our method was unclear to the reviewer and for the confusion this might have caused. Our aim in section 3 was to describe the different possibilities for implementation of our ML3 framework but given the limited space (max 8 pages) we had to restrict ourselves. In our updated draft we tried to make the explanation more crisp and clear, we would like to ask the reviewer to have a look at the updated section 3 and hope that now it is clearer how the parameters are updated at meta level.  \n\nQ4: Improving the convergence speed of training a policy in a reinforcement learning task (model free and model based ) is a desirable effect from different perspectives. First, it reduces the sample complexity, e.g the real rollouts in the environment which makes it cheaper to train from a computational perspective. It is especially beneficial from the hardware perspective, i.e. when the experiments are done on a real robot. Second, it allows achieving the desirable behaviour faster in terms of wall time, due to fewer rollouts needed.\n\nQ5: On one hand, we understand the desire of the reviewer to introduce the aforementioned constraints (properties). On the other hand, we respectfully disagree that the mentioned properties are mandatory for the loss functions in general. For that reason we don\u2019t explicitly enforce them. We would like to point out that some of these properties can be captured by an appropriate choice of the loss model. For example, output >= 0 could be captured by a proper choice of an output activation function. \n\nQ6: Due to the limited space we have decided to move the experimental details into the appendix. We will however make every effort during the rebuttal to include more details and explanations in the text after the reviewers remark on clarity. We would like to ask the reviewer to take a look at our updated paper draft.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper977/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryesZANKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper977/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper977/Authors|ICLR.cc/2020/Conference/Paper977/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163189, "tmdate": 1576860559938, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment"}}}, {"id": "ByeTlXM6Fr", "original": null, "number": 1, "cdate": 1571787509238, "ddate": null, "tcdate": 1571787509238, "tmdate": 1572972528126, "tddate": null, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a new meta-learning method (ML3) that meta-learns a loss function that is able to generalize across tasks. Building upon bi-level optimization framework as in MAML, instead of using a task-specific loss function in the inner loop, the authors compute adapted parameters of the model using a parametrized loss network and learn the loss network via backpropagation. Experiments are conducted on supervised sinusoid regression and binary digit classification as well as on model-based and model-free RL benchmarks.\n\nOverall, this paper is an extension to the gradient-based meta-learning algorithms such as MAML. While the idea is natural, there is a prior work [1] that has investigated the effectiveness of learned loss in gradient-based meta-learning, which seems pretty similar to this paper. I wonder how this method could be compared to [1] in various domains. \n\nBesides, I wonder how important the extra information added during the meta-training time is and the authors should present comparison to ML3 without the extra information.\n\nMoreover, I believe comparing ML3 to more recent meta-learning algorithms such as various MAML variants (e.g. MAML++), PEARL, LEO, etc. would be important to show the effectiveness of ML3. Right now, the method is only compared to ML3 with task loss, which seems not very conclusive.\n\n[1] Yu, T., Finn, C., Xie, A., Dasari, S., Zhang, T., Abbeel, P., & Levine, S. (2018). One-shot imitation from observing humans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557."}, "signatures": ["ICLR.cc/2020/Conference/Paper977/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper977/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574993520881, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper977/Reviewers"], "noninvitees": [], "tcdate": 1570237744190, "tmdate": 1574993520894, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper977/-/Official_Review"}}}, {"id": "Bkel5wCaKr", "original": null, "number": 2, "cdate": 1571837831761, "ddate": null, "tcdate": 1571837831761, "tmdate": 1572972528084, "tddate": null, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a mete-learning approach to learn a loss function from old tasks which can generalize well to new tasks. The benefits of the proposed approaches are 1) data-driven a loss function and 2) allowing the usage of extra side-information to design the loss function.\n\nOverall, the presentation in this paper is hard for me to understand technical details and see the difference with existing methods. Please see the questions below. I am glad to discuss problems with the authors' reply during the rebuttal.\n\nQ1. \"M\u03c6(y; f\u03b8(x)) that predicts the loss gave the ground truth target y and the predicted target f\u03b8(x)\", \"the purpose...loss function, or a meta-loss, which generalizes across multiple training contexts or tasks\".\n- It is better for the authors to visualize all y v.s. f_{\\theta}(x) for all experiments. They have done this only for \"Section 4.2.1 SHAPING LOSS FOR REGRESSION\". It is better to do this for all the experiments. In this way, we can see why meta-loss can better.\n- Besides, it is also better to show some example samples where the learning loss is significantly different from the human-designed one. This helps the reader better understand why the meta-loss can better.\n- Finally, the authors claim the extra-information used in the meta-training is helpful. How can we see this point? There is not a step-by-step ablation study on this point.\n\nQ2. Except for problem setup, i.e., learning a loss function, what are novelties in using meta-learning techniques?\n\nQ3. The authors present three usages of the proposed framework in Section 3. Could the authors describe one in detail and then briefly mention the other two usages instead of writing them with the same importance? In this way, readers can understand materials and novelties better. \n- For example, I do not understand how exactly gradients are updated on meta-level. The description in Section 3.1. is too brief. \n\nQ4. Why the convergence speed of the meta-learner is important? e.g., Figure 4(b-c).\n\nQ5. We have some basic restrictions for \"loss function\", i.e., loss(x, y) >= 0 for any x, y; loss(x, x) = 0. How such basic requirements are ensured by the learned meta-loss?\n\nQ6. Could the authors add more explanation in the experiments and motivation in the main text? Currently, the authors just describe what they have done in the proposed method and what they have observed in experiments, just a list of facts (see Q1)."}, "signatures": ["ICLR.cc/2020/Conference/Paper977/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper977/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574993520881, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper977/Reviewers"], "noninvitees": [], "tcdate": 1570237744190, "tmdate": 1574993520894, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper977/-/Official_Review"}}}, {"id": "BkgmKoYJ9B", "original": null, "number": 3, "cdate": 1571949434968, "ddate": null, "tcdate": 1571949434968, "tmdate": 1572972528041, "tddate": null, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper present a meta-learning method for learning parametric loss functions that can generalise across different tasks and model architectures, which is done by encode learning strategies into an adaptive high-dimensional loss.\n\nI think one interesting result is the utilisation of extra information that helps shape the loss landscapes at meta-train time, where as the authors said the extra information can take on various forms, such as exploratory signals or expert demonstrations for RL tasks. This enables a more efficient ways to optimise the original task loss.\n\nPotential improvements:\n\n(1) paper layout, to be honest, I'm not sure if Figure 1 is really needed\n\n(2) related work section seems very long, would be good if it can be shorten and use the extra space for results display\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper977/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper977/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574993520881, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper977/Reviewers"], "noninvitees": [], "tcdate": 1570237744190, "tmdate": 1574993520894, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper977/-/Official_Review"}}}, {"id": "r1xeAs9XcB", "original": null, "number": 1, "cdate": 1572215752135, "ddate": null, "tcdate": 1572215752135, "tmdate": 1572215752135, "tddate": null, "forum": "ryesZANKPB", "replyto": "SkgSqBllqS", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment", "content": {"title": "Re: Maybe a minor typo?", "comment": "Hi, in the case of one inner optimisation step the outer and the inner loop are the same. You could replace 'inner' with 'outer' in this case.  In general: the parameters of the meta-network are updated after each outer iterations, but the gradients of the task loss can be computed after each inner iteration and saved until the update. I hope this  helps!"}, "signatures": ["ICLR.cc/2020/Conference/Paper977/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryesZANKPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper977/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper977/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper977/Authors|ICLR.cc/2020/Conference/Paper977/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163189, "tmdate": 1576860559938, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper977/-/Official_Comment"}}}, {"id": "SkgSqBllqS", "original": null, "number": 1, "cdate": 1571976589224, "ddate": null, "tcdate": 1571976589224, "tmdate": 1571976589224, "tddate": null, "forum": "ryesZANKPB", "replyto": "ryesZANKPB", "invitation": "ICLR.cc/2020/Conference/Paper977/-/Public_Comment", "content": {"title": "Maybe a minor typo?", "comment": "I wonder if the last word in the fourth line below equation 1 is \"outer\" instead of \"inner\"?\n\nThank you!"}, "signatures": ["~Wuyang_Chen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Wuyang_Chen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sbechtle@tuebingen.mpg.de", "molchano@usc.edu", "ychebota@usc.edu", "egrefen@gmail.com", "ludovic.righetti@nyu.edu", "gaurav@usc.edu", "fmeier@fb.com"], "title": "Meta Learning via Learned Loss", "authors": ["Sarah Bechtle", "Artem Molchanov", "Yevgen Chebotar", "Edward Grefenstette", "Ludovic Righetti", "Gaurav Sukhatme", "Franziska Meier"], "pdf": "/pdf/58570956a3b37f43265e4fe9910a96f6e83c207f.pdf", "TL;DR": "Meta learning a loss function for optimisation for supervised learning tasks as well as reinforcement learning", "abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.", "keywords": ["Meta Learning", "Reinforcement Learning", "Loss Learning"], "paperhash": "bechtle|meta_learning_via_learned_loss", "original_pdf": "/attachment/c9db7760034dd7fc254751904c0bbc10ecaeea8c.pdf", "_bibtex": "@misc{\nbechtle2020meta,\ntitle={Meta Learning via Learned Loss},\nauthor={Sarah Bechtle and Artem Molchanov and Yevgen Chebotar and Edward Grefenstette and Ludovic Righetti and Gaurav Sukhatme and Franziska Meier},\nyear={2020},\nurl={https://openreview.net/forum?id=ryesZANKPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryesZANKPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504201400, "tmdate": 1576860592961, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper977/Authors", "ICLR.cc/2020/Conference/Paper977/Reviewers", "ICLR.cc/2020/Conference/Paper977/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper977/-/Public_Comment"}}}], "count": 11}