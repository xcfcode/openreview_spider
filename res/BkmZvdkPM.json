{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124452067, "tcdate": 1518466810626, "number": 246, "cdate": 1518466810626, "id": "BkmZvdkPM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BkmZvdkPM", "signatures": ["~Dzmitry_Bahdanau1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Jointly Learning \"What\" and \"How\" from Instructions and Goal-States", "abstract": "Training agents to follow instructions requires some way of rewarding them for behavior which accomplishes the intent of the instruction. For non-trivial instructions, which may be either underspecified or contain some ambiguity, it can be difficult or impossible to specify a reward function or obtain relatable expert trajectories for the agent to imitate. For these scenarios, we introduce a method which requires only pairs on instructions and examples of positive goal states, from which we can jointly learn a model of the instruction-conditional reward and a policy which executes instructions. Two sets of experiments in a gridworld compare the effectiveness of our method to that of RL when a reward function can be specified, and the application of our method when no reward function is defined. We furthermore evaluate the generalization of our approach to unseen instructions, and to scenarios where environment dynamics change outside of training, requiring fine-tuning of the policy ``in the wild''.", "paperhash": "bahdanau|jointly_learning_what_and_how_from_instructions_and_goalstates", "keywords": ["reinforcement learning", "imitation learning", "language grounding", "instruction following"], "_bibtex": "@misc{\n  bahdanau2018jointly,\n  title={Jointly Learning \"What\" and \"How\" from Instructions and Goal-States},\n  author={Dzmitry Bahdanau and Felix Hill and Jan Leike and Edward Hughes and Pushmeet Kohli and Edward Grefenstette},\n  year={2018},\n  url={https://openreview.net/forum?id=BkmZvdkPM}\n}", "authorids": ["dimabgv@gmail.com", "felixhill@google.com", "leike@google.com", "edwardhughes@google.com", "pushmeet@google.com", "etg@google.com"], "authors": ["Dzmitry Bahdanau", "Felix Hill", "Jan Leike", "Edward Hughes", "Pushmeet Kohli", "Edward Grefenstette"], "TL;DR": "We proposed an approach to train an agent to follow instructions using only examples of the respective goal-states.", "pdf": "/pdf/7cbb6cfcf2b089ac16f05c30397380e0e7cfcaa4.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582851069, "tcdate": 1520576455540, "number": 1, "cdate": 1520576455540, "id": "By1CwiytG", "invitation": "ICLR.cc/2018/Workshop/-/Paper246/Official_Review", "forum": "BkmZvdkPM", "replyto": "BkmZvdkPM", "signatures": ["ICLR.cc/2018/Workshop/Paper246/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper246/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a framework (AGILE) for jointly learning a model for the reward (using a discriminator) and the policy. The task involves an agent performing operations in a simulated grid world, given an initial state and some instructions. Experiments show that AGILE out-performs vanilla RL, but falls short of the RL-RP method (RL with auxiliary reward prediction). \n\nThis paper is relatively well-presented. Although the task is simple and the results are not very strong, the model seems interesting, and I think this is good contribution for a Workshop paper.\n\nMinor comment: \n1) It would be nice to move Figure 1 earlier so the readers would more clear about the task or the input/output space.\n2) The paper explains the model in a very high-level way. It would be helpful to provide a bit more detail (e.g. the architecture of the discriminator).\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jointly Learning \"What\" and \"How\" from Instructions and Goal-States", "abstract": "Training agents to follow instructions requires some way of rewarding them for behavior which accomplishes the intent of the instruction. For non-trivial instructions, which may be either underspecified or contain some ambiguity, it can be difficult or impossible to specify a reward function or obtain relatable expert trajectories for the agent to imitate. For these scenarios, we introduce a method which requires only pairs on instructions and examples of positive goal states, from which we can jointly learn a model of the instruction-conditional reward and a policy which executes instructions. Two sets of experiments in a gridworld compare the effectiveness of our method to that of RL when a reward function can be specified, and the application of our method when no reward function is defined. We furthermore evaluate the generalization of our approach to unseen instructions, and to scenarios where environment dynamics change outside of training, requiring fine-tuning of the policy ``in the wild''.", "paperhash": "bahdanau|jointly_learning_what_and_how_from_instructions_and_goalstates", "keywords": ["reinforcement learning", "imitation learning", "language grounding", "instruction following"], "_bibtex": "@misc{\n  bahdanau2018jointly,\n  title={Jointly Learning \"What\" and \"How\" from Instructions and Goal-States},\n  author={Dzmitry Bahdanau and Felix Hill and Jan Leike and Edward Hughes and Pushmeet Kohli and Edward Grefenstette},\n  year={2018},\n  url={https://openreview.net/forum?id=BkmZvdkPM}\n}", "authorids": ["dimabgv@gmail.com", "felixhill@google.com", "leike@google.com", "edwardhughes@google.com", "pushmeet@google.com", "etg@google.com"], "authors": ["Dzmitry Bahdanau", "Felix Hill", "Jan Leike", "Edward Hughes", "Pushmeet Kohli", "Edward Grefenstette"], "TL;DR": "We proposed an approach to train an agent to follow instructions using only examples of the respective goal-states.", "pdf": "/pdf/7cbb6cfcf2b089ac16f05c30397380e0e7cfcaa4.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582850878, "id": "ICLR.cc/2018/Workshop/-/Paper246/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper246/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper246/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper246/AnonReviewer1"], "reply": {"forum": "BkmZvdkPM", "replyto": "BkmZvdkPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper246/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper246/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582850878}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582590572, "tcdate": 1521215928553, "number": 2, "cdate": 1521215928553, "id": "rkgpKDKKG", "invitation": "ICLR.cc/2018/Workshop/-/Paper246/Official_Review", "forum": "BkmZvdkPM", "replyto": "BkmZvdkPM", "signatures": ["ICLR.cc/2018/Workshop/Paper246/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper246/AnonReviewer1"], "content": {"title": "The paper considers the problem of programming agents by providing statements in the form of relational statements. The proposed approach seems interesting, but a bit too basic and the experiments are too preliminary", "rating": "7: Good paper, accept", "review": "Instead of providing a reward function or full demonstrations, this paper consider the cases where an agent's task is provided as a command describing a goal state in terms of spatial relations. Training is performed by providing couples (c,s), where c is a command and s is a desired goal state that corresponds to the command. The proposed algorithm, AGILE, trains a discriminator network to distinguish what should be a valid goal state from the reste. It also learns a policy that maximizes the frequency of visiting the goal states.\nI think this work is overall based on a good idea. But I would be surprised if such an idea has not been extensively explored in the literature. The authors should clearly state what makes their work original. The experimental setup is too simple (5x5 grid) and the results are minimal. \nAre the policy and the discriminator trained iteratively? It seems like you can just train the discriminator  to learn where to go, and then train the policy using the reward defined by the output of the discriminator. In that case, this problem is not different from standard binary classification and RL combined. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jointly Learning \"What\" and \"How\" from Instructions and Goal-States", "abstract": "Training agents to follow instructions requires some way of rewarding them for behavior which accomplishes the intent of the instruction. For non-trivial instructions, which may be either underspecified or contain some ambiguity, it can be difficult or impossible to specify a reward function or obtain relatable expert trajectories for the agent to imitate. For these scenarios, we introduce a method which requires only pairs on instructions and examples of positive goal states, from which we can jointly learn a model of the instruction-conditional reward and a policy which executes instructions. Two sets of experiments in a gridworld compare the effectiveness of our method to that of RL when a reward function can be specified, and the application of our method when no reward function is defined. We furthermore evaluate the generalization of our approach to unseen instructions, and to scenarios where environment dynamics change outside of training, requiring fine-tuning of the policy ``in the wild''.", "paperhash": "bahdanau|jointly_learning_what_and_how_from_instructions_and_goalstates", "keywords": ["reinforcement learning", "imitation learning", "language grounding", "instruction following"], "_bibtex": "@misc{\n  bahdanau2018jointly,\n  title={Jointly Learning \"What\" and \"How\" from Instructions and Goal-States},\n  author={Dzmitry Bahdanau and Felix Hill and Jan Leike and Edward Hughes and Pushmeet Kohli and Edward Grefenstette},\n  year={2018},\n  url={https://openreview.net/forum?id=BkmZvdkPM}\n}", "authorids": ["dimabgv@gmail.com", "felixhill@google.com", "leike@google.com", "edwardhughes@google.com", "pushmeet@google.com", "etg@google.com"], "authors": ["Dzmitry Bahdanau", "Felix Hill", "Jan Leike", "Edward Hughes", "Pushmeet Kohli", "Edward Grefenstette"], "TL;DR": "We proposed an approach to train an agent to follow instructions using only examples of the respective goal-states.", "pdf": "/pdf/7cbb6cfcf2b089ac16f05c30397380e0e7cfcaa4.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582850878, "id": "ICLR.cc/2018/Workshop/-/Paper246/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper246/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper246/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper246/AnonReviewer1"], "reply": {"forum": "BkmZvdkPM", "replyto": "BkmZvdkPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper246/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper246/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582850878}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573571474, "tcdate": 1521573571474, "number": 125, "cdate": 1521573571117, "id": "rys6ARAFz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BkmZvdkPM", "replyto": "BkmZvdkPM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Jointly Learning \"What\" and \"How\" from Instructions and Goal-States", "abstract": "Training agents to follow instructions requires some way of rewarding them for behavior which accomplishes the intent of the instruction. For non-trivial instructions, which may be either underspecified or contain some ambiguity, it can be difficult or impossible to specify a reward function or obtain relatable expert trajectories for the agent to imitate. For these scenarios, we introduce a method which requires only pairs on instructions and examples of positive goal states, from which we can jointly learn a model of the instruction-conditional reward and a policy which executes instructions. Two sets of experiments in a gridworld compare the effectiveness of our method to that of RL when a reward function can be specified, and the application of our method when no reward function is defined. We furthermore evaluate the generalization of our approach to unseen instructions, and to scenarios where environment dynamics change outside of training, requiring fine-tuning of the policy ``in the wild''.", "paperhash": "bahdanau|jointly_learning_what_and_how_from_instructions_and_goalstates", "keywords": ["reinforcement learning", "imitation learning", "language grounding", "instruction following"], "_bibtex": "@misc{\n  bahdanau2018jointly,\n  title={Jointly Learning \"What\" and \"How\" from Instructions and Goal-States},\n  author={Dzmitry Bahdanau and Felix Hill and Jan Leike and Edward Hughes and Pushmeet Kohli and Edward Grefenstette},\n  year={2018},\n  url={https://openreview.net/forum?id=BkmZvdkPM}\n}", "authorids": ["dimabgv@gmail.com", "felixhill@google.com", "leike@google.com", "edwardhughes@google.com", "pushmeet@google.com", "etg@google.com"], "authors": ["Dzmitry Bahdanau", "Felix Hill", "Jan Leike", "Edward Hughes", "Pushmeet Kohli", "Edward Grefenstette"], "TL;DR": "We proposed an approach to train an agent to follow instructions using only examples of the respective goal-states.", "pdf": "/pdf/7cbb6cfcf2b089ac16f05c30397380e0e7cfcaa4.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}