{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392897900000, "tcdate": 1392897900000, "number": 1, "id": "YYMOsvf5O_svq", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "YHGzHsybzQU0l", "replyto": "hcqO1f7vjQh68", "signatures": ["Anjan Nepal"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Anonymous 198e:\r\nThank you for your comments.\r\nWe have added the citations in the paper that are relavant and also tried to clarify what is borrowed from the previous work and what is novel. We try to answer other comments below.\r\n\r\n\r\nThe terms corresponding to S_1 are missing from P({S_t}|{Y_t}) and Q({S_t}|{Y_t}).\r\n\r\n- We did this for simplicity. This is stated in the paper: 'initial distribution is defined the same way except that we drop the indicators for the previous time step and use parameters \theta_mk rather than \theta_mjk'.\r\n\r\n\r\nIn equations (3), (12), and (13), the denominator contains S^m_{t-1,j} instead of S^m_{t,k'}.\r\n\r\n- We believe that the equations are correct.\r\n\r\n\r\nIt would be cleaner to parameterize the multinomial distributions directly in terms probabilities (as was done by Ghahramani and Jordan) instead of log-unnormalized-probabilities.\r\n\r\n- In the derivation, we did not assume that these distributions in the original and the variational model are equal but the minimization of the KL-divergence shows that they are equal. However, your suggestion can make the equations look cleaner. We will update the paper soon to reflect the changes.\r\n\r\n\r\nThe parameterization of the variational posterior in equation 13 is odd. Why are the transition terms explicitly normalized but the observation terms are not?\r\n\r\n- Transition parameters are explicitly normalized to make them similar to the parameters in the original model. If we use the distributions directly in terms of probabilities (according to the previous comment), then this will also simplify. For the observation variational parameters, we did not include the normalization in the equations to make the calculations easier. First we find the phi parameters according to the equation 19. But to use them in the forward-backward, they need to provide proper probability distribution. Hence, we locally normalize them (per layer and timestep). i.e. divide by sum_k ( exp (phi_mtk) ) for all m and t.\r\n\r\n\r\nSince distributed but not context-dependent representations are not included in the evaluation, it is not possible to disentangle the effects of those two factors on the performance of the FHMM representations. \r\n\r\n- We have updated the paper with the results using the 50-dimensional word-embeddings trained using neural network based on Collobert and Weston (2008) on our data.\r\n \r\n\r\nRepresentations learned by neural language models as well as the 'non-temporal FHMM' (effectively a distributed mixture model) would be the natural baselines for this. \r\n- We agree that the non-temporal FHMM would be a nice comparison, and are working to try to include it in a final version.  We ask that in the mean time, the reviewer look at the new results we have added for the neural language model, which the FHMM outperforms.  Also note that Brown clusters (results included from our original submission) provide another form of 'non-temporal' clustering, in that once the clusters are trained, they are functions of word type only, and not the local context of a token."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "decision": "submitted, no decision", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "pdf": "https://arxiv.org/abs/1312.6168", "paperhash": "nepal|factorial_hidden_markov_models_for_learning_representations_of_natural_language", "keywords": [], "conflicts": [], "authors": ["Anjan Nepal", "Alexander Yates"], "authorids": ["rockanjan@gmail.com", "ayates@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392794640000, "tcdate": 1392794640000, "number": 1, "id": "3bUlnTHBjonIL", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "YHGzHsybzQU0l", "replyto": "Uy5xO0qPmo2CA", "signatures": ["Anjan Nepal"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments. We would like to make a minor correction to your statement that the word representations are dependent on both the left and right contexts in the sentence, not just the left-context.\r\nWe would like to answer few of your comments below:\r\n\r\n* before seeing domain adaptation performance results, I would have liked to know whether the FHMM representation leads to competitive performance when testing on similar data.\r\n- We performed an experiment with the PoS tagging experiment with an in-domain setting (WSJ, trained on 38K sentences and tested on 2K sentences). Compared to the baseline, use of 5 layered 10 states FHMM reduced the error on all words from 3.27 to 2.83 and on OOV words from 4.86 to 4.32. We did not include these results in the paper because we only focused on the use of FHMMs as representation model for domain adaptation. We wanted to see if FHMM can give representations which are common to both domains and hence helps the supervised classifier in improving the accuracy on the target domain. From the results, we see that is the case.\r\n\r\n* Also the domain adaptation performance is not compared with the performance afforded by some of the readily available word embeddings \r\n- We have updated the paper adding the results using 50-dimensional word embeddings trained using neural network based on Collobert and Weston (2008) on our data."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "decision": "submitted, no decision", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "pdf": "https://arxiv.org/abs/1312.6168", "paperhash": "nepal|factorial_hidden_markov_models_for_learning_representations_of_natural_language", "keywords": [], "conflicts": [], "authors": ["Anjan Nepal", "Alexander Yates"], "authorids": ["rockanjan@gmail.com", "ayates@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392742200000, "tcdate": 1392742200000, "number": 1, "id": "eXJaY4DxhuXnF", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "YHGzHsybzQU0l", "replyto": "rrPhwCd1KswBT", "signatures": ["Anjan Nepal"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments. We try to answer them below:\r\n1, 2 and 3: We have updated the paper by taking your suggestions in consideration. In short, 1)we have properly cited the original FHMM model where necessary, 2) They are found by setting the derivative of the objective function wrt phi to zero which has a closed form solution and 3) We updated the results by using the prefixes of size 4, 6, 10 and 20 in addition to the whole path of the brown clusters.\r\n4. Although it is an interesting idea, we have not used it in our current results.\r\n5. We have updated the paper with the results from using 50-dimensional word embedding trained on our data using Collobert and Weston (2008).\r\n6 and extra comment: Thank you for the pointers on the related work. We have added them in our previous work section."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "decision": "submitted, no decision", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "pdf": "https://arxiv.org/abs/1312.6168", "paperhash": "nepal|factorial_hidden_markov_models_for_learning_representations_of_natural_language", "keywords": [], "conflicts": [], "authors": ["Anjan Nepal", "Alexander Yates"], "authorids": ["rockanjan@gmail.com", "ayates@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391806560000, "tcdate": 1391806560000, "number": 4, "id": "hcqO1f7vjQh68", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YHGzHsybzQU0l", "replyto": "YHGzHsybzQU0l", "signatures": ["anonymous reviewer 198e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Factorial Hidden Markov Models for Learning Representations of Natural Language", "review": "The authors propose using a factorial hidden Markov model (FHMM), which is an HMM with multiple latent Markov chains cooperating to produce observations, to induce context-dependent word representations from text. They derive a variational EM algorithm for training these models efficiently and use a structured variational posterior consisting of independent Markov chains to capture the sequential structure of the exact posterior. Given a sequence of words, the authors obtain their representations by computing the variational posterior for the sequence and representing each word with the resulting distribution over the states of the latent variables associated with the timestep that generated the word.\r\n\r\nAlthough HMMs have been used to induce word representations before, using factorial HMMs for this task is novel. The variational algorithm for training FHMMs on multinomial observations, such as words, derived in the paper appears to be new as well.\r\n\r\nThe idea of using FHMMs to obtain context-dependent word representations is a sensible one. Unfortunately, the paper is not particularly well written and the experimental evaluation is not sufficiently convincing. The variational EM algorithm derived by the authors is a relatively minor modification of the algorithm of Ghahramani and Jordan (1997), something the paper does not state clearly enough. The form of the variational posterior comes from the same paper. The bound in Eq. 15 has already been used in a very similar setting by Blei and Lafferty (2007). The presentation of the derivation is unclear and contains a number of small mistakes. For example:\r\n-The terms corresponding to S_1 are missing from P({S_t}|{Y_t}) and Q({S_t}|{Y_t}).\r\n-In equations (3), (12), and (13), the denominator contains S^m_{t-1,j} instead of S^m_{t,k'}.\r\n-It would be cleaner to parameterize the multinomial distributions directly in terms probabilities (as was done by Ghahramani and Jordan) instead of log-unnormalized-probabilities.\r\n-The parameterization of the variational posterior in equation 13 is odd. Why are the transition terms explicitly normalized but the observation terms are not?\r\n\r\nThe experimental section does have some interesting results but suffers from a non-standard experimental protocol and absence of some obvious baselines. The representations produced by FHMM are interesting because they are distributed and context-dependent. Since distributed but not context-dependent representations are not included in the evaluation, it is not possible to disentangle the effects of those two factors on the performance of the FHMM representations. Representations learned by neural language models as well as the 'non-temporal FHMM' (effectively a distributed mixture model) would be the natural baselines for this. Until this issue is addressed, it will remain unclear how much context dependence contributes and whether using FHMMs instead of simpler models is worth the effort. Finally, it is unfortunate that the authors chose not to use exactly the same training data as in [25] and [6], making it impossible to compare their results to those in the literature."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "decision": "submitted, no decision", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "pdf": "https://arxiv.org/abs/1312.6168", "paperhash": "nepal|factorial_hidden_markov_models_for_learning_representations_of_natural_language", "keywords": [], "conflicts": [], "authors": ["Anjan Nepal", "Alexander Yates"], "authorids": ["rockanjan@gmail.com", "ayates@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391785140000, "tcdate": 1391785140000, "number": 3, "id": "rrPhwCd1KswBT", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YHGzHsybzQU0l", "replyto": "YHGzHsybzQU0l", "signatures": ["anonymous reviewer 9228"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "One extra comment: the approach bears some similarity to the model of Grave, Obozinski and Grave (CoNLL 2013), where they associate latent variables with nodes in a syntactic dependency tree and draw words conditioned on the variables. As in this paper, Grave et al. do not perform hard clustering (as in Brown clustering) but rather approximate posteriors at test time (based on the entire sentence)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "decision": "submitted, no decision", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "pdf": "https://arxiv.org/abs/1312.6168", "paperhash": "nepal|factorial_hidden_markov_models_for_learning_representations_of_natural_language", "keywords": [], "conflicts": [], "authors": ["Anjan Nepal", "Alexander Yates"], "authorids": ["rockanjan@gmail.com", "ayates@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391736360000, "tcdate": 1391736360000, "number": 2, "id": "Uy5xO0qPmo2CA", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YHGzHsybzQU0l", "replyto": "YHGzHsybzQU0l", "signatures": ["anonymous reviewer 590a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Factorial Hidden Markov Models for Learning Representations of Natural Language", "review": "The author use a Factorial Hidden Markov Model (FHMM) on two Natural Language Processing tasks, namely POS tagging and Chunking.  Such a model associates a factorial state (a tuple of states) with each word position in the corpus. This can be interpreted as a left-context dependent word representation. In order to evaluate the quality of this representation, the authors train the representation on WSJ data from the Penn repository and test on biomedical text. This domain adaptation performance is compared with that of a variety of systems (no learned features, HMM features, Brown clusters).   Although the approach makes sense, the empirical evaluation leaves questions open.  For instance, before seeing domain adaptation performance results, I would have liked to know whether the FHMM representation leads to competitive performance when testing on similar data. Also the domain adaptation performance is not compared with the performance afforded by some of the readily available word embeddings (other than Brown clusters.)   In conclusion, I do not really know what to think of the performance of FHMMs on such tasks."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "decision": "submitted, no decision", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "pdf": "https://arxiv.org/abs/1312.6168", "paperhash": "nepal|factorial_hidden_markov_models_for_learning_representations_of_natural_language", "keywords": [], "conflicts": [], "authors": ["Anjan Nepal", "Alexander Yates"], "authorids": ["rockanjan@gmail.com", "ayates@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391650680000, "tcdate": 1391650680000, "number": 1, "id": "_XBNcqV3Dycxk", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YHGzHsybzQU0l", "replyto": "YHGzHsybzQU0l", "signatures": ["anonymous reviewer 9228"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Factorial Hidden Markov Models for Learning Representations of Natural Language", "review": "The paper looks into representing words as (approximate) posterior distributions of states in a Factorial HMM estimated on a unlabeled dataset of a moderate size. Unlike standard word representations (e.g., obtained with the neural probabilistic model of Bengio et al. (2003)), these representations encode not only properties of a word but also properties of the word context (in theory unbounded) . For example, this may result in a form of word sense disambiguation.  Of course, this come at the cost of performing inference at test time.  The authors evaluate their approach on PoS tagging and chunking tasks. \r\n\r\nI find the paper quite interesting, as using some form of disambiguation does seem like a good idea to me. \r\n\r\nComments:\r\n\r\n1) The authors use a variational approximation which can be regarded as a form of the structured variational method for Factorial HMMs introduced in Ghahramani & Jordan (MLJ 1997) (not cited).  It would be good to explain which parts of the inference algorithm (e.g., relaxation for softmax) are novel, and which are borrowed from previous work.\r\n2)  I do not quite understand how exactly the variational parameters phi are computed -- something like Newton-Raphson? (In the paper: 'these expectations are used to find the new set of variational parameters', par 1, page 6)\r\n3) I wonder if Brown clusters are used as atomic variables or the features are paths in the binary tree (as in Koo et al. (ACL 2008)). Using paths may help substantially. \r\n4) Actually, factorial HMMs induce other types of word representations as well: parameters associated with emission distributions can perhaps be regarded as such representations. I am wondering if the authors considered using them as an additional baseline. (Of course, this representation would not be affected by the context.)\r\n5) It would be nice to see methods using word embeddings produced by more conventional methods as additional baselines. This would make the paper more convincing.\r\n6) Stochastic neural network models (namely sigmoid belief networks) for syntactic parsing of Titov & Henderson (ACL 2007) and Henderson & Titov (JMLR 2010) may also be relevant. They learn representations of parsing states (and some of their states corresponds to word emissions). The posterior distribution of the state variables are affected by global context. They also use variational approximation methods somewhat similar to the ones considered in this submission."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "decision": "submitted, no decision", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "pdf": "https://arxiv.org/abs/1312.6168", "paperhash": "nepal|factorial_hidden_markov_models_for_learning_representations_of_natural_language", "keywords": [], "conflicts": [], "authors": ["Anjan Nepal", "Alexander Yates"], "authorids": ["rockanjan@gmail.com", "ayates@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387865580000, "tcdate": 1387865580000, "number": 41, "id": "YHGzHsybzQU0l", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "YHGzHsybzQU0l", "signatures": ["rockanjan@gmail.com"], "readers": ["everyone"], "content": {"title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "decision": "submitted, no decision", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "pdf": "https://arxiv.org/abs/1312.6168", "paperhash": "nepal|factorial_hidden_markov_models_for_learning_representations_of_natural_language", "keywords": [], "conflicts": [], "authors": ["Anjan Nepal", "Alexander Yates"], "authorids": ["rockanjan@gmail.com", "ayates@gmail.com"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 8}