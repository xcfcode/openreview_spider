{"notes": [{"id": "jk1094_ZiN", "original": "V9znF9Y18ar", "number": 2722, "cdate": 1601308301751, "ddate": null, "tcdate": 1601308301751, "tmdate": 1614985642079, "tddate": null, "forum": "jk1094_ZiN", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "43LPTO5Vy7", "original": null, "number": 1, "cdate": 1610040519568, "ddate": null, "tcdate": 1610040519568, "tmdate": 1610474128115, "tddate": null, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers overall appreciated the efforts of the authors in making NAS more computationally efficient. The paper could greatly benefit from further editing/restructuring with the goal of improving clarity, as it\u2019s currently hard to navigate and understand in places. Future submissions of this work would benefit from more extensive empirical validation that motif networks mimic the original network. The reviewers also agreed that for the method to be appealing/useful, a general way to generate motif networks is needed. Overall, the outcome was that this is a very interesting idea but needs further development along the directions outlined above."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040519555, "tmdate": 1610474128100, "id": "ICLR.cc/2021/Conference/Paper2722/-/Decision"}}}, {"id": "ww06XfsqkDK", "original": null, "number": 8, "cdate": 1606298164851, "ddate": null, "tcdate": 1606298164851, "tmdate": 1606298164851, "tddate": null, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment", "content": {"title": "Response to all the reviewers", "comment": "We would like to thank all reviewers for their insightful comments! We recognize that reviewing is time-consuming work, and we are deeply appreciative. We are glad that the reviewers found 'Synthetic Petri Dish' to be a novel and well motivated method with a potential of real world impact. Below, we\u2019ve written responses to each reviewer individually.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jk1094_ZiN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2722/Authors|ICLR.cc/2021/Conference/Paper2722/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845147, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment"}}}, {"id": "ozucI0m11Rh", "original": null, "number": 7, "cdate": 1606298036401, "ddate": null, "tcdate": 1606298036401, "tmdate": 1606298036401, "tddate": null, "forum": "jk1094_ZiN", "replyto": "nzL156adSwV", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment", "content": {"title": "Part 2 of Response to AnonReviewer2", "comment": "$\\Vert$  Another shortcoming with the proposed approach is that the authors haven't proposed a 'general purpose' method to create petri dishes, and only width reduction is explored. Although this is a nice start, clearly this will not work if the architecture parameter being optimized over using NAS is 'width' itself. Have the authors considered such a scenario? \n\nThis is a very good point. We have demonstrated the effectiveness of our approach for searching activation functions and connectivity of cell microarchitectures. Modifying the Petri Dish to search over the width of the network is an interesting future-work direction. We have some initial ideas in this direction -- for example, architectures with variable layer-widths could be reduced in size by a fixed factor to create their corresponding motif-networks with variable widths. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jk1094_ZiN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2722/Authors|ICLR.cc/2021/Conference/Paper2722/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845147, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment"}}}, {"id": "nzL156adSwV", "original": null, "number": 6, "cdate": 1606297974786, "ddate": null, "tcdate": 1606297974786, "tmdate": 1606297974786, "tddate": null, "forum": "jk1094_ZiN", "replyto": "PmghpBeirh", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment", "content": {"title": "Part 1 of Response to AnonReviewer2", "comment": "$\\Vert$  the design section is written almost completely in prose without a well delineated algorithm. \n\nWe did summarize our method in Algorithm 1, but had to move it to the Appendix due to space constraints. This algorithm captures various aspects of Synthetic Petri Dish including initialization, training, inference and hybridization with other NAS methods. \n\n$\\Vert$   At first glance I believed that once the 'petri-dish' was initialized then ground truth evaluations would not be carried out again as it seems that is what the introduction is implying. However the authors combine the petri-dish with ground truth evaluations (Sec. 3.3). \n\nOnce the Petri Dish is warm-started with few ground-truth evaluations, it need not be re-trained with fresh ground-truth evaluations. For example, in Experiment 1 (search for the optimal slope of sigmoid), we train the Petri Dish only once and then utilize it to predict the performance of test points. \n\nIn Experiment 2, we combine Synthetic Petri Dish with popular NAS methods such as NAO (Luo et al., 2018) and Random Search. These NAS methods conduct their search in an iterative manner where each iteration involves ground-truth evaluation of the newly generated architectures. Since training the Petri Dish is cheap (requires just 2 hours on a CPU), we utilize the fresh ground-truth data from each NAS iteration to re-train the Petri Dish as well. This combined procedure of NAS with Petri Dish is explained in Figure 2 (b) and also in Algorithm 1. \n\n$\\Vert$  is it true that 'inference' carried out using this petri dish involves training a petri-dish network to convergence then evaluating its accuracy on the synthetic validation data? \n\nYes, that is correct. We also mention this detail in the paper. For example, section 3.1 states that -- \"Inner-loop training proceeds until individual BCE losses converge. Once trained, each motif-network is independently evaluated using the synthetic validation data to obtain individual validation loss values\". Subsequently, Section 3.2 states that during Petri Dish inference procedure, \"motif-networks are trained and evaluated using the optimized synthetic data with the same hyper-parameter settings as in the inner-loop training and evaluation.\" \n\n$\\Vert$  Although 'petri-dish inference' is defined in Sec 3.2, I see no usage of this exact verb in Sec 4.1\n\nSection 4.2 refers to Algorithm 1 and Figure 2 (b), both of which highlight the 'Petri Dish Inference' procedure. \n\n$\\Vert$  One concern I have is that the robustness of petri dish idea at mimicking the original network is not well established outside of a rather simple experiment in Fig. 1. \n\nBased on your feedback, we are currently working to add results from running Petri Dish for NASBench  in our paper. This will be included in our next revision very soon.\n\n$\\Vert$  Another concern with Fig. 1 is that the 'ground-truth' evaluations used to seed the two models appear to be artificially restricted to the shaded box. The training and test dataset on this experiment should be sampled i.i.d. unless the authors can provide a compelling reason why.\n\nWe restricted the training points for the performance prediction models to the blue-shaded region so that we can effectively test their generalization capability for unseen points. While training the NN model, we randomly sample batches of ground-truth points from the blue-shaded region. The experimental details are further described in Appendix A.3. \n\n$\\Vert$  With regards to Fig. 3, from what I can understand the authors considered usage of NAO over 3 iterations of 33 motifs each to yield 100 ground truth evaluation. Did the authors consider 5 iterations of NAO with 20 motifs each as in the petri-dish approach. What do the authors mean exactly with, \"For a fair comparison, original NAO is re-run in this limited ground-truth setting and the resulting performance is depicted by the red-curve in Figure 3\"? Can the authors exactly delineate their experimental settings in the appendix or in the main text.\n\nWe believe the reviewer has misunderstood our experimental setup for Experiment 2. All NAS variants in Experiment 2 (including NAO with Reduced Data) are run for the same number of iterations. Section 4.2 of the paper describes this in detail. For example, it states that -- \"In each NAS iteration, 100 newly generated motifs (variable M in algorithm 1) are evaluated using the Petri dish inference procedure and only the top 20 predicted motifs (variable K in algorithm 1) are evaluated for their ground-truth performance. The test perplexity of the best found motif at the end of each NAS iteration is plotted in Figure 3\". \n\nWe will try to further clarify this point in our revision. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jk1094_ZiN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2722/Authors|ICLR.cc/2021/Conference/Paper2722/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845147, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment"}}}, {"id": "tumAR6O112E", "original": null, "number": 4, "cdate": 1606290151310, "ddate": null, "tcdate": 1606290151310, "tmdate": 1606292988334, "tddate": null, "forum": "jk1094_ZiN", "replyto": "oP_Pb8Pcp1r", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for recognizing the novelty of the method and the clarity of our paper.\n\n$\\Vert$  Is there a way to standardize the motif generation procedure for different architectures?\n\nEach motif variant is extracted from the ground-truth network and instantiated in the corresponding motif-network by reducing the width of the motif, while ensuring that the depth and the internal connectivity of the motif remains unchanged. For example, in the RNN cell search experiment, the ground-truth network has a cell that is composed of 12 layers (layer width of 850) and has millions of parameters. Each cell is extracted from its ground-truth setting and is instantiated in the motif-network with just three neurons per layer. The width of the motif-network is a hyper-parameter that is selected using the procedure described in Appendix A.2. \n\nThe same motif-network generation procedure can be used for searching the connectivity of macro-architectures as well.  \n\n$\\Vert$ Result for only one task is shown in the result section. It's not clear if this method can be for other tasks? Adding more tasks might have strengthened the paper.\n\nBased on your feedback, we are currently working to add results from another benchmark in our paper. This will be included in our next revision very soon. \n \n\n$\\Vert$ Authors mentions that original NAO reaches 56 perplexity after 300 GPU days. How many days does it take for the synthetic petri dish with NAO to reach this result?\n\nIn the experiments performed in the paper, our goal was to evaluate the performance-prediction mechanism of Synthetic Petri Dish with limited compute resources. For a fair comparison, we also re-run the original NAO code in the same setting and report the numbers in the paper. Running these experiments longer becomes prohibitively expensive, especially given that each of them needs to be run five times with random seed for statistical significance.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jk1094_ZiN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2722/Authors|ICLR.cc/2021/Conference/Paper2722/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845147, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment"}}}, {"id": "Vp9MK17_M6S", "original": null, "number": 5, "cdate": 1606292539806, "ddate": null, "tcdate": 1606292539806, "tmdate": 1606292539806, "tddate": null, "forum": "jk1094_ZiN", "replyto": "no4FtOCeCPW", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you recognizing the promising idea presented in this paper. \n\n$\\Vert$   how the synthetic data are generated and how exactly the motifs are instantiated?\n\nWe would like to clarify that the synthetic training and validation data is not generated. Instead, the synthetic data is randomly initialized at the start of the experiment and subsequently optimized during each outer-loop iteration (as detailed in Equations (2) and (3)). It is also possible to train a generator for the synthetic data and that could be an interesting direction for future-work. \n\nEach motif variant is extracted from the ground-truth network and instantiated in the corresponding motif-network by reducing the width of the motif, while ensuring that the depth and the internal connectivity of the motif remains unchanged. For example, in the RNN cell search experiment, the ground-truth network has a cell that is composed of 12 layers (layer width of 850) and has millions of parameters. Each cell is extracted from its ground-truth setting and is instantiated in the motif-network with three neurons per layer. \n\nThe width of the motif-network and the number of synthetic data samples are chosen through the hyper-parameter selection procedure described in Appendix A.2.\n\n$\\Vert$ how good are the predictions the petri dish makes, and how does the derived ranking compare to the ground-truth ranking?\n\nOne of the goals of Experiment 1 in the paper (sigmoid slope search) is to compare the performance ordering (i.e. ranking) of Petri Dish predictions with the ground-truth performance ordering. \n\nAlso, based on your feedback, we are currently working to add results from another benchmark in our paper. This will be included in our next revision very soon.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jk1094_ZiN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2722/Authors|ICLR.cc/2021/Conference/Paper2722/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845147, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment"}}}, {"id": "qZfhb4RA50b", "original": null, "number": 3, "cdate": 1606286751496, "ddate": null, "tcdate": 1606286751496, "tmdate": 1606286751496, "tddate": null, "forum": "jk1094_ZiN", "replyto": "T5xJy2r8-s7", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for recognizing the applicability of Petri Dish to speed up a variety of hyper-parameter searches.\n\n$\\Vert$  Is it possible to just use standard metrics like accuracy instead for Figure 1?\n\nWe plot the normalized accuracy instead of the raw accuracy in Figure 1 because the raw ground-truth accuracy values need not match the raw motif-network accuracy values. Since the goal of Synthetic Petri Dish is to predict the performance ordering of motifs, we normalize the motif-performance values before plotting them. \n\n$\\Vert$  I do not understand why one has to define ground-truth loss and motif network loss.\n\nWe would like to clarify that we do not subtract the raw ground-truth loss and the raw motif-network loss. Instead, as explained in Section 3.1 and Equation (2), we construct the outer-loop loss by taking the difference between the normalized ground-truth loss and the normalized motif-network loss. The outer-loop loss tries to minimize the difference in the performance ordering of the motif networks and the ground-truth networks. To frame the outer-loop loss function, what is desired is for the validation loss of the motif-network to induce the same relative ordering as the validation loss of the ground-truth networks; such relative ordering is all that is needed to decide which new motif is likely to be best. One way to design such an outer-loop loss with this property is to penalize differences between normalized loss values in the Petri dish and ground-truth setting. To this end, the motif-network (inner-loop) loss values and their respective ground-truth loss values are first independently normalized to have zero-mean and unit-variance. Then, for each motif, a mean squared error (MSE) loss is computed between the normalized inner-loop validation loss and the normalized ground-truth validation loss. The MSE loss is averaged over all the motifs and used to compute a gradient step to improve the synthetic training and validation data. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jk1094_ZiN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2722/Authors|ICLR.cc/2021/Conference/Paper2722/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845147, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Comment"}}}, {"id": "PmghpBeirh", "original": null, "number": 1, "cdate": 1603782048880, "ddate": null, "tcdate": 1603782048880, "tmdate": 1605024146465, "tddate": null, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Review", "content": {"title": "Many shortcomings, and not yet ready for publication", "review": "This paper presents an approach to accelerating NAS with 'petri-dish' networks, which hope to mimic the response of original networks at a fraction of training time cost. The key idea is to evaluate an architectural setting on a miniaturized network as opposed to the original network. With this approach computational effort is saved by eschewing expensive 'ground truth' original network evaluations.\n\nThe largest hinderance by far in the paper is the quality of the writing. Much of the paper is not well written, and difficult to understand. In particular, the design section is written almost completely in prose without a well delineated algorithm. The paper does not do a good job of guiding the reader through their proposed approach, thus much hunting and guessing is required to understand exactly what it is that the authors are proposing. E.g., at first glance I believed that once the 'petri-dish' was initialized then ground truth evaluations would not be carried out again as it seems that is what the introduction is implying. However the authors combine the petri-dish with ground truth evaluations (Sec. 3.3) but this is not clearly presented in the introduction. Another difficulty is it's unclear exactly the precise experimental settings used, making it impossible to be sure whether the experiments are 'fair' to competing approaches. E.g., to generate Fig. 1 a 'petri-dish' is initialized and trained, however is it true that 'inference' carried out using this petri dish involves training a petri-dish network to convergence then evaluating its accuracy on the synthetic validation data? Although 'petri-dish inference' is defined in Sec 3.2, I see no usage of this exact verb in Sec 4.1. It's difficult to connect these concepts as a reader without a well delineated algorithm which concretizes with precision what is being proposed.\n\nBesides these issues, the proposed idea is not fully well explored which also leads to doubt on the robustness of the approach. One concern I have is that the robustness of petri dish idea at mimicking the original network is not well established outside of a rather simple experiment in Fig. 1. As this is a key linchpin to the proposed approach, I strongly require an extensive experiment in a complex setting (with multiple properties of a network being searched over). Another concern with Fig. 1 is that the 'ground-truth' evaluations used to seed the two models appear to be artificially restricted to the shaded box. Neural networks operate under the i.i.d. setting thus making this approach biased against the NN Model approach. The training and test dataset on this experiments should be sampled i.i.d. unless the authors can provide a compelling reason why.\n\nWith regards to Fig. 3, from what I can understand the authors considered usage of NAO over 3 iterations of 33 motifs each to yield 100 ground truth evaluation. Did the authors consider 5 iterations of NAO with 20 motifs each as in the petri-dish approach. What do the authors mean exactly with, \"For a fair comparison, original NAO is re-run in this limited ground-truth setting and\nthe resulting performance is depicted by the red-curve in Figure 3\"? Can the authors exactly delineate their experimental settings in the appendix or in the main text.\n\nI note that evaluation was not performed for CNNs. Also it would be nice to see the performance of this approach on standard NAS benchmarks such as NASBench.\n\n\nAnother shortcoming with the proposed approach is that the authors haven't proposed a 'general purpose' method to create petri dishes, and only width reduction is explored. Although this is a nice start, clearly this will not work if the architecture parameter being optimized over using NAS is 'width' itself. Have the authors considered such a scenario? How do the authors propose to create petri-dishes in the generalized setting? This shortcoming is glaring, and I'm not sure whether the proposed idea is well explored with a general purpose 'petri-dish' creation mechanism.\n\nPros:\n-Possibly neat idea if executed well.\n\nCons:\n-See above.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089999, "tmdate": 1606915805603, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2722/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Review"}}}, {"id": "no4FtOCeCPW", "original": null, "number": 2, "cdate": 1603824253541, "ddate": null, "tcdate": 1603824253541, "tmdate": 1605024146404, "tddate": null, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Review", "content": {"title": "Very nice idea, but important details unclear", "review": "tHe paper proposes a method for quickly and cheaply determining the value of a\nparticular motif in neural architecture search by isolating it in a \"petri dish\"\nthat allows it to be evaluated without having to train an entire network. The\nauthors describe their method and evaluate it empirically, showing the promise\nof the method.\n\nThe idea is interesting and seems very promising. As the authors say, it\naddresses one of the bottlenecks in neural architecture search -- the presented\nresearch deals with an important problem. There are clear reasons for preferring\nthe proposed method over alternatives.\n\nHowever, there are important details missing. In particular, it was unclear to\nme after reading the paper how the synthetic data are generated and how exactly\nthe motifs are instantiated. These are crucial parts of the proposed method, and\nits success hinges on representative data and embeddings. This should be\nexplained in some detail; in particular because the paper leaves the reader at a\nloss on how to apply this methodology to their own problems.\n\nFurther, the evaluation performed by the authors, albeit showing good results,\nis very small. Only anecdotal results are presented in two contexts, and it is\nunclear how the approach actually performs -- how good are the predictions the\npetri dish makes, and how does the derived ranking compare to the ground-truth\nranking?\n\nIn summary, while I feel that the idea is interesting and promising, there is\ninsufficient information for it to have a significant impact.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089999, "tmdate": 1606915805603, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2722/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Review"}}}, {"id": "oP_Pb8Pcp1r", "original": null, "number": 3, "cdate": 1603872150370, "ddate": null, "tcdate": 1603872150370, "tmdate": 1605024146222, "tddate": null, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Review", "content": {"title": "Interesting idea", "review": "##########################################################################\n\nSummary:\n\n \nThe paper provides a novel surrogate model method for Neural Architecture Search. Authors motivated the paper from biological study of cells in an artificial petri dish setting. Authors explains the concept of evaluation of small motifs with synthetic learned training and validation data that can predict the performance of larger network.\nLeveraging this, authors proposes speeding up the problem of Neural Architecture Search.\n\n##########################################################################\n\nPros:\n \n1. The problem of Neural Architecture Search is an impactful problem specially with the recent advances in neural networks. This paper tries to solve the scalability issue of this problem which I think has real world impact.\n\n2. The paper is generally well motivated. The motivation behind the methods are well explained. I liked the petri dish inspiration. Also I think the idea of motif for architecture search deserves more attention. I like the use of motifs to speed/scale up the search procedure such that we can train on smaller set of motifs but do inference on a larger set. Also as the motifs are directly evaluated rather than using some NN to evaluate them, it makes the case for their method more convincing. \n \n3. Overall the paper is well written.  Result section is also well structured. \n\n \n##########################################################################\n\nCons: \n\n1. Although the paper puts a lot of importance on motifs, but it does not explain a standard way to generate the motifs for any kind of networks, which makes the scope a bit narrow. Is there a way to standardize the motif generation procedure for different architectures?\n\n2. The performance improvement with time over other methods is not properly shown in the result section. I thought that would have shown more impact in the result. Although it shows perplexity with ground truth evaluations curve, but time vs performance could have probably shown the effect of cheap inference for motifs in these work.\n \n3. Result for only one task is shown in the result section. It's not clear if this method can be for other tasks? Adding more tasks might have strengthened the paper.\n\n4. Authors mentions that original NAO reaches 56 perplexity after 300 GPU days. How many days does it take for the synthetic petri dish with NAO to reach this result? What is its best performance? When does it converges if we keep running for a long time? What is the performance for RS without NAO if we keep running for a long time?\n\n##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n \n#########################################################################\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089999, "tmdate": 1606915805603, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2722/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Review"}}}, {"id": "T5xJy2r8-s7", "original": null, "number": 4, "cdate": 1603879210686, "ddate": null, "tcdate": 1603879210686, "tmdate": 1605024146094, "tddate": null, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "invitation": "ICLR.cc/2021/Conference/Paper2722/-/Official_Review", "content": {"title": "Official review", "review": "Summary:\nThe paper considers the problem of Neural Architectural Search, and proposes an efficient method called Synthetic Petri Dish inspired by how in-vitro experiments are done in biology. Numerical simulations are shown using two examples to demonstrate the validity of the proposed method.\n\nPros:\n- The paper introduces a nice idea to speed up the problem of architecture/hyper-parameter search. The current search methods are often too prohibitive computationally, so this is a useful contribution.\n- The idea is well-motivated, and also quite general in terms of its applicability. As in, this method could be used to speed up a variety of architecture/hyper-parameter searches.\n\nCons:\n- The numerical evidence provided in the paper is quite minimal. It would be very interesting to see the performance of the method across different datasets as well as across different hyper-parameters. For instance, how does the method perform when it comes to choice of layers in a CNN (like different kinds of residual blocks) or transformers, learning rates, number of filters, etc. While the evidence provided in the paper is alright, there is lots of room to strengthen the paper.\n\nComments:\n- While I understand why Figure 1 is introduced very early in the paper, I feel it should be explained better. In particular, the use of normalised validation accuracy without defining it properly could lead to confusion. Is it possible to just use standard metrics like accuracy instead?\n- Equation (2): I do not understand why one has to define ground-truth loss and motif network loss. Because the two are subtracted, wouldn't the result be just the difference between the ground truth prediction and motif network prediction? In other words, (a-b)-(c-b) = (a-c). So essentially you are trying to minimise the difference between the predictions of the two models right? Is this also why you only optimise on the synthetic input and not bother about the synthetic output?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2722/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2722/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "authorids": ["~Aditya_Rawal1", "~Joel_Lehman1", "~Felipe_Petroski_Such1", "~Jeff_Clune1", "~Kenneth_Stanley1"], "authors": ["Aditya Rawal", "Joel Lehman", "Felipe Petroski Such", "Jeff Clune", "Kenneth Stanley"], "keywords": ["Neural Architecture Search", "AutoML", "Meta-learning"], "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations.", "one-sentence_summary": "In order to speed-up NAS, this paper learns synthetic data to quickly train and evaluate miniaturized version of the full architectures. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rawal|synthetic_petri_dish_a_novel_surrogate_model_for_rapid_architecture_search", "supplementary_material": "/attachment/eac59f59b25687ed68449ad7b5a940514d1e5614.zip", "pdf": "/pdf/ecb176b5336e7e8298e6f5b192f4c2ba478ea7a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SEcZRp-26-", "_bibtex": "@misc{\nrawal2021synthetic,\ntitle={Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search},\nauthor={Aditya Rawal and Joel Lehman and Felipe Petroski Such and Jeff Clune and Kenneth Stanley},\nyear={2021},\nurl={https://openreview.net/forum?id=jk1094_ZiN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jk1094_ZiN", "replyto": "jk1094_ZiN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2722/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089999, "tmdate": 1606915805603, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2722/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2722/-/Official_Review"}}}], "count": 12}