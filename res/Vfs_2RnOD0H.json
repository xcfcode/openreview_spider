{"notes": [{"id": "Vfs_2RnOD0H", "original": "4hVv83im9Kq", "number": 1417, "cdate": 1601308157900, "ddate": null, "tcdate": 1601308157900, "tmdate": 1616046683255, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "SmwvDyUXdoO", "original": null, "number": 1, "cdate": 1610040528202, "ddate": null, "tcdate": 1610040528202, "tmdate": 1610474137468, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "The paper presents an online algorithm for dynamic tensor rematerialization.  The theoretic analysis on the tensor operation and memory budget bound of the proposed method, as well as on the relationship between the proposed method and optimal static analysis method is novel and interesting.  It covers a pretty comprehensive study across theory, simulation and system implementation. In addition, the paper is well written. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040528189, "tmdate": 1610474137454, "id": "ICLR.cc/2021/Conference/Paper1417/-/Decision"}}}, {"id": "cle3QQD1HKM", "original": null, "number": 10, "cdate": 1606256484384, "ddate": null, "tcdate": 1606256484384, "tmdate": 1606256484384, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "VFL_xa-nw_5", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment", "content": {"title": "Request for clarification on ablation study comment", "comment": "Thank you for your reply. Could you please clarify what additional ablation study you feel would strengthen the submission? We have included Capuchin's MSPS heuristic in our simulated evaluation in Section 4; it would be feasible to extend the ablation study in Appendix D to include variants of MSPS. We could also extend the prototype to use MSPS, though the results in Section 4 and Appendix D were what led us to use $h_{\\text{DTR}}^{\\text{eq}}$ in the prototype."}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vfs_2RnOD0H", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1417/Authors|ICLR.cc/2021/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859957, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment"}}}, {"id": "VFL_xa-nw_5", "original": null, "number": 9, "cdate": 1606203924507, "ddate": null, "tcdate": 1606203924507, "tmdate": 1606203924507, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "JhBE7JjK9P_", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment", "content": {"title": "Thanks to the authors for the clarification", "comment": "Based on the fact that Capuchin's code isn't available yet I agree that a full comparison to Capuchin isn't needed. My another concern on ablation study of replacing the heuristics used in Peng et al. (2020) with the proposed one still holds."}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vfs_2RnOD0H", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1417/Authors|ICLR.cc/2021/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859957, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment"}}}, {"id": "MEd9eHlmyNC", "original": null, "number": 7, "cdate": 1605324844264, "ddate": null, "tcdate": 1605324844264, "tmdate": 1605324844264, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "h0fmv_6rYmz", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment", "content": {"title": "Response to review", "comment": "We thank you for your careful reading and your detailed review, particularly for your questions and suggestions.\n\n### Multi-GPU Setting\n\nLike recent checkpointing work, such as Jain _et al._ (2020), Kumar _et al._ (2019), and [Beaumont _et al._ (2019)](https://hal.inria.fr/hal-02352969/document), we have focused on a single-GPU setting. However, nothing in DTR\u2019s design precludes supporting multiple GPUs. One way to add support would be to have the runtime system manage each GPU individually, treating values sent to a particular GPU as \"inputs\" (unevictable) and rematerializing values specific to that GPU to avoid incurring cross-GPU communication expenses.\n\n### Question on Datasets\n\nWe agree that the input to a dynamic model is very important because it can affect the model's control flow. To ensure controlled experiments in our evaluation of dynamic models, we were careful to specify and fix the input shape (sequences of a given length for LSTM and balanced binary trees of a specific depth for TreeLSTM) to ensure that we would not be introducing more variables into the experiment. That is, we used synthetic inputs of fixed shapes to ensure that in each batch, the models would follow the same control flow -- this was to ensure fair and clear comparisons. By design (and in practice using our prototype) DTR works regardless of the input shape and the resulting control flow, whereas static methods would need to perform planning for each distinct control flow pattern encountered. \n\n### Comparing against Swapping Techniques\n\nWe thank you for your suggestion about adding further discussion about how DTR specifically and checkpointing broadly differ from swapping techniques. Past swapping systems like Capuchin assume a static computation graph and a fixed access pattern (please also see our response to AnonReviewer5), which would prevent them from working with dynamic models. Additionally, as we note in our related work discussion, these systems first perform a profiling pass to gather information and then use the information they gather to plan a swapping schedule in advance so that they can overlap swapping with computation (crucial for obtaining good performance). It is thus not immediately apparent to us whether swapping can be handled entirely online on arbitrarily dynamic models.\n\nOne way to combine DTR and swapping, given a fixed swapping schedule, would be to use DTR to replace the rematerialization schemes used by systems like Capuchin (perhaps given a constraint like treating values that will be swapped out as unevictable). It may also be possible to incorporate such swapping into DTR by treating swapping as a different kind of \"eviction\": instead of replaying computations, swapped-out values would be rematerialized using communication and the communication time would be treated as the \"cost.\" Swapping would present interesting tradeoffs with rematerializations, as it would likely scale better than certain tensor operators (see our discussion of UNet\u2019s performance with AnonReviewer4). Swapping has some other additional constraints, such as requiring that there actually be a location to swap to (which may not be the case for edge devices) and that there be enough memory for swapping there. This would be an interesting direction for future work, though central to the issue would be ensuring that the communication could be efficiently overlapped with computation. We will expand our discussion of these points in the paper.\n\n### Comparing against Static Methods\n\nWe also thank you for your suggestion about making more explicit why past techniques would not support dynamic models in general and we agree that this would be useful for readers. The fundamental reason, as you note in your question about datasets, is that the computation graph can vary between inputs on many dynamic models (and in some more experimental models, can vary based on input _values_), while static techniques require there to be a single computation graph.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vfs_2RnOD0H", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1417/Authors|ICLR.cc/2021/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859957, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment"}}}, {"id": "vqemzaTQi8U", "original": null, "number": 6, "cdate": 1605324791751, "ddate": null, "tcdate": 1605324791751, "tmdate": 1605324791751, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "TId0UKb-YCS", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment", "content": {"title": "Response to review", "comment": "We thank you for carefully reading our work and we greatly appreciate your encouraging remarks. We hope that by making the prototype and our experimental infrastructure available, we can assist future work in this area. DTR is open source and publicly available; we also hope to upstream DTR to mainline PyTorch in coming months. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vfs_2RnOD0H", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1417/Authors|ICLR.cc/2021/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859957, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment"}}}, {"id": "XpS9lK4_Twh", "original": null, "number": 5, "cdate": 1605324760862, "ddate": null, "tcdate": 1605324760862, "tmdate": 1605324760862, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "aXFcfjqDjqp", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment", "content": {"title": "Response to review", "comment": "We thank you for your attentive reading of our paper and your insightful comments.\n\n### Motivating the Dynamic Approach\n\nWe believe DTR\u2019s simplicity, flexibility, and deployability make it an attractive target for practical use, especially for rapid prototyping and experimentation. We agree that, as you describe, in the particular case of static models that take very long to train on large datasets, investing minutes or hours to generate a checkpointing plan is a reasonable tradeoff. However, as we note in our reply to AnonReviewer5, optimal planning with tools like Checkmate can take much longer to generate plans for large models (e.g., more than a day for DenseNet161). Furthermore, many interesting dynamic models are not supported by static checkpointing approaches.\n\nAdditionally, as in our response to AnonReviewer5, we stress the conceptual contribution of our online algorithm: DTR is able to produce high-quality checkpointing schemes despite lacking any advance knowledge of the model (a very reduced setting). We hope that the insights of DTR will also be useful in future work; e.g., incorporating more facts into a planning heuristic could lead to even better results. (For example, a variant of DTR that learns from past batches might be able to make better eviction decisions at lower overhead than the purely online version.) Furthermore, as an approach that makes few assumptions and is capable of supporting arbitrarily dynamic models, DTR enables exploration of more diverse model architectures and learning techniques in memory-constrained settings, including training on edge devices or tasks with higher-order derivatives like metalearning. Providing more support for such applications expands the possibilities for deep learning models and applications.\n\n### Further Comparisons against Past Work\n\nUnfortunately, we could not find a system implemented in PyTorch that could automatically support checkpointing across the variety of models we included in our evaluation of the DTR prototype. We could potentially apply PyTorch's checkpointing API to manually checkpoint some of the models in our evaluation, but this would entail significant additional engineering effort and the quality of any such manual checkpointing schemes would depend heavily on specific knowledge of the models, threatening the fairness of the comparison.\n\nIn part of our evaluation, we relied on simulation as a baseline because some past systems are simply not available (see our discussion about Capuchin in response to AnonReviewer5) and others have been implemented in different frameworks, making it difficult to set up fair comparisons. By contrast, we were able to present a fair comparison against Checkmate by directly using their MLSys 2020 artifact.\n\n### Performance on UNet (decreased throughput)\n\nIndependently of any checkpointing scheme, increasing the batch size can only improve a model\u2019s throughput so long as the tensor operator performance scales at a lower rate than the batch size (for example, by exploiting hardware parallelism). If the tensor operators scale at the same rate or a higher rate, then increasing the batch size will at most maintain the throughput and any additional computations (e.g., rematerializations) will decrease the throughput. We further analyzed the logs DTR produced from the trials in Table 1 and observed such scaling in UNet: the `cudnn_convolution_backward` operator scaled linearly over the batch sizes we tested and accounted for 49% of UNet\u2019s total computation time. By contrast, the same `cudnn_convolution_backward` operator exhibited sublinear scaling in ResNet-1202, resulting in increased throughput (perhaps partly due to the difference in image sizes: 224x224 for ResNet and 416x608 for UNet).\n\nWhile this decrease in throughput is more appropriately attributed to the operators, and not to DTR, we note that it may be possible to apply systems like Halide, TVM, or Tiramisu to generate and tune operator implementations specialized to larger batch sizes and improve the throughput.\n\nWe increased the batch size in Table 1 to highlight that DTR can allow for processing larger inputs in the same amount of memory. This can represent handling larger problem sizes with models or simply running larger models, though perhaps there is a better way we can demonstrate DTR's capabilities in this regard.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vfs_2RnOD0H", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1417/Authors|ICLR.cc/2021/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859957, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment"}}}, {"id": "JhBE7JjK9P_", "original": null, "number": 4, "cdate": 1605324702539, "ddate": null, "tcdate": 1605324702539, "tmdate": 1605324702539, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "Vyzy7fCTQe", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment", "content": {"title": "Response to review", "comment": "We thank you for your detailed review and careful reading of our paper.\n\n### Comparisons against Capuchin\n\nUnfortunately, the Capuchin implementation has not been released. Earlier we wrote to the corresponding Capuchin author, who replied that he and his coauthors hope to make the code available but have not yet done so. Note that Capuchin was implemented in TensorFlow, which would make it difficult to set up fair head-to-head performance comparisons with DTR (that is, it would be difficult to isolate the impact of the systems in question from the differences between the underlying frameworks). We also note that Capuchin\u2019s checkpointing approach does not support dynamic models in general, as their runtime system first performs a profiling pass on the models and assumes that the computation graph and access patterns will not change between inputs. (Accordingly, the Capuchin author Dr. Shi confirmed in our correspondence that he would not expect Capuchin to support a dynamic model because it would not be able to \"identify a regular tensor access pattern to make memory management policy.\") DTR makes no such assumption, as we discuss in the paper.\n\n### Utility of Applying DTR to Static Models\n\nAs noted in your review, DTR has the advantage that it checkpoints both dynamic and static models but the disadvantage that it incurs runtime overhead. We contend that, in practice, DTR\u2019s high-quality plans, produced in real time, make it a convenient choice for many deep learning scenarios. In particular, returning to Figure 3, DTR\u2019s plans on static models are competitive with Checkmate\u2019s optimal plans and outperform some past static approaches. During rapid prototyping and experimentation, even a few minutes or hours of static planning may be a significant drawback. Even after the model development phase, when we might be encouraged to spend hours or days finding an optimal schedule, it is unclear that optimal methods like Checkmate can feasibly checkpoint large models. For example, Jain _et al._ (2020) note that \"[f]or DenseNet161 (Huang _et al._, 2017), no feasible solution was found within one day.\" Finally, we note that DTR can be used as a static checkpointing technique on static models by running the DTR simulator to create a checkpointing scheme. This usage of DTR would incur no runtime overhead.\n\nWe emphasize the conceptual point made by our presentation of DTR: Namely, that an online algorithm with no prior knowledge of the model is capable of producing very good checkpointing schemes in real time. While the online algorithm, implemented precisely as described in the paper, is immediately useful for both static and dynamic models (and can enable exploration of more diverse model architectures), it is our hope that the insights of DTR will also be useful in broader scenarios, e.g., to inspire further improvements in static checkpointing.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vfs_2RnOD0H", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1417/Authors|ICLR.cc/2021/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859957, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Comment"}}}, {"id": "h0fmv_6rYmz", "original": null, "number": 1, "cdate": 1603069527945, "ddate": null, "tcdate": 1603069527945, "tmdate": 1605024451027, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Review", "content": {"title": "The paper proposes an implementation of tensor rematerialization for PyTorch, in order to reduce GPU memory requirements by up to 50-80% depending on model. The implementation is dynamic (does not require pre-runtime analysis) but requires between 0-75% overhead during runtime. Several heuristics are proposed and analyzed to minimize overheads while maximizing memory saved.", "review": "Claims:\n\n-Better eviction heuristics provide noticeable improvement over earlier dynamic rematerialization schemes - up to 50-80% memory savings at the cost of up to 75% increase in computation\n\n-Seamless implementation with PyTorch; user does not need to change their code - highly impactful if incorporated into PyTorch\n\n-Upper bound analysis shows only O(N) operations are needed (constant factor more than without rematerialization)\n\n-Several eviction heuristics are studied, including from previous literature. Mathematical formalism unifies these heuristics and clearly describes their relationships\n\nPros:\n\n-It seems that previous rematerialization schemes provide up to 50% memory savings (i.e. can run 2x larger models), but the proposed work goes up to 80% memory savings (i.e. can run 5x larger models). If this is true, then the ability to run 5x larger models on the same hardware shifts rematerialization from a relatively minor optimization to a game-changing feature\n\n-Running time overhead below 100%, which is an acceptable tradeoff (authors consider runtime overhead >100% to be thrashing, which is sensible)\n\n-More so than typical papers, the design motivations are factually and intuitively explained, and performance claims are not exaggerated but rather put into perspective. The writing strikes me as convincing.\n\n-Comprehensive experiment design with 8 models and 7 eviction heuristics, covering RNNs, CNNs, Residual networks, Transformers, and unconventional models such as Unrolled GAN. This leaves little doubt as to the proposed method's effectiveness\n\nCons:\n\n-The paper did not study rematerialization in at least the multi-GPU, if not distributed-parallel setting. I think the latter can be excused given the typical scope of an ICLR paper, but I was expecting the former and was surprised to find the experiment design only involved one GPU. At minimum, a multi-GPU experiment is needed to confirm the implementation works with multi-GPU setups, if not distributed-parallel setups.\n\nQuestions:\n\n-The paper does not talk about datasets used. This is appropriate if all models in the experiment suite do not have dynamic structure, since dynamic structures could (depending on the implementation) cause the computational graph to differ with different datasets - e.g. I imagine this could happen with NLP and BiLSTM or recursive neural networks. Can the authors clarify if they limited their models to those with static (fixed computational graph) structures? How would the proposed method behave on models with dynamic structures that change with the input data?\n\nSuggestions for improvement:\n\n-The authors call out the relationship between rematerialization and tensor swapping only in the related work. While this is better than not bringing it up at all, it can be argued that since rematerialization and swapping are both dynamic memory management techniques, swapping should at least be mentioned in the introduction. Even if swapping is out of scope of the paper, I think some discussion on how DTR and swapping could be combined would make the paper even stronger.\n\n-Although the paper does explain and provide experiments as to why dynamic rematerialization is superior to static rematerialization, I think the argument could be made much more convincing if the authors showed evidence that static rematerialization does not work on certain dynamic models, limiting its applicability. For example, a statement to the effect of e.g. \"Checkmate doesn't work on model X, and here is why...\" could be instructive.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119145, "tmdate": 1606915807985, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1417/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Review"}}}, {"id": "aXFcfjqDjqp", "original": null, "number": 3, "cdate": 1603951345921, "ddate": null, "tcdate": 1603951345921, "tmdate": 1605024450961, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Review", "content": {"title": "A useful and practical solution for dynamic checkpointing but system evaluation needs to be well strengthened", "review": "The paper presents an online algorithm for dynamic tensor rematerialization.  Theoretically, it shows the same asymptotic order on the memory budget and tensor operations as of the optimal static approach.  By simulation, it shows the performance matches optimal static checkpointing in a few models.  A PyTorch prototype is implemented, which shows benefits of reducing memory footprint and increased batch size comparing with basic PyTorch models without checkpointing.\n\nMerits of the paper:\n- Address an important practical problem on how and when to perform checkpointing during DL training.\n- Cover a pretty comprehensive study across theory, simulation and system implementation.\n- The suggested system implementation looks simple and clean.\n- Clearly written paper, which is easy to understand and follow.\n\nPlaces to improve:\n- The need of having dynamic approach in this area is not very well motivated.  Since the computation in most of DL models is repetitive over iterations, static approach would work pretty well.  Furthermore, since many models take long to train, spending minutes on analyzing static graph and obtaining an optimal solution seems to be time well spent, comparing with a suboptimal dynamic solution.  The motivation of developing dynamic approach needs to be strengthened.  \n\n- Although some comparison with related work is done by simulation, really system evaluation is relatively weak - it only compares with the strawman PyTorch models without checkpointing.   It would be a lot more convincing with the results comparing with basic checkpointing approach (e.g., layer wise checkpointing) and some related work.\n\n- For some of the reported models, like Unet, the approach reduces memory footprint and helps increase batch size, which however reduces  throughput.  These are probably not good examples to show the benefits of the approach.  But it might be beneficial to point out the underlying reasons for decreased throughput so readers know when the approach would/wouldn't work well and why. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119145, "tmdate": 1606915807985, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1417/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Review"}}}, {"id": "TId0UKb-YCS", "original": null, "number": 2, "cdate": 1603748453234, "ddate": null, "tcdate": 1603748453234, "tmdate": 1605024450891, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Review", "content": {"title": "This submission is based on the observation that intermediate activations can be replayed (through local forward prop calculations), rather than kept in memory for backprop to reach them after the full forward path calculation. As a result, it helps memory capacity bound cases (such as large batch) by reducing the working set of DL training.", "review": "Contributions: a) analyzes multiple heuristics for which tensors to evict where compute overhead of rematerialization is minimal overall, b) suggested approach is just-in-time, and thus does not require any static analysis of the network. That is, unlike prior work in this area, it covers any network type with no prior knowledge, c) offers a good formal analysis of proposed heuristic in terms of its components: staleness, memory capacity and recursive replay cost - their formalization covers previously published heuristics as well.  Experimental framework is sound. And some encouraging results are shown delivering memory capacity saving of 30% to 90% with training slowdown of 2x or less.\n\nPrior such work all required static analysis and planning of the network - and hence were of limited use. Significant contribution of this work is summed up at the end of Sec 4.3. It achieves 'as good' results as prior state-of-the-art (Checkmate, published at MLSys) - but without any prior knowledge of the model. This significantly increases the practical significance of this work. NeuIPS 2019 work of Kumar is the other often-cited work, also based on static planning, where the authors had already noted the primary limitation keeping it from getting adopted: \"algorithm yields asymptotically better schedules, the schedule length and memory depend exponentially on the path width\". \n\nI am also happy to see authors offer hardware detail of their experimental platform (Figs 2 and 4).  And PyTorch software prototype should make it easier to follow through in other frameworks. There is a decent variety in the chosen set of benchmarks as well.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119145, "tmdate": 1606915807985, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1417/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Review"}}}, {"id": "Vyzy7fCTQe", "original": null, "number": 4, "cdate": 1604648544539, "ddate": null, "tcdate": 1604648544539, "tmdate": 1605024450826, "tddate": null, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "invitation": "ICLR.cc/2021/Conference/Paper1417/-/Official_Review", "content": {"title": "Interesting work", "review": "Summary: This paper proposed a simple yet effective greedy algorithm with a new heuristics on checkpointing deep learning models so that people could train large model with restricted GPU memory budgets. The proposed method operates in an online setting and do not need static analysis of computation graph, thus could be used for both static and dynamic models. In a restricted model setting of linear forward network and equal space and time cost for each node, the author proves the proposed method could reach the same bound on tensor operation and memory budget with previous static checkpointing methods. The author also establish a theorem on tensor operation numbers between the proposed dynamical method and an optimal static checkpointing algorithm. In experiment, the author compared the proposed method with static techniques including the optimal Checkmate tool of Jain et al. (2020), showing the proposed method gives competitive performance without static model analysis in prior. The author also compared the proposed heuristics with prior arts on several static and dynamic models. Finally, the author described a prototype of PyTorch implementation of the proposed method. \n\nPros: \n1. While goes under a limited setting, the theoretic analysis on the tensor operation and memory budget bound of the proposed method, as well as on the relationship between the proposed method and optimal static analysis method is novel and interesting. The experiment also shows the competitiveness of the proposed method by comparing to static methods. \n2. The author does a great job explaining the idea, concepts, procedures and experiments. \n\nCons: \n1. The author provides the comparison between the proposed heuristics and others with the same greedy algorithm, but it seems not to have the full comparison to other dynamic checkpointing approach(e.g. Peng et al. (2020) ). Although experiments in the paper shows competitive results with static model, the main use cases of the proposed method might still be in dynamic models as in normal static model use cases, the time overhead of static analysis could be ignored compared to actual model training time.\n2. As the proposed heuristics bears some similarity with the one used in Peng et al. (2020), it would be more convincing to also have an ablation study of replacing the heuristics used in Peng et al. (2020) with the proposed one. \n\nReferences:\n\n[1] Jain, Paras, et al. \"Checkmate: Breaking the memory wall with optimal tensor rematerialization.\" Proceedings of Machine Learning and Systems 2 (2020): 497-511.\n\n[2] Peng, Xuan, et al. \"Capuchin: Tensor-based GPU Memory Management for Deep Learning.\" Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems. 2020.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1417/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1417/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Tensor Rematerialization", "authorids": ["jerry96@cs.washington.edu", "~Steven_Lyubomirsky1", "altanh@cs.washington.edu", "jrb@cs.washington.edu", "dh63@cs.washington.edu", "jroesch@cs.washington.edu", "~Tianqi_Chen1", "~Zachary_Tatlock1"], "authors": ["Marisa Kirisame", "Steven Lyubomirsky", "Altan Haan", "Jennifer Brennan", "Mike He", "Jared Roesch", "Tianqi Chen", "Zachary Tatlock"], "keywords": ["Rematerialization", "Memory-saving", "Runtime Systems", "Checkpointing"], "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an  $\\Omega(\\sqrt{N})$ memory budget with only $\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "one-sentence_summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kirisame|dynamic_tensor_rematerialization", "supplementary_material": "/attachment/59fee74aed52a2d277f26b02624b0809ad6fcb04.zip", "pdf": "/pdf/241e988e3953566bc4fe0e6a974d29ff78dfcc2e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkirisame2021dynamic,\ntitle={Dynamic Tensor Rematerialization},\nauthor={Marisa Kirisame and Steven Lyubomirsky and Altan Haan and Jennifer Brennan and Mike He and Jared Roesch and Tianqi Chen and Zachary Tatlock},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vfs_2RnOD0H}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Vfs_2RnOD0H", "replyto": "Vfs_2RnOD0H", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119145, "tmdate": 1606915807985, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1417/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1417/-/Official_Review"}}}], "count": 12}