{"notes": [{"id": "3T9iFICe0Y9", "original": "uvpwr0ZMGb2", "number": 2870, "cdate": 1601308318555, "ddate": null, "tcdate": 1601308318555, "tmdate": 1616054552335, "tddate": null, "forum": "3T9iFICe0Y9", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "The Recurrent Neural Tangent Kernel", "authorids": ["~Sina_Alemohammad1", "~Zichao_Wang1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Sina Alemohammad", "Zichao Wang", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Neural Tangent Kernel", "Recurrent Neural Network", "Gaussian Process", "Overparameterization"], "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alemohammad|the_recurrent_neural_tangent_kernel", "supplementary_material": "/attachment/dd17167fccf9dfa0b2f8f6ce87b96205c1f2b33b.zip", "pdf": "/pdf/0ede6a7293a24c88d58e7542b3c44d97270a2a0c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nalemohammad2021the,\ntitle={The Recurrent Neural Tangent Kernel},\nauthor={Sina Alemohammad and Zichao Wang and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3T9iFICe0Y9}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Y-CP-K-ful4", "original": null, "number": 1, "cdate": 1610040506001, "ddate": null, "tcdate": 1610040506001, "tmdate": 1610474113235, "tddate": null, "forum": "3T9iFICe0Y9", "replyto": "3T9iFICe0Y9", "invitation": "ICLR.cc/2021/Conference/Paper2870/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Reviewers agreed on the value of theoretical contribution, especially the surprising conclusion that the weight-tied and untied RNTK are identical. The empirical results were updated in response to reviewer's suggestion. I believe this would be of interest to ICLR audience."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Recurrent Neural Tangent Kernel", "authorids": ["~Sina_Alemohammad1", "~Zichao_Wang1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Sina Alemohammad", "Zichao Wang", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Neural Tangent Kernel", "Recurrent Neural Network", "Gaussian Process", "Overparameterization"], "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alemohammad|the_recurrent_neural_tangent_kernel", "supplementary_material": "/attachment/dd17167fccf9dfa0b2f8f6ce87b96205c1f2b33b.zip", "pdf": "/pdf/0ede6a7293a24c88d58e7542b3c44d97270a2a0c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nalemohammad2021the,\ntitle={The Recurrent Neural Tangent Kernel},\nauthor={Sina Alemohammad and Zichao Wang and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3T9iFICe0Y9}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3T9iFICe0Y9", "replyto": "3T9iFICe0Y9", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040505987, "tmdate": 1610474113220, "id": "ICLR.cc/2021/Conference/Paper2870/-/Decision"}}}, {"id": "8rnTixldWsY", "original": null, "number": 5, "cdate": 1605418292127, "ddate": null, "tcdate": 1605418292127, "tmdate": 1605418292127, "tddate": null, "forum": "3T9iFICe0Y9", "replyto": "6V1l0pBZjUD", "invitation": "ICLR.cc/2021/Conference/Paper2870/-/Official_Comment", "content": {"title": "Some clarifications regarding the experiments ", "comment": "We thank the reviewers for their supportive feedback and are delighted that you found our proposed sensitivity analysis useful for studying infinite width RNNs.  Below we address each of your comments. \n\n**The proposed method is restricted to the small data setting**: Indeed, a downside shared by all kernel methods is that their computational complexity scales quadratically with the signal length and number of samples. In this first paper on the RNTK, we focused on its derivation, theoretical analysis, interpretation, and proof-of-concept validation experiments. However, we agree that developing a computational framework for the RNTK (and other NTK kernels) to large-scale data is an interesting direction for future research. We also would like to emphasize that, despite its computational limitations, the RNTK can handle data of varying lengths, as highlighted in Theorem 1 and the Remark on page 4, in sharp contrast with other kernels, including the NTK.\n\n**It seems that the proposed RNTK cannot outperform other state-of-the-art (SOA) methods**: We respectfully disagree.  Table 1 demonstrates that, for a wide range of datasets, the RNTK outperforms SOA methods.  We report in Table 1 the aggregate statistics, i.e., the average accuracies, the Friedman ranking, and P90/50 across all datasets, which are common metrics used in the literature to show the superiority of a classifier compared to other classifiers on a set of datasets. The much larger Table 2 in the Appendix reports on the per-dataset accuracies to provide transparency in the results and simpler model comparison on a per-dataset basis. \n\n**Why are GRU and identity RNNs chosen?** The main goal of our paper is to propose a novel kernel based on the RNN and to compare its performance against standard kernels and other NTKs. That being said, comparing RNTK performances with the finite width regime RNNs with Gaussian initialization is also important to see how finite versus infinite width plays into the performances of generalization of architectures, for both theoretical and practical purposes. However, as finite width RNNs can suffer from training instabilities such as vanishing or exploding gradients, we see in Table 1 that Gaussian RNN has the worst performance among all other methods. We added improved versions of recurrent architectures that have more stable training and better generalization ability, such as Identity RNNs and GRU, to see these versions perform in comparison to kernels methods, and more importantly RNTK, mainly to hint practitioners on which method to use on such small size time-series datasets. \n\n**Clarification in Table 2**: Thanks for pointing out the difficulty of interpreting Table 2. We have highlighted the best performer for each dataset in the revised version. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2870/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Recurrent Neural Tangent Kernel", "authorids": ["~Sina_Alemohammad1", "~Zichao_Wang1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Sina Alemohammad", "Zichao Wang", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Neural Tangent Kernel", "Recurrent Neural Network", "Gaussian Process", "Overparameterization"], "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alemohammad|the_recurrent_neural_tangent_kernel", "supplementary_material": "/attachment/dd17167fccf9dfa0b2f8f6ce87b96205c1f2b33b.zip", "pdf": "/pdf/0ede6a7293a24c88d58e7542b3c44d97270a2a0c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nalemohammad2021the,\ntitle={The Recurrent Neural Tangent Kernel},\nauthor={Sina Alemohammad and Zichao Wang and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3T9iFICe0Y9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3T9iFICe0Y9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2870/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2870/Authors|ICLR.cc/2021/Conference/Paper2870/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843626, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2870/-/Official_Comment"}}}, {"id": "gIcUaJP8dND", "original": null, "number": 4, "cdate": 1605418077349, "ddate": null, "tcdate": 1605418077349, "tmdate": 1605418077349, "tddate": null, "forum": "3T9iFICe0Y9", "replyto": "DHh3gCEavs5", "invitation": "ICLR.cc/2021/Conference/Paper2870/-/Official_Comment", "content": {"title": "Thank you", "comment": "We thank the reviewers for their positive comments about our paper. We are pleased that you find the paper clear and heartily agree that our work provides a new foundation for not only the study of infinite width RNNs but also avenues to explore improved RNN and NTK architectures.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2870/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Recurrent Neural Tangent Kernel", "authorids": ["~Sina_Alemohammad1", "~Zichao_Wang1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Sina Alemohammad", "Zichao Wang", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Neural Tangent Kernel", "Recurrent Neural Network", "Gaussian Process", "Overparameterization"], "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alemohammad|the_recurrent_neural_tangent_kernel", "supplementary_material": "/attachment/dd17167fccf9dfa0b2f8f6ce87b96205c1f2b33b.zip", "pdf": "/pdf/0ede6a7293a24c88d58e7542b3c44d97270a2a0c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nalemohammad2021the,\ntitle={The Recurrent Neural Tangent Kernel},\nauthor={Sina Alemohammad and Zichao Wang and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3T9iFICe0Y9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3T9iFICe0Y9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2870/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2870/Authors|ICLR.cc/2021/Conference/Paper2870/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843626, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2870/-/Official_Comment"}}}, {"id": "I3QtHBS0blx", "original": null, "number": 3, "cdate": 1605418039819, "ddate": null, "tcdate": 1605418039819, "tmdate": 1605418039819, "tddate": null, "forum": "3T9iFICe0Y9", "replyto": "4OkzS-X4Flk", "invitation": "ICLR.cc/2021/Conference/Paper2870/-/Official_Comment", "content": {"title": "More empirical results are added", "comment": "We thank the reviewers for their constructive comments. Below are our responses for each point.\n\n**Clarifications and baseline Google stock price regression** : We have included more details on the experimental setup in the main text in the revised version. We thank the reviewer for the suggested baseline predictor for the Google stock price regression; we validated it for both the synthetic and the real world datasets and added the corresponding SNRs (Figures 4 a-d). This baseline strategy indeed provides competitive performance for the stock data, since it is well matched to this data. However, it significantly underperforms the other methods we consider (including the RNTK) for the sinusoid example we considered. We added the full details of this experiment in the revised version of the manuscript. \n\n**How we choose datasets from the UCR corpus**: We definitely did not cherry pick the datasets that we showcase in the paper. Our rationale behind the selection of the 53 datasets we considered was as follows. \nFirst, we selected the datasets from the UCR corpus with fewer than 1000 samples and 1000 time steps so that we could compare the RNTK to a number of other kernel methods (as we point out below, kernel methods of any kind tend to be computationally expensive, and we had to limit the run time involved in comparing a large number of kernels on a large number of datasets -- see our reply to Reviewer 3 below).\nSecond, from 88 remaining datasets, we removed datasets with an atypical number of samples versus number of classes (which consequently contain datasets with very few samples), such as the two datasets you have mentioned: Beef with 6 samples per class, Adiac with 10.54 samples per class, and other datasets like WordSynonyms with 10.68 samples per class. In our revised Table 1 and 2, we report results including these three datasets in the revised version:\n\nBeef - RNTK: 90%, NTK: 73.33%, RBF:83.33%, **Poly: 93.33%**, Gaussian RNN: 26.67%, GRU: 36.67%, identity RNN: 46.67%  \nAdiac - RNTK: 76.73%, NTK 71.87%, RBF: 73.40%, Poly: **77.75%**, Gaussian RNN: 51.4%, GRU: 60.61%, identity RNN: 16.88%  \nWordSynonyms - RNTK: 57.99%, NTK: 58.46%, RBF: 61.13%, Poly: **62.07%**, Gaussian RNN: 17.71%, GRU: 53.76%, identity RNN: 45.77%  \n\nIn the submitted paper, we also explored the RNTK\u2019s performance with two slightly longer signals \u2013 StarLightCures (1024 times steps) and SemgHandSubjectCh2 (1500 time steps) \u2013 and observed that the RNTK provides competitive performance. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2870/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Recurrent Neural Tangent Kernel", "authorids": ["~Sina_Alemohammad1", "~Zichao_Wang1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Sina Alemohammad", "Zichao Wang", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Neural Tangent Kernel", "Recurrent Neural Network", "Gaussian Process", "Overparameterization"], "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alemohammad|the_recurrent_neural_tangent_kernel", "supplementary_material": "/attachment/dd17167fccf9dfa0b2f8f6ce87b96205c1f2b33b.zip", "pdf": "/pdf/0ede6a7293a24c88d58e7542b3c44d97270a2a0c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nalemohammad2021the,\ntitle={The Recurrent Neural Tangent Kernel},\nauthor={Sina Alemohammad and Zichao Wang and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3T9iFICe0Y9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3T9iFICe0Y9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2870/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2870/Authors|ICLR.cc/2021/Conference/Paper2870/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843626, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2870/-/Official_Comment"}}}, {"id": "DHh3gCEavs5", "original": null, "number": 2, "cdate": 1603841118475, "ddate": null, "tcdate": 1603841118475, "tmdate": 1605032430291, "tddate": null, "forum": "3T9iFICe0Y9", "replyto": "3T9iFICe0Y9", "invitation": "ICLR.cc/2021/Conference/Paper2870/-/Official_Review", "content": {"title": "Limited in scope, but interesting, and excellent in the presentation", "review": "This paper studies the NTK of RNNs in the infinite-width limit, and shows a number of interesting features of such networks, that are somehow surprising knowing the problems of exploding gradients, or knowing the need for independence of parameters in proofs involving the NTK (for the tied weights giving the same as the untied case). While the techniques are not particularly new (they are relying on the applciation of techniques appearing in earlier papers), and the idea to look at RNNs is fairly straightforward, I think this is an interesting and useful paper, with nontrivial estimates (that look correct, although I may need a bit more time to check). The quality of the writing is extremely high, the notations are optimal, and it is overall a pleasure to read, and this paper will serve as an excellent basis for future investigations.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2870/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Recurrent Neural Tangent Kernel", "authorids": ["~Sina_Alemohammad1", "~Zichao_Wang1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Sina Alemohammad", "Zichao Wang", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Neural Tangent Kernel", "Recurrent Neural Network", "Gaussian Process", "Overparameterization"], "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alemohammad|the_recurrent_neural_tangent_kernel", "supplementary_material": "/attachment/dd17167fccf9dfa0b2f8f6ce87b96205c1f2b33b.zip", "pdf": "/pdf/0ede6a7293a24c88d58e7542b3c44d97270a2a0c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nalemohammad2021the,\ntitle={The Recurrent Neural Tangent Kernel},\nauthor={Sina Alemohammad and Zichao Wang and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3T9iFICe0Y9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3T9iFICe0Y9", "replyto": "3T9iFICe0Y9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2870/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086982, "tmdate": 1606915801754, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2870/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2870/-/Official_Review"}}}, {"id": "6V1l0pBZjUD", "original": null, "number": 1, "cdate": 1603825776456, "ddate": null, "tcdate": 1603825776456, "tmdate": 1605024113923, "tddate": null, "forum": "3T9iFICe0Y9", "replyto": "3T9iFICe0Y9", "invitation": "ICLR.cc/2021/Conference/Paper2870/-/Official_Review", "content": {"title": "overall this is a good submission", "review": "This paper extends NTK to RNN to explain behavior of RNNs in overparametrized case. It\u2019s a good extension study and interesting to see RNN with infinite-width limit converges to a kernel. The paper proves the same RNTK formula when the weights are shared and not shared. The proposed sensitivity for computationally friendly RNTK hyperparameter tuning is also insightful.\n\nWeakness:\n\nIn the experimental part, the paper claims they restrict the data with shorter and fewer samples. This may be a downside of the proposed method as the goal of RNNs is to handle various length of data samples. Also, it seems that the proposed RNTK cannot outperform other SOA methods. Any reason? Why are GRU and identity RNNs chosen? Please highlight the best number for each experiment for an easier comparison. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2870/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Recurrent Neural Tangent Kernel", "authorids": ["~Sina_Alemohammad1", "~Zichao_Wang1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Sina Alemohammad", "Zichao Wang", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Neural Tangent Kernel", "Recurrent Neural Network", "Gaussian Process", "Overparameterization"], "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alemohammad|the_recurrent_neural_tangent_kernel", "supplementary_material": "/attachment/dd17167fccf9dfa0b2f8f6ce87b96205c1f2b33b.zip", "pdf": "/pdf/0ede6a7293a24c88d58e7542b3c44d97270a2a0c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nalemohammad2021the,\ntitle={The Recurrent Neural Tangent Kernel},\nauthor={Sina Alemohammad and Zichao Wang and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3T9iFICe0Y9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3T9iFICe0Y9", "replyto": "3T9iFICe0Y9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2870/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086982, "tmdate": 1606915801754, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2870/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2870/-/Official_Review"}}}, {"id": "4OkzS-X4Flk", "original": null, "number": 3, "cdate": 1603953570673, "ddate": null, "tcdate": 1603953570673, "tmdate": 1605024113789, "tddate": null, "forum": "3T9iFICe0Y9", "replyto": "3T9iFICe0Y9", "invitation": "ICLR.cc/2021/Conference/Paper2870/-/Official_Review", "content": {"title": "Theoretical results are valuable,  empirical results are insufficient.", "review": "In this paper the neural tangent kernel for RNNs is derived. It is emphasized how this results in a proper kernel that can handle samples of different lengths.\n\nThe  theoretical derivations are correct to the best of my knowledge and help to complete the picture of deep architectures for which the NTK has been derived. This is a valuable contribution.\n\nMy main concern is on the empirical side. Since the sinusoid regression is a toy problem I'll focus on the other two:\n\n- Google stock price regression: Appendix A2 doesn't provide much insight into the experiment setup, but it seems that the objective is to predict the stock price of the next day based on the previous ones. I would assume that a strong baseline for this task will be to simply predict the stock price to be the same as the previous day. This is a trivial predictor that requires a training set size of zero. So if you were to plot its performance on Figs 4c and 4d, which (constant) SNR would it yield? I'd be surprised if it is beaten (and that would be a very interesting result), but also I'd be surprised if it's not matched (seems a simple enough predictor to discover if there is no overfitting). However, those graphs show a big spread between different predictors, which is not intuitive given the previous sentences. Can you pleaser report the SNR of \"my\" trivial predictor and explain the apparent discrepancy?\n\n- The 53 UCR classification datasets: Why those 53? The database contains 128, and your paper states that you took \"data sets with fewer than 1000 training samples and fewer than 1000 time steps\". But datasets \"Adiac\" and \"Beef\" fulfill those requirements and were not taken. On the other hand, \"StarLightCurves\" doesn't seem to fulfill them (1024 steps) and yet you took it. Can you clearly state how you chose those 53 datasets out of the 128 available?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2870/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2870/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Recurrent Neural Tangent Kernel", "authorids": ["~Sina_Alemohammad1", "~Zichao_Wang1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Sina Alemohammad", "Zichao Wang", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Neural Tangent Kernel", "Recurrent Neural Network", "Gaussian Process", "Overparameterization"], "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alemohammad|the_recurrent_neural_tangent_kernel", "supplementary_material": "/attachment/dd17167fccf9dfa0b2f8f6ce87b96205c1f2b33b.zip", "pdf": "/pdf/0ede6a7293a24c88d58e7542b3c44d97270a2a0c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nalemohammad2021the,\ntitle={The Recurrent Neural Tangent Kernel},\nauthor={Sina Alemohammad and Zichao Wang and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=3T9iFICe0Y9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3T9iFICe0Y9", "replyto": "3T9iFICe0Y9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2870/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086982, "tmdate": 1606915801754, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2870/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2870/-/Official_Review"}}}], "count": 8}