{"notes": [{"id": "rygiEL8FOV", "original": "r1ljopl7u4", "number": 33, "cdate": 1553716787033, "ddate": null, "tcdate": 1553716787033, "tmdate": 1562083044152, "tddate": null, "forum": "rygiEL8FOV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Context Mover's Distance & Barycenters: Optimal transport of contexts for building representations", "authors": ["Sidak Pal Singh", "Andreas Hug", "Aymeric Dieuleveut", "Martin Jaggi"], "authorids": ["sidak.singh@epfl.ch", "andreas.hug@epfl.ch", "aymeric.dieuleveut@epfl.ch", "martin.jaggi@epfl.ch"], "keywords": ["optimal transport", "Wasserstein distance and barycenters", "representation learning", "NLP", "sentence similarity", "entailment"], "TL;DR": "Represent each entity as a probability distribution over contexts embedded in a ground space.", "abstract": "We present a framework for building unsupervised representations of entities and their compositions, where each entity is viewed as a probability distribution rather than a fixed length vector. In particular, this distribution is supported over the contexts which co-occur with the entity and are embedded in a suitable low-dimensional space. This enables us to consider the problem of representation learning with a perspective from Optimal Transport and take advantage of its numerous tools such as Wasserstein distance and Wasserstein barycenters. We elaborate how the method can be applied for obtaining unsupervised representations of text and illustrate the performance quantitatively as well as qualitatively on tasks such as measuring sentence similarity and word entailment, where we empirically observe significant gains (e.g., 4.1% relative improvement over Sent2vec and GenSen).\n\nThe key benefits of the proposed approach include: (a) capturing uncertainty and polysemy via modeling the entities as distributions, (b) utilizing the underlying geometry of the particular task (with the ground cost), (c) simultaneously providing interpretability with the notion of optimal transport between contexts and (d) easy applicability on top of existing point embedding methods. In essence, the framework can be useful for any unsupervised or supervised problem (on text or other modalities); and only requires a co-occurrence structure inherent to many problems. The code, as well as pre-built histograms, are available under https://github.com/context-mover.", "pdf": "/pdf/9f83f973d86df64a6927fccab80d88cdb0675306.pdf", "paperhash": "singh|context_movers_distance_barycenters_optimal_transport_of_contexts_for_building_representations"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "HJenDaCGcN", "original": null, "number": 2, "cdate": 1555389795567, "ddate": null, "tcdate": 1555389795567, "tmdate": 1556906128247, "tddate": null, "forum": "rygiEL8FOV", "replyto": "rygiEL8FOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper33/Official_Review", "content": {"title": "Review", "review": "This paper proposes to construct word embeddings from a histogram over context words, instead of as point vectors, which allows for measuring distances between two words in terms of optimal transport between the histograms. On sentence similarity and word entailment tasks, the method is competitive with previous approaches, although not by a huge margin. \n\nThe paper proposes a method to augment representation of an entity (such as a word) from standard \"point in a vector space\" to a histogram with bins located at some points in that vector space. In this model, the bins correspond the context objects, the location of which are the standard point embedding of those objects, and the histogram weights correspond to the strength of the contextual association. The distance between two representations is then measured with, Context Mover Distance, based on the theory of optimal transport, which is suitable for computing the discrepancy between distributions. \n\nPros\n- Mathematically elegant method to represent words as distributional estimates of context words.\n- Novel idea to use wasserstein barycenter to measure sentence similarity\n- Novel idea to use Wasserstein distance for hypernym detection.\n\nCons:\n- Results do not show significant improvement over baselines.\n- Potentially complicated for practitioners in the community. Computing CMD and wasserstein barycenters is not trivial and can be inefficient. For this method to be practically useful (and see wide adoption), I believe there has to be a compelling use case for using distributional estimates as oppose to standard point estimates, which isn't demonstrated in the paper. Nevertheless, I believe this paper makes an important contribution. ", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context Mover's Distance & Barycenters: Optimal transport of contexts for building representations", "authors": ["Sidak Pal Singh", "Andreas Hug", "Aymeric Dieuleveut", "Martin Jaggi"], "authorids": ["sidak.singh@epfl.ch", "andreas.hug@epfl.ch", "aymeric.dieuleveut@epfl.ch", "martin.jaggi@epfl.ch"], "keywords": ["optimal transport", "Wasserstein distance and barycenters", "representation learning", "NLP", "sentence similarity", "entailment"], "TL;DR": "Represent each entity as a probability distribution over contexts embedded in a ground space.", "abstract": "We present a framework for building unsupervised representations of entities and their compositions, where each entity is viewed as a probability distribution rather than a fixed length vector. In particular, this distribution is supported over the contexts which co-occur with the entity and are embedded in a suitable low-dimensional space. This enables us to consider the problem of representation learning with a perspective from Optimal Transport and take advantage of its numerous tools such as Wasserstein distance and Wasserstein barycenters. We elaborate how the method can be applied for obtaining unsupervised representations of text and illustrate the performance quantitatively as well as qualitatively on tasks such as measuring sentence similarity and word entailment, where we empirically observe significant gains (e.g., 4.1% relative improvement over Sent2vec and GenSen).\n\nThe key benefits of the proposed approach include: (a) capturing uncertainty and polysemy via modeling the entities as distributions, (b) utilizing the underlying geometry of the particular task (with the ground cost), (c) simultaneously providing interpretability with the notion of optimal transport between contexts and (d) easy applicability on top of existing point embedding methods. In essence, the framework can be useful for any unsupervised or supervised problem (on text or other modalities); and only requires a co-occurrence structure inherent to many problems. The code, as well as pre-built histograms, are available under https://github.com/context-mover.", "pdf": "/pdf/9f83f973d86df64a6927fccab80d88cdb0675306.pdf", "paperhash": "singh|context_movers_distance_barycenters_optimal_transport_of_contexts_for_building_representations"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper33/Official_Review", "cdate": 1554234174028, "reply": {"forum": "rygiEL8FOV", "replyto": "rygiEL8FOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234174028, "tmdate": 1556906090381, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "BylxXI3M9N", "original": null, "number": 1, "cdate": 1555379735761, "ddate": null, "tcdate": 1555379735761, "tmdate": 1556906128027, "tddate": null, "forum": "rygiEL8FOV", "replyto": "rygiEL8FOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper33/Official_Review", "content": {"title": "A reasonable proposal for distributionally representing words. Some issues with clarity in writing.", "review": "Positives:\n- The results are straightforward, and compare to the standard baselines (SIF, infersent etc). I think these are not SoTA numbers but the relative comparisons seem fine, especially for a workshop paper. The addition of supplementary ablation experiments is also much appreciated.\n- The approach is fairly clear, and there's no real big holes in terms of how the problem is set up and solved. \n\nConceptual:\n- I'm a bit unhappy with how the entailment experiments were set up. The narrative up to the last paragraph (\"For this purpose, ..\") is fairly clear that one wants to find distributions that are 'contained in the support' of another. You could directly do this on the PPMI matrix you have and define a measure of 'containment' in terms of distributions. Instead of this, you end up plugging in a (heurisitic) ground metric and computing embeddings. This seems haphazard, and I'm not terribly convinced this makes sense. Are you doing consistency checks like making sure you dont have negative-cost-cycles?\n\nClarity:\n- Throughout the paper, there are all sorts of minor unsupported side-remarks and claims that should be stripped. The paper has a fairly straightforward main story and positive results; the addition of these remarks just detract from the rest of the paper. The authors should ask themselves - would I be willing to write a supplemental proof to make this statement precise? or is it just a side-remark I can remove. \n- The explanation surrounding equation 4 is fairly confusing. I guess what's happening is that you're taking each context, embedding it to some metric space, and weighting it by its bin count. This is quite the sudden leap, since until this point it wasn't really clear that you were going to embed the contexts with a base embedding. Is this an approximation to the actual thing you want to compute (OT over contexts?) or a way to induce the distances? I think its a little bit of both, but the explanation here needs to be more carefully thought out and laid out. I think it would help to make the inputs and outputs to your method precise. \n- 'We also consider adding the information from point estimate into the distributional estimate to get best of both the worlds.' - you should motivate why you want to do this ('best of both worlds' is extremely unclear.. what problem are you solving exactly?). You should then precisely state what you're doing. Your setup Eq (4) is already a bit confusing, so you need to be a bit careful when building on it. I guess the point estimate being added here is the original Glove embedding?\n- 'Since the contexts are dense embeddins' - you really need to explain this. A context is a context, not an embedding - I think the more precise statement is that a context can be mapped to a dense embedding. I assume it's something like, you start with point estimates (i.e. glove vectors) of contexts, so you can treat each context as being assigned an associated vector. You then cluster this, and sum over the clusters. I'm also not sure if summing normalized SPPMI values makes sense as an object. Shouldn't you merge the contexts and then compute the SPPMI again over the 'combined' context? Either way, this part can be made much more clear.\n- I'm not sure why barycenters is obviously better.. if you have polysemy, you'd want to select the meaning that's implied by all the words, and reject any others. The barycenter does not do this, because you're still incurring costs from the word sense that's not being used in the sentence. I guess it's better than the alternatives?\n- In the paragraph connecting SIF and CoMB - i have no idea what the precise connection is. You should write down propositions and equations for any precise statements like this one (or remove it).\n\nMinor comment:\n- 'Also, KL-divergence isn\u2019t defined when the supports of distributions under comparison don\u2019t fully overlap' - is false, you only need absolute continuity, meaning you don't need full overlap - just that the support of the distribution inside the log must be a subset of the support of the distribution outside the log. \n- 'Hence, one potential application could be in checking for the implicit bias in point estimates (Bolukbasi et al., 2016) and then correcting it via the ground cost.' - this is a pretty vacuous statement - you could have compared pairwise distances for word embeddings to correct it, for example. I honestly think the throwaway comments like this one and the one above should just be stripped from the paper. \n- The section 'Relation between the histogram and point estimates.' is similarly vacuous. Yes, count based methods use histograms and neural networks use vectors. Yes, your paper kind of uses both. I really do not think pointing this out adds much insight to your paper. It may be that you had something more profound to say, but it doesn't quite come though.\n- 'A practical take-home message of this work thus is to not throw away the co-occurrence information e.g. when using GloVe, but to instead pass it on to our method.' - should be moved to the discussion. \n- You may also want to put the timings in the experiment - runtimes are somewhat useless without matching accuracy numbers for approximate algorithms such as sinkhorn. What's the relative (percent) error on your wasserstein distance estimates?\n- ' In fact, Figure 3 says it all,' - in fact, figure 3 does not say it all because it's a particular example projected into 2d without very much explanation. In fact i'd argue that it's not a terribly enlightening - what is the ground truth supposed to look like? why is euclidean averaging bad? \n", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context Mover's Distance & Barycenters: Optimal transport of contexts for building representations", "authors": ["Sidak Pal Singh", "Andreas Hug", "Aymeric Dieuleveut", "Martin Jaggi"], "authorids": ["sidak.singh@epfl.ch", "andreas.hug@epfl.ch", "aymeric.dieuleveut@epfl.ch", "martin.jaggi@epfl.ch"], "keywords": ["optimal transport", "Wasserstein distance and barycenters", "representation learning", "NLP", "sentence similarity", "entailment"], "TL;DR": "Represent each entity as a probability distribution over contexts embedded in a ground space.", "abstract": "We present a framework for building unsupervised representations of entities and their compositions, where each entity is viewed as a probability distribution rather than a fixed length vector. In particular, this distribution is supported over the contexts which co-occur with the entity and are embedded in a suitable low-dimensional space. This enables us to consider the problem of representation learning with a perspective from Optimal Transport and take advantage of its numerous tools such as Wasserstein distance and Wasserstein barycenters. We elaborate how the method can be applied for obtaining unsupervised representations of text and illustrate the performance quantitatively as well as qualitatively on tasks such as measuring sentence similarity and word entailment, where we empirically observe significant gains (e.g., 4.1% relative improvement over Sent2vec and GenSen).\n\nThe key benefits of the proposed approach include: (a) capturing uncertainty and polysemy via modeling the entities as distributions, (b) utilizing the underlying geometry of the particular task (with the ground cost), (c) simultaneously providing interpretability with the notion of optimal transport between contexts and (d) easy applicability on top of existing point embedding methods. In essence, the framework can be useful for any unsupervised or supervised problem (on text or other modalities); and only requires a co-occurrence structure inherent to many problems. The code, as well as pre-built histograms, are available under https://github.com/context-mover.", "pdf": "/pdf/9f83f973d86df64a6927fccab80d88cdb0675306.pdf", "paperhash": "singh|context_movers_distance_barycenters_optimal_transport_of_contexts_for_building_representations"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper33/Official_Review", "cdate": 1554234174028, "reply": {"forum": "rygiEL8FOV", "replyto": "rygiEL8FOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234174028, "tmdate": 1556906090381, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper33/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "ryevD4dw9N", "original": null, "number": 1, "cdate": 1555690591024, "ddate": null, "tcdate": 1555690591024, "tmdate": 1556906127775, "tddate": null, "forum": "rygiEL8FOV", "replyto": "rygiEL8FOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper33/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context Mover's Distance & Barycenters: Optimal transport of contexts for building representations", "authors": ["Sidak Pal Singh", "Andreas Hug", "Aymeric Dieuleveut", "Martin Jaggi"], "authorids": ["sidak.singh@epfl.ch", "andreas.hug@epfl.ch", "aymeric.dieuleveut@epfl.ch", "martin.jaggi@epfl.ch"], "keywords": ["optimal transport", "Wasserstein distance and barycenters", "representation learning", "NLP", "sentence similarity", "entailment"], "TL;DR": "Represent each entity as a probability distribution over contexts embedded in a ground space.", "abstract": "We present a framework for building unsupervised representations of entities and their compositions, where each entity is viewed as a probability distribution rather than a fixed length vector. In particular, this distribution is supported over the contexts which co-occur with the entity and are embedded in a suitable low-dimensional space. This enables us to consider the problem of representation learning with a perspective from Optimal Transport and take advantage of its numerous tools such as Wasserstein distance and Wasserstein barycenters. We elaborate how the method can be applied for obtaining unsupervised representations of text and illustrate the performance quantitatively as well as qualitatively on tasks such as measuring sentence similarity and word entailment, where we empirically observe significant gains (e.g., 4.1% relative improvement over Sent2vec and GenSen).\n\nThe key benefits of the proposed approach include: (a) capturing uncertainty and polysemy via modeling the entities as distributions, (b) utilizing the underlying geometry of the particular task (with the ground cost), (c) simultaneously providing interpretability with the notion of optimal transport between contexts and (d) easy applicability on top of existing point embedding methods. In essence, the framework can be useful for any unsupervised or supervised problem (on text or other modalities); and only requires a co-occurrence structure inherent to many problems. The code, as well as pre-built histograms, are available under https://github.com/context-mover.", "pdf": "/pdf/9f83f973d86df64a6927fccab80d88cdb0675306.pdf", "paperhash": "singh|context_movers_distance_barycenters_optimal_transport_of_contexts_for_building_representations"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper33/Decision", "cdate": 1554814604946, "reply": {"forum": "rygiEL8FOV", "replyto": "rygiEL8FOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814604946, "tmdate": 1556906100339, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}