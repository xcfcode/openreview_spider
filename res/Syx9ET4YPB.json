{"notes": [{"id": "Syx9ET4YPB", "original": "rygozjmwDH", "number": 497, "cdate": 1569439026339, "ddate": null, "tcdate": 1569439026339, "tmdate": 1577168238568, "tddate": null, "forum": "Syx9ET4YPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "fVnqGoAxk", "original": null, "number": 1, "cdate": 1576798698179, "ddate": null, "tcdate": 1576798698179, "tmdate": 1576800937632, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "Syx9ET4YPB", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposed to evaluate the robustness of CNN models on similar video frames. The authors construct two carefully labeled video databases. Based on extensive experiments, they conclude that the state of the art classification and detection models are not robust when testing on very similar video frames. While Reviewer #1 is overall positive about this work, Reviewer #2 and #3 rated weak reject with various concerns. Reviewer #2 concerns limited contribution since the results are similar to our intuition. Reviewer #3 appreciates the value of the databases, but concerns that the defined metrics make the contribution look huge. The authors and Reviewer #3 have in-depth discussion on the metric, and Reviewer #3 is not convinced. Given the concerns raised by the reviewers, the ACs agree that this paper can not be accepted at its current state.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syx9ET4YPB", "replyto": "Syx9ET4YPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729410, "tmdate": 1576800281994, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper497/-/Decision"}}}, {"id": "r1lxp3pjjr", "original": null, "number": 6, "cdate": 1573801144412, "ddate": null, "tcdate": 1573801144412, "tmdate": 1573801144412, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "Hke6nQt5ir", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment", "content": {"title": "Adversarial Accuracy is Worst Case accuracy in Lp ball.", "comment": " > \u201cAdversarial papers do not \"reports the maximum error made in a \\delta ball around each image\": there is no exhaustive search, where one would feed the network with all possible images at \\delta distance. Each adversarial method returns one and only one image, which is estimated to be the worst case error in this \\delta radius. The accuracy is then computed as: (number of images for which the network got the right answer) / (number of image given to the network) In other words, the canonical definition of accuracy.\u201d\n \nWe respectfully disagree with this assertion made by reviewer 3.\n\nThe definition of adversarial robustness involves the maximum possible error in a perturbation set. For Lp adversaries, this would be the existence of a misclassified example within a delta ball *irrespective* of how many examples within that delta ball are tested.  A perfect attack would by definition have a higher error rate than the 1e9 example attack mentioned by the reviewer, and be considered a valid attack giving a score of 0% on that example if a single adversarial example exists in the delta ball that is misclassified.\n\nWith regards to the experiment Reviewer 3 outlined. We refer the reviewer to https://arxiv.org/abs/1902.02322.  In the evaluation section, Carlini performs an almost identical experiment as the reviewer suggested and shows that the attack is 100% successful (Carlini only generates 25 examples not 1e9). This method is seen by the community as a valid way to attack models. This technique is also related to random-restarts, another commonly used approach in generating adversarial examples. Here, noise is added to the input image and a search procedure such as PGD is started from each instantiation of the noise (see Madry et al, 2017 https://arxiv.org/pdf/1706.06083.pdf, Section 3.1). \n\n\nFurthermore, the predominant technique to generate adversarial examples, Projected Gradient Descent, often searches through *many* examples before finding an incorrectly classified adversarial example. In fact, Carlini et al\u2019s monograph \u201cOn Evaluating Adversarial Robustness\u201d (https://arxiv.org/pdf/1902.06705.pdf)  mentions in Section 4.8:\n\n \" ```For example, on CIFAR-10 or ImageNet with a maximum \u2113\u221e distortion of 8/255, white-box optimization attacks generally converge in under 100 or 1000 iterations with a step size of 1. However, black-box attacks often take orders of magnitude more queries, with attacks requiring over 100,000 queries not being abnormal. For a different dataset, or a different distortion metric, attacks will require a different number of iterations. While it is possible to perform an attack correctly with very few iterations of gradient descent, it requires much more thought and care (Engstrom et al., 2018). For example, in some cases even a million iterations of white-box gradient descent have been found necessary (Qian & Wegman, 2018).```\"\n\nEach step of PGD generates a potential adversarial example (a different image), and up 1e6 iterations are necessary in some cases. If we measure accuracy as reviewer 3 suggests, an adversarial example that takes 1e6 PGD iterations to generate had (1e6 - 1) correctly classified examples and 1 incorrect example, but the adversarial accuracy on this example is reported as 0% not 99.9999% (note all (1e6 - 1) correctly classified examples are in fact within the delta ball).\n\nWe thus stand by the definition of \u201caccuracy drop\u201d used in our paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx9ET4YPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper497/Authors|ICLR.cc/2020/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170606, "tmdate": 1576860551742, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment"}}}, {"id": "Hke6nQt5ir", "original": null, "number": 5, "cdate": 1573716917110, "ddate": null, "tcdate": 1573716917110, "tmdate": 1573716917110, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "SkxbfbuVjS", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment", "content": {"title": "Accuracy is accuracy", "comment": "> However, we argue that the PM-k \u201caccuracy drop\u201d is analogous to the \u201cadversarial accuracy drop\u201d claimed in the adversarial example and robustness literature\n\nAdversarial papers do not \"reports the maximum error made in a \\delta ball around each image\": there is no exhaustive search, where one would feed the network with all possible images at \\delta distance. Each adversarial method returns one and only one image, which is estimated to be the worst case error in this \\delta radius. The accuracy is then computed as: (number of images for which the network got the right answer) / (number of image given to the network)\nIn other words, the canonical definition of accuracy.\n\nNonetheless, an illustrative parallel can be drawn with adversarial attacks. Suppose that one invent a new adversarial approach, which goes as follow:\n1. For each image in the dataset, generate 1,000,000,000 adversarial images inside the \\delta radius, using various existing methods and adding a bit of randomness to obtain slightly different perturbed image each time. If the network misclassifies a _single_ one of these 1,000,000,000 tries, then we consider it to have failed.\n2. Compute the accuracy as M/N, where N is the number of images in the dataset and M the number of images for which the network correctly classified every single one of the 1B associated adversarial examples.\n\nSuch methodology would definitely obtain a quite spectacular drop in \"accuracy\" compared to the state of the art. Is it still meaningful to talk about accuracy in this context, though? I do not think so, and it is the same thing here. \n\nThe second sentence of my point 2 above was indeed incorrect. Overall, the contributions of the paper are thus:\n1) A human validation of two existing datasets of video perturbations, which results in a 4% relative accuracy improvement for the reference frame and ~6% for the frames around.\n2) Results for different networks on a metric which cannot be compared to any other work.\n\nWe add to this the misleading abstract wording and intro (\"We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets.\"). Realistically speaking, what would a reasonably knowledgeable reader think when reading the words \"median accuracy drop of 16%\":\nA) wow, that's a significant drop compared to the mere 3% reported in previous work, they probably uncover something very important and up to now unseen, or \nB) well, \"median accuracy drop\" probably means something different than in any textbook or previous work on the matter, nothing to see here.\n\nSure, presenting the actual median accuracy drop would have resulted in less impressive numbers. And that's precisely my point.\n\nMy opinion thus remains. The results presented in the paper are not plainly wrong, but the contributions are very slim yet worded in a way that makes them look huge. I do not see value in such an overstated paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper497/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper497/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx9ET4YPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper497/Authors|ICLR.cc/2020/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170606, "tmdate": 1576860551742, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment"}}}, {"id": "SkxbfbuVjS", "original": null, "number": 4, "cdate": 1573318920879, "ddate": null, "tcdate": 1573318920879, "tmdate": 1573318920879, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "rkxcWM73FH", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "We understand R3\u2019s concern that calling the worst-case pm-k metric an \u201caccuracy drop\u201d might be misleading. However, we argue that the PM-k \u201caccuracy drop\u201d is analogous to the \u201cadversarial accuracy drop\u201d claimed in the adversarial example and robustness literature [Athalye et al (2018) https://arxiv.org/pdf/1802.00420.pdf,  Madry et al. (2017) https://arxiv.org/pdf/1706.06083.pdf]. When reporting adversarial accuracy of a classifier on a dataset, one reports the maximum error made in a \\delta ball around each image; in this work we are reporting the maximum error over a set of naturally perturbed images temporally and visually close to the anchor frame. The K in PM-K behaves analogously to \\delta in L_p adversaries in that the adversarial accuracy is monotonically decreasing in \\delta. \n\nUnfortunately, we cannot follow R3's comment that \"[human review] resulted in a relative accuracy improvement of about 4% for the reference frame (Table 4, column Original). The improvement for the perturbed frames are not actually provided.\" In Table 4, we do show the improvement on the perturbed frames in the column labeled \"Perturbed\", and show that human review results in a 5-6% improvement on ImageNet-Vid-Robust and YTBB-Robust.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx9ET4YPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper497/Authors|ICLR.cc/2020/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170606, "tmdate": 1576860551742, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment"}}}, {"id": "B1l7zeOVir", "original": null, "number": 3, "cdate": 1573318667292, "ddate": null, "tcdate": 1573318667292, "tmdate": 1573318667292, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "Skx8zfYntr", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for their comments.\n\nRegarding the stringency of our metric: the worst-case nature of our metric is inspired by adversarial robustness, where worst-case notions are commonplace. Moreover, adversarial robustness is more stringent than our metric since it allows *any* l_p bounded perturbation, not only images occurring naturally in videos. The goal of our paper is to find a middle ground between standard accuracy (which does not measure robustness) and l_p adversarial robustness. We refer the reviewer to our more detailed response to R3 regarding the metric.\n\nFurther, while we are happy to hear that the results match R2's expectations, we argue that systematically measuring robustness is critical for progress in this direction. Especially for the question of robustness, it is crucial to rigorously quantify various phenomena. We believe a key contribution of our work is in verifying the intuitions of R2 (and possibly others in the field). \n\nUpon R2's request, we compute the accuracy drop on a random frame in the nearest 20 frames for the best performing models on the two datasets (a fine-tuned Resnet-152 in both cases).\n\nFor Youtube-BB-robust the anchor accuracy was 92.8%, the PM-K accuracy was 84.6%, and the random neighbor frame accuracy was 93.1%.\n\nFor ImageNet-Vid-Robust, anchor accuracy was 84.8%, PM-K accuracy was 70.1%, and a random neighbor frame accuracy was 84.1%.\n\nThe accuracy differences between the anchor frame accuracy and sampling a random frame are within error bars.  This is to be expected: Our anchor frames are randomly sampled, so sampling random frames within a neighborhood of the anchor frames is analogous to a new, iid draw of anchor frames, resulting in little change in accuracy. The adversarial nature of our PM-K metric is the reason for the large drops.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx9ET4YPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper497/Authors|ICLR.cc/2020/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170606, "tmdate": 1576860551742, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment"}}}, {"id": "S1lLhkuVjH", "original": null, "number": 2, "cdate": 1573318574263, "ddate": null, "tcdate": 1573318574263, "tmdate": 1573318574263, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "S1eNC80O5B", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for their detailed comments and are happy to hear that they found the paper well organized with a thorough evaluation.\n\nRegarding details about the curation process: The expert annotators in this case were the authors of the paper. We designed a custom interface for annotating frames, which displayed the anchor frame next to a single frame that is nearby in the video. For each pair, the annotators stated whether the nearby frame was similar or dissimilar to the anchor frame. We will add these details, along with a depiction of the interface, in the appendix in the next revision. A few screenshots of the UI can be found here: https://pictureweb.s3-us-west-2.amazonaws.com/iclr/ui.html\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper497/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx9ET4YPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper497/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper497/Authors|ICLR.cc/2020/Conference/Paper497/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170606, "tmdate": 1576860551742, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper497/Authors", "ICLR.cc/2020/Conference/Paper497/Reviewers", "ICLR.cc/2020/Conference/Paper497/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper497/-/Official_Comment"}}}, {"id": "rkxcWM73FH", "original": null, "number": 1, "cdate": 1571725825661, "ddate": null, "tcdate": 1571725825661, "tmdate": 1572972588075, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "Syx9ET4YPB", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents new datasets based on ImageNet and Youtube-BB to assert networks performance consistency across time. Compared to previous work, it uses human labeler to further validate the dataset and discard frames that are deemed too different from the reference one. It provides results on image classification and detection using popular network architectures. Based on these results, the paper claims an accuracy drop of 10 to 16%.\n\nThe main contribution of this paper is to introduce a new, human annotated dataset for robustness assessment of image classifiers. In itself, it is valuable work, but it is not clear if the contribution is important enough for ICLR. However, I would still be ok with accepting the paper (better datasets are always useful) if it was not for the way the results are reported. I do not specifically have issues with \"more stringent robustness metric\" but it should not be used to claim incredible results (like an accuracy drop of 10 to 16% instead of 3% for previous work (Real et al. 2017)).\n\nThere is one thing for sure: using \"accuracy drop\" in this context is just misleading. The underlying concept to which \"accuracy\" refers is _not_ the \"maximum error made by the network over the whole set of images\". By this definition of accuracy, if the number of images around the reference frame were 100, missing a _single one_ each time (that is 99% of actual accuracy) would result, according to this peculiar redefinition, to a _0%_ accuracy. This is actually highlighted in Appendix G.1: the \"accuracy\" trend can only go down, since every supplemental frame brings one more chance to fail and obtain a 0% accuracy for this set of perturbed images.\nSame thing goes for the detection, where the only frame that matters among all is the one _minimizing_ the AP. Same thing in Table 4, where the \"accuracy\" of the Original column means one thing (the amount of correctly identified images over the total number of images) while the \"accuracy\" of the Perturbed column right next to it means something completely different. Same thing in Table 2, which even provides a \"delta\" between two unrelated metrics.\nI cannot see how this can be justified. Sure, there could be some usage for such strict metric, but again, this is _not_ accuracy and cannot be compared to any previous results. Having a more stringent metric is one thing, but in this case it just seems like a justification to get high drop numbers.\n\nKeeping that in mind, these are the actual conclusions we can make from the paper:\n1) Human reviewers removed or changed about 20% of the frames\n2) This resulted in a relative accuracy improvement of about 4% for the reference frame (Table 4, column Original). The improvement for the perturbed frames are not actually provided.\n3) The comparison (and improvements) to previous work due to the dataset cleaning remains unclear.\n4) Comparison between different networks and training procedures\n\nOverall, the paper presents impressive numbers but does not actually back them up. I am open to eventually consider acceptance given the value of the datasets, but the paper would then require a significant overhaul to remove all confusing aspects."}, "signatures": ["ICLR.cc/2020/Conference/Paper497/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper497/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx9ET4YPB", "replyto": "Syx9ET4YPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575600895975, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper497/Reviewers"], "noninvitees": [], "tcdate": 1570237751277, "tmdate": 1575600895987, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper497/-/Official_Review"}}}, {"id": "Skx8zfYntr", "original": null, "number": 2, "cdate": 1571750414143, "ddate": null, "tcdate": 1571750414143, "tmdate": 1572972588042, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "Syx9ET4YPB", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper targets on the evaluation of model robustness on similar video frames. The authors build two carefully labeled video datasets, and extensive experiments are conducted to show that the state-of-the-art classification and detection models are not robust enough when dealing with very similar video frames.  The results are similar to my intuitive feelings.\n\nThe authors propose acc_orig (the average acc) and acc_pmk (which chooses the worse one in nearest 2k frames) to amplify the gap. I personally think acc_pmk is too stringent. I wonder if there is still large gaps if we choose a random frame in the nearest 2k frames.\n\nThe authors have tried fine-tuning and data augmentation techniques to improve the robustness, although the performance is improved, the gap between acc_orig and acc_pmk does not change much.\n\nThe paper has done many work to analyze the robustness of image classification and detection models, however, the results are expected and no effective methods are proposed to improve the results. Overall, the contribution is limited. "}, "signatures": ["ICLR.cc/2020/Conference/Paper497/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper497/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx9ET4YPB", "replyto": "Syx9ET4YPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575600895975, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper497/Reviewers"], "noninvitees": [], "tcdate": 1570237751277, "tmdate": 1575600895987, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper497/-/Official_Review"}}}, {"id": "S1eNC80O5B", "original": null, "number": 3, "cdate": 1572558540083, "ddate": null, "tcdate": 1572558540083, "tmdate": 1572972588010, "tddate": null, "forum": "Syx9ET4YPB", "replyto": "Syx9ET4YPB", "invitation": "ICLR.cc/2020/Conference/Paper497/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary\nIn this paper, the authors curated two datasets: ImageNet-Vid and Youtube-BB in order to create human-reviewed perceptibly similar sets (Imagenet-Vid-Robust and YTBB-Robust). The obtained datasets are evaluated over 45 different models pre-trained on ImageNet in order to see their drop in accuracy on natural perturbations. Three detection models are also evaluated and show that not only classification models are sensitive to these perturbations, but that it also yields to localization errors.\n\nComments\nThe paper is clear, well organized, well written and easy to follow.\nThe authors present two novel datasets grouped in sets of perceptibly similar images and answer to the following hypothesis: Can the perturbations occurring naturally in videos be a realistic robustness challenge?\nThe thorough evaluation over the curated datasets shows pretty well that the changes in the model prediction are indeed due to a lack of robustness of the models themselves rather than the difference occurring from one frame to the other (occlusion etc).\nThe authors mention the curation was done with the help of expert human annotators. Details could be added as to how the annotators are considered experts and what process they went through (mturk? Handmade application to select the frames?).\nOverall I think the paper adds an interesting contribution, with the datasets themselves which can be used for image similarity tasks for example\nAlthough the contribution of the paper is important, it seems limited for the conference with no novel method proposed. \n\nTypo\nSection 3, l 4: using use -> using\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper497/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper497/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Image Classifiers Generalize Across Time?", "authors": ["Vaishaal Shankar", "Achal Dave", "Rebecca Roelofs", "Deva Ramanan", "Ben Recht", "Ludwig Schmidt"], "authorids": ["vaishaal@berkeley.edu", "achald@cs.cmu.edu", "roelofs@cs.berkely.edu", "deva@cs.cmu.edu", "brecht@berkeley.edu", "ludwigschmidt2@gmail.com"], "keywords": ["robustness", "image classification", "distribution shift"], "TL;DR": "We systematically measure the sensitivity of image classifiers to temporal perturbations by introducing two human-reviewed benchmarks of similar video frames.", "abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.", "pdf": "/pdf/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "paperhash": "shankar|do_image_classifiers_generalize_across_time", "original_pdf": "/attachment/6a70b8252f8901af3e6b77af6fe6ac6ecc30b2d4.pdf", "_bibtex": "@misc{\nshankar2020do,\ntitle={Do Image Classifiers Generalize Across Time?},\nauthor={Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Ben Recht and Ludwig Schmidt},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx9ET4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx9ET4YPB", "replyto": "Syx9ET4YPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper497/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575600895975, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper497/Reviewers"], "noninvitees": [], "tcdate": 1570237751277, "tmdate": 1575600895987, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper497/-/Official_Review"}}}], "count": 10}