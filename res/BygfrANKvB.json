{"notes": [{"id": "BygfrANKvB", "original": "H1gr3KU_wr", "number": 1103, "cdate": 1569439290027, "ddate": null, "tcdate": 1569439290027, "tmdate": 1577168284862, "tddate": null, "forum": "BygfrANKvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zOP1lwF384", "original": null, "number": 1, "cdate": 1576798714530, "ddate": null, "tcdate": 1576798714530, "tmdate": 1576800921977, "tddate": null, "forum": "BygfrANKvB", "replyto": "BygfrANKvB", "invitation": "ICLR.cc/2020/Conference/Paper1103/-/Decision", "content": {"decision": "Reject", "comment": "The authors present a new approach to improve performance for retro-synthesis using a seq2seq model, achieving significant improvement over the baseline. There are a number of lingering questions regarding the significance and impact of this work. Hence, my recommendation is to reject. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BygfrANKvB", "replyto": "BygfrANKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722273, "tmdate": 1576800273539, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1103/-/Decision"}}}, {"id": "HJlAN9CrjH", "original": null, "number": 5, "cdate": 1573411381873, "ddate": null, "tcdate": 1573411381873, "tmdate": 1573489939562, "tddate": null, "forum": "BygfrANKvB", "replyto": "HkeLifuQ5B", "invitation": "ICLR.cc/2020/Conference/Paper1103/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the feedback and comments. We address each of them in turn:\n\n- \"Novelty and domain-specificity of pre-training methods\"\n\nConventional pre-training methods use a masked language modeling (MLM) objective (Devlin et., 2018), but this kind of objective does not work for our problem. When we try a pre-training method that only masks tokens and decode the target molecule, we do not see the same performance gains. \n\nHere are some numbers after running new experiments using masked pre-training:\nNo pre-training: \t\t                 Top-1: 42.0, Top-5: 57.0, Top-10: 65.7\n(new) Masked pre-training: \t Top-1: 38.7, Top-5: 57.9, Top-10: 61.6\nRandom pre-training: \t  \t Top-1: 43.3, Top-5: 60.1, Top-10: 69.0\nTemplate-based pre-training:    Top-1: 43.5, Top-5: 61.5, Top-10: 71.3\n\nWhile these masking objectives have worked well in NLP and vision tasks, we see that it does not work for our chemistry-domain application. In fact, we see a performance drop, probably because this pre-trained model provides a poor initialization for the task. \n\nChemistry is a domain that involves a lot of small data problems, for which developing effective pre-training and transfer learning techniques is critically important. Therefore, we think that our effective pre-training methods are an important contribution and hope they will inspire more chemically relevant pre-training tasks for other applications.\n\n- \"mixture models for encouraging diversity is a simple instance of ensembling.\u201d\n\nWe argue that our mixture model is more than a simple instance of ensembling. The mixture model encourages different latent classes to learn different reaction types and thus leads to diverse predictions. Ensembling, however, does not have this kind of inductive bias. Furthermore, an ensemble of K models would have K times more parameters, but our mixture model only requires K embeddings of the latent variable, which is a negligible amount of additional parameters.\n\nTo see that the ensemble would not produce as diverse results as the mixture model, we ran our model 5 times with different random seeds, and combined the outputs based on their predicted likelihoods to produce the top 10 predictions (mirroring the setting of mixture model and results presented in Table 3).\n\nUsing the reaction class predictor (we recognize that this is an imperfect metric, but is nonetheless a good proxy), we find that a single model predicts, on average, 2.7 unique reactions. The ensemble predicts 3.02 unique reactions. Our mixture model predicts 3.32 unique reactions, which is much better than that of the ensemble. While the mixture model is a simple idea, it works well to learn to make diverse predictions without additional supervision. \n\n- \u201cUsing deep learning to this application area is also not novel\u201d\n\nWe take the application of deep learning to retrosynthesis in novel directions. First, we note that diversity of predictions for this task is very important, because it is much more helpful for chemists if they have a diverse range of useful predictions. Diversity for this task has not been explored in literature and other models that focus on accuracy of their models do not capture this important facet of the problem. Second, as mentioned earlier, conventional pre-training methods are not effective for this task, so we think that our pre-training methods are a novel contribution.\n\n-\u201cThe experimental results are based only on the USPTO dataset.\u201d\n\nThe USPTO dataset is the benchmark and the only publicly available dataset used in previous work on retrosynthesis (Schwaller et al., 2019; Karpov et al., 2018, Liu et al., 2017). One work, Lee et al. (2019), did experiments on Pfizer electronic lab notebooks, but these datasets are not publicly available.\n\n- \u201cWhat good is diversity if the prediction is incorrect?\u201d\n\nWe agree with the reviewer that diversity matters only when the predictions are correct. We note that our mixture models does have high accuracy, which is a good indicator that the model generally produces correct reaction outputs. We also supplement our diversity metrics with human evaluations that measure diversity only on correct predictions, see below.\n\n- \u201cHow does a human determine something to be more diverse? What is the rubric they use? How qualified is the human in being able to judge this task?\u201d\n\nThe human evaluations were done by a senior (5+ years) chemistry PhD student. They judged diversity based on the number of different reaction types and location of the reaction on the input target. The same type of reaction pathway that uses slightly different precursors is considered as identical, and this evaluation was done taking the correctness of the reaction into account. This information has been added to the appendix.\n\n- \u201cFigure 6 does not have a color scale.\u201d\n\nWe just added it in the revision. Thank you for pointing this out."}, "signatures": ["ICLR.cc/2020/Conference/Paper1103/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfrANKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference/Paper1103/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1103/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1103/Reviewers", "ICLR.cc/2020/Conference/Paper1103/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1103/Authors|ICLR.cc/2020/Conference/Paper1103/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161210, "tmdate": 1576860533512, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference/Paper1103/Reviewers", "ICLR.cc/2020/Conference/Paper1103/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1103/-/Official_Comment"}}}, {"id": "HyeBVs0Bir", "original": null, "number": 8, "cdate": 1573411628938, "ddate": null, "tcdate": 1573411628938, "tmdate": 1573411628938, "tddate": null, "forum": "BygfrANKvB", "replyto": "rJl66yICKB", "invitation": "ICLR.cc/2020/Conference/Paper1103/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the feedback and comments.\n\nFor baselines, the vanilla transformer model from Schwaller et al. (2019) is the state-of-the-art model for retrosynthesis. Previous work using LSTM models (Liu et al., 2017) have been vastly outperformed by these transformer models. We have added the recent work of Zheng et al. (2019) in Table 1. While their method is orthogonal to our techniques, our model outperforms theirs by a substantial margin, especially for top-10 accuracy.\n\nReferences:\n\n(Schwaller et al., 2019): Molecular Transformer for Chemical Reaction Prediction and Uncertainty Estimation\n(Liu et al., 2017): Retrosynthetic Reaction Prediction Using Neural Sequence-to-Sequence Models\n(Zeng et al., 2019): Predicting Retrosynthetic Reaction using Self-Corrected Transformer Neural Networks"}, "signatures": ["ICLR.cc/2020/Conference/Paper1103/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfrANKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference/Paper1103/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1103/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1103/Reviewers", "ICLR.cc/2020/Conference/Paper1103/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1103/Authors|ICLR.cc/2020/Conference/Paper1103/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161210, "tmdate": 1576860533512, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference/Paper1103/Reviewers", "ICLR.cc/2020/Conference/Paper1103/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1103/-/Official_Comment"}}}, {"id": "BkxAJiArir", "original": null, "number": 7, "cdate": 1573411557532, "ddate": null, "tcdate": 1573411557532, "tmdate": 1573411557532, "tddate": null, "forum": "BygfrANKvB", "replyto": "BkxFpvFMqS", "invitation": "ICLR.cc/2020/Conference/Paper1103/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for the positive feedback and comments. If the reviewer has any questions or additional comments, please let us know."}, "signatures": ["ICLR.cc/2020/Conference/Paper1103/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfrANKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference/Paper1103/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1103/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1103/Reviewers", "ICLR.cc/2020/Conference/Paper1103/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1103/Authors|ICLR.cc/2020/Conference/Paper1103/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161210, "tmdate": 1576860533512, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference/Paper1103/Reviewers", "ICLR.cc/2020/Conference/Paper1103/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1103/-/Official_Comment"}}}, {"id": "r1gJ2cCSoS", "original": null, "number": 6, "cdate": 1573411495165, "ddate": null, "tcdate": 1573411495165, "tmdate": 1573411495165, "tddate": null, "forum": "BygfrANKvB", "replyto": "HJlAN9CrjH", "invitation": "ICLR.cc/2020/Conference/Paper1103/-/Official_Comment", "content": {"title": "(continued)", "comment": "References:\n\n(Devlin et al., 2018): BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n(Schwaller et al., 2019): Molecular Transformer for Chemical Reaction Prediction and Uncertainty Estimation\n(Karpov et al., 2018): A Transformer Model for Retrosynthesis\n(Liu et al., 2017): Retrosynthetic Reaction Prediction Using Neural Sequence-to-Sequence Models\n(Lee et al., 2019): Molecular Transformer unifies reaction prediction and retrosynthesis across pharma chemical space\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1103/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygfrANKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference/Paper1103/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1103/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1103/Reviewers", "ICLR.cc/2020/Conference/Paper1103/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1103/Authors|ICLR.cc/2020/Conference/Paper1103/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161210, "tmdate": 1576860533512, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1103/Authors", "ICLR.cc/2020/Conference/Paper1103/Reviewers", "ICLR.cc/2020/Conference/Paper1103/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1103/-/Official_Comment"}}}, {"id": "rJl66yICKB", "original": null, "number": 1, "cdate": 1571868613189, "ddate": null, "tcdate": 1571868613189, "tmdate": 1572972512539, "tddate": null, "forum": "BygfrANKvB", "replyto": "BygfrANKvB", "invitation": "ICLR.cc/2020/Conference/Paper1103/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is very well-written and combines the state-of-the-art NLP model and the domain knowledge in retrosynthetic reaction predictions.  The authors propose pre-training models to help improve the model's generation to rare reactions. In addition, a discrete latent variable model is used in the model to encourage the model to produce a diverse set of alternative predictions.  The experiments in the paper also show the effectiveness of the two main contributions.\n\nThe main contribution of this paper is to apply the state-of-the-art Transformer model and other techniques in NLP to address the specific issues in retrosynthesis. Both the pre-training model and the mixture model are combining the specific domain knowledge to improve the generalization and diversity of retrosynthesis. There is not a huge algorithm novelty for the methods proposed in this paper, but they can well address the domain issues and improve the performance.\n\nMy only concern is that the baseline model compared in the paper is Schwaller's work. I am not pretty sure if this baseline achieves state-of-the-art performance. It would be very interesting to see more comparisons with other state-of-the-art \n work.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1103/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1103/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygfrANKvB", "replyto": "BygfrANKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666724347, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1103/Reviewers"], "noninvitees": [], "tcdate": 1570237742332, "tmdate": 1575666724361, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1103/-/Official_Review"}}}, {"id": "BkxFpvFMqS", "original": null, "number": 2, "cdate": 1572145088812, "ddate": null, "tcdate": 1572145088812, "tmdate": 1572972512494, "tddate": null, "forum": "BygfrANKvB", "replyto": "BygfrANKvB", "invitation": "ICLR.cc/2020/Conference/Paper1103/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Given a target compound, the authors suggest a method to predict likely chemical reactants to produce the target. The authors provide a transformer based model to predict the reactants. Existing methods do not generalize well for rare reactions and the training data has only one reactant set for each target even though that may not be the only way to synthesize the compound. To solve this problem, the authors use a pretraining method similar to BERT. Instead of just using token masking, they provide alternate proxy decompositions for a target molecule by randomly removing bond types that are likely to break and by transforming the target based on known templates.\n\nThis is a well written paper with good baselines.They use multiple techniques that are both domain specific (data augmentation) as well as methods from NLP adapted for this task. The experiments are carefully designed and show that both pretraining and data augmentation helps. Overall, I think the community will benefit from this work."}, "signatures": ["ICLR.cc/2020/Conference/Paper1103/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1103/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygfrANKvB", "replyto": "BygfrANKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666724347, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1103/Reviewers"], "noninvitees": [], "tcdate": 1570237742332, "tmdate": 1575666724361, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1103/-/Official_Review"}}}, {"id": "HkeLifuQ5B", "original": null, "number": 3, "cdate": 1572205213779, "ddate": null, "tcdate": 1572205213779, "tmdate": 1572972512447, "tddate": null, "forum": "BygfrANKvB", "replyto": "BygfrANKvB", "invitation": "ICLR.cc/2020/Conference/Paper1103/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present an approach to improve performance for retro-synthesis of chemical targets in a seq2seq setting using transformers. The authors have encouraging results and the paper was fairly easy to read and follow. However there are a variety of concerns that the authors need to address:\n\nThe technical contributions in this paper are somewhat thin. The main contributions are data augmentation techniques, pre-training and a mixture model that seems to improve performance on the USPTO-50K dataset. The novelty is quite low and it\u2019s not clear if this will transfer to another domain. The impact is also low as the pre-training techniques using bond breaking and template-based are specific to this problem task. Additionally, mixture models for encouraging diversity is a simple instance of ensembling.\n\nUsing deep learning to this application area is also not novel. This paper largely builds upon previous work with Transformers from Schwaller et. al and Karpov et. al. \n\nOther clarifications/issues:\n - The experimental results are based only on the USPTO dataset. It\u2019s unclear how significant the results are. The authors can consider using diverse datasets or applying their techniques to another application domain to bolster their claims.\n - Table 3, lists the average number of unique reactions classes. The authors say \u201c \u2026 we predict the reaction class for every output of our models\u2026 \u201c . It\u2019s not clear how it makes sense to calculate diversity when there\u2019s no ground truth available for determining if the predicted output is a valid synthesis for the target. To say this another way, what good is diversity if the prediction is incorrect?\n - Table 3, lists human eval results. The details here seem quite vague. How does a human determine something to be more diverse? What is the rubric they use? How qualified is the human in being able to judge this task?\n - Figure 6 does not have a color scale.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1103/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1103/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bensonc@mit.edu", "tianxiao@mit.edu", "tommi@csail.mit.edu", "regina@csail.mit.edu"], "title": "Learning to Make Generalizable and Diverse Predictions for Retrosynthesis", "authors": ["Benson Chen", "Tianxiao Shen", "Tommi S. Jaakkola", "Regina Barzilay"], "pdf": "/pdf/995c97f417013804a8791ec1f6f60e389c69169e.pdf", "TL;DR": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.", "abstract": "We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.", "code": "https://github.com/iclr-2020-retro/retro_smiles_transformer", "keywords": ["Chemistry", "Retrosynthesis", "Transformer", "Pre-training", "Diversity"], "paperhash": "chen|learning_to_make_generalizable_and_diverse_predictions_for_retrosynthesis", "original_pdf": "/attachment/8aa8c464dfe106db7e9fecd16efdf6131b8db8ac.pdf", "_bibtex": "@misc{\nchen2020learning,\ntitle={Learning to Make Generalizable and Diverse Predictions for Retrosynthesis},\nauthor={Benson Chen and Tianxiao Shen and Tommi S. Jaakkola and Regina Barzilay},\nyear={2020},\nurl={https://openreview.net/forum?id=BygfrANKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygfrANKvB", "replyto": "BygfrANKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1103/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666724347, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1103/Reviewers"], "noninvitees": [], "tcdate": 1570237742332, "tmdate": 1575666724361, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1103/-/Official_Review"}}}], "count": 9}