{"notes": [{"id": "ryx6daEtwr", "original": "ByxusO2PDB", "number": 650, "cdate": 1569439093244, "ddate": null, "tcdate": 1569439093244, "tmdate": 1577168230835, "tddate": null, "forum": "ryx6daEtwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "GPNET: MONOCULAR 3D VEHICLE DETECTION BASED ON LIGHTWEIGHT WHEEL GROUNDING POINT DETECTION NETWORK", "authors": ["zizhang.wu"], "authorids": ["wuzizhang87@gmail.com"], "keywords": ["applications in vision", "audio", "speech", "natural language processing", "robotics"], "TL;DR": "Method for detecting vehicle 3D information based on fisheye camera with high efficiency", "abstract": "We present a method to infer 3D location and orientation of vehicles on a single image. To tackle this problem, we optimize the mapping relation between the vehicle\u2019s wheel grounding point on the image and the real location of the wheel in the 3D real world coordinate. Here we also integrate three task priors, including a ground plane constraint and vehicle wheel grounding point position, as well as a small projection error from the image to the ground plane. And a robust light network for grounding point detection in autopilot is proposed based on the vehicle and wheel detection result. In the light grounding point detection network, the DSNT key point regression method is used for balancing the speed of convergence and the accuracy of position, which has been proved more robust and accurate compared with the other key point detection methods. With more, the size of grounding point detection network is less than 1 MB, which can be executed quickly on the embedded environment. The code will be available soon.", "pdf": "/pdf/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "paperhash": "zizhangwu|gpnet_monocular_3d_vehicle_detection_based_on_lightweight_wheel_grounding_point_detection_network", "original_pdf": "/attachment/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "_bibtex": "@misc{\nzizhang.wu2020gpnet,\ntitle={{\\{}GPNET{\\}}: {\\{}MONOCULAR{\\}} 3D {\\{}VEHICLE{\\}} {\\{}DETECTION{\\}} {\\{}BASED{\\}} {\\{}ON{\\}} {\\{}LIGHTWEIGHT{\\}} {\\{}WHEEL{\\}} {\\{}GROUNDING{\\}} {\\{}POINT{\\}} {\\{}DETECTION{\\}} {\\{}NETWORK{\\}}},\nauthor={zizhang.wu},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx6daEtwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "yUuhK3jPri", "original": null, "number": 1, "cdate": 1576798702320, "ddate": null, "tcdate": 1576798702320, "tmdate": 1576800933693, "tddate": null, "forum": "ryx6daEtwr", "replyto": "ryx6daEtwr", "invitation": "ICLR.cc/2020/Conference/Paper650/-/Decision", "content": {"decision": "Reject", "comment": "This paper aims to estimate the 3D location and orientation of vehicle from a 2D image. Instead of using a CNN-based 3D detection pipeline, the authors propose to detect the vehicle\u2019s wheel grounding points and then using the ground plane constraint for the estimation. All three reviewers provided unanimous rating of rejection. Many concerns are raised by the reviewers, including poor generalization to new situations, small improvement over prior work, low presentation quality, the lack of detailed description of the experiments, etc. The authors did not respond to the reviewers\u2019 comments. The AC agrees with the reviewers\u2019 comments, and recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GPNET: MONOCULAR 3D VEHICLE DETECTION BASED ON LIGHTWEIGHT WHEEL GROUNDING POINT DETECTION NETWORK", "authors": ["zizhang.wu"], "authorids": ["wuzizhang87@gmail.com"], "keywords": ["applications in vision", "audio", "speech", "natural language processing", "robotics"], "TL;DR": "Method for detecting vehicle 3D information based on fisheye camera with high efficiency", "abstract": "We present a method to infer 3D location and orientation of vehicles on a single image. To tackle this problem, we optimize the mapping relation between the vehicle\u2019s wheel grounding point on the image and the real location of the wheel in the 3D real world coordinate. Here we also integrate three task priors, including a ground plane constraint and vehicle wheel grounding point position, as well as a small projection error from the image to the ground plane. And a robust light network for grounding point detection in autopilot is proposed based on the vehicle and wheel detection result. In the light grounding point detection network, the DSNT key point regression method is used for balancing the speed of convergence and the accuracy of position, which has been proved more robust and accurate compared with the other key point detection methods. With more, the size of grounding point detection network is less than 1 MB, which can be executed quickly on the embedded environment. The code will be available soon.", "pdf": "/pdf/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "paperhash": "zizhangwu|gpnet_monocular_3d_vehicle_detection_based_on_lightweight_wheel_grounding_point_detection_network", "original_pdf": "/attachment/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "_bibtex": "@misc{\nzizhang.wu2020gpnet,\ntitle={{\\{}GPNET{\\}}: {\\{}MONOCULAR{\\}} 3D {\\{}VEHICLE{\\}} {\\{}DETECTION{\\}} {\\{}BASED{\\}} {\\{}ON{\\}} {\\{}LIGHTWEIGHT{\\}} {\\{}WHEEL{\\}} {\\{}GROUNDING{\\}} {\\{}POINT{\\}} {\\{}DETECTION{\\}} {\\{}NETWORK{\\}}},\nauthor={zizhang.wu},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx6daEtwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryx6daEtwr", "replyto": "ryx6daEtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728977, "tmdate": 1576800281491, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper650/-/Decision"}}}, {"id": "SJgLFl3CFB", "original": null, "number": 1, "cdate": 1571893373718, "ddate": null, "tcdate": 1571893373718, "tmdate": 1572972569231, "tddate": null, "forum": "ryx6daEtwr", "replyto": "ryx6daEtwr", "invitation": "ICLR.cc/2020/Conference/Paper650/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a method to detect cars from a single image. The method imposes several handcrafted constraints specific to the dataset in order to achieve higher improvement and efficiency. These constraints are quite strong and they not generalize to new situations (eg. a car in the sky, a car upside down a car with multiple wheels). The results do not seem particularly strong because the dataset seems easy and the improvements over previous works is small. \n\nI would suggest to emphasise the improvements over previous works and send this paper to a specialized journal or venue in vehicle detection. "}, "signatures": ["ICLR.cc/2020/Conference/Paper650/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper650/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GPNET: MONOCULAR 3D VEHICLE DETECTION BASED ON LIGHTWEIGHT WHEEL GROUNDING POINT DETECTION NETWORK", "authors": ["zizhang.wu"], "authorids": ["wuzizhang87@gmail.com"], "keywords": ["applications in vision", "audio", "speech", "natural language processing", "robotics"], "TL;DR": "Method for detecting vehicle 3D information based on fisheye camera with high efficiency", "abstract": "We present a method to infer 3D location and orientation of vehicles on a single image. To tackle this problem, we optimize the mapping relation between the vehicle\u2019s wheel grounding point on the image and the real location of the wheel in the 3D real world coordinate. Here we also integrate three task priors, including a ground plane constraint and vehicle wheel grounding point position, as well as a small projection error from the image to the ground plane. And a robust light network for grounding point detection in autopilot is proposed based on the vehicle and wheel detection result. In the light grounding point detection network, the DSNT key point regression method is used for balancing the speed of convergence and the accuracy of position, which has been proved more robust and accurate compared with the other key point detection methods. With more, the size of grounding point detection network is less than 1 MB, which can be executed quickly on the embedded environment. The code will be available soon.", "pdf": "/pdf/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "paperhash": "zizhangwu|gpnet_monocular_3d_vehicle_detection_based_on_lightweight_wheel_grounding_point_detection_network", "original_pdf": "/attachment/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "_bibtex": "@misc{\nzizhang.wu2020gpnet,\ntitle={{\\{}GPNET{\\}}: {\\{}MONOCULAR{\\}} 3D {\\{}VEHICLE{\\}} {\\{}DETECTION{\\}} {\\{}BASED{\\}} {\\{}ON{\\}} {\\{}LIGHTWEIGHT{\\}} {\\{}WHEEL{\\}} {\\{}GROUNDING{\\}} {\\{}POINT{\\}} {\\{}DETECTION{\\}} {\\{}NETWORK{\\}}},\nauthor={zizhang.wu},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx6daEtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryx6daEtwr", "replyto": "ryx6daEtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper650/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper650/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575515921116, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper650/Reviewers"], "noninvitees": [], "tcdate": 1570237749073, "tmdate": 1575515921129, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper650/-/Official_Review"}}}, {"id": "HkgHJ8y1cB", "original": null, "number": 2, "cdate": 1571907037182, "ddate": null, "tcdate": 1571907037182, "tmdate": 1572972569189, "tddate": null, "forum": "ryx6daEtwr", "replyto": "ryx6daEtwr", "invitation": "ICLR.cc/2020/Conference/Paper650/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I find it very hard to review this paper. The idea of using keypoints to carry pose estimation is is more than 15 years old, and for the car examples reported in this paper, I'm wondering why not just you SURF or SIFT - these would certainly have been reasonable baselines. The convnets cited in this paper are mostly targeted at the harder problem of estimating human body poses.\n\nThe paper is very hard to read. It is full of typos and far from ready for submission to ICLR. The equations (eg eqn 1) are impossible to parse.\n\nBased on all this I find it hard to trust the results. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper650/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper650/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GPNET: MONOCULAR 3D VEHICLE DETECTION BASED ON LIGHTWEIGHT WHEEL GROUNDING POINT DETECTION NETWORK", "authors": ["zizhang.wu"], "authorids": ["wuzizhang87@gmail.com"], "keywords": ["applications in vision", "audio", "speech", "natural language processing", "robotics"], "TL;DR": "Method for detecting vehicle 3D information based on fisheye camera with high efficiency", "abstract": "We present a method to infer 3D location and orientation of vehicles on a single image. To tackle this problem, we optimize the mapping relation between the vehicle\u2019s wheel grounding point on the image and the real location of the wheel in the 3D real world coordinate. Here we also integrate three task priors, including a ground plane constraint and vehicle wheel grounding point position, as well as a small projection error from the image to the ground plane. And a robust light network for grounding point detection in autopilot is proposed based on the vehicle and wheel detection result. In the light grounding point detection network, the DSNT key point regression method is used for balancing the speed of convergence and the accuracy of position, which has been proved more robust and accurate compared with the other key point detection methods. With more, the size of grounding point detection network is less than 1 MB, which can be executed quickly on the embedded environment. The code will be available soon.", "pdf": "/pdf/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "paperhash": "zizhangwu|gpnet_monocular_3d_vehicle_detection_based_on_lightweight_wheel_grounding_point_detection_network", "original_pdf": "/attachment/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "_bibtex": "@misc{\nzizhang.wu2020gpnet,\ntitle={{\\{}GPNET{\\}}: {\\{}MONOCULAR{\\}} 3D {\\{}VEHICLE{\\}} {\\{}DETECTION{\\}} {\\{}BASED{\\}} {\\{}ON{\\}} {\\{}LIGHTWEIGHT{\\}} {\\{}WHEEL{\\}} {\\{}GROUNDING{\\}} {\\{}POINT{\\}} {\\{}DETECTION{\\}} {\\{}NETWORK{\\}}},\nauthor={zizhang.wu},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx6daEtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryx6daEtwr", "replyto": "ryx6daEtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper650/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper650/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575515921116, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper650/Reviewers"], "noninvitees": [], "tcdate": 1570237749073, "tmdate": 1575515921129, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper650/-/Official_Review"}}}, {"id": "rkg6-V6B9B", "original": null, "number": 3, "cdate": 1572357124903, "ddate": null, "tcdate": 1572357124903, "tmdate": 1572972569145, "tddate": null, "forum": "ryx6daEtwr", "replyto": "ryx6daEtwr", "invitation": "ICLR.cc/2020/Conference/Paper650/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "General:  The proposed method tries to improve vehicle identification and tracking by combining model-based and data-driven methods. The idea is appealing, especially the use of domain knowledge combined with a data driven method to improve a model-based task.\n\n The authors propose a set of model configurations on the waypoint detection, optimization techniques, a deep learning network topology and a data driven and domain knowledge based wheel detection mechanism.\n\nImprovements on vehicle identification and tracking are not shown in the study, but different computer vision methods  are compared to each other.\n\nThe study has several shortcomings in consistently stating the problem and the result, completeness, reproducibility, quantification of results and the experimental methodology lacks  a systematic approach to isolate the effects of the different components of the proposed method.\n\nIt seems that the full method proposed yields better results compared to other way point based methods in a certain phase space. However, from the material shown, especially the lack of the experimental description and its systematic shortcomings,  I cannot judge if the components of the method can contribute to improved vehicle detection and tracking.\n\nThe paper can be strengthened by the following:\n    Better framing of the problem\n    Improve the descriptions on the experiments carried out\n    Including a quantitative discussion on the results and the corresponding uncertainties\n    Systematic studies on the behavior of the proposed method components including a mathematical description in the probability space\n\n \n\nMore detailed comments:\n\nBetter framing of the problem:\n\n    The framing of the problem and the task lack  at least a qualitative discussion on the issues arising in using visual based vs Lidar/Radar. I disagree with the authors, that a visual system can replace these other systems, but rather enhance results under certain conditions. I am missing the discussion on the shortcomings of camera sensors with regards to the overall task: Day/Night, Fog, wheels are not visible from all angles and the impact on the target phase space. The authors give an outline, that there is a study to come tackling the performance on vehicle identification, therefore a qualitative discussion could be enough at this point.\n\nImprove the descriptions on the experiments carried out\n    So that someone else can reproduce it\n\nSystematic studies on the behavior of the proposed method components including a mathematical description in the probability space\n\n    The Experiment is not described at all and is therefore not reproducible. The results shown are in general the overall Precision and Recall on some dataset. The authors state at several occasions, that isolated measures on the method show better performance. E.g. the choice of the \"..online hard keypoints mining method..\", \"..the fixed range of softmax inputs..\", \"..ensure the accuracy io vehicle yaw angle\u2026\". However, I cannot find proof of these statements in the material shown. Usually only the overall task performance is stated from which deductions on isolated effects cannot be drawn.\n    Albeit mentioning in the introduction priors in a Gaussian Process framework, I have not noticed a notion of a random variable, a probability density distribution or the impact of the experiment on the prior pdfs.  A systematic study on the behavior of those pdfs is missing and only a few example picture of bounding boxes are shown.\n\nIncluding a quantitative discussion on the results and the corresponding uncertainties\n    Insert a quantitative  discussion on the experiments.  Insert estimates on uncertainties on the results or give at least a qualitative statement, if these are negligible.\n\nOther: The paper is an application paper and does not offer novel advances for the ICLR community.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper650/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper650/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GPNET: MONOCULAR 3D VEHICLE DETECTION BASED ON LIGHTWEIGHT WHEEL GROUNDING POINT DETECTION NETWORK", "authors": ["zizhang.wu"], "authorids": ["wuzizhang87@gmail.com"], "keywords": ["applications in vision", "audio", "speech", "natural language processing", "robotics"], "TL;DR": "Method for detecting vehicle 3D information based on fisheye camera with high efficiency", "abstract": "We present a method to infer 3D location and orientation of vehicles on a single image. To tackle this problem, we optimize the mapping relation between the vehicle\u2019s wheel grounding point on the image and the real location of the wheel in the 3D real world coordinate. Here we also integrate three task priors, including a ground plane constraint and vehicle wheel grounding point position, as well as a small projection error from the image to the ground plane. And a robust light network for grounding point detection in autopilot is proposed based on the vehicle and wheel detection result. In the light grounding point detection network, the DSNT key point regression method is used for balancing the speed of convergence and the accuracy of position, which has been proved more robust and accurate compared with the other key point detection methods. With more, the size of grounding point detection network is less than 1 MB, which can be executed quickly on the embedded environment. The code will be available soon.", "pdf": "/pdf/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "paperhash": "zizhangwu|gpnet_monocular_3d_vehicle_detection_based_on_lightweight_wheel_grounding_point_detection_network", "original_pdf": "/attachment/90edccd5a7024f7371668eb1af7fc077cf6459c7.pdf", "_bibtex": "@misc{\nzizhang.wu2020gpnet,\ntitle={{\\{}GPNET{\\}}: {\\{}MONOCULAR{\\}} 3D {\\{}VEHICLE{\\}} {\\{}DETECTION{\\}} {\\{}BASED{\\}} {\\{}ON{\\}} {\\{}LIGHTWEIGHT{\\}} {\\{}WHEEL{\\}} {\\{}GROUNDING{\\}} {\\{}POINT{\\}} {\\{}DETECTION{\\}} {\\{}NETWORK{\\}}},\nauthor={zizhang.wu},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx6daEtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryx6daEtwr", "replyto": "ryx6daEtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper650/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper650/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575515921116, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper650/Reviewers"], "noninvitees": [], "tcdate": 1570237749073, "tmdate": 1575515921129, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper650/-/Official_Review"}}}], "count": 5}