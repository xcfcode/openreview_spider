{"notes": [{"id": "ryGkSo0qYm", "original": "rJl6V6TUKQ", "number": 51, "cdate": 1538087734907, "ddate": null, "tcdate": 1538087734907, "tmdate": 1548054947595, "tddate": null, "forum": "ryGkSo0qYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1ldR_A7e4", "original": null, "number": 1, "cdate": 1544968400148, "ddate": null, "tcdate": 1544968400148, "tmdate": 1545354489450, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Meta_Review", "content": {"metareview": "The paper is proposed as probable accept based on current ratings with a majority accept (7,7,5).", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Probable accept based on majority vote."}, "signatures": ["ICLR.cc/2019/Conference/Paper51/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper51/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353354577, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper51/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353354577}}}, {"id": "H1gzJMz5Am", "original": null, "number": 9, "cdate": 1543279066024, "ddate": null, "tcdate": 1543279066024, "tmdate": 1543280107375, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "content": {"title": "Summary of changes in new version", "comment": "Dear reviewers,\n\nWe would like to sincerely thank you for your constructive comments. After implementing them, we believe that the current version of the paper is not only stronger, but also clearer.\n\nOur main focus was to strengthen the experimental section. The new version better illustrates  the advantages of large scale graph learning.\n\n- For better clarity, we have re-organized the experiment by type : a) approximation quality of our scaled model compared to the original non-scaled version (section 5.1), b) quality of our automatic parameter selection (section 5.2), c) benefit from learning versus A-NN for large scale applications, i.e. assessment of the quality of the model (section 5.3, 5.4, 5.5), and d) scalability of the model (section 5.6)\u2028\n- We added a comparison with Daitch \u201csoft\u201d and \u201chard\u201d models both in terms of quality and scalability. To do so, we first implemented their original algorithms, but also proposed new, scalable versions based on our Section 3.\n- We added a new experiment on larger graphs (262,144 nodes), where we focus both on time but also on graphs accuracy. We demonstrate how large-scale graph learning can recover a manifold, much better than usual A-NN graphs. \n- We showed the accuracy of the theta parameter estimation for the new, larger dataset and also its robustness against outliers or duplicates (the latter part is in the appendix due to space limitations).\n\nEventually, we fixed other minor issues, including the addition of the missing reference and the clarification of some specific points.\n\nWe thank you again for your time and suggestions.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611316, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGkSo0qYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper51/Authors|ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611316}}}, {"id": "rklxB0GinX", "original": null, "number": 3, "cdate": 1541250616269, "ddate": null, "tcdate": 1541250616269, "tmdate": 1543255847248, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Review", "content": {"title": "Neat contribution", "review": "Learning graphs from data fine tunes standard similarity graph constructions such as k-nearest neighbor graphs.  \nThere has been a line of research works that focuses on learning graphs and that shows that this results in superior\nresults in various machine learning tasks.  The current state-of-the-art method is the method proposed by Kalofolias, \nwhich however is slow.   The authors suggest a method to avoid searching for the parameters that achieve a desired\nlevel of sparsity by providing closed a formula. The parameter that determines the sparsity is theta, see proposition 1 on page 4. This was originally shown by Kalofolias. To achieve their goal, the authors first consider the degree of any given node by looking at equation (8), page 4. They prove theorem 1, that is intuitive and  provides the form of the optimal \nweights that connect this node to the rest of the nodes in the graph.  The proof is based on applying the KKT conditions on\nthe objective (8), with the single constraint that there are no negative weights.  Finally, since we care about the \nsparsity of the graph as a whole, the authors use the average of the parameter theta over all nodes. The authors perform \nexperiments on real-world graphs, and show basic properties of their method, as well as the main source of mistakes ,i.e., disconnected nodes, figure 5.\n\nEssentially, this paper starts from the work of Kalofolias  and improves it significantly. This by itself is \na neat contribution, but the authors could improve their paper by showing a more complete view  of graph \nlearning methods, with respect to the quality of the produced graphs and the scalability. I find this aspect of the paper narrowing its contribution, hence my evaluation. Some remarks follow.\n\n- A different family of graph learning methods is based on the objective ||LX||_F^2 or equivalently tr(X^TLLX). \nFor this objective, Daitch et al. proved certain neat properties, such as the existence of a sparse optimal graph. \nThis allows Daitch et al. to solve the primal dual significantly faster than O(n^2) since by their theorem, \nO(nd) edges are required where d is the dimension of the data points. When d is large, a random projection can be applied. \nThe paper should compare with this family of methods that are more scalable both with respect to the accuracy, \nand to the runtimes. \n\n- While the proposed method scales significantly better than Kalofolias, the datasets used are small. \n\n- Using LSH for k-nn graphs results in a  scalable, practical way to construct similarity graphs. The authors should cite\nthe following related work, and compare with such methods.\n\u201cEfficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures\u201c by Dong, Charikar, Li. \n\n- An interesting experiment would be to inject outliers in the dataset, or use some dataset with outliers. \nWould this affect the tightness of the interval in equation (17)? ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Review", "cdate": 1542234549007, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335638080, "tmdate": 1552335638080, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJgXinpl0m", "original": null, "number": 7, "cdate": 1542671514769, "ddate": null, "tcdate": 1542671514769, "tmdate": 1542671603286, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "content": {"title": "The reference of the spherical data we mentioned", "comment": "This is the reference to the paper [Perraudin etal 2018] where the spherical data were used for deep learning:\n\nNathana\u00ebl Perraudin, Micha\u00ebl Defferrard, Tomasz Kacprzak, Raphael Sgier\nDeepSphere: Efficient spherical Convolutional Neural Network with HEALPix sampling for cosmological applications\n\nLink: https://arxiv.org/abs/1810.12186"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611316, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGkSo0qYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper51/Authors|ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611316}}}, {"id": "SkeZBo6g0m", "original": null, "number": 6, "cdate": 1542671161096, "ddate": null, "tcdate": 1542671161096, "tmdate": 1542671161096, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "BkgZllZ52X", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "content": {"title": "Answer to reviewer", "comment": "Dear reviewer,\nThank you for your review and your positive comments. \n\nComparing the performance of GCN using different graphs (k-nn, A-NN, learned) is definitely an interesting and relevant topic. While going deep to GCNs is out of the scope of this paper, we took a step in this direction by learning a graph that could be used for deep learning. \n\nIn the new version of the paper we are adding a new experiment, where we work with the data used by [Perraudin etal 2018] with the DeepSphere GCN. Because the graph used in DeepSphere is constructed using empirical rules and is not a real ground truth, one could ask themselves, if there exists a better way to construct it. The graph we learned using our algorithm would be a candidate. We show in our experiments that the actual graph that the authors used for deep learning (that they constructed knowing the coordinates) is close to the one we learned from smooth signals, without having any information of coordinates. Furthermore, the sphere is a 2D manifold, and our graph has properties similar to a 2D manifold graph.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611316, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGkSo0qYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper51/Authors|ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611316}}}, {"id": "Bkxn9caeRQ", "original": null, "number": 5, "cdate": 1542670996107, "ddate": null, "tcdate": 1542670996107, "tmdate": 1542670996107, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "HkgeCuI5nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "content": {"title": "Answer to reviewer (Part 2)", "comment": "\nPerformance verification for more datasets:\n---------------------------------------------------------\nAs we wrote in the answer to reviewer 1, it is difficult to assess the quality of graphs when the ground truth is not really known. To assess quality, we tried to use a wide variety of both datasets and proxies for graph quality. We showed the distribution quality of edges for MNIST (Figure 3), the edge accuracy for MNIST (Figure 4) and the classification error of label propagation on MNIST (Figure 5). In the latter we also compared the number of disconnected nodes. We measured the diameter of the graph between word2vec representations (Figure 6) that is best for our large scale graph, and showed qualitatively the effects of the graph in Figure 7. \n\nFollowing your comment regarding performance verification, we added a further experiment, for larger data this time (262,000 nodes). We used our learned graph for embedding all nodes on a 2-D plane. The signals were known to reside on a small part of the surface of the sphere, which is a 2-dimensional manifold. Without giving any notion of coordinates, but only smooth signals of the sphere, we were able to recover a very good 2-D embedding by using the first two non-zero eigenvectors of our learned sparse Laplacian. This is an important result, since we used no coordinate information whatsoever, only the similarity between different nodes, and the structure was very well recovered. \n\nIn this experiment it is clear that the large scale Log model works best. The large scale L-2 graph is able to recover a meaningful 2D manifold only if we remove the disconnected nodes, and in that case, it gives erroneous results in the middle of the manifold. The A-NN has no disconnected nodes, but gives an embedding that is far from 2D, as many of the edges are erroneous. \n\nIn their work [Perraudin etal 2018] used in a weighted 8 exact K-NN graph for their experiments (Figure B.13 in their paper). Knowing this graph, we can see we can compare different algorithms with respect to the f1 measure. Again, we observed that the learned graphs perform significantly better than A-NN.\n\nWe are adding the plots of the different embeddings, as well as the f1 scores in the body or in the appendix of our paper depending on the available space. Furthermore, we are adding a plot of expected versus obtained degrees of the graph using our theta approximations.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611316, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGkSo0qYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper51/Authors|ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611316}}}, {"id": "H1ek1FaxRX", "original": null, "number": 4, "cdate": 1542670551416, "ddate": null, "tcdate": 1542670551416, "tmdate": 1542670551416, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "HkgeCuI5nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "content": {"title": "Answer to reviewer (Part 1)", "comment": "Dear reviewer,\nWe would like to first of all thank you for your review. We completely agree that graph construction is a significant problem for the wide range of ML community. From your comments we understand that we have to make more obvious for the reader the novelty of our paper and its importance for the ML community.\n\nNovelty/contribution:\n--------------------------\nUntil now, there was no real graph learning algorithm in the literature that actually scales to problems larger than a few thousands of nodes. The previous state of the art algorithms, but also classic algorithms like the ones of Daitch etal (see comments to Reviewer 1) cannot scale due to their computational complexity that is in the order at least O(n^2). For our experiments with the MNIST dataset (60,000 nodes) none of them could be computed. In the new version of the paper, we were able to compare with Daitch only because we modified significantly their algorithms. \n\nAlternative to graph learning, people could resort to approximate nearest neighbor (A-NN) algorithms, that do not weight the edges, but only return a binary approximate adjacency matrix. These algorithms are more practical for large scale problems, but suffer in quality of the edges.\n\nOur solution is the first actual graph learning solution (based on tr(X^TLX)), that thanks to A-NN scales with O(dn logn) for n nodes and d features. Due to this complexity it can scale to learn graphs of 1 million nodes in reasonable time on a laptop running Matlab. To the best of our knowledge, there is no other graph learning algorithm that could scale to this size of graphs. \n\nTo achieve this scalability, our contribution is twofold (first, Section 3 and second, Section 4). \n\nFirst, in Section 3, we show how the optimization problem of the previous state of the art can be reduced if we know a reduced support of edges. While this is small part of our contribution, we show the details of the optimization, analyze the new computational complexity, and provide experiments to show the quality of the approximate graphs in realistic scenarios one would come across in ML. Furthermore, we will provide online our Matlab code so that the ML community can learn large scale graphs for their own data.\n\nSecondly, in Section 4, we reduce a big drawback of the previous state of the art model. The latter suffers from the problem of how to set the two parameters $\\alpha$ and $\\beta$. This is a real problem when we don't even know the order of magnitude of the parameters, and the only solution seems to be trial and error, a.k.a. grid search over both parameters, in a logarithmic scale. In real ML scenarios, we want to be able to set the density of the graph, for example to 5 or 10 nearest neighbors in average. Having to try 25 or 100 different settings (5 or 10 for $\\alpha$ times 5 or 10 for $\\beta$ for [Kalofolias 2016]) would be a prohibitive factor for large scale problems. \n\nEven when these two parameters are reduced to the more intuitive ones $\\theta, \\delta$ (Proposition 1 in our paper), there was no way to know the order of magnitude of $\\theta$ for controlling sparsity. In our experiments we had to use values of theta in the order of magnitude of $1e-6$ (Figure 2), but also of $1e2$ (Figure 10). As you propose, following the regularization path was the only way available until now, but this needs to run the algorithm many more times (like [Kalofolias, 2016] did in his paper). Hence our contribution is to propose a natural way to set theta in order to control the sparsity. We basically propose a method to link $theta$ and $k$, the number of desired edges per node.  While one may argue that $k$ still needs to be tuned, this parameter can be interpreted and is way simpler to set based on data assumptions than is impossible with $\\theta$. \n\nParameter value range:\n-------------------------------\nAs you say, we give a range of parameters that shall give approximately a requested graph sparsity. In our experiments, we always use the geometric mean between the upper and lower bounds for each given k. This is equivalent to using the arithmetic mean in the log scale, that represents the order of magnitude of $\\theta$, and is what we are plotting in Figure 2 and Figure 10 (logarithmic $\\theta$ scale).\n\nWe are adding this information in the main text, as you pointed out it is important to make it clear, and we thank you for this comment.\n\n[Continues in Part 2]"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611316, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGkSo0qYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper51/Authors|ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611316}}}, {"id": "HyezC8TlC7", "original": null, "number": 3, "cdate": 1542670025785, "ddate": null, "tcdate": 1542670025785, "tmdate": 1542670025785, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "rklxB0GinX", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "content": {"title": "Answer to reviewer (Part 2)", "comment": "About the dataset sizes:\n---------------------------------\nWhen comparing graph quality on real data, we usually face the challenge of not having in hand an actual ground truth graph. To have concrete quantitative comparisons, we chose the MNIST experiment as we could use classification error of label propagation as a proxy for graph quality. Note that for 60,000 nodes, saving a full adjacency matrix would need 28.8 GB of storage (for 64-bit floats, square form of W), therefore all non scalable algorithms (including Dong etal, Kalofolias, and Daitch) would run into problems.\n\nTo add comparisons for larger data, as you suggested, we learned a large scale graph where we know the underlying node structure. We used 1000 signals from data from [Perraudin etal. 2018]. It consists of simulated cosmological mass maps, i.e. the amount of mass in the universe observed from earth in every direction. Hence  this data resides on the sphere, which can be considered as its underlying manifold. For this experiment, we learned a graph for a subpart of the sphere, i.e between 262,000 nodes (512x512 grid). For this new graph, we first see how well the theta bounds approximate the final graph sparsity we obtain. We then compute the first few eigenvectors of the Laplacian and see that the two first non trivial eigenvectors give an almost square 2-D grid embedding (like Laplacian eigenmaps), even if there was no information of x,y,z coordinates in our original data. We will add plots in the appendix (due to space constraints) showing the 2-D embedding we obtain. We note that the visual quality of the obtained embedding is significantly worse using A-NN and slightly worse using l2.\n\nIn terms of time complexity, as we mentioned in Section 5.5, our algorithm can learn in reasonable time a graph of 1 million nodes on a desktop running Matlab. We extended slightly this experiment by adding a scalability figure in the appendix. While this is a proof of scalability, for really large data one will have to consider implementation details like memory management, and using a faster programming language (like C++). \n\n\nLocality Sensitive Hashing:\n------------------------------------\n\nWe are happy to cite LSH among other possibilities for finding A-NN. While a different A-NN technique might provide a better initial support and hence overall results, we believe that providing that A-NN is above some quality threshold, the final quality of the results will not be affected so much. Indeed the task of the optimization problem is to select which edges should be kept from the initial support (that is much larger than the final graph). So the optimization should compensate for at least some of the A-NN errors.\nTake for example Figure 4. To obtain the L2 and Log graphs of degree k=10, we started from an A-NN graph of degree k=30 (yellow line, up right) and set to zero many erroneous edges by learning. \n\nWhile studying the effect of different A-NNs to the final quality of the graph is interesting, this contribution focuses on scaling the problem of graph learning. Doing a fair and complete comparison with more A-NN models would be a long publication itself, and could make our submission lose its focus.\n\n\nTightness of theta intervals with outliers in data:\n---------------------------------------------------------------\nThe theta intervals of equation (17) are indeed robust to outliers. We run experiments adding outliers (10% images contaminated with large amounts of Gaussian noise) in the MNIST dataset, and plotted the theoretical versus obtained sparsity of graphs versus the choice of parameter theta. The result is almost identical with the one of Figure 2b. The explanation is that the theta intervals of eq. (17) are really dependent on the smallest distances in $Z$, rather than the largest ones: $Z^\\hat$ is sorted, and B as well. Adding outliers induces additional nodes distances larger than usually, that never make it in the equation of 17, except for the few outlier nodes.\n\nTo complete the experiment, we also tried adding 10% duplicate images instead of outliers. In that case, we have 10% pairs of images that have essentially zero distances with each other. We thought this could affect the intervals much more, but we were wrong: the theta intervals are again very robust to this duplicates in the data: while the values of theta proposed are an order of magnitude larger in the case of duplicates, these values obtain graphs with degree within distance 1 from the desired one, as is the case for the outliers data, and the original MNIST data. We have added this new experiment in the appendix, but could also try to fit it in the main paper material. \n\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611316, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGkSo0qYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper51/Authors|ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611316}}}, {"id": "SJxmCraxAX", "original": null, "number": 2, "cdate": 1542669771140, "ddate": null, "tcdate": 1542669771140, "tmdate": 1542669771140, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "rklxB0GinX", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "content": {"title": "Answer to reviewer (Part 1)", "comment": "Dear reviewer,\nWe really appreciate your thorough review and the positive comments, as well as your propositions for improvements. We tried to address the points that you made in your review, and we believe that the paper is, as you suggested, stronger with these additions.\n\n||LX||_F^2:\n----------------\nThe models of Daitch etal are indeed very important ones in the graph learning literature. Following your comments, we tried to compare both in accuracy and scalability. Note that there are actually two models in their paper, that they call \u201chard\u201d and \u201csoft\u201d graphs. Hard graphs have the hard constraint that each node has degree at least 1. Soft graphs allow for degrees lower than 1, but penalize them quadratically.\n\nIn their original paper, Daitch etal solve their optimization problem using SDPT3, which has a complexity significantly higher than O(v^2) for v variables. So even when v=o(n), the complexity is not better than O(n^2). Furthermore, in order to solve their problem in a more scalable way, Daitch etal restrict the support of the graph to a subset of edges. Then, by checking the KKT conditions on the dual variable, they can assess if the some edges should be added to the optimization scheme. If so, the optimization is run again. This is actually a very nice idea and it allows for solving larger problems, (particularly if SDPT3 is used). However, the search of the next relevant support costs O(n^2) again, since they have to compute the KKT conditions for all possible edges.\n\nWe started by implementing the original hard and soft Daitch algorithm and obtained similar speed as reported in the original paper. Unfortunately, we were not able to run the algorithms for more than a few thousands of nodes. Hence, in order to cope with their scalability issues and provide some comparison with our models, we have derived a \u201cmore\u201d scalable variant of Daitch algorithm. Essentially, we did two modifications. First, we removed the support optimization using the KKT conditions and use the same support as our algorithm.  Second, we used FISTA and primal dual optimization schemes respectively for the soft and hard graph optimization instead of SDPT3. This time, our implementation scaled to the order of a 100\u2019000 nodes for a powerful desktop computer with 64Gb of RAM. The running times of optimization are still significantly higher than our models, because the term ||LX||_F^2 =||M w||_2^2 takes p times more computation than tr(X^T L X) = ||Z w||_1, where p is the number of signals.\n\nWe can add the resulting time in Figure 1 of our paper: within 30 seconds (the new y limit of the plot), we were only able to learn a hard graph of 250 nodes using quadratic programming, and a soft graph of 2000 nodes when proximal splitting method were used. When the KKT conditions trick is not used, our version of Daitch algorithm were much faster but still significantly slower than our algorithm because of the dependencies on p. Note that we also used random projections to reduce the dimension from 300 to 20 only for the Daitch algorithms, while ours was running on the full set of 300 features\n\nIn terms of quality, in our paper we focus on scalable algorithms (A-NN, scaled L2-degrees, scaled Log-degrees). Figures 4 and 5 of our paper show this comparison, that now we also run for the scaled version of the Daitch-soft model. In our experiments, we see that the Daitch model has many wrong edges in Figure 4, while in Figure 5 it performs slightly better than the L2 scaled model for specific edge densities, but always worse than the A-NN and the scaled Log-degrees model. Also, we see that Daitch suffers the same problem as the scaled L2 model: it has many disconnected nodes (less than L2, more than Log). This is expected, as the soft constraint is a quadratic one similar to the one of the L2 model, that allows node degrees to be zero.\n\nWe believe that one major issue of the Daitch hard algorithm is that it does not provide a way to control sparsity. Hence we varied the size of the support to get different graphs. As for the soft algorithm, the regularization parameter controls the strength of the constraint, but it is very difficult to obtain arbitrary sparsity levels outside a small interval. \n\n[Continues in Part 2]"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611316, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGkSo0qYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper51/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper51/Authors|ICLR.cc/2019/Conference/Paper51/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers", "ICLR.cc/2019/Conference/Paper51/Authors", "ICLR.cc/2019/Conference/Paper51/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611316}}}, {"id": "HkgeCuI5nQ", "original": null, "number": 2, "cdate": 1541200072153, "ddate": null, "tcdate": 1541200072153, "tmdate": 1541534328216, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Review", "content": {"title": "marginally bellow threshold", "review": "The paper proposes a scalable approximate calculation of graph construction. Based on the sparse optimization formulation of a graph construction, the authors provide a way to select parameter automatically based on user desired connectivity of graph.\n\nThe problem setting, graph construction, is significant for the wide range of ML community. Overall, however, advantage/novelty of the proposed method is unclear for me.\n\nScalability is main advantage of the proposed method, but the authors just employed known nearest neighbor approximation methods, and thus here no technical novelty is shown.\n\nI couldn't find connection between Section 3 and 4, these seem to be an independent topics. Main claim of the paper would be in Section 3, but the novelty would be weak as mentioned above.\n\nSolving reverse problem is interesting, but it just provide the parameter value range which results in given sparsity level k. This doesn't provide exact value of \\theta (and user still have to specify k), and selection would be possible easily without the analytical formula (e.g., by following the regularization path)\n\nPerformance verification is not convincing. Showing accuracy gain for more wide variety of datasets would be convincing.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Review", "cdate": 1542234549007, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335638080, "tmdate": 1552335638080, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgZllZ52X", "original": null, "number": 1, "cdate": 1541177320725, "ddate": null, "tcdate": 1541177320725, "tmdate": 1541534328015, "tddate": null, "forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "invitation": "ICLR.cc/2019/Conference/-/Paper51/Official_Review", "content": {"title": "Good paper, very well written", "review": "This paper proposed an approximation technique to learn the large-scale graph with the desired edge density. It was well-written and contains thorough experimental results and analysis.\n\nA minor drawback is that while this work was motivated by the use of k-NN graph in graph convolution network (GCN), there was no evidence on how well A-NN performs in compare to k-NN with GCN.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper51/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Large Scale Graph Learning From Smooth Signals", "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.", "paperhash": "kalofolias|large_scale_graph_learning_from_smooth_signals", "authorids": ["v.kalofolias@gmail.com", "nathanael.perraudin@sdsc.ethz.ch"], "authors": ["Vassilis Kalofolias", "Nathana\u00ebl Perraudin"], "keywords": ["Graph learning", "Graph signal processing", "Network inference"], "pdf": "/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf", "_bibtex": "@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathana\u00ebl Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper51/Official_Review", "cdate": 1542234549007, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGkSo0qYm", "replyto": "ryGkSo0qYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper51/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335638080, "tmdate": 1552335638080, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper51/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}