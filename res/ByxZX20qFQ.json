{"notes": [{"id": "ByxZX20qFQ", "original": "HJlqU5j5Ym", "number": 1334, "cdate": 1538087961393, "ddate": null, "tcdate": 1538087961393, "tmdate": 1550878959105, "tddate": null, "forum": "ByxZX20qFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1xIRD66yN", "original": null, "number": 1, "cdate": 1544570830225, "ddate": null, "tcdate": 1544570830225, "tmdate": 1545354500313, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Meta_Review", "content": {"metareview": "There is a clear consensus among the reviews to accept this submission thus I am recommending acceptance. The paper makes a clear, if modest, contribution to language modeling that is likely to be valuable to many other researchers.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "clear consensus to accept"}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1334/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352874881, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1334/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1334/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352874881}}}, {"id": "SJgnkDkMCm", "original": null, "number": 5, "cdate": 1542743779926, "ddate": null, "tcdate": 1542743779926, "tmdate": 1542743779926, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "H1lJ9-Pdn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "The primary goal of the projections is to project all embeddings into the model dimension d so that we can have variable sized embeddings. Our goal was not to make the model model expressive. Compared to the rest of the model, these projections add very little overhead compared to the rest of the model. Doing without them is an interesting future direction though!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616904, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxZX20qFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1334/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1334/Authors|ICLR.cc/2019/Conference/Paper1334/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616904}}}, {"id": "ryloqL1G0Q", "original": null, "number": 4, "cdate": 1542743699094, "ddate": null, "tcdate": 1542743699094, "tmdate": 1542743699094, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "HJxeCIut2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the comments! \n\nQ: \u201cADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4)\u201d\nThe differences in training time are due to the size of the models: Weight tying saves a lot more parameters for the Billion Word model due to the larger vocab compared to the WikiText-103 models which have a smaller vocab. On WikiText-103, tying saves 15% of parameters (Table 3, ADP vs ADP-T, 291M vs 247M) and training time is reduced by about 13%. On Billion Word, tying saves 27% of parameters (Table 4) and training time is reduced by about 34%. The slight discrepancy may be due to multi-machine training for Billion Word compared to the single machine setup for WikiText-103.\n\nQ1: \"I am curious about what would you get if you use ADP on BPE vocab set?\"\nWe tried adaptive input embeddings with BPE but the results were worse than softmax. This is likely because 'rare' BPE units are in some sense not rare enough compared to a word vocabulary. In that case, the regularization effect of assigning less capacity to 'rare' BPE tokens through adaptive input embeddings is actually harmful.\n\nQ2: \"How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?\"\nFor WikiText-103 (Table 3) we measured 24.92 on test with a full softmax model (a 5.2 PPL improvement over the previous SOTA). This corresponds to a Transformer model including our tuned optimization scheme. Adding tied adaptive input embeddings (ADP-T) to this configuration reduces this perplexity to 20.51, which is another reduction of 4.4 PPL."}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616904, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxZX20qFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1334/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1334/Authors|ICLR.cc/2019/Conference/Paper1334/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616904}}}, {"id": "Hke6aByfA7", "original": null, "number": 3, "cdate": 1542743493365, "ddate": null, "tcdate": 1542743493365, "tmdate": 1542743493365, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "H1xamJD0hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for the comments! \n\nQ: \u201ccomparing directly to Merity et al.'s approach\u201d\nMerity et al. share the input and output embeddings via an adaptive softmax where all words have the same embedding size. We reimplemented their approach and found that it did not perform very well in our experiments (25.48 PPL; Appendix A, Table 6, last row). We found that sharing fixed size input and output embeddings for a flat softmax performs better (22.63 PPL; second to last row of Table 6). This is likely because we train all words at every time step, which is not the case for an adaptive softmax with fixed size embeddings.\n\nQ: \u201cThe discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing\u201d\nWe updated the paper and hope that the discussion is clearer now. Thank you for the feedback!\n\nQ: \u201cthoughts as to why full-softmax BPE is worse than adaptive softmax word level\u201d\nFull-softmax BPE is worse because we measure perplexity on the word-level. This involves multiplying the probabilities of the individual BPE tokens. BPE token-level perplexity itself is actually significantly lower than word-level PPL (around 21.5 for GBW and around 18 for WikiText-103 for the models presented in the paper) but the two are not comparable.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616904, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxZX20qFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1334/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1334/Authors|ICLR.cc/2019/Conference/Paper1334/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616904}}}, {"id": "S1ePmKlW0Q", "original": null, "number": 1, "cdate": 1542682911081, "ddate": null, "tcdate": 1542682911081, "tmdate": 1542743372698, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "content": {"title": "Updated version of the paper", "comment": "We updated the paper with the following changes:\n* Table 3 contains new (better) validation results for WikiText-103. Note that only the validation numbers are updated, the test results were not affected. As described in the paper, we form training examples by taking 512 contiguous words from the training data with no regard for sentence boundaries. Evaluation is the same except that we require blocks to contain complete sentences of up to 512 tokens. Previously reported validation numbers did not always contain complete sentences because samples were built the same way as during training. We have corrected this so that validation is conducted the same way as testing.\n* We also added new (and better) Billion word results with a bigger model achieving 23.7 perplexity.\n* We added a comparison to Merity et al. fixed size adaptive softmax to the Appendix (Table 6).\n* Clarified discussion around tying and not tying projections/word embeddings. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616904, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxZX20qFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1334/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1334/Authors|ICLR.cc/2019/Conference/Paper1334/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616904}}}, {"id": "S1gSrFe-R7", "original": null, "number": 2, "cdate": 1542682940900, "ddate": null, "tcdate": 1542682940900, "tmdate": 1542682940900, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "SkxTKpJwa7", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "content": {"title": "open sourcing", "comment": "We are planning to open source the code and pre-trained models in the future."}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616904, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxZX20qFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1334/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1334/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1334/Authors|ICLR.cc/2019/Conference/Paper1334/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616904}}}, {"id": "SkxTKpJwa7", "original": null, "number": 1, "cdate": 1542024581100, "ddate": null, "tcdate": 1542024581100, "tmdate": 1542024581100, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Public_Comment", "content": {"comment": "Code and pre-trained models are available at http://anonymized.\n\nit is not available, would you fix it and I am very interested in your paper.", "title": "Hello the source code link in your paper is unaccessible?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311621996, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByxZX20qFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1334/Authors", "ICLR.cc/2019/Conference/Paper1334/Reviewers", "ICLR.cc/2019/Conference/Paper1334/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311621996}}}, {"id": "H1xamJD0hQ", "original": null, "number": 3, "cdate": 1541463845243, "ddate": null, "tcdate": 1541463845243, "tmdate": 1541533221308, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Official_Review", "content": {"title": "Solid contribution to the language modeling literature", "review": "The authors extend an existing approach to adaptive softmax classifiers used for the output component of neural language models into the input component, once again allowing tying between the embedding and softmax. This fills a significant gap in the language modeling architecture space, and the perplexity results bear out the advantages of combining adaptively-sized representations with weight tying. While the advance is in some sense fairly incremental, the centrality of unsupervised language modeling to modern deep NLP (ELMo, BERT, etc.) implies that perplexity improvements as large as this one may have meaningful downstream effects on performance on other tasks. Some things I noticed:\n\n- One comparison that I believe is missing (I could be misreading the tables) is comparing directly to Merity et al.'s approach (adaptive softmax but fixed embedding/softmax dimension among the bands). Presumably you're faster, but is there a perplexity trade-off?\n\n- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.\n\n- The loss by frequency-bin plots are really fantastic. You could also try a scatterplot of log freq vs. average loss by individual word/BPE token.\n\n- Do you have thoughts as to why full-softmax BPE is worse than adaptive softmax word level? That goes against the current (industry) conventional wisdom in machine translation and large-scale language modeling that BPE is solidly better than word-level approaches because it tackles the softmax bottleneck while also sharing morphological information between words.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Official_Review", "cdate": 1542234252542, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1334/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335924301, "tmdate": 1552335924301, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1lJ9-Pdn7", "original": null, "number": 1, "cdate": 1541071239338, "ddate": null, "tcdate": 1541071239338, "tmdate": 1541533221104, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Official_Review", "content": {"title": "Reasonable increment from Grave et al. (2017)", "review": "This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations.\n\nThe article is well written and I find the contribution simple, but interesting. It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017).\n\nMy question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space. I understand that for two matrices A and B we have rank(AB) <= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?\n\nReferences\nJoulin, A., Ciss\u00e9, M., Grangier, D. and J\u00e9gou, H., 2017, July. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp. 1302-1310).", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Official_Review", "cdate": 1542234252542, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1334/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335924301, "tmdate": 1552335924301, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJxeCIut2Q", "original": null, "number": 2, "cdate": 1541142215690, "ddate": null, "tcdate": 1541142215690, "tmdate": 1541533220896, "tddate": null, "forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1334/Official_Review", "content": {"title": "simple model architecture changed and extensive experiments", "review": "This paper introduced a new architecture for input embeddings of neural language models: adaptive input representation (ADP). ADP allowed a model builder to define a set of bands of input words with different frequency where frequent words have larger embedding size than the others. The embeddings of each band are then projected into the same size. This resulted in lowering the number of parameters. \n\nExtensive experiments with the Transformer LM on WikiText-103 and Billion Word corpus showed that ADP achieved competitive perplexities. While tying weight with the output did not benefit the perplexity, it lowered the runtime significantly on Billion Word corpus. Further analyses showed that ADP gained performance across all word frequency ranges.\n\nOverall, the paper was well-written and the experiments supported the claim. The paper was very clear on its contribution. The variable-size input of this paper was novel as far as I know. However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax. The weight sharing was also needed further investigation and experimental data on sharing different parts.\n\nThe experiments compared several models with different input levels (characters, BPE, and words). The perplexities of the proposed approach were competitive with the character model with an advantage on the training time. However, the runtimes were a bit strange. For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4). The runtime of ADP seemed to lose in term of scaling as well to BPE. Perhaps, the training time was an artifact of multi-GPU training. \n\nQuestions:\n1. I am curious about what would you get if you use ADP on BPE vocab set?\n2. How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1334/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Input Representations for Neural Language Modeling", "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "keywords": ["Neural language modeling"], "authorids": ["alexei.b@gmail.com", "michael.auli@gmail.com"], "authors": ["Alexei Baevski", "Michael Auli"], "TL;DR": "Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.", "pdf": "/pdf/b9798d0b95f2dbcbdf08c448d18eaad3d2ec2c29.pdf", "paperhash": "baevski|adaptive_input_representations_for_neural_language_modeling", "_bibtex": "@inproceedings{\nbaevski2018adaptive,\ntitle={Adaptive Input Representations for Neural Language Modeling},\nauthor={Alexei Baevski and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZX20qFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1334/Official_Review", "cdate": 1542234252542, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxZX20qFQ", "replyto": "ByxZX20qFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1334/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335924301, "tmdate": 1552335924301, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1334/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}