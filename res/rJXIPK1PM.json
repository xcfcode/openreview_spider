{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124281870, "tcdate": 1518470986931, "number": 289, "cdate": 1518470986931, "id": "rJXIPK1PM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rJXIPK1PM", "signatures": ["~David_Lopez-Paz2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Easing non-convex optimization with neural networks", "abstract": "Despite being non-convex, deep neural networks are surprisingly amenable to optimization by gradient descent. In this note, we use a deep neural network with $D$ parameters to parametrize the input space of a generic $d$-dimensional non-convex optimization problem. Our experiments show that minimizing the over-parametrized $D \\gg d$ variables provided by the deep neural network eases and accelerates the optimization of various non-convex test functions.", "paperhash": "lopezpaz|easing_nonconvex_optimization_with_neural_networks", "keywords": ["nonconvex optimization", "deep neural networks", "overparametrized models"], "_bibtex": "@misc{\n  lopez-paz2018easing,\n  title={Easing non-convex optimization with neural networks},\n  author={David Lopez-Paz and Levent Sagun},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXIPK1PM}\n}", "authorids": ["dlp@fb.com", "levent.sagun@ipht.fr"], "authors": ["David Lopez-Paz", "Levent Sagun"], "TL;DR": "deep neural networks can be used to ease generic nonconvex optimization problems", "pdf": "/pdf/21aa6295e0bcea1d6dc3b4f72b95b1a5bd88738f.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582978401, "tcdate": 1519841391745, "number": 1, "cdate": 1519841391745, "id": "B1_uxuNOG", "invitation": "ICLR.cc/2018/Workshop/-/Paper289/Official_Review", "forum": "rJXIPK1PM", "replyto": "rJXIPK1PM", "signatures": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer1"], "content": {"title": "Interesting Results on Overparametrization", "rating": "6: Marginally above acceptance threshold", "review": "This paper provides interesting new angle on over-paramterization. Existing works mostly try to explain why over-parametrization works for neural network. This paper tries over-paramterization on classical optimization problems and demonstrate its effectiveness.\nThe experiments are interesting. However, I hope to see more theoretical (not necessary rigorous) analysis.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Easing non-convex optimization with neural networks", "abstract": "Despite being non-convex, deep neural networks are surprisingly amenable to optimization by gradient descent. In this note, we use a deep neural network with $D$ parameters to parametrize the input space of a generic $d$-dimensional non-convex optimization problem. Our experiments show that minimizing the over-parametrized $D \\gg d$ variables provided by the deep neural network eases and accelerates the optimization of various non-convex test functions.", "paperhash": "lopezpaz|easing_nonconvex_optimization_with_neural_networks", "keywords": ["nonconvex optimization", "deep neural networks", "overparametrized models"], "_bibtex": "@misc{\n  lopez-paz2018easing,\n  title={Easing non-convex optimization with neural networks},\n  author={David Lopez-Paz and Levent Sagun},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXIPK1PM}\n}", "authorids": ["dlp@fb.com", "levent.sagun@ipht.fr"], "authors": ["David Lopez-Paz", "Levent Sagun"], "TL;DR": "deep neural networks can be used to ease generic nonconvex optimization problems", "pdf": "/pdf/21aa6295e0bcea1d6dc3b4f72b95b1a5bd88738f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582978200, "id": "ICLR.cc/2018/Workshop/-/Paper289/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper289/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper289/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper289/AnonReviewer2"], "reply": {"forum": "rJXIPK1PM", "replyto": "rJXIPK1PM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper289/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper289/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582978200}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582827342, "tcdate": 1520608822260, "number": 2, "cdate": 1520608822260, "id": "ByCELQxtM", "invitation": "ICLR.cc/2018/Workshop/-/Paper289/Official_Review", "forum": "rJXIPK1PM", "replyto": "rJXIPK1PM", "signatures": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer3"], "content": {"title": "Overall good but needs some improvements", "rating": "7: Good paper, accept", "review": "This paper considers the problem of solving non-convex problems using deep neural network. The authors propose that the deep neural networks can ease and accelerate the optimization process because of the redundancy of parameters. This idea is interesting and the results are promising. The experiments show that if D>>d, using deep neural networks can achieve faster convergence. My major concerns are as follows.\n(1) The test data is small. The authors may want to conduct experiments on large scale real world data.\n(2) The authors may try their method to solve more non-convex problems.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Easing non-convex optimization with neural networks", "abstract": "Despite being non-convex, deep neural networks are surprisingly amenable to optimization by gradient descent. In this note, we use a deep neural network with $D$ parameters to parametrize the input space of a generic $d$-dimensional non-convex optimization problem. Our experiments show that minimizing the over-parametrized $D \\gg d$ variables provided by the deep neural network eases and accelerates the optimization of various non-convex test functions.", "paperhash": "lopezpaz|easing_nonconvex_optimization_with_neural_networks", "keywords": ["nonconvex optimization", "deep neural networks", "overparametrized models"], "_bibtex": "@misc{\n  lopez-paz2018easing,\n  title={Easing non-convex optimization with neural networks},\n  author={David Lopez-Paz and Levent Sagun},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXIPK1PM}\n}", "authorids": ["dlp@fb.com", "levent.sagun@ipht.fr"], "authors": ["David Lopez-Paz", "Levent Sagun"], "TL;DR": "deep neural networks can be used to ease generic nonconvex optimization problems", "pdf": "/pdf/21aa6295e0bcea1d6dc3b4f72b95b1a5bd88738f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582978200, "id": "ICLR.cc/2018/Workshop/-/Paper289/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper289/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper289/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper289/AnonReviewer2"], "reply": {"forum": "rJXIPK1PM", "replyto": "rJXIPK1PM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper289/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper289/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582978200}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582741266, "tcdate": 1520663969747, "number": 3, "cdate": 1520663969747, "id": "r1qjTeZtz", "invitation": "ICLR.cc/2018/Workshop/-/Paper289/Official_Review", "forum": "rJXIPK1PM", "replyto": "rJXIPK1PM", "signatures": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer2"], "content": {"title": "Review for \"Easing non-convex optimization with neural networks\"", "rating": "6: Marginally above acceptance threshold", "review": "This paper studies using neural networks to assist non-convex optimization. The main idea is to replace the parameter vector x (to be optimized) by the output of a neural network g(z), then optimize both z and the parameters of network g. The paper also proposes a parallelized scheme. That is, instead of optimizing f(g(z)), it defines the objective function to be f(g(z_1)) + ... + f(f(z_n)), then jointly optimize (z_1,...,z_n) and the parameters of g. Then the best solution among g(z_1),...,g(z_n) is taken as the final solution.\n\nThe idea behind the optimization algorithm is interesting. The problem will be solved if one of the n sub-problems for optimizing f(g(z_1)), ..., f(g(z_n)) is solved. The computation for solving these problems are correlated. Indeed, if one of z_1, ... , z_n reaches a region of low function value, it will help shaping the shared network g, which can help other sub-problems attaining a low function value as well. Another benefit of this optimization scheme is to utilize the existing linear algebra and parallel computing frameworks, which have been highly optimized for neural network computation. Since the algorithm is very simple, it is widely applicable to a broad range of non-convex optimization problems.\n\nAlthough the idea is promising, the empirical justification is preliminary. There is no experiments on real problems, or large-scale synthetic data. \n\nIn addition, the proposed method should be compared with the following simple baseline:\n\noptimize f(x_1) + .... + f(x_n)\n\nwhere x_1,...,x_n have different random initializations. The hyper-parameter n should be set such that the total number of parameters in the above problem is equal to the total number of parameters in the proposed neural network method. The proposed method should be compared with this baseline in CPU time.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Easing non-convex optimization with neural networks", "abstract": "Despite being non-convex, deep neural networks are surprisingly amenable to optimization by gradient descent. In this note, we use a deep neural network with $D$ parameters to parametrize the input space of a generic $d$-dimensional non-convex optimization problem. Our experiments show that minimizing the over-parametrized $D \\gg d$ variables provided by the deep neural network eases and accelerates the optimization of various non-convex test functions.", "paperhash": "lopezpaz|easing_nonconvex_optimization_with_neural_networks", "keywords": ["nonconvex optimization", "deep neural networks", "overparametrized models"], "_bibtex": "@misc{\n  lopez-paz2018easing,\n  title={Easing non-convex optimization with neural networks},\n  author={David Lopez-Paz and Levent Sagun},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXIPK1PM}\n}", "authorids": ["dlp@fb.com", "levent.sagun@ipht.fr"], "authors": ["David Lopez-Paz", "Levent Sagun"], "TL;DR": "deep neural networks can be used to ease generic nonconvex optimization problems", "pdf": "/pdf/21aa6295e0bcea1d6dc3b4f72b95b1a5bd88738f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582978200, "id": "ICLR.cc/2018/Workshop/-/Paper289/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper289/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper289/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper289/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper289/AnonReviewer2"], "reply": {"forum": "rJXIPK1PM", "replyto": "rJXIPK1PM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper289/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper289/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582978200}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573565771, "tcdate": 1521573565771, "number": 100, "cdate": 1521573565428, "id": "SyLpCARtz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rJXIPK1PM", "replyto": "rJXIPK1PM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Easing non-convex optimization with neural networks", "abstract": "Despite being non-convex, deep neural networks are surprisingly amenable to optimization by gradient descent. In this note, we use a deep neural network with $D$ parameters to parametrize the input space of a generic $d$-dimensional non-convex optimization problem. Our experiments show that minimizing the over-parametrized $D \\gg d$ variables provided by the deep neural network eases and accelerates the optimization of various non-convex test functions.", "paperhash": "lopezpaz|easing_nonconvex_optimization_with_neural_networks", "keywords": ["nonconvex optimization", "deep neural networks", "overparametrized models"], "_bibtex": "@misc{\n  lopez-paz2018easing,\n  title={Easing non-convex optimization with neural networks},\n  author={David Lopez-Paz and Levent Sagun},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXIPK1PM}\n}", "authorids": ["dlp@fb.com", "levent.sagun@ipht.fr"], "authors": ["David Lopez-Paz", "Levent Sagun"], "TL;DR": "deep neural networks can be used to ease generic nonconvex optimization problems", "pdf": "/pdf/21aa6295e0bcea1d6dc3b4f72b95b1a5bd88738f.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}