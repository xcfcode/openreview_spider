{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487970865377, "tcdate": 1478276540541, "number": 214, "id": "S1VaB4cex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1VaB4cex", "signatures": ["~Michael_Maire1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396437285, "tcdate": 1486396437285, "number": 1, "id": "BJTGhMLde", "invitation": "ICLR.cc/2017/conference/-/paper214/acceptance", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper describes a novel and simple way of constructing deep neural networks using a fractal expansion rule. It is evaluated on several datasets (ImageNet, CIFAR 10/100, SVHN) with promising results which are on par with ResNets. As noted by the reviewers, FractalNets would benefit from additional exploration and analysis.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396437791, "id": "ICLR.cc/2017/conference/-/paper214/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1VaB4cex", "replyto": "S1VaB4cex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396437791}}}, {"tddate": null, "tmdate": 1485295250177, "tcdate": 1485295250177, "number": 4, "id": "S1cqRHSPg", "invitation": "ICLR.cc/2017/conference/-/paper214/public/comment", "forum": "S1VaB4cex", "replyto": "ryKsOZQvl", "signatures": ["~Michael_Maire1"], "readers": ["everyone"], "writers": ["~Michael_Maire1"], "content": {"title": "Re: rebuttal response", "comment": "> only the technicality in the paper is the fractal network architecture with intuitive claims\n\nOur experiments demonstrate two novel technical capabilities:\n(1) The ability to train fractal networks to exhibit an anytime property.  As the rebuttal clarifies, residual networks do not have this property.\n(2) The ability to \u201cpeel-off\u201d the deepest column of such a fractal network for use as a stand-alone network with high performance.  The resulting network has the same connectivity as a plain network, yet performs substantially better than a plain network trained in isolation.\n\nBoth claims are backed by quantitative performance numbers in the respective experiments.\n\nFrom (2), we can extrapolate that the fractal structure is a training scaffold, with its purpose being to allow the network to evolve from being effectively shallow to deep over the course of training.\n\n> Authors state in the paper that \" In experiments, fractal networks **match the state-of-the-art\nperformance held by residual networks** on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks\"\n\nYes, at the same depth (34-layers), FractalNet-34 matches the performance of ResNet-34 C.  \n\n> Veit et al demonstrates systematical empirical support for their claims about analyzing residual networks (which seems to be on arxiv since May but still not cited on paper ).\n\nAs detailed in the \"Explanatory power\" section of our rebuttal, we disagree with [Veit et al]\u2019s conclusions, disagreement shared with, e.g.,  [Greff et al, arXiv:1612.07771, ICLR 2017 submission].\n\nSince [Veit et al]\u2019s goals (analyzing ResNet) differ from ours (demonstrating alternatives, understanding very deep networks in general), we opted to be non-confrontational.  We now agree that it is better to cite [Veit et al] and [Greff et al] and state our disagreement.  We will add the citations and points made in our rebuttal to the final version of the paper.\n\n> can not report results on dozens of layers\n\nWe are puzzled by this comment. Table 1: results for 20 and 40 layer FractalNets; Table 2: 34 layers; Table 3: 20, 40, 80, 160 layers; Table 4: 20 and 40 layer networks. That\u2019s \"dozens\".\n\n> also in Table 3, error increases as depth increase to 160 layers\n\nThis characterization is inaccurate.  Error decreases gradually from 10 through 80 layers, then only slightly increases when going from 80 to 160 layers.  This shows diminishing returns with increased depth, but also robustness to being too deep; extra depth does not break trainability.\n\n> As an answer to lack of comparison to DenseNet, authors argue in rebuttal that DenseNet cites FractalNet but this can not be a reason for the lack of comparison as DenseNet was published in August and well-known as holding state of art results as a resnet variant. \n\nThe rebuttal clearly stated that we are happy to add DenseNet to our comparison tables (and we will).\n\nNote: like the Wide Residual Networks paper, *at the time of ICLR submission*, the DenseNet paper did not include results on ImageNet (https://arxiv.org/abs/1608.06993v2 revised on Nov 29 added ImageNet results for DenseNet).\n\n> Authors state in rebuttal that \"Many of the variants only reported results on CIFAR/SVHN\u2026 it is a bit difficult to have already compared to results that did not exist at submission time.\" .  However there were clearly published ones, for example two results by Huang et al, 2016b on july (arxiv) ..\n\nTable 1 (CIFAR/SVHN) has always included results for (Huang et al, 2016b, Stochastic Depth).  \n\nOn ImageNet, (Huang et al, 2016b) state that their model fails to improve over a baseline ResNet.  See Table 1 and Section 4 in their latest version: https://arxiv.org/abs/1603.09382v3\n\n> Regarding the \"simplifying power\" claim of authors.  It is not very convincing that it is simplifying. how can it simplify with a harder training procedure with many parameters that can not scale as good as baselines?\n\nOur training procedure is exactly as simple as ResNet\u2019s: train a network with a single attached loss in an end-to-end manner via SGD.\n\nDrop-path is optional for FractalNet, analogous to the manner in which stochastic depth is optional for ResNet.  They can each be used as additional regularization, at the cost of more complicated training.  Drop-path, in addition, can optionally enforce the anytime property.\n\nWe also note that, in terms of parameters, choosing to extract the deepest FractalNet column as a stand-alone high-performance network approximately halves the number of parameters used at test time.\n\n> The rebuttal lists unsatisfactory answers with somehow manipulative arguments\n\nWe are not sure how to respond to this unfortunate characterization of our arguments.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682127, "id": "ICLR.cc/2017/conference/-/paper214/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1VaB4cex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper214/reviewers", "ICLR.cc/2017/conference/paper214/areachairs"], "cdate": 1485287682127}}}, {"tddate": null, "tmdate": 1485146273283, "tcdate": 1485146273283, "number": 5, "id": "ryKsOZQvl", "invitation": "ICLR.cc/2017/conference/-/paper214/official/comment", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["ICLR.cc/2017/conference/paper214/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper214/AnonReviewer1"], "content": {"title": "rebuttal response", "comment": "Authors claim that the reviewers reflect \"narrow view\" of on the number of parameters and fractions of accuracy and  \"complete focus on engineering\" etc . Authors claims about focusing on engineering are open to debate as only the technicality in the paper is the fractal network architecture with intuitive claims. One can either formulate the system with rigorous analysis with clear assumptions (which would require satisfactory theoretical analysis and relatively small scale empirical sanity check without heavy experiments) or propose a comprehensive empirical analysis by designing careful experiments to support intuitive claims. Therefore, as the approach clearly does not belong to the first category,  such an approach needs strong experimental evidence to support intuitive claims without a doubt.  The rebuttal lists unsatisfactory answers with somehow manipulative arguments about narrow reviewing/ dates of the baselines( which will be discussed below).. \n\n\nAuthors state in the paper that \" In experiments, fractal networks **match the state-of-the-art performance held by residual networks** on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks\" and \"Fractal architectures are arguably the simplest means of satisfying this requirement, and match or exceed residual networks in experimental performance\". Therefore the main support behind all the claims is that fractal network match (or exceed) performance of state of art residual networks, indeed the empirical study only compares accuracy to some baselines without designing empirical analysis for the claims about differences to resnets (or ensemble explanation of resnet by Veit et al(NIPS 2016) ). \nHowever even this comparison lacks many results as explained in my review.   Therefore the expectation of the fair comparison is completely reasonable and it is not about only focusing on fractions of accuracy or number of parameters. As a good example, Veit et al demonstrates systematical empirical support for their claims about analyzing residual networks (which seems to be on arxiv since May but still not cited on paper ). Table 4 and Figure 3 provide good preliminary sanity check but compares only to plain networks, do not support claims about differences between residual and fractal networks.\n\nThe paper claims that the fractal network scales to \"ultra\" deep networks, however authors can not report results on dozens of layers. Authors also claim that extra depth may slow training (not clear how much) but does not decrease accuracy but this is not clear as there are no results for dozens of layers, also in Table 3, error increases as depth increase to 160 layers.  Number of parameters of the proposed architecture is significantly greater than the state of art resnet variants as I explained in the review but the authors argue that it is slightly more parameters. \n\nAs an answer to lack of comparison to DenseNet, authors argue in rebuttal that DenseNet cites FractalNet but this can not be a reason for the lack of comparison as DenseNet was published in August and well-known as holding state of art results as a resnet variant. \n\nAuthors state in rebuttal that \"Many of the variants only reported results on CIFAR/SVHN\u2026 it is a bit difficult to have already compared to results that did not exist at submission time.\" .  However there were clearly published ones, for example two results by Huang et al, 2016b on july (arxiv) ..\n\nRegarding the \"simplifying power\" claim of authors.  It is not very convincing that it is simplifying. how can it simplify with a harder training procedure with many parameters that can not scale as good as baselines?\n\nIn essence, my evaluation did not change as rebuttal did not provide satisfactory clarification or improvement.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682001, "id": "ICLR.cc/2017/conference/-/paper214/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1VaB4cex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper214/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper214/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper214/reviewers", "ICLR.cc/2017/conference/paper214/areachairs"], "cdate": 1485287682001}}}, {"tddate": null, "tmdate": 1484929335077, "tcdate": 1484929335077, "number": 4, "id": "SJkBY2yve", "invitation": "ICLR.cc/2017/conference/-/paper214/official/comment", "forum": "S1VaB4cex", "replyto": "HJcpm-LIe", "signatures": ["ICLR.cc/2017/conference/paper214/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper214/AnonReviewer2"], "content": {"title": "Response to author rebuttal", "comment": "I thank the authors for their response and explanations. My primary objections to this paper are not based on performance, but (in my view) lack of sufficiently clear and careful testing of ideas.\n\nI partly agree with the authors regarding comparison to Inception networks specifially due to the join operation used by FractalNet which Inception does not use. Clearly, there are differences, and my use of the word 'direct' was incorrect. Nevertheless, I think the (indirect) relations are important because they may point the authors towards helpful baselines. More on this later.\n\nI also agree that the any time property enabled by FractalNet+drop-path appears to be stonger than what's been shown for Resnets/Highways. However, firstly, this does not mean that they do not have this property at all.\n\n(Due to the above two points, I am raising my score by one point)\n\nThe second point relates to my earlier concern about clear and careful comparisons and testing, which the authors have not addressed. For example, it is possible that the reason FractalNets have a better anytime property compared to Resnets is that they have many more parameters (or utilize many more computations) in the first place. In principle, if the Resnet/Highway was trained with many more layers than necessary, then one could also remove many more layers without hurting performance. Why should the anytime property be measured in terms of layers instead of number of computations required? This hypothesis could be tested and negated/confirmed using comparison to a well constructed baseline.\n\nSimilar arguments apply to other claimed attributes of the FractalNet design. As another example, the authors mention that although there are similarities with Inception, FractalNets have much longer paths. But they do not show (again, using carefully constructed baseline experiments) that this gives them an advantage over Inception design principles. What if, due to underlying design issues, FractalNets are unable to utilize these longer paths to reasonable advantage?\n\nMany such important scientific questions come to mind while reviewing this paper. On one hand, the authors encourage readers to not to focus too much on benchmarks. On the other hand, the paper itself primarily provides comparisons to pre-published numbers instead of comparable baselines through which the importance of ideas can be isolated and observed. Table 4 and Section 4.3 are good steps in this direction, but in my view several important scientific questions remain unanswered. The fact that overall, networks with a fractal design, batch norm, drop-path and dropout can perform competitively (given sufficient parameters) with other published networks is a good and interesting starting point, but immediately requires more careful investigation in my view."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682001, "id": "ICLR.cc/2017/conference/-/paper214/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1VaB4cex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper214/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper214/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper214/reviewers", "ICLR.cc/2017/conference/paper214/areachairs"], "cdate": 1485287682001}}}, {"tddate": null, "tmdate": 1484928792294, "tcdate": 1481932670179, "number": 1, "id": "rkUFJWfNl", "invitation": "ICLR.cc/2017/conference/-/paper214/official/review", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["ICLR.cc/2017/conference/paper214/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper214/AnonReviewer2"], "content": {"title": "Unconvincing experimental comparisons", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.\n\nHowever, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.\nThe number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:\n\n- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.\n\n- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\n- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].\n\n- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\n- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nOverall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. \n\n[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016).\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512662069, "id": "ICLR.cc/2017/conference/-/paper214/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper214/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper214/AnonReviewer2", "ICLR.cc/2017/conference/paper214/AnonReviewer1", "ICLR.cc/2017/conference/paper214/AnonReviewer3"], "reply": {"forum": "S1VaB4cex", "replyto": "S1VaB4cex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper214/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper214/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512662069}}}, {"tddate": null, "tmdate": 1484679641278, "tcdate": 1484679641278, "number": 1, "id": "H1ZJ513Ig", "invitation": "ICLR.cc/2017/conference/-/paper214/public/review", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["~George_Philipp1"], "readers": ["everyone"], "writers": ["~George_Philipp1"], "content": {"title": "From an interested reader: I agree with the authors rebuttals", "rating": "7: Good paper, accept", "review": "Looking through the comment section here, I agree to a large degree with the author's standpoint on many issues discussed. Points (1) through (4) in the authors comment below are, in my opinion, a good summary of the contributions of the paper. While I don't think those contributions are groundbreaking, I believe they are significant enough to merit acceptance.\n\nThe reason I am commenting here is because, having looked at several comment sections for this ICLR, I am seeing a general trend that reviews have a strong focus on performance, i.e. reviews tend to be very short and judge papers, to a large degree, on whether they are a few percentage points better or worse than the reported baseline. E.g. see the comments \"the experimental evaluation is not convincing, e.g. no improvement on SVHN\" or \"the effect of drop-path seems to vanish with data augmentation\" below.\n\nI believe that papers should be judged more on their scientific contributions (see points (1), (2) and (4) below), especially when those papers themselves state that their focus is on those scientific contributions, not on amazing performance.\n\nFurther, I believe the trend to focus excessively on performance is problematic for a number of reasons:\n\n - The Deep Learning community has focused very heavily on a few datasets (MNIST, ImageNet, CIFAR-10, CIFAR-100, SVHN). This means that at any time, a large chunk of the deep learning literature is battling for 5 SOTA titles. Hence, expecting any new model to attain one of those titles is a very high bar.\n\n - It is an arbitrary standard. Say the SOTA on ImageNet improves by 2% a year. Then a paper that outperforms by 1% in 2014 would underperform by 1% in 2015. By the performance standard, the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year. Is that really true?\n\n - How does one even draw a \"fair comparison\" on these standard datasets at this point? The bag of tricks for neural networks includes: drop-out, l2, l1, ensembling, various forms of data augmentation, various forms of normalization and initialization, various non-linearities, various learning rate schedules, various forms of pooling, label smoothing, gradient clipping etc. etc. There are a gazillion ways to eke out fractions of percentage points of performance. And - every single paper has a unique combination of tricks that they use for their model, even though the tricks themselves are unrelated to the model. Hence, the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used, which would take an exorbitant amount of time. What's worse, many papers do not even report all of the tricks they used. One would have to get the authors code and reverse engineer the model, not to mention slight differences introduced by using e.g. TensorFlow vs. Torch vs. Caffe. In this light, the request from one of the reviewers to have a baseline \"against which the improvements can be clearly demonstrated by making isolated changes\" seems unrealistic to me.\n\n - The ML community should not make excessive fine-tuning of models mandatory for publication. By requiring models to beat SOTA, we force each author to fine-tune their model ad nauseum, which leads to an arms race. To get a publications, authors would spend ever more time fine-tuning their models. This can not only lead to \"training on the test set\", but also wastes the time of researcher that could be better spent exploring new ideas.\n\n - It gives too much power to bad research. In science, there is always a certain background rate of \"bad\" results published: either the numbers are outright fake or the experimental protocol was invalid, e.g. someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result. What's worse, these \"bad\" results are far more likely to hold the SOTA title at any given time than a \"good\" result. By requiring new publications to beat SOTA, we give too much power to bad results.\n\n - It punishes authors for reporting many or strong baselines. In this paper, authors were careful to report many recent results. Table 1 is thorough. And now they are criticized for not beating all of those baselines. I have a feeling that if the authors of this paper had been more selective about which baselines they report, i.e. those that they can beat, they would have received higher scores on the paper. I have written an in-depth review for another paper at this conference that used, in my opinion, very weak baselines and ended up getting high reviewer marks. I don't think that was a coincidence. \n\nThe same arguments apply, though I think to a lesser degree, to judging models excessively on how many parameters they have or their runtime. However, I agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper. I would like to see a discussion of that in the final version. In general, I think this paper would benefit from an appendix with more details on model and training procedure. I also agree with reviewers that 80 layers, which is the deepest that authors can go while improving test error (Table 3), is not ultra-deep. Hence putting \"ultra-deep\" in the paper title seems exaggerated and I would recommend scaling back the language. However, I don't think being ultra-deep (~1000 layers) is necessary, because as Veit et al showed, networks that appear ultra deep might not be ultra deep in practice. Training an 80-layer net that functions at test time without residual connections seems to be enough of an achievement.\n\nIn summary, I think if a paper makes scientific contribution (see points (1), (2) and (4) below) independent of performance, then competitive performance should be enough for publication, instead of requiring SOTA. I believe this paper achieves that mark.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1484679642248, "id": "ICLR.cc/2017/conference/-/paper214/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1VaB4cex", "replyto": "S1VaB4cex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu", "ICLR.cc/2017/conference/paper214/reviewers", "ICLR.cc/2017/conference/paper214/areachairs", "~George_Philipp2"], "cdate": 1484679642248}}}, {"tddate": null, "tmdate": 1484293058248, "tcdate": 1484293058248, "number": 3, "id": "HJcpm-LIe", "invitation": "ICLR.cc/2017/conference/-/paper214/public/comment", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["~Michael_Maire1"], "readers": ["everyone"], "writers": ["~Michael_Maire1"], "content": {"title": "Rebuttal", "comment": "We thank the reviewers for their time, but disagree with many points in the reviews.  We first address what we feel is the overall disconnect between the reviews and the contributions of the paper, then rebut specific points below.\n\nMachine learning research is a mix of engineering, mathematics, and science.  Complete focus on engineering can restrict one to the narrow view of judging work by counting parameters and fractions of a percent in accuracy.  The reviews unfortunately reflect this mode of thinking.  However, long-term progress also requires scientific advancement, including new ideas as well as experiments that enhance understanding by revealing simple principles underlying complex phenomena.\n\nFractalNet is a new idea with:\n\n(1) Explanatory power:\n\nAs stated in the abstract, our experiments show that for the success of very deep networks, the \"key may be the ability to transition, during training, from effectively shallow to deep\".  This is the property shared by ResNet and FractalNet.\n\nFurthermore, this observation is an important counterpoint to [Veit et al., NIPS 2016] which claims that: (a) ensemble-like behavior is key and (b) the fraction of available effectively short paths is somehow related to performance.  FractalNet provides a counterexample: we can extract a single column (plain network topology with one long path) and it alone (no ensembling) performs as well the entire network.  In FractalNet, the rest of the network is a training apparatus for transitioning from shallow to deep.  This strongly suggests that, with more careful analysis, the same may be true for ResNet.\n\nIn fact, unlike [Veit et al., NIPS 2016] our explanation is consistent with the view that very deep networks, such as ResNet and FractalNet, learn unrolled iterative estimation [Greff et al., Highway and Residual Networks learn Unrolled Iterative Estimation, arXiv:1612.07771 and ICLR 2017 submission].  In this view, the longest path is most important; the ensemble can be discarded after training.  Moreover, our Section 4.3 and Figure 3 provide insight into the dynamics of this learning process.\n\nWe believe that the research community values understanding these mechanisms and would benefit from the evidence our work injects into the debate.\n\n(2) Simplifying power:\n\nFractalNet shows how a simple design principle produces structure similar to the mishmash of hacks (hand-designed modules, manually-attached auxiliary losses at intermediate depths) required in prior architectures like Inception.  It also expands the set of simple tricks for building very deep networks to include an option other than residual connections.\n\n(3) Good performance:\n\nFractalNet matches a ResNet of equal depth in terms of performance on ImageNet.  Training either of these networks from scratch takes weeks on modern GPUs.  Yet, because FractalNet uses slightly more parameters or does not beat the absolute best ResNet variant (developed after 7 or so papers iterating on ResNet) on the smaller and less important CIFAR dataset, the reviews declare experiments unsatisfactory.  There is a line between demanding high experimental standards and strangling promising ideas; we are all for the former, but hope not to fall victim to the latter.\n\n(4) New capabilities:\n\nContrary to AnonReviewer2's claim, prior work does not demonstrate an anytime property for ResNet; see our specific response below.  FractalNet, in combination with drop-path training, provides a novel anytime output capability, which could prove useful in real-world latency sensitive applications.\n\n\n-----\nResponse to AnonReviewer1\n\nPlease see our comments about experiments above.\n\n> Therefore the empirical effectiveness of drop-path is not convincing too.\n\nDrop-path serves two purposes: (a) additional regularization in the absence of data augmentation and (b) regularization that allows the resulting network to have the anytime property.  Yes, additional data augmentation can compensate for drop-path if one only cares about (a), but drop-path is essential for and the only effective means of enabling anytime output.\n\n> DenseNets (Huang et al, 2016a) should be also included in the comparison\n\nDenseNet cites FractalNet as the original version of FractalNet (arXiv:1605.07648) was published on arXiv three months prior to the original version of DenseNet (arXiv:1608.06993).  We are happy to alert readers to subsequent work, but please keep in mind the historical sequence of development.\n\n> Table 1 has Res-Net variants as baselines however Table 2 has only ResNet.\n\nMany of the variants only reported results on CIFAR/SVHN.  For example, the Wide Residual Networks paper (arXiv:1605.07146), at the time of the ICLR deadline (November 5), did not include ImageNet results.  It was updated weeks later, on November 28, to include ImageNet results.  We can expand the table, but it is a bit difficult to have already compared to results that did not exist at submission time.\n\n> there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.\n\nSection 4.2 observes \"most methods perform similarly on SVHN\".  The errors rates on SVHN are so low as to be uninformative.  We believe SVHN is simply too easy of a dataset to be a challenge to any of the modern techniques in Table 1.  Much like MNIST, SVHN is now a useful sanity check, but not a differentiator.\n\n> Also, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis.\n\nOur point is merely to draw connections between the FractalNet architecture and some hand-designed components of Inception, suggesting a more fundamental explanation for why those particular hand-designed choices are effective.  Widespread adoption of ResNet by the community has displaced Inception, VGG, and other architectures, making ResNet the clear leader and appropriate target for experimental comparison.\n\n\n-----\nResponse to AnonReviewer2\n\nPlease see our comments about experiments and the main contributions of the paper above.\n\n> The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\nWe are demonstrating feasibility of parameter reduction tricks for FractalNet.  Much of the iterative improvement in the ResNet variants themselves, developed over many papers, already has a component of optimizing the architecture to reduce parameters and/or computational load.\n\n> Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts.\n\nThis statement is incorrect.  Our model is not directly related to Inception.  It reproduces some local connectivity reminiscent of Inception modules, but the global connectivity structure is entirely different. Even locally, FractalNet has a recursive join structure which Inception modules lack.  As consequence of this is that at global scale, FractalNet contains many paths whose length range over many orders of magnitude (all powers of 2 up to the maximum depth).  In contrast, the shortest path through an Inception network grows linearly with depth (number of modules stacked).\n\n> As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance\n\nImageNet performance is a useful benchmark tool, not the end goal of network design.  Mass adoption of ResNet by the community suggests that simplicity and scalability are also legitimate concerns.  With regard to scalability in depth, the only networks previously demonstrated to easily extend to the 100-1000 layer regime rely on some form of residual connection (ResNet/Highway Networks).  We demonstrate a 160-layer FractalNet, placing it in this group.\n\nMoreover, FractalNet and ResNet/Highway all have a mechanism for evolving from being effectively shallow to effectively deep over the course of training. This shallow to deep evolution seems critical, but unfortunately the design of Inception prevents its effective depth from being less than the number of stacked Inception modules.\n\n> It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\nWe disagree.  The lesioning experiments in Veit et al. demonstrate the opposite: ResNet does not have an anytime property.  If deleting a few layers, then yes, ResNet can recover.  However, Section 4.2 and Figure 5 of Veit et al, show that deleting 10 blocks of a 54 block ResNet increases CIFAR-10 error to 0.2, deleting 20 blocks pushes error above 0.5.  So one cannot maintain a reasonable error if halving the ResNet depth.  Compare this to our Table 4: subnetwork columns of one half (20) and even one fourth (10) the layers of a full FractalNet (40) maintain low error on CIFAR-100.  This robustness across many orders of magnitude in depth (and thus time) is required for a useful anytime property and is unique to FractalNet (with drop-path training).\n\n> The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nDrop-path is essential for and the only effective means of enabling anytime output.  The fact that it can contribute as a regularizer in other scenarios is a bonus.\n\n\n-----\nResponse to AnonReviewer3\n\n> the experimental evaluation is not convincing, e.g. no improvement on SVHN\n\nAs we replied to AnonReviewer1: Section 4.2 observes \"most methods perform similarly on SVHN\".  The errors rates on SVHN are so low as to be uninformative.  We believe SVHN is simply too easy of a dataset to be a challenge to any of the modern techniques in Table 1.  Much like MNIST, SVHN is now a useful sanity check, but not a differentiator.\n\n> number of parameters should be mentioned for all models for fair comparison\n\nWe can add this to the table, but please see our larger discussion of experiments above.\n\n> the effect of drop-path seems to vanish with data augmentation\n\nDrop-path is essential for and the only effective means of enabling anytime output.  The fact that it can contribute as a regularizer in the absence of data augmentation is a bonus.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682127, "id": "ICLR.cc/2017/conference/-/paper214/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1VaB4cex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper214/reviewers", "ICLR.cc/2017/conference/paper214/areachairs"], "cdate": 1485287682127}}}, {"tddate": null, "tmdate": 1482447219171, "tcdate": 1482447219171, "number": 3, "id": "HJoOt0F4g", "invitation": "ICLR.cc/2017/conference/-/paper214/official/review", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["ICLR.cc/2017/conference/paper214/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper214/AnonReviewer3"], "content": {"title": "weak comparison", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a strategy for building deep neural networks via rules for expansion and merging of sub-networks.\npros:\n- the idea is novel\n- the approach is described clearly\ncons:\n- the experimental evaluation is not convincing, e.g. no improvement on SVHN\n- number of parameters should be mentioned for all models for fair comparison\n- the effect of drop-path seems to vanish with data augmentation", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512662069, "id": "ICLR.cc/2017/conference/-/paper214/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper214/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper214/AnonReviewer2", "ICLR.cc/2017/conference/paper214/AnonReviewer1", "ICLR.cc/2017/conference/paper214/AnonReviewer3"], "reply": {"forum": "S1VaB4cex", "replyto": "S1VaB4cex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper214/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper214/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512662069}}}, {"tddate": null, "tmdate": 1482122080695, "tcdate": 1482122080695, "number": 2, "id": "r1tPX1rVg", "invitation": "ICLR.cc/2017/conference/-/paper214/official/review", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["ICLR.cc/2017/conference/paper214/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper214/AnonReviewer1"], "content": {"title": "Unsatisfactory experiments and restrictively large number of parameters", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations. Using the fractal architecture,  authors argue and try to demonstrate that the large nominal network depth with many short paths is the key for 'training 'ultra-deep\u201d networks while residuals are incidental.\n\nThe main bottleneck of this paper is that number of parameters needed for the FractalNet is significantly higher than the baselines which makes it hard to scale to ''ultra-deep\u201d networks.  Authors replied that Wide ResNets also require many parameters but this is not the case for ResNet and other ResNet variants. ResNet and ResNet with Stochastic depth scales to depth of 110 with 1.7M parameters and to depth of 1202 with 10.2M parameters which is much less than the number of parameters for depths of 20 and 40 in Table 1(Huang et al, 2016a).   It is not clear whether FractalNet can perform better than these depths with a reasonable computation. Authors report less parameters for 40 layers but this scaling trick is not validated for other depths including depth 20 in Table 1. On the other hand, the number of parameters for 40 layers with scaling trick is clearly still large compared to most of the baselines. Unsatisfactory comparison to these baselines makes the claims of authors unconvincing.\n\nAuthors also claim that drop-path to provide improvement compared to layer dropping procedure in Huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when well-known data augmentation techniques applied. Therefore the empirical effectiveness of drop-path is not convincing too.\n\nDenseNets (Huang et al, 2016a) should be also included in the comparison since it outperforms most of the state of art Res Nets on both CIFAR10 and ImageNet and more importantly outperforms the proposed FractalNet significantly and it requires significantly less computation. \n\nTable 1 has Res-Net variants as baselines however Table 2 has only ResNet.  Therefore ImageNet comparison only shows that one can run FractalNet on ImageNet and can perform comparably well to ResNet which is not a satisfactory result given the improvements of other baselines over ResNet.  In addition, there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.\n\nAlso, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis. \n\nAlthough the paper attempts to explore many interesting intuitive directions using the proposed architecture, the empirical results are not support the given claims and the large number of parameters makes the model restrictive in practice hence the contribution does not seem to be significant. \n\nPros:\nProvides an interesting architecture compared to ResNet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis\n\ncons:\n     -    Number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters\nThe claims are intuitive but not supported well with empirical evidence\nPath regularization does not yield improvement when the data augmentation is used\n     -     The empirical results do not show whether the method is promising for \u201cultra-deep\u201d networks ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512662069, "id": "ICLR.cc/2017/conference/-/paper214/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper214/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper214/AnonReviewer2", "ICLR.cc/2017/conference/paper214/AnonReviewer1", "ICLR.cc/2017/conference/paper214/AnonReviewer3"], "reply": {"forum": "S1VaB4cex", "replyto": "S1VaB4cex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper214/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper214/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512662069}}}, {"tddate": null, "tmdate": 1481076712410, "tcdate": 1481076712404, "number": 2, "id": "BylglxHme", "invitation": "ICLR.cc/2017/conference/-/paper214/public/comment", "forum": "S1VaB4cex", "replyto": "HJYdQPeme", "signatures": ["~Michael_Maire1"], "readers": ["everyone"], "writers": ["~Michael_Maire1"], "content": {"title": "Re: parameters and comparison", "comment": "Table 1 includes ResNet variants developed through a total of 7 different papers.  The Wide Residual Networks paper alone reports results for 10 different variants in its Table 4, and we are showing their best (28-layer) one here.  This wide ResNet also required many parameters (36.5M).  While CIFAR is a useful dataset, at some point such exploration devolves into training on the test set.  \n\nOur own architectural exploration on CIFAR is far more restrained.  Ours is the first paper introducing the fractal architecture idea; perhaps subsequent ones can further optimize it.  We set B=5 because that is a natural choice for 32x32 input images.  A series of 5 spatial reductions, each by a factor of 2, reduces a 32x32 spatial grid to a 1x1 spatial grid, so a classification layer can be attached without making any additional decisions about pooling.  Table 3 shows graceful scaling with any value for C, while Table 1 shows good performance with C=3 with a simple 20-layer design whose channel count progression mirrors that of VGG/ResNet.  The 40-layer design in Table 2 is our first effort at reducing parameters in a smart way while increasing depth.  We believe there is room for further exploration here as results appear stable for this variant.  We have not yet run the 40-layer without augmentation, but will do so to fill in those entries of Table 1.\n\nImageNet training, being more time consuming, is less susceptible to the implicit leakage of test into training; we also tie ResNet performance in a fair comparison on ImageNet (Table 2).\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682127, "id": "ICLR.cc/2017/conference/-/paper214/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1VaB4cex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper214/reviewers", "ICLR.cc/2017/conference/paper214/areachairs"], "cdate": 1485287682127}}}, {"tddate": null, "tmdate": 1481076543397, "tcdate": 1481076543388, "number": 1, "id": "B1Dr1eSme", "invitation": "ICLR.cc/2017/conference/-/paper214/public/comment", "forum": "S1VaB4cex", "replyto": "H1R1WQyXe", "signatures": ["~Michael_Maire1"], "readers": ["everyone"], "writers": ["~Michael_Maire1"], "content": {"title": "Re: Questions about nomenclature and evaluations", "comment": "> Why is a novel adjective 'ultra-deep' needed to describe these networks?\n\nOur intention is to be descriptive, not to define a new class of networks.  We are open to suggestions for revising terminology.  We want to convey the fact that fractal networks scale gracefully as depth increases.  Like residual networks, and unlike plain networks, fractal networks of any depth are trainable; our experiments show scaling from 5 to 160 layers.  A subtitle of \"deep neural networks without residuals\" is not informative; it would just mean any network other than residual networks.  Our architectural design is one of the few demonstrated suitable for training beyond 100 layers.\n\nPlease note that [Veit et al., Residual Networks Behave Like Ensembles of Relatively Shallow Networks, NIPS 2016] use \u201cultra-deep\u201d in reference to residual networks, so this terminology is not original to our paper.\n\n> What was the main motivation for experiment design?\n\nCIFAR-100/10 and SVHN have served as standard benchmark datasets for evaluating a wide variety of proposed neural network architectures.  We centered our experiments on these standard tasks in order to facilitate comparison with an extensive body of prior work, as shown in Table 1.  Training on ImageNet requires significantly more computational resources, but given its importance, we provide a performance comparison of FractalNet and ResNet on it in Table 2.  Our remaining experiments focus on examining the properties of fractal networks: scaling with depth (Table 3), anytime output (Table 4), and behavior during training (Figure 3).\n\n> What knowledge does this paper add over Inception networks, which already have long and short paths (as authors note)?\n\n(1) Unlike Inception networks, fractal networks contain paths whose lengths span over many orders of magnitude (all powers of two within each repeated block).  The Inception architecture has some slightly shorter paths, but does not have this great diversity in path length.\n\n(2) A simpler construction rule.  We show that repeated application of the fractal expansion rule generates network architectures that exhibit both paths of all lengths and cross sections reminiscent of Inception modules.  This elucidates principles behind the otherwise ad-hoc nature of Inception modules.\n\n(3) No auxiliary losses.  The Inception architecture required manual introduction of auxiliary losses at intermediate levels of the network in order to facilitate training.  Fractal networks eliminate this hack and are instead trained via a single loss attached at the end of the network.\n\n(4) An anytime property when trained with drop-path regularization.  We introduce a novel regularization scheme (drop-path) and show that after training with it, fractal networks can be partially evaluated at test time to yield a quick answer.  That is, evaluating a short path (treating a column within a fractal network as a \u201cplain\u201d network) yields output with decent accuracy.  Evaluating a longer path (column) yields even higher accuracy.  For real-time systems or latency-sensitive applications, fractal networks could provide a stream of outputs with increasing accuracy up to some limit on compute time.  There is no analogue of such capability in either Inception networks or residual networks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682127, "id": "ICLR.cc/2017/conference/-/paper214/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1VaB4cex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper214/reviewers", "ICLR.cc/2017/conference/paper214/areachairs"], "cdate": 1485287682127}}}, {"tddate": null, "tmdate": 1480778608779, "tcdate": 1480778608772, "number": 2, "id": "HJYdQPeme", "invitation": "ICLR.cc/2017/conference/-/paper214/official/comment", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["ICLR.cc/2017/conference/paper214/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper214/AnonReviewer1"], "content": {"title": "parameters and comparison", "comment": "- Number of parameters needed is dramatically increasing with respect to competitors for state of art results. Can you also give comparison on depth, parameters,training times to Resnet and its variants in Table 1 and Table 2 and elaborate on this?\n Did you also try to decrease the number of parameters of 20 layers ?\n\n-In Table 1, why didn't you report any result for no augmentation for 40 layers?\n\n- Why did you set B values and corresponding layers and C values to given numbers, what else have you tried and what did you get in terms of training efficiency and misclassification error for getting the results on Table 1 and Table 2? Are the results stable or very sensitive according to these parameters? \n\n- Can you elaborate on the cases where you could not getter better results than best res-net variant in Table 1?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682001, "id": "ICLR.cc/2017/conference/-/paper214/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1VaB4cex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper214/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper214/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper214/reviewers", "ICLR.cc/2017/conference/paper214/areachairs"], "cdate": 1485287682001}}}, {"tddate": null, "tmdate": 1480696037838, "tcdate": 1480696037832, "number": 1, "id": "H1R1WQyXe", "invitation": "ICLR.cc/2017/conference/-/paper214/pre-review/question", "forum": "S1VaB4cex", "replyto": "S1VaB4cex", "signatures": ["ICLR.cc/2017/conference/paper214/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper214/AnonReviewer2"], "content": {"title": "Questions about nomenclature and evaluations", "question": "Why is a novel adjective 'ultra-deep' needed to describe these networks?\n\nWhat was the main motivation for experiment design?\n\nWhat knowledge does this paper add over Inception networks, which already have long and short paths (as authors note)?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "pdf": "/pdf/264c99a54fcdf9dbe210d0936483fe25a1880573.pdf", "paperhash": "larsson|fractalnet_ultradeep_neural_networks_without_residuals", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu", "berkeley.edu", "caltech.edu"], "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959402104, "id": "ICLR.cc/2017/conference/-/paper214/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper214/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper214/AnonReviewer2"], "reply": {"forum": "S1VaB4cex", "replyto": "S1VaB4cex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper214/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper214/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959402104}}}], "count": 14}