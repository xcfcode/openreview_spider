{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396605658, "tcdate": 1486396605658, "number": 1, "id": "BJ86hMIOl", "invitation": "ICLR.cc/2017/conference/-/paper475/acceptance", "forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The consensus of the reviewers, although their reviews where somewhat succinct, was that the paper proposes an interesting research direction by training neural networks to approximate datastructures by constraining them to (attempt to) respect the axioms of the structure, but is thin on the ground in terms of evaluation and comparison to existing work in the domain (both in terms of models and \"standard\" experiments\"). The authors have not sought to defend their paper against the reviewers' critique, and thus I am happy to accept the consensus and reject the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, i.e., which exploit non-uniformity in their usage patterns to reduce time or space complexity.\nAbstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior.\nWe extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some probability.\nWe employ a number of approximations to synthesize several distribution-sensitive data structures from probabilistic specification as deep neural networks, such as a stack, queue, natural number, set, and binary tree.", "pdf": "/pdf/26c4b7d083283a4341a08dfde779756c2c9267c6.pdf", "TL;DR": "We model mental representations as abstract distribution-sensitive data types and synthesize concrete implementations using deep networks from specification", "paperhash": "tavares|learning_approximate_distributionsensitive_data_structures", "keywords": ["Unsupervised Learning"], "conflicts": ["mit.edu"], "authors": ["Zenna Tavares", "Armando Solar-Lezama"], "authorids": ["zenna@mit.edu", "asolar@csail.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396606201, "id": "ICLR.cc/2017/conference/-/paper475/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396606201}}}, {"tddate": null, "tmdate": 1481925588469, "tcdate": 1481925588469, "number": 3, "id": "Sy20Q1MNl", "invitation": "ICLR.cc/2017/conference/-/paper475/official/review", "forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "signatures": ["ICLR.cc/2017/conference/paper475/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper475/AnonReviewer3"], "content": {"title": "Interesting direction, but not there yet.", "rating": "4: Ok but not good enough - rejection", "review": "A method for training neural networks to mimic abstract data structures is presented. The idea of training a network to satisfy an abstract interface is very interesting and promising, but empirical support is currently too weak. The paper would be significantly strengthened if the method could be shown to be useful in a realistic application, or be shown to work better than standard RNN approaches on algorithmic learning tasks.\n\nThe claims about mental representations are not well supported. I would remove the references to mind and brain, as well as the more philosophical points, or write a paper that really emphasizes one of these aspects and supports the claims.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, i.e., which exploit non-uniformity in their usage patterns to reduce time or space complexity.\nAbstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior.\nWe extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some probability.\nWe employ a number of approximations to synthesize several distribution-sensitive data structures from probabilistic specification as deep neural networks, such as a stack, queue, natural number, set, and binary tree.", "pdf": "/pdf/26c4b7d083283a4341a08dfde779756c2c9267c6.pdf", "TL;DR": "We model mental representations as abstract distribution-sensitive data types and synthesize concrete implementations using deep networks from specification", "paperhash": "tavares|learning_approximate_distributionsensitive_data_structures", "keywords": ["Unsupervised Learning"], "conflicts": ["mit.edu"], "authors": ["Zenna Tavares", "Armando Solar-Lezama"], "authorids": ["zenna@mit.edu", "asolar@csail.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512573694, "id": "ICLR.cc/2017/conference/-/paper475/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper475/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper475/AnonReviewer2", "ICLR.cc/2017/conference/paper475/AnonReviewer1", "ICLR.cc/2017/conference/paper475/AnonReviewer3"], "reply": {"forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512573694}}}, {"tddate": null, "tmdate": 1481903737604, "tcdate": 1481903737604, "number": 2, "id": "ryztRFW4e", "invitation": "ICLR.cc/2017/conference/-/paper475/official/review", "forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "signatures": ["ICLR.cc/2017/conference/paper475/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper475/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents a framework to formulate data-structures in a learnable way. It is an interesting and novel approach that could generalize well to interesting datastructures and algorithms. In its current state (Revision of Dec. 9th), there are two strong weaknesses remaining: analysis of related work, and experimental evidence.\n\nReviewer 2 detailed some of the related work already, and especially DeepMind (which I am not affiliated with) presented some interesting and highly related results with its neural touring machine and following work. While it may be of course very hard to make direct comparisons in the experimental section due to complexity of the re-implementation, it would at least be very important to mention and compare to these works conceptually.\n\nThe experimental section shows mostly qualitative results, that do not (fully) conclusively treat the topic. Some suggestions for improvements:\n* It would be highly interesting to learn about the accuracy of the stack and queue structures, for increasing numbers of elements to store.\n* Can a queue / stack be used in arbitrary situations of push-pop operations occuring, even though it was only trained solely with consecutive pushes / consecutive pops? Does it in this enhanced setting `diverge' at some point?\n* The encoded elements from MNIST, even though in a 28x28 (binary?) space, are elements of a ten-element set, and can hence be encoded a lot more efficiently just by `parsing' them, which CNNs can do quite well. Is the NN `just' learning to do that? If so, its performance can be expected to strongly degrade when having to learn to stack more than 28*28/4=196 numbers (in case of an optimal parser and loss-less encoding). To argue more in this direction, experiments would be needed with an increasing number of stack / queue elements. Experimenting with an MNIST parsing NN in front of the actual stack/queue network could help strengthening or falsifying the claim.\n* The claims about `mental representations' have very little support throughout the paper. If indication for correspondence to mental models, etc., could be found, it would allow to hold the claim. Otherwise, I would remove it from the paper and focus on the NN aspects and maybe mention mental models as motivation.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, i.e., which exploit non-uniformity in their usage patterns to reduce time or space complexity.\nAbstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior.\nWe extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some probability.\nWe employ a number of approximations to synthesize several distribution-sensitive data structures from probabilistic specification as deep neural networks, such as a stack, queue, natural number, set, and binary tree.", "pdf": "/pdf/26c4b7d083283a4341a08dfde779756c2c9267c6.pdf", "TL;DR": "We model mental representations as abstract distribution-sensitive data types and synthesize concrete implementations using deep networks from specification", "paperhash": "tavares|learning_approximate_distributionsensitive_data_structures", "keywords": ["Unsupervised Learning"], "conflicts": ["mit.edu"], "authors": ["Zenna Tavares", "Armando Solar-Lezama"], "authorids": ["zenna@mit.edu", "asolar@csail.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512573694, "id": "ICLR.cc/2017/conference/-/paper475/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper475/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper475/AnonReviewer2", "ICLR.cc/2017/conference/paper475/AnonReviewer1", "ICLR.cc/2017/conference/paper475/AnonReviewer3"], "reply": {"forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512573694}}}, {"tddate": null, "tmdate": 1481885576524, "tcdate": 1481885576524, "number": 1, "id": "ryg9PB-Vg", "invitation": "ICLR.cc/2017/conference/-/paper475/official/review", "forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "signatures": ["ICLR.cc/2017/conference/paper475/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper475/AnonReviewer2"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "The paper presents a way to \"learn\" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).\n\nThere are several flaws:\n - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.\n - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...\n - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.\n - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of \"success rate\" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.\n\nWhile an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, i.e., which exploit non-uniformity in their usage patterns to reduce time or space complexity.\nAbstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior.\nWe extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some probability.\nWe employ a number of approximations to synthesize several distribution-sensitive data structures from probabilistic specification as deep neural networks, such as a stack, queue, natural number, set, and binary tree.", "pdf": "/pdf/26c4b7d083283a4341a08dfde779756c2c9267c6.pdf", "TL;DR": "We model mental representations as abstract distribution-sensitive data types and synthesize concrete implementations using deep networks from specification", "paperhash": "tavares|learning_approximate_distributionsensitive_data_structures", "keywords": ["Unsupervised Learning"], "conflicts": ["mit.edu"], "authors": ["Zenna Tavares", "Armando Solar-Lezama"], "authorids": ["zenna@mit.edu", "asolar@csail.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512573694, "id": "ICLR.cc/2017/conference/-/paper475/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper475/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper475/AnonReviewer2", "ICLR.cc/2017/conference/paper475/AnonReviewer1", "ICLR.cc/2017/conference/paper475/AnonReviewer3"], "reply": {"forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512573694}}}, {"tddate": null, "tmdate": 1481320824955, "tcdate": 1481320824947, "number": 1, "id": "BkWKYjOQl", "invitation": "ICLR.cc/2017/conference/-/paper475/public/comment", "forum": "BJVEEF9lx", "replyto": "HkwMKgkQl", "signatures": ["~Zenna_Tavares1"], "readers": ["everyone"], "writers": ["~Zenna_Tavares1"], "content": {"title": "Response to comments", "comment": "Thank you for your questions; let us address them one by one.\n\n- Could you briefly state why you selected these particular ones for more detailed discussion than the others?\n\nWe compared a stack and queue because they are more meaningfully comparable in the sense that push and pop have similar meanings to enqueue and dequeue.\nFor this reason we hypothesized that the internal representations could turn out to be similar or even identical (with only the interface functions differing).\nThis turned out not to be the case; the internal representations in the examples are markedly different.\n\nA second reason is that there are a number of referenced examples in the literature of differentiable stacks and queues.\nThese examples showcase the difference (both in result and motivation) between this approach and manually designing a differentiable data structure.\n\nThat said, our architecture makes it extremely simple to synthesize new data-structures.\nWe have already updated the paper with observations on the internal representation of natural numbers (learned from peano's axioms); and will do so for the remaining data-structures.\n\n- For the implementation, you used a specific single layer architecture. Did you experiment with others than this specific one? If so, can you speculate why the one you used in the paper performed better than the others?\n\nWe experimented with a small number of architectures, in particular a number of residual convolutional networks and a conventional residual neural network.\nIn practice, the convolutional networks performed better substantially better, but increasing the number of layers, or size of residual blocks did not have much effect.\nOne reason may be that because of the nested nature of the axioms (e.g. push(push(push(empty_stack, image1), image2), image3)), which makes training similar to that of recurrent networks.\nFor all the examples we tried however, we were able to synthesize a solution with the architecture stated.\n\n\n- In Figure 1 and Figure 3 the initialization for the stack data-structure is different. Does this originate from different training runs, i.e., do you backpropagate up to the initialization of the datastructure to find an optimal initialization? If not, how do you find the initialization for the empty datastructure? Do you think this influences the model performance?\n\nWe do simultaneously learn the representation of the initial (empty) stack / queue and potentially any constant; we will update the paper to make this more clear.\nAs you have observed these initial representations differ, which wee highlighted this in figure 3's caption.  It suggests there are actually multiple good kinds of representations, and which one is found depends on the optimization procedure.\n\nFor both examples we get good performance, essentially almost zero error.  One question we can investigate is if we make the problem substantially more difficult, (e.g. to stack 10 digits rather than 3), whether the representations that are learned are most consistent."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, i.e., which exploit non-uniformity in their usage patterns to reduce time or space complexity.\nAbstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior.\nWe extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some probability.\nWe employ a number of approximations to synthesize several distribution-sensitive data structures from probabilistic specification as deep neural networks, such as a stack, queue, natural number, set, and binary tree.", "pdf": "/pdf/26c4b7d083283a4341a08dfde779756c2c9267c6.pdf", "TL;DR": "We model mental representations as abstract distribution-sensitive data types and synthesize concrete implementations using deep networks from specification", "paperhash": "tavares|learning_approximate_distributionsensitive_data_structures", "keywords": ["Unsupervised Learning"], "conflicts": ["mit.edu"], "authors": ["Zenna Tavares", "Armando Solar-Lezama"], "authorids": ["zenna@mit.edu", "asolar@csail.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287561345, "id": "ICLR.cc/2017/conference/-/paper475/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJVEEF9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper475/reviewers", "ICLR.cc/2017/conference/paper475/areachairs"], "cdate": 1485287561345}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481320504176, "tcdate": 1478296629131, "number": 475, "id": "BJVEEF9lx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJVEEF9lx", "signatures": ["~Zenna_Tavares1"], "readers": ["everyone"], "content": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, i.e., which exploit non-uniformity in their usage patterns to reduce time or space complexity.\nAbstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior.\nWe extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some probability.\nWe employ a number of approximations to synthesize several distribution-sensitive data structures from probabilistic specification as deep neural networks, such as a stack, queue, natural number, set, and binary tree.", "pdf": "/pdf/26c4b7d083283a4341a08dfde779756c2c9267c6.pdf", "TL;DR": "We model mental representations as abstract distribution-sensitive data types and synthesize concrete implementations using deep networks from specification", "paperhash": "tavares|learning_approximate_distributionsensitive_data_structures", "keywords": ["Unsupervised Learning"], "conflicts": ["mit.edu"], "authors": ["Zenna Tavares", "Armando Solar-Lezama"], "authorids": ["zenna@mit.edu", "asolar@csail.mit.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480706351128, "tcdate": 1480706351123, "number": 2, "id": "SyPNYSkQl", "invitation": "ICLR.cc/2017/conference/-/paper475/pre-review/question", "forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "signatures": ["ICLR.cc/2017/conference/paper475/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper475/AnonReviewer3"], "content": {"title": "Unrolling & Evidence for psychological claims", "question": "I'm not entirely sure I understand the unrolling procedure. Do you create a loss function as a finite composition of neural networks representing e.g. push / pop, and optimize that? A summary of the algorithm along with a concrete example for e.g. a stack would be useful.\n\nThe paper is motivated as a computational model of mental representations, but little to no evidence is provided for the psychological plausibility of the model. Is there any such evidence? (If not, the psychological / philosophical points should be removed in my opinion; the work could be motivated from an AI perspective instead)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, i.e., which exploit non-uniformity in their usage patterns to reduce time or space complexity.\nAbstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior.\nWe extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some probability.\nWe employ a number of approximations to synthesize several distribution-sensitive data structures from probabilistic specification as deep neural networks, such as a stack, queue, natural number, set, and binary tree.", "pdf": "/pdf/26c4b7d083283a4341a08dfde779756c2c9267c6.pdf", "TL;DR": "We model mental representations as abstract distribution-sensitive data types and synthesize concrete implementations using deep networks from specification", "paperhash": "tavares|learning_approximate_distributionsensitive_data_structures", "keywords": ["Unsupervised Learning"], "conflicts": ["mit.edu"], "authors": ["Zenna Tavares", "Armando Solar-Lezama"], "authorids": ["zenna@mit.edu", "asolar@csail.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959261500, "id": "ICLR.cc/2017/conference/-/paper475/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper475/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper475/AnonReviewer1", "ICLR.cc/2017/conference/paper475/AnonReviewer3"], "reply": {"forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959261500}}}, {"tddate": null, "tmdate": 1480685838695, "tcdate": 1480685838688, "number": 1, "id": "HkwMKgkQl", "invitation": "ICLR.cc/2017/conference/-/paper475/pre-review/question", "forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "signatures": ["ICLR.cc/2017/conference/paper475/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper475/AnonReviewer1"], "content": {"title": "Details for experiments and discussion", "question": "While you state that you \"successfully synthesized approximate distribution-sensitive data-structures from a number of\nabstract data types:\n* Natural number (from Peano\u2019s axioms)\n* Stack\n* Queue\n* Set\n* Binary tree\"\nyou showcase results for queue and stack.\n\n1. Could you briefly state why you selected these particular ones for more detailed discussion than the others?\n2. For the implementation, you used a specific single layer architecture. Did you experiment with others than this specific one? If so, can you speculate why the one you used in the paper performed better than the others?\n3. In Figure 1 and Figure 3 the initialization for the stack data-structure is different. Does this originate from different training runs, i.e., do you backpropagate up to the initialization of the datastructure to find an optimal initialization? If not, how do you find the initialization for the empty datastructure? Do you think this influences the model performance?\n\nThank you!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Approximate Distribution-Sensitive Data Structures", "abstract": "We present a computational model of mental representations as data-structures which are distribution sensitive, i.e., which exploit non-uniformity in their usage patterns to reduce time or space complexity.\nAbstract data types equipped with axiomatic specifications specify classes of concrete data structures with equivalent logical behavior.\nWe extend this formalism to distribution-sensitive data structures with the concept of a probabilistic axiomatic specification, which is implemented by a concrete data structure only with some probability.\nWe employ a number of approximations to synthesize several distribution-sensitive data structures from probabilistic specification as deep neural networks, such as a stack, queue, natural number, set, and binary tree.", "pdf": "/pdf/26c4b7d083283a4341a08dfde779756c2c9267c6.pdf", "TL;DR": "We model mental representations as abstract distribution-sensitive data types and synthesize concrete implementations using deep networks from specification", "paperhash": "tavares|learning_approximate_distributionsensitive_data_structures", "keywords": ["Unsupervised Learning"], "conflicts": ["mit.edu"], "authors": ["Zenna Tavares", "Armando Solar-Lezama"], "authorids": ["zenna@mit.edu", "asolar@csail.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959261500, "id": "ICLR.cc/2017/conference/-/paper475/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper475/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper475/AnonReviewer1", "ICLR.cc/2017/conference/paper475/AnonReviewer3"], "reply": {"forum": "BJVEEF9lx", "replyto": "BJVEEF9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper475/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959261500}}}], "count": 8}