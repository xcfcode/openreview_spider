{"notes": [{"id": "SkgODpVFDr", "original": "r1lDlGoDvr", "number": 602, "cdate": 1569439072294, "ddate": null, "tcdate": 1569439072294, "tmdate": 1577168280156, "tddate": null, "forum": "SkgODpVFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kishida@nlab.ci.i.u-tokyo.ac.jp", "nakayama@nlab.ci.i.u-tokyo.ac.jp"], "title": "Incorporating Horizontal Connections in Convolution by Spatial Shuffling", "authors": ["Ikki Kishida", "Hideki Nakayama"], "pdf": "/pdf/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "TL;DR": "We propose spatially shuffled convolution that the regular convolution incorporates the information from outside of its receptive field.", "abstract": "Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks.\nThe design of the regular convolution is based on the Receptive Field (RF) where the information within a specific region is processed.\nIn the view of the regular convolution's RF, the outputs of neurons in lower layers with smaller RF are bundled to create neurons in higher layers with larger RF. \nAs a result, the neurons in high layers are able to capture the global context even though the neurons in low layers only see the local information.\nHowever, in lower layers of the biological brain, the information outside of the RF changes the properties of neurons.\nIn this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution).\nIn ss convolution, the regular convolution is able to use the information outside of its RF by spatial shuffling which is a simple and lightweight operation.\nWe perform experiments on CIFAR-10 and ImageNet-1k dataset, and show that ss convolution improves the classification performance across various CNNs.", "code": "https://github.com/AccountForSubmission/ICLR2020_Spatially_Shuffled_Convolution", "keywords": ["shuffle", "convolution", "receptive field", "classification", "horizontal connections"], "paperhash": "kishida|incorporating_horizontal_connections_in_convolution_by_spatial_shuffling", "original_pdf": "/attachment/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "_bibtex": "@misc{\nkishida2020incorporating,\ntitle={Incorporating Horizontal Connections in Convolution by Spatial Shuffling},\nauthor={Ikki Kishida and Hideki Nakayama},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgODpVFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_uCSxPPz7Z", "original": null, "number": 1, "cdate": 1576798701023, "ddate": null, "tcdate": 1576798701023, "tmdate": 1576800934947, "tddate": null, "forum": "SkgODpVFDr", "replyto": "SkgODpVFDr", "invitation": "ICLR.cc/2020/Conference/Paper602/-/Decision", "content": {"decision": "Reject", "comment": "The paper is well-motivated by neuroscience that our brains use information from outside the receptive field of convolutive processes through top-down mechanisms. However, reviewers feel that the results are not near the state of the art and the paper needs further experiments and need to scale to larger datasets. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kishida@nlab.ci.i.u-tokyo.ac.jp", "nakayama@nlab.ci.i.u-tokyo.ac.jp"], "title": "Incorporating Horizontal Connections in Convolution by Spatial Shuffling", "authors": ["Ikki Kishida", "Hideki Nakayama"], "pdf": "/pdf/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "TL;DR": "We propose spatially shuffled convolution that the regular convolution incorporates the information from outside of its receptive field.", "abstract": "Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks.\nThe design of the regular convolution is based on the Receptive Field (RF) where the information within a specific region is processed.\nIn the view of the regular convolution's RF, the outputs of neurons in lower layers with smaller RF are bundled to create neurons in higher layers with larger RF. \nAs a result, the neurons in high layers are able to capture the global context even though the neurons in low layers only see the local information.\nHowever, in lower layers of the biological brain, the information outside of the RF changes the properties of neurons.\nIn this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution).\nIn ss convolution, the regular convolution is able to use the information outside of its RF by spatial shuffling which is a simple and lightweight operation.\nWe perform experiments on CIFAR-10 and ImageNet-1k dataset, and show that ss convolution improves the classification performance across various CNNs.", "code": "https://github.com/AccountForSubmission/ICLR2020_Spatially_Shuffled_Convolution", "keywords": ["shuffle", "convolution", "receptive field", "classification", "horizontal connections"], "paperhash": "kishida|incorporating_horizontal_connections_in_convolution_by_spatial_shuffling", "original_pdf": "/attachment/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "_bibtex": "@misc{\nkishida2020incorporating,\ntitle={Incorporating Horizontal Connections in Convolution by Spatial Shuffling},\nauthor={Ikki Kishida and Hideki Nakayama},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgODpVFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkgODpVFDr", "replyto": "SkgODpVFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729136, "tmdate": 1576800281679, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper602/-/Decision"}}}, {"id": "B1l-8NElcH", "original": null, "number": 1, "cdate": 1571992649042, "ddate": null, "tcdate": 1571992649042, "tmdate": 1572972574961, "tddate": null, "forum": "SkgODpVFDr", "replyto": "SkgODpVFDr", "invitation": "ICLR.cc/2020/Conference/Paper602/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: The authors extended the regular convolution and proposed spatially shuffled convolution to use the information outside of its RF, which is inspired by the idea that horizontal connections are believed to be important for visual processing in the visual cortex in biological brain. The authors proposed ss convolution for regular convolution and group convolution. The authors tested the proposed ss convolution on multiple CNN models and show improvement of results. Finally, detailed analysis of spatial shuffling and ablation study was conducted.\nStrengths: The authors proposed spatially shuffled convolution to use the information outside of its RF. The operation only requires small amount of extra shuffling operations and without extra learnable parameters. The idea is straightforward and easy to understand. I especially like the visualization analysis of the receptive field and the layer ablation study suggested that all layers can benefit from the proposed operation, though more for middle and higher layers.\nWeakness: The proposed method seems to be a reasonable alternative for regular convolution, but  By fixing the permutation, not much insight is gained from this technique. \nSuggestions: It would be good to include the standard deviation for the results as the final results were average of 5 runs and can help to see if it\u2019s consistently useful. In the current setting, the permutation matrix is generated randomly, considering the proposed method is based on shuffling, how would different permutation affect the performance? In addition, I think it could be helpful if the authors can show more utilities of the proposed method, for example, are the proposed method capable of doing tasks similar to this paper **Learning long-range spatial dependencies with horizontal gated recurrent units**, which argues that the ability of CNNs to learn long-range spatial dependencies is limited by their localized receptive fields. One minor thing, in the main paper, the abbreviation for spatial shuffled convolution (ss convolution) is mentioned multiple times."}, "signatures": ["ICLR.cc/2020/Conference/Paper602/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper602/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kishida@nlab.ci.i.u-tokyo.ac.jp", "nakayama@nlab.ci.i.u-tokyo.ac.jp"], "title": "Incorporating Horizontal Connections in Convolution by Spatial Shuffling", "authors": ["Ikki Kishida", "Hideki Nakayama"], "pdf": "/pdf/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "TL;DR": "We propose spatially shuffled convolution that the regular convolution incorporates the information from outside of its receptive field.", "abstract": "Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks.\nThe design of the regular convolution is based on the Receptive Field (RF) where the information within a specific region is processed.\nIn the view of the regular convolution's RF, the outputs of neurons in lower layers with smaller RF are bundled to create neurons in higher layers with larger RF. \nAs a result, the neurons in high layers are able to capture the global context even though the neurons in low layers only see the local information.\nHowever, in lower layers of the biological brain, the information outside of the RF changes the properties of neurons.\nIn this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution).\nIn ss convolution, the regular convolution is able to use the information outside of its RF by spatial shuffling which is a simple and lightweight operation.\nWe perform experiments on CIFAR-10 and ImageNet-1k dataset, and show that ss convolution improves the classification performance across various CNNs.", "code": "https://github.com/AccountForSubmission/ICLR2020_Spatially_Shuffled_Convolution", "keywords": ["shuffle", "convolution", "receptive field", "classification", "horizontal connections"], "paperhash": "kishida|incorporating_horizontal_connections_in_convolution_by_spatial_shuffling", "original_pdf": "/attachment/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "_bibtex": "@misc{\nkishida2020incorporating,\ntitle={Incorporating Horizontal Connections in Convolution by Spatial Shuffling},\nauthor={Ikki Kishida and Hideki Nakayama},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgODpVFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgODpVFDr", "replyto": "SkgODpVFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper602/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper602/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575769991194, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper602/Reviewers"], "noninvitees": [], "tcdate": 1570237749752, "tmdate": 1575769991207, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper602/-/Official_Review"}}}, {"id": "SJelH-XZcH", "original": null, "number": 2, "cdate": 1572053303601, "ddate": null, "tcdate": 1572053303601, "tmdate": 1572972574916, "tddate": null, "forum": "SkgODpVFDr", "replyto": "SkgODpVFDr", "invitation": "ICLR.cc/2020/Conference/Paper602/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "[Overview]\n\nIn this paper, the authors proposed a shuffle strategy for convolution layers in convolutional neural networks (CNNs). Specifically, the authors argued that the receptive field (RF) of each convolutional filter should be not constrained in the small patch. Instead, it should also cover other locations beyond the local patch and also the single channel. Based on this motivation, the authors proposed a spatial shuffling layer which is aimed at shuffling the original feature responses. In the experimental results, the authors evaluated the proposed ss convolutional layer on CIFAR-10 and ImageNet-1k and compared with various baseline architectures. Besides, the authors further did some ablated analysis and visualizations for the proposed ss convolutional layer.\n\n[Pros]\n\n1. The authors proposed a new strategy for convolutional layers. The idea is borrowed from the biological domain, and then transformed to a spatial shuffling layer which can shuffle the feature response at each convolutional layer.\n\n2. The authors performed experiments on both small-scale dataset (CIFAR-10) and large-scale data (CIFAR-100) for evaluations.\n\n3. The authors further added some ablated analysis on the proposed model. Specifically, the authors visualize the receptive field which can be used in ss layer compared with the original convolutional layer, which indicates that ss layer can incorporate the global context at the very beginning.\n\n[Cons]\n\n1. The motivation behind the proposed ss layer is not explained very well. Though the authors mentioned that it is biologically inspired, I would not buy that since it is still a unclear phenomenon, and even it is true, using a randomized shuffling seems not align with the observations to some extent.\n\n2. The paper is poorly written in general. The motivation behind the proposed method, and the presentation of method section part are cluttered much. In the model analysis in experiment section, the presentation and explanations are also vague and not clear to me.\n\n3. The proposed model seems increase the baseline models' performance very marginally on all architectures. It is hard to say that it is because the shuffling layer enable the neurons to incorporate the global context information. Instead, it might just because the randomization which would increase the generalization ability of the trained model.\n\n4. Finally, the comparison with previous models, such as SENet, ShuffleNet, etc are not systematically. I would like to see a more comprehensive summarization of the differences between the proposed ss layer and other architectures, because all of them are trying to incorporate more contextual information from other channels or locations.\n\n[Summary]\n\nOverall, I think the proposed ss layer is still a reasonable way to incorporate the contextual information in CNNs. However, the poor presentation and the weak experimental results and analysis make the paper overall a one under the bar of the venue. I would suggest the authors revise the paper with more well-motivated formula and more solid experiments and analysis in the next submission."}, "signatures": ["ICLR.cc/2020/Conference/Paper602/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper602/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kishida@nlab.ci.i.u-tokyo.ac.jp", "nakayama@nlab.ci.i.u-tokyo.ac.jp"], "title": "Incorporating Horizontal Connections in Convolution by Spatial Shuffling", "authors": ["Ikki Kishida", "Hideki Nakayama"], "pdf": "/pdf/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "TL;DR": "We propose spatially shuffled convolution that the regular convolution incorporates the information from outside of its receptive field.", "abstract": "Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks.\nThe design of the regular convolution is based on the Receptive Field (RF) where the information within a specific region is processed.\nIn the view of the regular convolution's RF, the outputs of neurons in lower layers with smaller RF are bundled to create neurons in higher layers with larger RF. \nAs a result, the neurons in high layers are able to capture the global context even though the neurons in low layers only see the local information.\nHowever, in lower layers of the biological brain, the information outside of the RF changes the properties of neurons.\nIn this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution).\nIn ss convolution, the regular convolution is able to use the information outside of its RF by spatial shuffling which is a simple and lightweight operation.\nWe perform experiments on CIFAR-10 and ImageNet-1k dataset, and show that ss convolution improves the classification performance across various CNNs.", "code": "https://github.com/AccountForSubmission/ICLR2020_Spatially_Shuffled_Convolution", "keywords": ["shuffle", "convolution", "receptive field", "classification", "horizontal connections"], "paperhash": "kishida|incorporating_horizontal_connections_in_convolution_by_spatial_shuffling", "original_pdf": "/attachment/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "_bibtex": "@misc{\nkishida2020incorporating,\ntitle={Incorporating Horizontal Connections in Convolution by Spatial Shuffling},\nauthor={Ikki Kishida and Hideki Nakayama},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgODpVFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgODpVFDr", "replyto": "SkgODpVFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper602/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper602/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575769991194, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper602/Reviewers"], "noninvitees": [], "tcdate": 1570237749752, "tmdate": 1575769991207, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper602/-/Official_Review"}}}, {"id": "rJeDcCUb9S", "original": null, "number": 3, "cdate": 1572069006808, "ddate": null, "tcdate": 1572069006808, "tmdate": 1572972574872, "tddate": null, "forum": "SkgODpVFDr", "replyto": "SkgODpVFDr", "invitation": "ICLR.cc/2020/Conference/Paper602/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a simple \u201cspatial shuffling\u201d operation for modifying CNNs \nbased on a permutation matrix created at initialization time.  \n\nThe approach is motivate by a very high-level discussion of biological brains.\nImprovements are claimed on Cifar10 but results are not near the state of the art.\nThe results do seem to improve incrementally over the previous vanilla results \non the particular architecures on which they are applied.\n\nThe authors dedicate some space to a qualitative analysis of why \nthe models improve although it is at best intuition-y.\n\nIn the end I\u2019m left with inconclusive results, a weakly motivated story, \nand a paper that despite exceeding the page limit by a page lacks \ninformation density. \n\n\nMinor:\n\nConvolutional neural networks achieve \u2026 \u201celegant performance in computer vision tasks\u201d\n>>> \twrong adjective.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper602/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper602/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kishida@nlab.ci.i.u-tokyo.ac.jp", "nakayama@nlab.ci.i.u-tokyo.ac.jp"], "title": "Incorporating Horizontal Connections in Convolution by Spatial Shuffling", "authors": ["Ikki Kishida", "Hideki Nakayama"], "pdf": "/pdf/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "TL;DR": "We propose spatially shuffled convolution that the regular convolution incorporates the information from outside of its receptive field.", "abstract": "Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks.\nThe design of the regular convolution is based on the Receptive Field (RF) where the information within a specific region is processed.\nIn the view of the regular convolution's RF, the outputs of neurons in lower layers with smaller RF are bundled to create neurons in higher layers with larger RF. \nAs a result, the neurons in high layers are able to capture the global context even though the neurons in low layers only see the local information.\nHowever, in lower layers of the biological brain, the information outside of the RF changes the properties of neurons.\nIn this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution).\nIn ss convolution, the regular convolution is able to use the information outside of its RF by spatial shuffling which is a simple and lightweight operation.\nWe perform experiments on CIFAR-10 and ImageNet-1k dataset, and show that ss convolution improves the classification performance across various CNNs.", "code": "https://github.com/AccountForSubmission/ICLR2020_Spatially_Shuffled_Convolution", "keywords": ["shuffle", "convolution", "receptive field", "classification", "horizontal connections"], "paperhash": "kishida|incorporating_horizontal_connections_in_convolution_by_spatial_shuffling", "original_pdf": "/attachment/666968d34917d4b3de10c2f0db58f5fc268d1628.pdf", "_bibtex": "@misc{\nkishida2020incorporating,\ntitle={Incorporating Horizontal Connections in Convolution by Spatial Shuffling},\nauthor={Ikki Kishida and Hideki Nakayama},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgODpVFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgODpVFDr", "replyto": "SkgODpVFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper602/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper602/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575769991194, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper602/Reviewers"], "noninvitees": [], "tcdate": 1570237749752, "tmdate": 1575769991207, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper602/-/Official_Review"}}}], "count": 5}