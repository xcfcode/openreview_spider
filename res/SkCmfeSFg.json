{"notes": [{"tddate": null, "tmdate": 1490909548141, "tcdate": 1490909548141, "number": 2, "id": "rkNOtxihl", "invitation": "ICLR.cc/2017/workshop/-/paper165/public/comment", "forum": "SkCmfeSFg", "replyto": "H1s9EJLje", "signatures": ["~Alexey_Romanov1"], "readers": ["everyone"], "writers": ["~Alexey_Romanov1"], "content": {"title": "We added the figure for DeCov loss", "comment": "Dear reviewer,\n\nWe have updated the paper with the figure for the DeCov loss. The observed trend (a strong activation of one of the neurons in the penultimate layer) is even clearer in the case of DeCov, which corresponds to the lowest AMI score on the clustering task."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels", "abstract": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used for  clustering and introduce a novel loss component that substantially improves the quality of produced clusters, is simple to apply to an arbitrary cost function, and does not require a complicated training procedure. ", "pdf": "/pdf/15b4095eb28376ef7a4ab39b80519a8eabc3a153.pdf", "TL;DR": "A novel loss component that leads to substantial improvement of KMeans clustering over the learned representations.", "paperhash": "romanov|forced_to_learn_discovering_disentangled_representations_without_exhaustive_labels", "conflicts": ["cs.uml.edu"], "authors": ["Alexey Romanov", "Anna Rumshisky"], "keywords": [], "authorids": ["aromanov@cs.uml.edu", "arum@cs.uml.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487368742450, "tcdate": 1487368742450, "id": "ICLR.cc/2017/workshop/-/paper165/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper165/reviewers"], "reply": {"forum": "SkCmfeSFg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487368742450}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1490909252351, "tcdate": 1487368741603, "number": 165, "id": "SkCmfeSFg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "SkCmfeSFg", "signatures": ["~Alexey_Romanov1"], "readers": ["everyone"], "content": {"title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels", "abstract": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used for  clustering and introduce a novel loss component that substantially improves the quality of produced clusters, is simple to apply to an arbitrary cost function, and does not require a complicated training procedure. ", "pdf": "/pdf/15b4095eb28376ef7a4ab39b80519a8eabc3a153.pdf", "TL;DR": "A novel loss component that leads to substantial improvement of KMeans clustering over the learned representations.", "paperhash": "romanov|forced_to_learn_discovering_disentangled_representations_without_exhaustive_labels", "conflicts": ["cs.uml.edu"], "authors": ["Alexey Romanov", "Anna Rumshisky"], "keywords": [], "authorids": ["aromanov@cs.uml.edu", "arum@cs.uml.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028644775, "tcdate": 1490028644775, "number": 1, "id": "H16wuYTog", "invitation": "ICLR.cc/2017/workshop/-/paper165/acceptance", "forum": "SkCmfeSFg", "replyto": "SkCmfeSFg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels", "abstract": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used for  clustering and introduce a novel loss component that substantially improves the quality of produced clusters, is simple to apply to an arbitrary cost function, and does not require a complicated training procedure. ", "pdf": "/pdf/15b4095eb28376ef7a4ab39b80519a8eabc3a153.pdf", "TL;DR": "A novel loss component that leads to substantial improvement of KMeans clustering over the learned representations.", "paperhash": "romanov|forced_to_learn_discovering_disentangled_representations_without_exhaustive_labels", "conflicts": ["cs.uml.edu"], "authors": ["Alexey Romanov", "Anna Rumshisky"], "keywords": [], "authorids": ["aromanov@cs.uml.edu", "arum@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028645313, "id": "ICLR.cc/2017/workshop/-/paper165/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkCmfeSFg", "replyto": "SkCmfeSFg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028645313}}}, {"tddate": null, "tmdate": 1489694929057, "tcdate": 1489694929057, "number": 1, "id": "SyFCldOse", "invitation": "ICLR.cc/2017/workshop/-/paper165/public/comment", "forum": "SkCmfeSFg", "replyto": "SkCmfeSFg", "signatures": ["~Alexey_Romanov1"], "readers": ["everyone"], "writers": ["~Alexey_Romanov1"], "content": {"title": "Response to reviewers", "comment": "We would like to thank the reviewers for their thoughtful comments. We will update the abstract according to your suggestions.\n\n\nReviewer1: \n\n- \u201cI don't see how weight matrices with \"different rows\" would yield better representations. There is a complex interaction with other layers,  learning algorithm, nonlinearities and hidden states which could easily destroy  the intended benefits of this regularizer.\u201d\n\nConsider what the learned representation looks like without the loss. When trained for binary classification, the network basically learns the weights that correspond to two patterns of activations.  That means, the result of multiplication of the weight matrix by the input vector by will produce a vector that will be very uniform except for two neurons activated for the corresponding outcomes.\n\nThe only way to get better, disentangled representations for a fixed input from the previous layer, is to have the rows of this matrix be sufficiently different from each other.  Our loss component forces the rows to be different effectively leading to disentangled representations for different underlying classes in the input data.\n\nHowever, we agree that in more complex deep networks it might not be enough apply the loss component to the penultimate layer only.  In fact, in the full version of the paper, we propose a modified version of the loss that works on the full network and achieves better results. \n\n- \u201cIt's also not clear why looking at the  rows in probabilistic sense (through the softmax and KL) is necessary at all.  Why not simply taking the L2 for example?\u201d\nIndeed, the proposed approach is universal in that sense. We conducted our initial experiments using the KL divergence as a \u201cmeasure\u201d of similarity; we expect other metrics to work as well.\n\n\nReivewer2:\n\n\u201c- Needs benchmarking with several tasks and datasets.\u201d\nWe agree, and we have done more experiments since the abstract was submitted.  In particular, we wanted to make sure that we validate the proposed loss component on two most common types of models: RNN and CNN.   We therefore experimented with a CNN model on the CIFAR-10 dataset and found that the proposed loss leads to a better clustering in terms of AMI scores in case of CNN as well. \n\n\u201c- Can you also plot Figure 2 with DeCov loss?\u201d\nYes, we will be happy to update the abstract with a figure for DeCov loss."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels", "abstract": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used for  clustering and introduce a novel loss component that substantially improves the quality of produced clusters, is simple to apply to an arbitrary cost function, and does not require a complicated training procedure. ", "pdf": "/pdf/15b4095eb28376ef7a4ab39b80519a8eabc3a153.pdf", "TL;DR": "A novel loss component that leads to substantial improvement of KMeans clustering over the learned representations.", "paperhash": "romanov|forced_to_learn_discovering_disentangled_representations_without_exhaustive_labels", "conflicts": ["cs.uml.edu"], "authors": ["Alexey Romanov", "Anna Rumshisky"], "keywords": [], "authorids": ["aromanov@cs.uml.edu", "arum@cs.uml.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487368742450, "tcdate": 1487368742450, "id": "ICLR.cc/2017/workshop/-/paper165/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper165/reviewers"], "reply": {"forum": "SkCmfeSFg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487368742450}}}, {"tddate": null, "tmdate": 1489527955520, "tcdate": 1489527955520, "number": 2, "id": "H1s9EJLje", "invitation": "ICLR.cc/2017/workshop/-/paper165/official/review", "forum": "SkCmfeSFg", "replyto": "SkCmfeSFg", "signatures": ["ICLR.cc/2017/workshop/paper165/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper165/AnonReviewer2"], "content": {"title": "A new auxiliary cost function to learn representations useful for clustering.", "rating": "7: Good paper, accept", "review": "This paper introduces an auxiliary cost function which forces the representation learnt in a particular layer to be useful for clustering. This can be added to any classification model or unsupervised model like autoencoder. Authors show that adding this loss helps in better clustering of examples in a binary classification task and auto-encoder task. The model does not have access to any cluster information and it learns to group examples based on their characteristics.\n\nThe proposed objective function roughly maximizes the KL-Divergence between the probability distributions induced from the row vectors in a weight matrix. I like the idea of directly considering weights instead of considering the unit activations as done in DeCov. Also from experiments, proposed approach does better than DeCov.\n\nWhile this is an interesting idea, I encourage the authors to verify the benefits of the proposed loss in a variety of datasets. Also it looks like one can combine both DeCov and the proposed loss. They look complementary.\n\nCan you also plot Figure 2 with DeCov loss?\n\nPros:\n+ New cost function for learning representations useful for clustering\n+ Proof of concept experiments that show the method works.\n\nCons:\n- Needs benchmarking with several tasks and datasets.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels", "abstract": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used for  clustering and introduce a novel loss component that substantially improves the quality of produced clusters, is simple to apply to an arbitrary cost function, and does not require a complicated training procedure. ", "pdf": "/pdf/15b4095eb28376ef7a4ab39b80519a8eabc3a153.pdf", "TL;DR": "A novel loss component that leads to substantial improvement of KMeans clustering over the learned representations.", "paperhash": "romanov|forced_to_learn_discovering_disentangled_representations_without_exhaustive_labels", "conflicts": ["cs.uml.edu"], "authors": ["Alexey Romanov", "Anna Rumshisky"], "keywords": [], "authorids": ["aromanov@cs.uml.edu", "arum@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489527956097, "id": "ICLR.cc/2017/workshop/-/paper165/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper165/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper165/AnonReviewer1", "ICLR.cc/2017/workshop/paper165/AnonReviewer2"], "reply": {"forum": "SkCmfeSFg", "replyto": "SkCmfeSFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper165/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489527956097}}}, {"tddate": null, "tmdate": 1489058893975, "tcdate": 1489058893975, "number": 1, "id": "BkI82h0ql", "invitation": "ICLR.cc/2017/workshop/-/paper165/official/review", "forum": "SkCmfeSFg", "replyto": "SkCmfeSFg", "signatures": ["ICLR.cc/2017/workshop/paper165/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper165/AnonReviewer1"], "content": {"title": "Insufficient motivation for the proposed solution", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a regularizer to promote representations that \ncan easily be clustered in the context of time series classification.\n\nThe regularizer proposed by the authors looks at the pairwise KL between \nthe softmax-induced probability distribution from each row of the weight matrix. \nThe rationale is that a weight matrix with rows that are \"different\" enough \nfrom each other would yield more diversity in the representation. \n\nWhile favorable experiments are provided, I am not convinced in the reasoning underlying \nthe proposed cost function. I don't see how weight matrices with \"different rows\"\nwould yield better representations. There is a complex interaction with other layers, \nlearning algorithm, nonlinearities and hidden states which could easily destroy \nthe intended benefits of this regularizer. It's also not clear why looking at the \nrows in probabilistic sense (through the softmax and KL) is necessary at all. \nWhy not simply taking the L2 for example ? \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels", "abstract": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used for  clustering and introduce a novel loss component that substantially improves the quality of produced clusters, is simple to apply to an arbitrary cost function, and does not require a complicated training procedure. ", "pdf": "/pdf/15b4095eb28376ef7a4ab39b80519a8eabc3a153.pdf", "TL;DR": "A novel loss component that leads to substantial improvement of KMeans clustering over the learned representations.", "paperhash": "romanov|forced_to_learn_discovering_disentangled_representations_without_exhaustive_labels", "conflicts": ["cs.uml.edu"], "authors": ["Alexey Romanov", "Anna Rumshisky"], "keywords": [], "authorids": ["aromanov@cs.uml.edu", "arum@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489527956097, "id": "ICLR.cc/2017/workshop/-/paper165/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper165/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper165/AnonReviewer1", "ICLR.cc/2017/workshop/paper165/AnonReviewer2"], "reply": {"forum": "SkCmfeSFg", "replyto": "SkCmfeSFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper165/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489527956097}}}], "count": 6}