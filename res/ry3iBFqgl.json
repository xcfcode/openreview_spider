{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396614594, "tcdate": 1486396614594, "number": 1, "id": "H1JCnfUul", "invitation": "ICLR.cc/2017/conference/-/paper489/acceptance", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. All reviewers agree that the paper is not convincingly above the acceptance threshold. The paper will be stronger, and the benefit of this dataset over SQuAD will likely be more clear once authors incorporate reviewers' comments and finish evaluation of inter-human agreement."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396615163, "id": "ICLR.cc/2017/conference/-/paper489/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396615163}}}, {"tddate": null, "tmdate": 1484926397966, "tcdate": 1484926397966, "number": 2, "id": "BJU6aoJPg", "invitation": "ICLR.cc/2017/conference/-/paper489/official/comment", "forum": "ry3iBFqgl", "replyto": "SktIUDqIl", "signatures": ["ICLR.cc/2017/conference/paper489/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper489/AnonReviewer1"], "content": {"title": "Response", "comment": "Thanks for carrying out the above studies and experiments. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555223, "id": "ICLR.cc/2017/conference/-/paper489/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper489/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper489/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555223}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484753706344, "tcdate": 1478296996616, "number": 489, "id": "ry3iBFqgl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ry3iBFqgl", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "content": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484580433257, "tcdate": 1484580433257, "number": 8, "id": "SktIUDqIl", "invitation": "ICLR.cc/2017/conference/-/paper489/public/comment", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "writers": ["~Tong_Wang1"], "content": {"title": "Expanded human evaluation, more sentence-level experiments, etc.", "comment": "Thank you everyone for your patience. Here's a list of updates we've made per the comments and suggestions of our reviewers.\nMajor updates:\n* Increased our human evaluation of NewsQA from 100 to 1000 samples with reasoning type annotation.\n* Performed the same human evaluation on 1000 SQuAD with reasoning type annotation.\n* Updated stratification results (by reasoning type) for both NewsQA and SQuAD on the expanded subsets evaluated by human annotators.\n* Added more sentence-level scoring experiments for a more conclusive demonstration of NewsQA's increased level of difficulty over SQuAD (eliminating article length as a factor in performance difference).\n* Performed hyper-parameter tuning for both BARB and MatchLSTM model on the NewsQA dataset.\nMinor updates:\n* Added machine performance on the human-evaluated subsets for both NewsQA and SQuAD.\n* Updated various statistics to be consistent with NewsQA's public release.\n* Various minor updates, corrections, and additional statistics per requested in the reviews.\n\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555349, "id": "ICLR.cc/2017/conference/-/paper489/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555349}}}, {"tddate": null, "tmdate": 1483551783975, "tcdate": 1483551783975, "number": 7, "id": "rJeN43cHx", "invitation": "ICLR.cc/2017/conference/-/paper489/public/comment", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "writers": ["~Tong_Wang1"], "content": {"title": "Quick update", "comment": "Dear readers and reviewers,                                                                                                                                                                                                                   \n  Happy holidays! Before the final revision, we'd like to give you a quick update on our latest efforts and progress. We have currently completed 95% of the planned expansion on human evaluations (on both NewsQA and SQuAD), and we're in the process of hyperparameter tuning for both MatchLSTM and our BARB model on NewsQA. Our plan is to update relevant statistics and results no later than the end of next week.                                                                         \n  Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555349, "id": "ICLR.cc/2017/conference/-/paper489/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555349}}}, {"tddate": null, "tmdate": 1482506767593, "tcdate": 1482506767593, "number": 6, "id": "B1_zfp9Vg", "invitation": "ICLR.cc/2017/conference/-/paper489/public/comment", "forum": "ry3iBFqgl", "replyto": "H1h6iHzEx", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "writers": ["~Tong_Wang1"], "content": {"title": "Re human evaluation, question variety, test set size, etc.", "comment": "1. As mentioned in an earlier response from us, we acknowledge that human performance is currently measured on a small and potentially insignificant subset. This limitation arose from time constraints on the original submission. To remedy the issue, we are currently expanding the human evaluation to a much larger subset using in-house data analysts. We will add model performance on human-evaluated data and update the paper accordingly once the process is completed.\n\n2. The way human performance is calculated on SQuAD is not applicable to NewsQA since NewsQA has only one answer per question after validation, and the level of noise before validation discourages any agreement-based evaluation. The current human evaluation on SQuAD in our paper is conducted the same way as on NewsQA, and we do plan to expand this as well as the NewsQA human evaluation.\n\n3. This was an oversight and has been corrected. We accidentally discounted null answers with agreement before validation (3.7%), with which the two percentages in Section 4.1 shall sum to 14.0% \u2014 the complement of the percentage (86.0%) in Section 3.5.\n\n4. Yes, the same article is presented to multiple questioners. We applied an array of techniques to minimize redundancy among questions collected for the same article, like measuring edit distance between the questions and between a question and the summary points, etc. These methods were applied in pilot studies and/or as post-hoc filtering steps throughout the data collection process to eliminate repeat questions.\n\n5. We will shortly start hyper-parameter tuning on NewsQA for both models and update the paper accordingly upon completion.\n\n6. We will expand the reasoning-type stratification results together with our planned expansion on human evaluation (and add model performance on this). The model used in Figure 1 is BARB and this information has been added to the paper.\n\n7. They are researchers.\n\n8. Considering the increased difficulty of NewsQA, we adopted a 90-5-5% split on NewsQA simply to maximize the training data size (in contrast to the 80-10-10% split on SQuAD). The resulting 5k+ testing samples are at least on par with several existing QA datasets (e.g., training/test size for CNN: 380k / 3.2k; for CBT-CN: 121k / 2.5k; for CBT-NE: 109k / 2,5k).\n\n9. The released version of the dataset (https://github.com/Maluuba/newsqa) in fact contains annotations for the pre-validation answer spans, so it is possible to recover the two versions of the dataset that the reviewers suggests.\n\nThank you.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555349, "id": "ICLR.cc/2017/conference/-/paper489/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555349}}}, {"tddate": null, "tmdate": 1482506539138, "tcdate": 1482506498057, "number": 5, "id": "Bk9ZbacNe", "invitation": "ICLR.cc/2017/conference/-/paper489/public/comment", "forum": "ry3iBFqgl", "replyto": "B11QX774e", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "writers": ["~Tong_Wang1"], "content": {"title": "Re human performance, summary points, etc.", "comment": "Annotation noise is an unfortunate but inevitable feature of all crowd-sourced resources, so although this issue no doubt affects NewsQA, it is reasonable to assume that it is no more prominent in NewsQA than in SQuAD. We employed two measures to counteract annotation noise, i.e., answer agreement between separate turkers (in the answer-sourcing stage) and the validation process. Consequently, we believe that the measured difference in human performance should reflect the difference in complexity and level of difficulty between the two datasets.\nAs mentioned in an earlier response, we are also in the process of expanding our human evaluation to a much larger subset using in-house data analysts. We will update the paper accordingly once this process is completed.\n\nThere are on average 3.56 summary points for each CNN article, which, in combination with the headline, offer a fairly informative context summarizing the full article (see examples in Appendix B) \u2014 informative at least for the purpose of soliciting interesting, non-generic questions. Meanwhile, judging from the variety in the reasoning types involved in answering the questions, the design appears to have limited factoid-oriented questions that can be answered by simple string matching (a limitation of the SQuAD dataset we aim to overcome). Overall, the choice appears to have struck a good balance between avoiding generic and trivial questions. Nonetheless, the suggestion of random sentence/phrase redaction is interesting and well received among the authors. We will certainly consider the hybrid approach in future work for improving the dataset.\n\nThe suggestion of engaging a QA system in crowd-sourcing is also an interesting direction we might consider in the future.\n\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555349, "id": "ICLR.cc/2017/conference/-/paper489/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555349}}}, {"tddate": null, "tmdate": 1482012107873, "tcdate": 1482007319104, "number": 3, "id": "B11QX774e", "invitation": "ICLR.cc/2017/conference/-/paper489/official/review", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["ICLR.cc/2017/conference/paper489/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper489/AnonReviewer2"], "content": {"title": "potentially great dataset with some flaws", "rating": "6: Marginally above acceptance threshold", "review": "It would seem that the shelf life of a dataset has decreased rapidly in recent literature. SQuAD dataset has been heavily pursued as soon as it hit online couple months ago, the best performance on their leaderboard now reaching to 82%. This is rather surprising when taking into account the fact that the formal conference presentation of the dataset took place only a month ago at EMNLP\u201916, and that the reported machine performance (at the time of paper submission) was only at 51%. One reasonable speculation is that the dataset may have not been hard enough.\n\nNewsQA, the paper in submission, aims to address this concern by presenting a dataset of a comparable scale created through different QA collection strategies. Most notably, the authors solicit questions without requiring answers from the same turkers, in order to promote more diverse and hard-to-answer questions. Another notable difference is that the questions are gathered without showing the content of the news articles, and the dataset makes use of a bigger subset of CNN/Daily corpus (12K / 90K), as opposed to a much smaller subset (500 / 90K) used by SQuAD.\n\nIn sum, I think NewsQA dataset presents an effort to construct a harder, large-scale reading comprehension challenge, a recently hot research topic for which we don\u2019t yet have satisfying datasets. While not without its own weaknesses, I think this dataset presents potential values compared to what are available out there today.\n\nThat said, the paper does read like it was prepared in a hurry, as there are numerous small things that the authors could have done better. As a result, I do wonder about the quality of the dataset. For one, human performance of SQuAD measured by the authors (70.5 - 82%) is lower than that reported by SQuAD (80.3 - 90.5%). I think this sort of difference can easily happen depending on the level of carefulness the annotators can maintain. After all, not all humans have the same level of carefulness or even the same level of reading comprehension. I think it\u2019d be the best if the authors can try to explain the reason behind these differences, and if possible, perform a more careful measurement of human performance. If anything, I don\u2019t think it looks favorable for NewsQA if the human performance is only at the level of 74.9%, as it looks as if the difficulty of the dataset comes mainly from the potential noise from the QA collection process, which implies that the low model performance could result from not necessarily because of the difficulty of the comprehension and reasoning, but because of incorrect answers given by human annotators.\n\nI\u2019m also not sure whether the design choice of not presenting the news article when soliciting the questions was a good one. I can imagine that people might end up asking similar generic questions when not enough context has been presented. Perhaps taking a hybrid, what I would like to suggest is to present news articles where some sentences or phrases are randomly redacted, so that the question generators can have a bit more context while not having the full material in front of them.\n\nYet another way of encouraging the turkers from asking too trivial questions is to engage an automatic QA system on the fly \u2014 turkers must construct a QA pair for which an existing state-of-the-art system cannot answer correctly.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512567641, "id": "ICLR.cc/2017/conference/-/paper489/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper489/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper489/AnonReviewer1", "ICLR.cc/2017/conference/paper489/AnonReviewer3", "ICLR.cc/2017/conference/paper489/AnonReviewer2"], "reply": {"forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512567641}}}, {"tddate": null, "tmdate": 1481952195865, "tcdate": 1481952195865, "number": 2, "id": "H1h6iHzEx", "invitation": "ICLR.cc/2017/conference/-/paper489/official/review", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["ICLR.cc/2017/conference/paper489/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper489/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "Paper Summary: \nThis paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.\n\nPaper Strengths: \n-- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.\n-- The proposed dataset is sufficiently large for data hungry deep learning models to train. \n-- The inclusion of questions with null answers is a nice property to have.\n-- A good amount of thought has gone into formulating the four-stage data collection process.\n-- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    \n\nPaper Weaknesses: \n-- Human evaluation is weak. Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?\n-- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. \n-- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?\n-- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?\n-- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?\n-- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?\n-- Which model's performance has been shown in Figure 1?\n-- Are the two \"students\" graduate/undergraduate students or researchers?\n-- Test set seems to be very small.\n-- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. \n\nPreliminary Evaluation: \nThe proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512567641, "id": "ICLR.cc/2017/conference/-/paper489/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper489/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper489/AnonReviewer1", "ICLR.cc/2017/conference/paper489/AnonReviewer3", "ICLR.cc/2017/conference/paper489/AnonReviewer2"], "reply": {"forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512567641}}}, {"tddate": null, "tmdate": 1481922482904, "tcdate": 1481922442432, "number": 4, "id": "SJG9DAWNx", "invitation": "ICLR.cc/2017/conference/-/paper489/public/comment", "forum": "ry3iBFqgl", "replyto": "Sy2vE3xNl", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "writers": ["~Tong_Wang1"], "content": {"title": "Human evaluation, SQuAD comparison, and other updates", "comment": "1. We acknowledge that human performance is currently measured on a small, potentially insignificant subset. This limitation arose from time constraints on the original submission. To remedy the issue, we are currently evaluating human performance on a larger subset of the corpus with in-house data analysts. We will update the paper accordingly once this process is completed.\n\n2. It is true that NewsQA is similar to SQuAD in terms of the characteristics cited. On the other hand, there are differences we believe to be significant: namely, NewsQA's larger article coverage (12,744 documents vs. 536), longer articles (31 sentences on average vs. 5), and unique collection methodology (i.e., questioners do not see the full article). These are in addition to the raw difficulty level. To address the difficulty issues raised in the review:\n(a) The expanded human evaluation will give a better estimate of NewsQA's challenge level with respect to SQuAD.\n(b) We believe that in reading comprehension, document length may be a challenge in and of itself. Increasing length decreases the 'signal-to-noise' ratio for salient information; it is harder to select an answer span when the number of non-answer spans is higher.\n    Nonetheless, to address the above-mentioned concerns, we are conducting an additional experiment where document length is artificially increased in SQuAD by concatenating consecutive paragraphs from the same Wikipedia article. Preliminary results suggest that even with comparable document length, sentence-level accuracy with simple methods is still significantly lower on NewsQA than on SQuAD, confirming our original beliefs on the level of difficulty in NewsQA.\n\n3. The lack of a numerical comparison was an oversight. We hence added a speed comparison between BARB and mLSTM (footnote 2, page 6) for the experiments reported in Section 6.2.\n\n4. This was a typo and has been corrected. The value in question (\"n_s\") represents the number of tokens in the document (previously denoted by \"n\").\n\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555349, "id": "ICLR.cc/2017/conference/-/paper489/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555349}}}, {"tddate": null, "tmdate": 1481849019642, "tcdate": 1481847908540, "number": 1, "id": "Sy2vE3xNl", "invitation": "ICLR.cc/2017/conference/-/paper489/official/review", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["ICLR.cc/2017/conference/paper489/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper489/AnonReviewer1"], "content": {"title": "Need better human evaluation and comparison with SQuAD", "rating": "6: Marginally above acceptance threshold", "review": "Summary: The paper proposes a novel machine comprehension dataset called NEWSQA. The dataset consists of over 100,000 question answer pairs based on over 10,000 news articles from CNN. The paper analyzes the different types of answers and the different types of reasoning required to answer questions in the dataset. The paper evaluates human performance and the performance of two baselines on the dataset and compares them with the performance on SQuAD dataset. \n\nStrengths:\n\n1. The paper presents a large scale dataset for machine comprehension. \n\n2. The question collection method seems reasonable to collect exploratory questions. Having an answer validation step is desirable.\n\n3. The paper proposes a novel (computationally more efficient) implementation of the match-LSTM model.\n\nWeaknesses:\n\n1. The human evaluation presented in the paper is not satisfactory because the human performance is reported on a very small subset (200 questions). So, it seems unlikely that these 200 questions will provide a reliable measure of the human performance on the entire dataset (which consists of thousands of questions).\n\n2. NEWSQA dataset is very similar to SQuAD dataset in terms of the size of the dataset, the type of dataset -- natural language questions posed by crowdworkers, answers comprising of spans of text from related paragraphs. The paper presents two empirical ways to show that NEWSQA is more challenging than SQuAD -- 1) the gap between human and machine performance in NEWSQA is larger than that in SQuAD. However, since the human performance numbers are reported on very small subset, these trends might not carry over when human performance is computed on all of the dataset.\n2) the sentence-level accuracy on SQuAD is higher than that in NEWSQA. However, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets. So, even this measure does not truly reflect that SQuAD is less challenging than NEWSQA.\nSo, it is not clear if NEWSQA is truly more challenging than SQuAD.\n\n3. Authors mention that BARB is computationally more efficient and faster compared to match-LSTM. However, the paper does not report how much faster BARB is compared to match-LSTM.\n\n4. On page 7, under \"Boundary pointing\" paragraph, the paper should clarify what \"s\" in \"n_s\" refers to.\n\nReview summary: While the dataset collection method seems interesting and promising, I would be more convinced after I see the following --\n1. Human performance on all (or significant percentage of the dataset).\n2. An empirical study that fairly shows that NEWSQA is more challenging (or better in some other way) than SQuAD.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512567641, "id": "ICLR.cc/2017/conference/-/paper489/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper489/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper489/AnonReviewer1", "ICLR.cc/2017/conference/paper489/AnonReviewer3", "ICLR.cc/2017/conference/paper489/AnonReviewer2"], "reply": {"forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512567641}}}, {"tddate": null, "tmdate": 1481833898961, "tcdate": 1481833898955, "number": 3, "id": "ryQ26_gVe", "invitation": "ICLR.cc/2017/conference/-/paper489/public/comment", "forum": "ry3iBFqgl", "replyto": "ry6XAkZXg", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "writers": ["~Tong_Wang1"], "content": {"title": "RE multi-span answers, question types", "comment": "1. The crowdsource workers are encouraged instead of required to mark single, continuous spans. Possibly as a result, the average number of spans is only slightly greater than 1 (around 1.05), and only about 0.6% of the answers span across multiple sentences.\n2. Due to limited space, we omitted some statistics from the paper. However, these stats (include the wh-word questions type distribution) are available on the dataset website, to which we will add a reference in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555349, "id": "ICLR.cc/2017/conference/-/paper489/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555349}}}, {"tddate": null, "tmdate": 1481123910353, "tcdate": 1481123910344, "number": 2, "id": "H1RB_jH7x", "invitation": "ICLR.cc/2017/conference/-/paper489/public/comment", "forum": "ry3iBFqgl", "replyto": "rJfGocy7e", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "writers": ["~Tong_Wang1"], "content": {"title": "Re crowd sourcing stats, human evaluation, etc.", "comment": "1. In answer sourcing, each question is answered by an average of 2.73 workers.\n2. In validation, each worker is asked to either pick the best answer or reject all answers for each (article, question) tuple, rather than validating an (article, question, answer) triple. The average number of workers per (article, question) tuple is 2.48. We have add both stats to the paper.\n3. The five reasoning types are implicitly considered \"ordered\" in the sense that by the order of their appearance in the paper, the later types subsumes the earlier types. As a result, the annotators were asked to choose only the most subsuming category when more than one category apply. Essentially all questions require word matching and paraphrasing, as can be seen in the examples in Table 2. In this sense, defaulting to the least challenging type would be somewhat degenerate---all questions would be classified into word matching. It would probably be clearer for us to discuss these reasoning types as \"stacking\" on top of or subsuming each other. The synthesis questions certainly involve word matching, and often paraphrasing, and sometimes inference (as defined) as well.\n4. Human performance was tested on 200 questions drawn uniformly at random from the development set (100 questions each for two humans). It is true that this is a relatively small sample. We could increase the sample size for human evaluation given more time.\n5. We did not use SQuAD's evaluation method because our process for collecting (and keeping) answers was different. Rather than maintaining a set of distinct answers for each question, we keep only answers with interworker agreement whenever possible. Including the validation stage, this amounts to 86% of the answers in the dataset, so this could be considered our human performance according to the SQuAD metric. The other 14% of answers are specially marked and considered ambiguous. However, we wanted to get a more accurate measure of human performance on these agreed answers, hence our in-house testing.\n6. They are on 200 questions from SQuAD's dev set."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555349, "id": "ICLR.cc/2017/conference/-/paper489/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555349}}}, {"tddate": null, "tmdate": 1480951304499, "tcdate": 1480951304493, "number": 1, "id": "B1gfU-7Qe", "invitation": "ICLR.cc/2017/conference/-/paper489/public/comment", "forum": "ry3iBFqgl", "replyto": "r1kX-0k7x", "signatures": ["~Tong_Wang1"], "readers": ["everyone"], "writers": ["~Tong_Wang1"], "content": {"title": "The reason for concatenating annotation", "comment": "The intuition behind the annotation mechanism is that we hope the model (in particular the re-encoding RNN) can learn how to best use this multi-dimensional annotation, rather than imposing a particular operation as the way the attention information is used  (e.g., input weighting by multiplication, as in conventional attention models).\nEmpirically, the mechanism is observed to improve model performance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287555349, "id": "ICLR.cc/2017/conference/-/paper489/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry3iBFqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper489/reviewers", "ICLR.cc/2017/conference/paper489/areachairs"], "cdate": 1485287555349}}}, {"tddate": null, "tmdate": 1480814116583, "tcdate": 1480814116579, "number": 3, "id": "ry6XAkZXg", "invitation": "ICLR.cc/2017/conference/-/paper489/pre-review/question", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["ICLR.cc/2017/conference/paper489/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper489/AnonReviewer2"], "content": {"title": "QA types", "question": "- The crowdsourcing instruction required workers to mark a single continuous span. I wonder for answers that require synthesis, how often the annotated span covers multiple sentences that are far apart. It'd be helpful if the authors can provide the average length of the span for each answer type.\n  \n- Can authors provide statistics on question types? e.g., Who, When, Where, What, Which, Why, How? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959253376, "id": "ICLR.cc/2017/conference/-/paper489/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper489/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper489/AnonReviewer1", "ICLR.cc/2017/conference/paper489/AnonReviewer3", "ICLR.cc/2017/conference/paper489/AnonReviewer2"], "reply": {"forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959253376}}}, {"tddate": null, "tmdate": 1480742184069, "tcdate": 1480741142972, "number": 2, "id": "r1kX-0k7x", "invitation": "ICLR.cc/2017/conference/-/paper489/pre-review/question", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["ICLR.cc/2017/conference/paper489/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper489/AnonReviewer3"], "content": {"title": "Clarification", "question": "For the BARB model, the authors mention -- \"Contrasting the multiplicative application of attention vectors, this annotation matrix is to be concatenated to the encoder RNN input in the re-encoding stage.\" What is the reason behind this?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959253376, "id": "ICLR.cc/2017/conference/-/paper489/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper489/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper489/AnonReviewer1", "ICLR.cc/2017/conference/paper489/AnonReviewer3", "ICLR.cc/2017/conference/paper489/AnonReviewer2"], "reply": {"forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959253376}}}, {"tddate": null, "tmdate": 1480727346700, "tcdate": 1480727305842, "number": 1, "id": "rJfGocy7e", "invitation": "ICLR.cc/2017/conference/-/paper489/pre-review/question", "forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "signatures": ["ICLR.cc/2017/conference/paper489/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper489/AnonReviewer1"], "content": {"title": "Clarification Questions", "question": "Dear Authors,\n\nCould you please clarify the following --\n\n1. How many workers answered each question in the answer sourcing step? \n\n2. How many workers validated each (article, question, answer) triple in the validation step?\n\n3. When classifying the questions based on reasoning required, what percentage of questions fall into more than one category? Also, what would be the new distributions across different reasoning types, if the questions falling into more than one category were defaulted to the least challenging type?\n\n4. The human performance on NEWSQA is reported on only 100 questions. How were these 100 questions selected? It seems unlikely that these 100 questions will provide a reliable measure of the human performance on the entire dataset (which consists of thousands of questions).\n \n5. Since every question in the NEWSQA dataset was answered by more than one worker, why was the same evaluation method as SQuAD not used for computing interhuman agreement for NEWSQA? \n \n6. In table 3 (for SQuAD), are the human (ours) numbers on 100 sentences?\n\nThanks.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "pdf": "/pdf/458bb7690cd230d9285b4c8b5b04c52991c9de7f.pdf", "TL;DR": "Crowdsourced QA dataset with natural language questions and multi-word answers", "paperhash": "trischler|newsqa_a_machine_comprehension_dataset", "conflicts": ["maluuba.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "authorids": ["adam.trischler@maluuba.com", "tong.wang@maluuba.com", "eric.yuan@maluuba.com", "justin.harris@maluuba.com", "alessandro.sordoni@maluuba.com", "phil.bachman@maluuba.com", "k.suleman@maluuba.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959253376, "id": "ICLR.cc/2017/conference/-/paper489/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper489/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper489/AnonReviewer1", "ICLR.cc/2017/conference/paper489/AnonReviewer3", "ICLR.cc/2017/conference/paper489/AnonReviewer2"], "reply": {"forum": "ry3iBFqgl", "replyto": "ry3iBFqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper489/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959253376}}}], "count": 17}