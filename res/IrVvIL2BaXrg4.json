{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392165900000, "tcdate": 1392165900000, "number": 1, "id": "eexW18S30F1K6", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "IrVvIL2BaXrg4", "replyto": "qLyDi4fHDriEo", "signatures": ["Yanpeng Li"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for your comments. Your suggestions are helpful.\r\n\r\nBut I would like to discuss with you about the following comments:\r\n\r\n- ' it is hard to understand why it should perform better than state of the art linear methods. The proposed heuristic could be interpreted as a prior on classifier weights, but again in the context of large training sets like the ones used in the experiments, this should not be a real advantage'\r\n\r\n1. Compared to supervised learning methods, the proposed method can be interpreted as feature weight learning for linear classifier only in some special cases where a) the feature vocabularies of training and test/unlabeled data are the same, and b) the classifier for the learned RDE features is a linear classifier.\r\n\r\nHowever, in practice there are a number of tasks that have 'out-of-vocabulary' (OOV) features, such as the word/n-gram/phrase based features in IR and NLP, sparse visual word features obtained by clustering or pooling in image representation, or other sparse signal features learned from universal data... That is, there must be a lot of features in the unlabeled data but not in the training data, since they usually follow a power law distribution. These ignored features cannot be utilized well in supervised learning but can be activated in the RDE based semi-supervised learning. See the following example: \r\n\r\nWe have the following Boolean features: f1, f2, f3, f4, f5... In the instance xi, the features f1, f2, and f3 appear. We select f4 and f5 as reference features. According to the first two steps of Section 2.2, the new representation for xi has two features. The feature values are RDE(xi, f4) and RDE(xi, f5), which are not the simple weighted combination of the old features in labeled data. For example, if f3, a highly indicative feature, does not appear in labeled data but in unlabeled data, it cannot be utilized in supervised learning method but can be learnt from RDE (xi, f4) if f4 is a good reference feature. Also it will be the similar case if f2 is an extremely low-frequency in labeled data. It provides a powerful strategy to overcome the data sparseness problem in machine learning, and also makes it feasible to design high-order features from the combination of existing features, e.g., n-gram features (Figure 1b). \r\n\r\n2. Even if in some cases it is \u201ca linear combination of the initial data features\u201d or feature reweighting method, is it \u201chard to understand why it should perform better than state of the art linear methods\u201d? It is easy to understand that a Na\u00efve Bayes classifier trained with 13 million labeled examples could be better than a linear SVM trained with 5000 labeled ones. The Na\u00efve Bayes classifier can be seen as feature reweighting of the SVM classifier (where the OOV features can be viewed as zero weights), which improves the performance greatly (Figure 1a and 1b). The reason is the law of large number. RDE based semi-supervised learning just aims to move towards this goal (Figure 1 and Table 1). \r\n\r\n3. \u201c\u2026in the context of large training sets like the ones used in the experiments, this should not be a real advantage\u2026\u201d \r\nIt is against all the semi-supervised learning methods but not just for the RDE based method. My opinion is that \u201clarge\u201d or \u201csmall\u201d is just a relative concept, so if you have large labeled data there tends to be much larger unlabeled data (waiting to be labeled). Otherwise, the task is not very suitable for machine learning since most of the work has already been done by humans, and manual annotation of the rest seems not too difficult.  Also, the \u201clarge\u201d or \u201csmall\u201d data depends on features. For example, for word features 10 million documents should be a large set, since most words appears at least several times. But for high order n-grams it appears to be a small set and we need to learn n-gram features from much more unlabeled data. In this experiment if we do not use the huge labeled MEDLINE dataset, we cannot get a good estimation of semi-perfect classifier, which is important to justify the theorems. \r\n\r\n\r\n-\t\u201cThe development of these features is neither intuitive nor theoretically founded.\u201d\r\n\r\nIt is well known that the performance of individual features is a very important factor to determine the quality of a set of features. All the theorems and the experiment (Figure2) are concerned with the performance of individual RDEs. The ensemble algorithm is to select the best RDEs as individual features and combine them together for better performance. Although we could ignore the diversity of the RDE features (our future study), the development of these features are supported by strong intuition and at least partially by theory and experiments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference Distance Estimator", "decision": "submitted, no decision", "abstract": "A theoretical study is presented for a simple linear classifier called reference distance estimator (RDE), which assigns the weight of each feature j as P(r|j)-P(r), where r is a reference feature relevant to the target class y. The analysis shows that if r performs better than random guess in predicting y and is conditionally independent with each feature j, the RDE will have the same classification performance as that from P(y|j)-P(y), a classifier trained with the gold standard y. Since the estimation of P(r|j)-P(r) does not require labeled data, under the assumption above, RDE trained with a large number of unlabeled examples would be close to that trained with infinite labeled examples. For the case the assumption does not hold, we theoretically analyze the factors that influence the closeness of the RDE to the perfect one under the assumption, and present an algorithm to select reference features and combine multiple RDEs from different reference features using both labeled and unlabeled data. The experimental results on 10 text classification tasks show that the semi-supervised learning method improves supervised methods using 5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its performance is even close to a classifier trained with 13 million labeled examples. In addition, the bounds in the theorems provide good estimation of the classification performance and can be useful for new algorithm design.", "pdf": "https://arxiv.org/abs/1308.3818", "paperhash": "li|reference_distance_estimator", "authors": ["Yanpeng Li"], "authorids": ["liyanpeng.lyp@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391918640000, "tcdate": 1391918640000, "number": 5, "id": "J9nSyRXqSM9Tj", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "IrVvIL2BaXrg4", "replyto": "IrVvIL2BaXrg4", "signatures": ["Yanpeng Li"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you for your comments. \r\n\r\nSome of your suggestions are what we are currently working on, e.g., the theoretical analysis about supervised/semi-supervised RDE, the relation with other classifiers and much more experiments on various challenge evaluation data sets. I must say it is a very promising direction technically no matter people like or not the description of the manuscript. \r\n\r\nHere we just aim to report the preliminary findings in theory and experiment. Since there is page limitation in this conference, we are not able to include a lot of details in this one."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference Distance Estimator", "decision": "submitted, no decision", "abstract": "A theoretical study is presented for a simple linear classifier called reference distance estimator (RDE), which assigns the weight of each feature j as P(r|j)-P(r), where r is a reference feature relevant to the target class y. The analysis shows that if r performs better than random guess in predicting y and is conditionally independent with each feature j, the RDE will have the same classification performance as that from P(y|j)-P(y), a classifier trained with the gold standard y. Since the estimation of P(r|j)-P(r) does not require labeled data, under the assumption above, RDE trained with a large number of unlabeled examples would be close to that trained with infinite labeled examples. For the case the assumption does not hold, we theoretically analyze the factors that influence the closeness of the RDE to the perfect one under the assumption, and present an algorithm to select reference features and combine multiple RDEs from different reference features using both labeled and unlabeled data. The experimental results on 10 text classification tasks show that the semi-supervised learning method improves supervised methods using 5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its performance is even close to a classifier trained with 13 million labeled examples. In addition, the bounds in the theorems provide good estimation of the classification performance and can be useful for new algorithm design.", "pdf": "https://arxiv.org/abs/1308.3818", "paperhash": "li|reference_distance_estimator", "authors": ["Yanpeng Li"], "authorids": ["liyanpeng.lyp@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391902020000, "tcdate": 1391902020000, "number": 4, "id": "Vx9IwXzYeBRDk", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "IrVvIL2BaXrg4", "replyto": "IrVvIL2BaXrg4", "signatures": ["anonymous reviewer 2bb3"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Reference Distance Estimator", "review": "This paper proposes an interesting approach: identify features which are likely to correlate with a class predictor on a small set and learn to predict the presence of this binary features on a large unlabeled dataset. The paper is however not very clear and its experimental section is far from thorough. \r\n\r\nThe paper is dense but misses important points. In particular,\r\n  - what are the modeling assumptions of RDEs?\r\n  - what types of data are likely to succeed?\r\n\r\nIt could be more fluent and better structured. In particular, the theorems follow each other without much transitions. The reader could be guided more telling what you are going to show and why, on results are going to be linked toward the final goal. I also feel that the results of supervised RDE on their own seems very good, better than SVM and logistic regression. It might be good to invest more work to propose RDEs in a supervised setting with extensive empirical comparison on reference dataset, before then extending them to semi-supervised learning. A paper with a single message is likely to be clearer. \r\n\r\nThe quality of its experimental section could greatly be improved. The experiments are performed on a single dataset. Reporting results on standard text classification datasets like RCV1 and other types of data would greatly improve the paper. I feel this is necessary since your results indicate that SVM and logistic regression, which have been state-of-the-art text classification algorithms for many years seem significantly outperformed by the simple supervised RDE. If this is confirmed over benchmark dataset this could be a game changer. However, this single result in non-standard setting is likely to rise doubts. I would also appreciate an analysis of the different step of the algorithm: \r\n- is the reference feature selection step important (e.g. you could compare with other feature selection strategies like information gain) ?\r\n- are the individual RDE good predictors of their reference features ? is that important or it does not matter for the final task?\r\n- what is the impact of k on the generalization performance?\r\n\r\nAlso, the baseline algorithm SVM and logistic regression are not trained on the full dataset (this is easily feasible with an efficient SGD implementation like http://leon.bottou.org/projects/sgd). Moreover, I do not understand why no learning curve is reported for semi-supervised RDEs (only the point at 5K labeled samples is reported). Finally, you mention that the hyperparameters of the baselines have been tuned on the test set, which is usually discouraged as it could a very optimistic estimate of the generalization performance. It also raises the question of the methodology used for tuning of k, t and the regularization parameter 'ridge' for your technique. Could you detail your procedure and provide more detail on hyper-parameter sensitivity?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference Distance Estimator", "decision": "submitted, no decision", "abstract": "A theoretical study is presented for a simple linear classifier called reference distance estimator (RDE), which assigns the weight of each feature j as P(r|j)-P(r), where r is a reference feature relevant to the target class y. The analysis shows that if r performs better than random guess in predicting y and is conditionally independent with each feature j, the RDE will have the same classification performance as that from P(y|j)-P(y), a classifier trained with the gold standard y. Since the estimation of P(r|j)-P(r) does not require labeled data, under the assumption above, RDE trained with a large number of unlabeled examples would be close to that trained with infinite labeled examples. For the case the assumption does not hold, we theoretically analyze the factors that influence the closeness of the RDE to the perfect one under the assumption, and present an algorithm to select reference features and combine multiple RDEs from different reference features using both labeled and unlabeled data. The experimental results on 10 text classification tasks show that the semi-supervised learning method improves supervised methods using 5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its performance is even close to a classifier trained with 13 million labeled examples. In addition, the bounds in the theorems provide good estimation of the classification performance and can be useful for new algorithm design.", "pdf": "https://arxiv.org/abs/1308.3818", "paperhash": "li|reference_distance_estimator", "authors": ["Yanpeng Li"], "authorids": ["liyanpeng.lyp@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391889240000, "tcdate": 1391889240000, "number": 3, "id": "-t2JA1q4M5znO", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "IrVvIL2BaXrg4", "replyto": "IrVvIL2BaXrg4", "signatures": ["Yanpeng Li"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "To Anonymous 7977: \r\n\r\n-  '..just selects features are (on average) as correlated to other features as possible...'\r\n\r\n1) You could have missed the I(r) in the formula, an important measure of the discriminating ability of reference feature, which is also obtained from the theory. \r\n\r\n2) When you look at the Step 2 of Section 2.2, you will know it is not a feature selection method, because each RDE (Formula 1) incorporates new information (beyond labeled data) from unlabeled data via P(r|j) and P(r). In other words, this method learns new features from the co-occurrence of existing features in huge unlabeled data. \r\n\r\n3) For example, we have the following Boolean features: f1, f2, f3, f4, f5...  In the instance xi, the features f1, f2, and f3 appear. We select f4 and f5 as reference features. According to the first two steps of Section 2.2, the new representation for xi has two features. The feature values are RDE(xi, f4) and RDE(xi, f5), which are not the simple weighted combination of the old features in labeled data. For example, if f3, a highly indicative feature, does not appear in labeled data but in unlabeled data, it cannot be utilized in supervised learning method but can be learnt from RDE (xi, f4) if f4 is a good reference feature. Also it will be the similar case if f2 is an extremely low-frequency in labeled data. It provides a powerful strategy to overcome the data sparseness problem in machine learning, and also makes it feasible to design high-order features from the combination of existing features, e.g., n-gram features (Figure 1b).   \r\n\r\n\r\n-\t'There is little motivation on using a reference feature --- which is not available in practice'\r\n\r\nCan you prove that it is not available in practice? Actually, the assumptions in Theorem 1 are much weaker than those in Naive Bayes and Co-training. Also, most machine learning method aims to find a prediction label as close as the gold standard label. RDE aims to find a good reference feature which is not necessarily the gold standard. In this sense, it should be more practical. \r\n\r\n\r\n-\t\u201cMoreover, how Theorem 3's condition is generally true?\u201d \r\n\r\nIt is a much weaker assumption than that in Theorem 1, and found generally true in the text classification experiments. Due to the page limitation, I am not able to give a detailed analysis in this paper. Theoretical study needs to make assumptions at first even if there could be a gap with the practice such as the IID assumption and conditional independence assumption. At least they can inspire people to move towards the correct direction. \r\n\r\n Looking forward to your response."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference Distance Estimator", "decision": "submitted, no decision", "abstract": "A theoretical study is presented for a simple linear classifier called reference distance estimator (RDE), which assigns the weight of each feature j as P(r|j)-P(r), where r is a reference feature relevant to the target class y. The analysis shows that if r performs better than random guess in predicting y and is conditionally independent with each feature j, the RDE will have the same classification performance as that from P(y|j)-P(y), a classifier trained with the gold standard y. Since the estimation of P(r|j)-P(r) does not require labeled data, under the assumption above, RDE trained with a large number of unlabeled examples would be close to that trained with infinite labeled examples. For the case the assumption does not hold, we theoretically analyze the factors that influence the closeness of the RDE to the perfect one under the assumption, and present an algorithm to select reference features and combine multiple RDEs from different reference features using both labeled and unlabeled data. The experimental results on 10 text classification tasks show that the semi-supervised learning method improves supervised methods using 5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its performance is even close to a classifier trained with 13 million labeled examples. In addition, the bounds in the theorems provide good estimation of the classification performance and can be useful for new algorithm design.", "pdf": "https://arxiv.org/abs/1308.3818", "paperhash": "li|reference_distance_estimator", "authors": ["Yanpeng Li"], "authorids": ["liyanpeng.lyp@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391864040000, "tcdate": 1391864040000, "number": 2, "id": "vtDg5wREyrtEf", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "IrVvIL2BaXrg4", "replyto": "IrVvIL2BaXrg4", "signatures": ["anonymous reviewer 7977"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Reference Distance Estimator", "review": "The paper proposes a feature selection algorithm and learn a new representation based on the selected features.  The new representation is then used for constructing classifiers. \r\n\r\nThis is a very difficult paper to read, due to the exposition style. As far as I can gather, the feature selection algorithm just selects features are (on average) as correlated to other features as possible, cf. step 1 in section 2.2. The new representation is then weighted combination of the old features. \r\n\r\nI have difficulty in understanding either contributions.  There is little motivation on using a reference feature --- which is not available in practice.  Moreover, how Theorem 3's condition is generally true?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference Distance Estimator", "decision": "submitted, no decision", "abstract": "A theoretical study is presented for a simple linear classifier called reference distance estimator (RDE), which assigns the weight of each feature j as P(r|j)-P(r), where r is a reference feature relevant to the target class y. The analysis shows that if r performs better than random guess in predicting y and is conditionally independent with each feature j, the RDE will have the same classification performance as that from P(y|j)-P(y), a classifier trained with the gold standard y. Since the estimation of P(r|j)-P(r) does not require labeled data, under the assumption above, RDE trained with a large number of unlabeled examples would be close to that trained with infinite labeled examples. For the case the assumption does not hold, we theoretically analyze the factors that influence the closeness of the RDE to the perfect one under the assumption, and present an algorithm to select reference features and combine multiple RDEs from different reference features using both labeled and unlabeled data. The experimental results on 10 text classification tasks show that the semi-supervised learning method improves supervised methods using 5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its performance is even close to a classifier trained with 13 million labeled examples. In addition, the bounds in the theorems provide good estimation of the classification performance and can be useful for new algorithm design.", "pdf": "https://arxiv.org/abs/1308.3818", "paperhash": "li|reference_distance_estimator", "authors": ["Yanpeng Li"], "authorids": ["liyanpeng.lyp@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391695980000, "tcdate": 1391695980000, "number": 1, "id": "qLyDi4fHDriEo", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "IrVvIL2BaXrg4", "replyto": "IrVvIL2BaXrg4", "signatures": ["anonymous reviewer 5b4f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Reference Distance Estimator", "review": "The paper describes a heuristic algorithm for learning representation features from labeled and unlabeled data. These features are then used as inputs to a classifier trained on labeled data. The main motivation of the algorithm is semi-supervised learning when unlabeled data can be used for estimating and selecting the representation features. Properties of these new features are analyzed under different hypothesis and in particular, a bound is provided for selecting the new features among a set of candidates. Experiments are then performed on a large dataset of Medline abstract and the proposed approach is compared to supervised and semi-supervised classifiers.\r\nThe paper introduces new ideas for learning intermediate representations.  They appear to be quite effective in the experimental comparison and the proposed method performs better than alternative semi-supervised baselines. The author also develops a series of properties of the learned features. On the other hand, the paper presents different weaknesses. The development of these features is neither intuitive nor theoretically founded. The final classifier performs a linear combination of the initial data features, and it is hard to understand why it should perform better than state of the art linear methods. The proposed heuristic could be interpreted as a prior on classifier weights, but again in the context of large training sets like the ones used in the experiments, this should not be a real advantage. Some of the experimental results are surprising. It is said that logistic regression and linear SVMs cannot be trained on large amounts of data and the author does not present results with these classifiers beyond 200 k training data. This should be reconsidered.  Results of semi-supervised classifiers are also doubtful. The baselines used for comparison are not able to leverage the use of unlabeled data when usually for text classification, semi-supervised learning is quite effective.\r\nThe form of the paper should also be improved.  In particular the experimental illustration of the distance and bound statistics is not clearly explained.\r\nGlobally, there are interesting ideas in this method, but also conceptual and experimental weaknesses that should be improved upon."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference Distance Estimator", "decision": "submitted, no decision", "abstract": "A theoretical study is presented for a simple linear classifier called reference distance estimator (RDE), which assigns the weight of each feature j as P(r|j)-P(r), where r is a reference feature relevant to the target class y. The analysis shows that if r performs better than random guess in predicting y and is conditionally independent with each feature j, the RDE will have the same classification performance as that from P(y|j)-P(y), a classifier trained with the gold standard y. Since the estimation of P(r|j)-P(r) does not require labeled data, under the assumption above, RDE trained with a large number of unlabeled examples would be close to that trained with infinite labeled examples. For the case the assumption does not hold, we theoretically analyze the factors that influence the closeness of the RDE to the perfect one under the assumption, and present an algorithm to select reference features and combine multiple RDEs from different reference features using both labeled and unlabeled data. The experimental results on 10 text classification tasks show that the semi-supervised learning method improves supervised methods using 5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its performance is even close to a classifier trained with 13 million labeled examples. In addition, the bounds in the theorems provide good estimation of the classification performance and can be useful for new algorithm design.", "pdf": "https://arxiv.org/abs/1308.3818", "paperhash": "li|reference_distance_estimator", "authors": ["Yanpeng Li"], "authorids": ["liyanpeng.lyp@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387400280000, "tcdate": 1387400280000, "number": 8, "id": "IrVvIL2BaXrg4", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "IrVvIL2BaXrg4", "signatures": ["liyanpeng.lyp@gmail.com"], "readers": ["everyone"], "content": {"title": "Reference Distance Estimator", "decision": "submitted, no decision", "abstract": "A theoretical study is presented for a simple linear classifier called reference distance estimator (RDE), which assigns the weight of each feature j as P(r|j)-P(r), where r is a reference feature relevant to the target class y. The analysis shows that if r performs better than random guess in predicting y and is conditionally independent with each feature j, the RDE will have the same classification performance as that from P(y|j)-P(y), a classifier trained with the gold standard y. Since the estimation of P(r|j)-P(r) does not require labeled data, under the assumption above, RDE trained with a large number of unlabeled examples would be close to that trained with infinite labeled examples. For the case the assumption does not hold, we theoretically analyze the factors that influence the closeness of the RDE to the perfect one under the assumption, and present an algorithm to select reference features and combine multiple RDEs from different reference features using both labeled and unlabeled data. The experimental results on 10 text classification tasks show that the semi-supervised learning method improves supervised methods using 5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its performance is even close to a classifier trained with 13 million labeled examples. In addition, the bounds in the theorems provide good estimation of the classification performance and can be useful for new algorithm design.", "pdf": "https://arxiv.org/abs/1308.3818", "paperhash": "li|reference_distance_estimator", "authors": ["Yanpeng Li"], "authorids": ["liyanpeng.lyp@gmail.com"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}