{"notes": [{"id": "r1eWdlBFwS", "original": "H1eM_3ltDB", "number": 2387, "cdate": 1569439849070, "ddate": null, "tcdate": 1569439849070, "tmdate": 1577168248789, "tddate": null, "forum": "r1eWdlBFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jddavison@g.harvard.edu", "kristen.severson@ibm.com", "ghoshso@us.ibm.com"], "title": "Isolating Latent Structure with Cross-population Variational Autoencoders", "authors": ["Joe Davison", "Kristen A. Severson", "Soumya Ghosh"], "pdf": "/pdf/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "TL;DR": "A variant of the VAE which models data from differing distributions, isolating the latent factors which are unique to each set as well as the shared structure", "abstract": "A significant body of recent work has examined variational autoencoders as a powerful approach for tasks which involve modeling the distribution of complex data such as images and text. In this work, we present a framework for modeling multiple data sets which come from differing distributions but which share some common latent structure. By incorporating architectural constraints and using a mutual information regularized form of the variational objective, our method successfully models differing data populations while explicitly encouraging the isolation of the shared and private latent factors. This enables our model to learn useful shared structure across similar tasks and to disentangle cross-population representations in a weakly supervised way. We demonstrate the utility of our method on several applications including image denoising, sub-group discovery, and continual learning.", "keywords": ["variational autoencoder", "latent variable model", "probabilistic graphical model", "machine learning", "deep learning", "continual learning"], "paperhash": "davison|isolating_latent_structure_with_crosspopulation_variational_autoencoders", "original_pdf": "/attachment/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "_bibtex": "@misc{\ndavison2020isolating,\ntitle={Isolating Latent Structure with Cross-population Variational Autoencoders},\nauthor={Joe Davison and Kristen A. Severson and Soumya Ghosh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eWdlBFwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "uMu1stIs41", "original": null, "number": 1, "cdate": 1576798747815, "ddate": null, "tcdate": 1576798747815, "tmdate": 1576800888252, "tddate": null, "forum": "r1eWdlBFwS", "replyto": "r1eWdlBFwS", "invitation": "ICLR.cc/2020/Conference/Paper2387/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a hierarchical Bayesian model over multiple data sets that                                                      \nhas both data set specific as well as shared parameters.                                                                           \nThe data set specific parameters are further encouraged to only capture aspects                                                    \nthat vary across data sets by an addition mutual information contribution to the                                                   \ntraining loss.                                                                                                                     \nThe proposed method is compared to standard VAEs on multiple data sets.                                                            \n                                                                                                                                   \nThe reviewers agree that the main approach of the paper is sensible. However,                                                      \nconcerns were raised about general novelty, about the theoretical justification                                                    \nfor the proposed loss function and about the lack of non-trivial baselines.                                                         \nThe authors' rebuttal did not manage to full address these points.                                                                 \n                                                                                                                                   \nBased on the reviews and my own reading, I think this paper is slightly                                                            \nbelow acceptance threshold.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jddavison@g.harvard.edu", "kristen.severson@ibm.com", "ghoshso@us.ibm.com"], "title": "Isolating Latent Structure with Cross-population Variational Autoencoders", "authors": ["Joe Davison", "Kristen A. Severson", "Soumya Ghosh"], "pdf": "/pdf/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "TL;DR": "A variant of the VAE which models data from differing distributions, isolating the latent factors which are unique to each set as well as the shared structure", "abstract": "A significant body of recent work has examined variational autoencoders as a powerful approach for tasks which involve modeling the distribution of complex data such as images and text. In this work, we present a framework for modeling multiple data sets which come from differing distributions but which share some common latent structure. By incorporating architectural constraints and using a mutual information regularized form of the variational objective, our method successfully models differing data populations while explicitly encouraging the isolation of the shared and private latent factors. This enables our model to learn useful shared structure across similar tasks and to disentangle cross-population representations in a weakly supervised way. We demonstrate the utility of our method on several applications including image denoising, sub-group discovery, and continual learning.", "keywords": ["variational autoencoder", "latent variable model", "probabilistic graphical model", "machine learning", "deep learning", "continual learning"], "paperhash": "davison|isolating_latent_structure_with_crosspopulation_variational_autoencoders", "original_pdf": "/attachment/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "_bibtex": "@misc{\ndavison2020isolating,\ntitle={Isolating Latent Structure with Cross-population Variational Autoencoders},\nauthor={Joe Davison and Kristen A. Severson and Soumya Ghosh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eWdlBFwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1eWdlBFwS", "replyto": "r1eWdlBFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724893, "tmdate": 1576800276617, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2387/-/Decision"}}}, {"id": "r1lMN7j9FS", "original": null, "number": 1, "cdate": 1571627818419, "ddate": null, "tcdate": 1571627818419, "tmdate": 1572972344824, "tddate": null, "forum": "r1eWdlBFwS", "replyto": "r1eWdlBFwS", "invitation": "ICLR.cc/2020/Conference/Paper2387/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper studied the problem of learning the latent representation from a complex data set which followed the independent but not identically distributions. The main contributions of this paper are to explicitly learn the commonly shared and private latent factors for different data populations in a unified VAE framework, and propose a mutual information regularized inference in order to avoid the \u201cleaking\u201d induced by the shared representations across different populations. The isolation of the commonly shared and population specific latent representations learned by the proposed are empirically demonstrated on several applications. However, I have some concerns regarding this paper as follows.\n(1) It is not clear why the private representation exhibits latent features from the shared space when using equation 3 and how this phenomenon hurts this CPVAE model.\n(2) In equation 1, how to define the isotropic diagonal covariance matrix in the Gaussian distribution p? Is it parameterized by g?\n(3) In equation 3, what is the prior distribution of p(z_ki, t_ki)?\n(4) In equation (4)(5), why could the marginal KL term be canceled out when using I_q(x_k; t_k) - I_q(x_-k; \\tilde{t}_k)?\n(5) The mutual information regularized inference involved the KL term between any two private factors from different populations. It might be not efficient for optimization. Thus, it will be helpful if the authors provide the model efficiency analysis compared with other baseline methods.\n\nMinor comments:\n(1) what is the symbol \u201cn_k\u201d? Did it denote the number of examples for the k-th population?\n(2) For mutual information regularized inference, it used two different notations: \u201cI_q(x_k; t_k) - I_q(x_-k; t_k)\u201d and \u201cI_q(x^k; t^k) - I_q(x^-k; t^k)\u201d.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2387/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2387/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jddavison@g.harvard.edu", "kristen.severson@ibm.com", "ghoshso@us.ibm.com"], "title": "Isolating Latent Structure with Cross-population Variational Autoencoders", "authors": ["Joe Davison", "Kristen A. Severson", "Soumya Ghosh"], "pdf": "/pdf/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "TL;DR": "A variant of the VAE which models data from differing distributions, isolating the latent factors which are unique to each set as well as the shared structure", "abstract": "A significant body of recent work has examined variational autoencoders as a powerful approach for tasks which involve modeling the distribution of complex data such as images and text. In this work, we present a framework for modeling multiple data sets which come from differing distributions but which share some common latent structure. By incorporating architectural constraints and using a mutual information regularized form of the variational objective, our method successfully models differing data populations while explicitly encouraging the isolation of the shared and private latent factors. This enables our model to learn useful shared structure across similar tasks and to disentangle cross-population representations in a weakly supervised way. We demonstrate the utility of our method on several applications including image denoising, sub-group discovery, and continual learning.", "keywords": ["variational autoencoder", "latent variable model", "probabilistic graphical model", "machine learning", "deep learning", "continual learning"], "paperhash": "davison|isolating_latent_structure_with_crosspopulation_variational_autoencoders", "original_pdf": "/attachment/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "_bibtex": "@misc{\ndavison2020isolating,\ntitle={Isolating Latent Structure with Cross-population Variational Autoencoders},\nauthor={Joe Davison and Kristen A. Severson and Soumya Ghosh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eWdlBFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eWdlBFwS", "replyto": "r1eWdlBFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575582878047, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2387/Reviewers"], "noninvitees": [], "tcdate": 1570237723544, "tmdate": 1575582878062, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2387/-/Official_Review"}}}, {"id": "HJg6uOEatH", "original": null, "number": 2, "cdate": 1571797109206, "ddate": null, "tcdate": 1571797109206, "tmdate": 1572972344788, "tddate": null, "forum": "r1eWdlBFwS", "replyto": "r1eWdlBFwS", "invitation": "ICLR.cc/2020/Conference/Paper2387/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a novel model called \"Cross-Population Variational Autoencoder (CPVAE), which is designed to model data from different distributions sharing some common structure. The proposed generative model utilizes both shared and private per-population latent variables. In order to restrict shared latent variables from \"leaking\" into private representations, the authors introduce an information-theoretic regularizer. This regularizer forces private population representations to: (a) maximize mutual information with input samples from their population, and (b) minimize mutual information with input samples from other populations. In other words, private representations are forced to be \"meaningful\" on the corresponding population alone.\n\nQuality:\nThe paper is well-written. I find the proposed method to be quite interesting. The derivations appear to be correct except for possibly the cancellation in Eq. 5, which I originally missed.\n\nSignificance:\nIn my opinion, if sound, the approach discussed in this paper may lead to interesting practical applications and may inspire other methods based on similar ideas.\n\nOriginality:\nEven though, as authors point out, there is a substantial amount of work in this field, I believe that their approach is novel and has its own merits.\n\nClarity:\nThe paper is well written and the material is presented with clarity. In my opinion, the only exception is Section 4.4, which could definitely benefit from a few additional sentences describing the training procedure in more detail. Right now I find it a bit confusing. It would appear that the shared encoder / decoder continue to be trained as new populations arrive. Would this mean that catastrophic forgetting can actually impact this shared representation? And if it changes by a sufficient degree, can it reduce the quality of the generated samples for older populations? If so, I think these points should be mentioned in the text.\n\nQuestions and suggestions:\nExperiments described in the paper are sufficiently convincing, but there are a few questions that could potentially be better clarified in the paper.\n\n1. After reviewing the text again and seeing the comment of Reviewer #1, I am also confused about the cancellation in  I_q(x_k; t_k) - I_q(x_{-k}; \\tilde{t}_k). Is it not true that marginal distributions q_\\phi(t) and q_\\phi(\\tilde{t}) in the KL divergence term in Eq. 5 are different? Unfortunately, the final optimization objective relies on the cancellation of these terms and if they do not cancel, the approach may not be theoretically justified despite producing interesting and compelling results. (This affected the final rating. I will be able to change the rating once this point is clarified.)\n\n2. Another issue is related to the special case when there are several very similar populations. Consider, for example, a case when there are two nearly-identical populations out of many. Using very similar latent variables for two similar populations would be penalized by the regularizer (not too significantly though). I assume that depending on the embedding sizes and the value of alpha (which authors introduce in Section 2.3), the model would either choose to use shared latent variables to encode these populations, or would allow for two nearly-identical private latent representations to exist. I think this is a conceptually important special case that could be mentioned and possibly explained in the paper.\n\n3. I think the paper would also benefit from a clarification regarding the \"mixing\" function g. Choosing this function to be a simple sum of arguments seems restricting and may be insufficient for some datasets. It does not appear to be the case, but are there any restrictions on g? Can it come from a parametrized function family with parameters being optimized during training?\n\n4. I think the paper would benefit from a more detailed discussion in Section 4.4 (see above)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2387/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2387/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jddavison@g.harvard.edu", "kristen.severson@ibm.com", "ghoshso@us.ibm.com"], "title": "Isolating Latent Structure with Cross-population Variational Autoencoders", "authors": ["Joe Davison", "Kristen A. Severson", "Soumya Ghosh"], "pdf": "/pdf/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "TL;DR": "A variant of the VAE which models data from differing distributions, isolating the latent factors which are unique to each set as well as the shared structure", "abstract": "A significant body of recent work has examined variational autoencoders as a powerful approach for tasks which involve modeling the distribution of complex data such as images and text. In this work, we present a framework for modeling multiple data sets which come from differing distributions but which share some common latent structure. By incorporating architectural constraints and using a mutual information regularized form of the variational objective, our method successfully models differing data populations while explicitly encouraging the isolation of the shared and private latent factors. This enables our model to learn useful shared structure across similar tasks and to disentangle cross-population representations in a weakly supervised way. We demonstrate the utility of our method on several applications including image denoising, sub-group discovery, and continual learning.", "keywords": ["variational autoencoder", "latent variable model", "probabilistic graphical model", "machine learning", "deep learning", "continual learning"], "paperhash": "davison|isolating_latent_structure_with_crosspopulation_variational_autoencoders", "original_pdf": "/attachment/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "_bibtex": "@misc{\ndavison2020isolating,\ntitle={Isolating Latent Structure with Cross-population Variational Autoencoders},\nauthor={Joe Davison and Kristen A. Severson and Soumya Ghosh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eWdlBFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eWdlBFwS", "replyto": "r1eWdlBFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575582878047, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2387/Reviewers"], "noninvitees": [], "tcdate": 1570237723544, "tmdate": 1575582878062, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2387/-/Official_Review"}}}, {"id": "HJxnlUtN9B", "original": null, "number": 3, "cdate": 1572275699847, "ddate": null, "tcdate": 1572275699847, "tmdate": 1572972344742, "tddate": null, "forum": "r1eWdlBFwS", "replyto": "r1eWdlBFwS", "invitation": "ICLR.cc/2020/Conference/Paper2387/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to model multiple datasets from differing distributions with shared latent structure and private latent factors. The main techniques include architecture design which encourages the isolation of shared and private latent factors and a mutual information-based regularizer. The paper is clearly written and easy to follow. I enjoyed reading it. The experiments support the claim of learned population-specific representations.\n\nHowever, I found that the paper has some weaknesses:\n1. The novelty is not enough. All the techniques involved in the paper are not new but from existing literature. The idea is not new. The authors also mentioned several previous works in Section 3, e.g. Multi-level  VAEs, oi-VAEs.\n\n2. More importantly, I did not see any baselines in the experiments except vanilla VAE. As far as I understand, previous methods can be easily adapted to these tasks. For example, [1] tried continual generative modeling for a sequence of distinct distributions. Many important baselines are missing in the experiments, which makes it hard for me to evaluate how significant the work is.\n\n3. What if the populations are not exclusive? The regularizer enforces them to be isolated but they are not in fact.\n\n4. How did you choose the annealing schedule of $\\alpha$ in Section B.1?\n\nMinor:\n\npage 2  eq (1) z_i -> z_{ki}\n\npage 3 last paragraph \u201cit may desirable\u201d\n\nReferences:\n\n[1] https://arxiv.org/pdf/1705.08395.pdf\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2387/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2387/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jddavison@g.harvard.edu", "kristen.severson@ibm.com", "ghoshso@us.ibm.com"], "title": "Isolating Latent Structure with Cross-population Variational Autoencoders", "authors": ["Joe Davison", "Kristen A. Severson", "Soumya Ghosh"], "pdf": "/pdf/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "TL;DR": "A variant of the VAE which models data from differing distributions, isolating the latent factors which are unique to each set as well as the shared structure", "abstract": "A significant body of recent work has examined variational autoencoders as a powerful approach for tasks which involve modeling the distribution of complex data such as images and text. In this work, we present a framework for modeling multiple data sets which come from differing distributions but which share some common latent structure. By incorporating architectural constraints and using a mutual information regularized form of the variational objective, our method successfully models differing data populations while explicitly encouraging the isolation of the shared and private latent factors. This enables our model to learn useful shared structure across similar tasks and to disentangle cross-population representations in a weakly supervised way. We demonstrate the utility of our method on several applications including image denoising, sub-group discovery, and continual learning.", "keywords": ["variational autoencoder", "latent variable model", "probabilistic graphical model", "machine learning", "deep learning", "continual learning"], "paperhash": "davison|isolating_latent_structure_with_crosspopulation_variational_autoencoders", "original_pdf": "/attachment/249a60ef9ed36783cbbfbb9bbfd4205ad5250c9f.pdf", "_bibtex": "@misc{\ndavison2020isolating,\ntitle={Isolating Latent Structure with Cross-population Variational Autoencoders},\nauthor={Joe Davison and Kristen A. Severson and Soumya Ghosh},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eWdlBFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eWdlBFwS", "replyto": "r1eWdlBFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575582878047, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2387/Reviewers"], "noninvitees": [], "tcdate": 1570237723544, "tmdate": 1575582878062, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2387/-/Official_Review"}}}], "count": 5}