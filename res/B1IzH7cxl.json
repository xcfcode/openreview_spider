{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396422375, "tcdate": 1486396422375, "number": 1, "id": "B1R-3GIdl", "invitation": "ICLR.cc/2017/conference/-/paper192/acceptance", "forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper presents an interesting application of variational methods for time series, in particular VRNN-like approaches, for stochastic volatility. Applications such as these are clearly in the scope of the conference, which was a point that one of the reviewer brought up. That said, questions of context, especially with regards to relation to different approaches, especially smoothing are relevant. Also, the number of methods GRACH and stochastic volatility is immense and this makes assessing the impact of this very hard to do and is more is needed to address this point. This paper is certainly interesting, but given these concerns, the paper is not yet rady for acceptance at the conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396422884, "id": "ICLR.cc/2017/conference/-/paper192/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396422884}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484331639340, "tcdate": 1478272270000, "number": 192, "id": "B1IzH7cxl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1IzH7cxl", "signatures": ["~Rui_Luo1"], "readers": ["everyone"], "content": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484331596227, "tcdate": 1484331596227, "number": 4, "id": "HJ4Lqc8Ue", "invitation": "ICLR.cc/2017/conference/-/paper192/public/comment", "forum": "B1IzH7cxl", "replyto": "Hy5E206El", "signatures": ["~Rui_Luo1"], "readers": ["everyone"], "writers": ["~Rui_Luo1"], "content": {"title": "Feedback from the authors and update of the paper", "comment": "Dear Reviewer,\n\nThank you very much for your detailed comment and advice.\n\nWe have modified the manuscript according to your suggestions:\n\n1. As we have known, for models that evolve explicitly in terms of the squares of the residuals (e^2), e.g. GARCH, the multi-step-ahead forecasts have closed-form solutions, which means that those forecasts can be efficiently computed in a recursive fashion due to the linear formulation of the model and the exploitation of relation E_{t-1}[e^2_t] = sigma^2_t.\n\nOn the other hand, for models that are not linear or do not explicitly evolve in terms of e^2, e.g. EGARCH (linear but not evolve in terms of e^2), our NSVM (nonlinear and not evolve in terms of e^2), the closed-form solutions are absent and thus the analytical forecast is not available. We will instead use simulation-based forecast, which uses random number generator to simulate draws from the predicted distribution and build up a pre-specified number of paths of the variances at 1 step ahead. The draws are then averaged to produce the forecast of the next step. For n-step-ahead forecast, it requires n iterations of 1-step-ahead forecast to get there.\n\nNSVM is designed as an end-to-end model for volatility estimation and forecast. It takes the price of stocks as input and outputs the distribution of the price at next step. It learns the dynamics using RNN, leading to an implicit, highly nonlinear formulation, where only simulation-based forecast is available. In order to obtain reasonably accurate forecasts, the number of draws should be relatively large, which will be very expensive for computation. Moreover, the number of draws will increase exponentially as the forecast horizon grows, so it will be infeasible to forecast several time steps ahead and that is why we focus on 1-step-ahead forecast.\n\n2. For GARCH volatility forecasts with longer horizon, i.e. multi-step-ahead forecasts, we usually exploit the relationship E_{t-1} [e^2_t] = sigma^2_t to substitute corresponding terms related to the absent observations and calculate the next sigma^2_{t+1} in a recursive fashion, which means that we use the predicted variances of the former steps to calculate the predictions of the latter steps. \n\n3. We have exploit the R packages ``stochvol'' and ``fGarch'' to extend the experiment by including more baselines such as stochvol, ARCH, TARCH, APARCH, AGARCH, NAGARCH, IGARCH, IAVARCH, FIGARCH. The result shows that our model still has the best performance on NLL metric.\n\n4. We have added more details about the network specification as well as the data generating procedure in Sec 5.2 and 5.3 as required.\n\n5. We believe the reason of the ``puzzling'' drops in Fig 4(b) and (c) is that NSVM has captured the jumps and drops of the stock price using its nonlinear dynamics and modelled the sudden changes as part of the trend: the estimated trend ``mu'' goes very close to the real observed price even around the jumps and drops (see the upper figure of Fig 4(b) and (c) around step 1300 and 1600). The residual (i.e. difference between the real value of observation and the trend of prediction) therefore becomes quite small, which lead to a lower volatility estimation.\n\nOn the other hand, for the baselines, we adopt AR as the trend model, which is a relatively simple linear model compared with the nonlinear NSVM. AR would not capture the sudden changes and leave those spikes in the residual; GARCH then took the residuals as input for volatility modelling, resulting in the spikes in volatility estimation.\n\nOur NSVM outperforms GARCH(1,1) on 142 out of 162 stocks on the metric of NLL. In particular, NSVM obtains -2.609 and -1.939 on the stocks corresponding to Fig 4(b) and (c) respectively, both of which are better than the results of GARCH (0.109 and 0.207 lower on NLL).\n\nYour comment enlightened us to carry out more investigations about multistep forecast in the future.\n\nThank you very much for your time and valuable advice.\n\n\nBest regards,\nThe authors"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287692776, "id": "ICLR.cc/2017/conference/-/paper192/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1IzH7cxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper192/reviewers", "ICLR.cc/2017/conference/paper192/areachairs"], "cdate": 1485287692776}}}, {"tddate": null, "tmdate": 1482710065674, "tcdate": 1482710065674, "number": 3, "id": "Hy5E206El", "invitation": "ICLR.cc/2017/conference/-/paper192/official/review", "forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "signatures": ["ICLR.cc/2017/conference/paper192/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper192/AnonReviewer3"], "content": {"title": "A relevant contribution to time series modelling with flexible nonlinear models", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose a recurrent variational neural network approach to modelling volatility in financial time series. This model consists of an application of Chung et al.\u2019s (2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series. \n\nThe paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). \n\nThe paper\u2019s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. This appears to be a novel, if minor contribution. The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful.\n\nI have a few comments and reservations with the paper:\n1) Although not  mentioned explicitly, the authors\u2019 framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. It would be interesting to discuss how this model could be extended to forecast at longer horizons.\n \n2) In Section 4.4, there\u2019s a mention that a GARCH(1,1) is conditionally deterministic. This is true only when forecasting 1 time-step in the future. At longer horizons, the GARCH(1,1) volatility forecast is not deterministic. \n\n3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model. However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. It would be well advised to look into R packages such as `stochvol\u2019 and \u2018fGarch\u2019 to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature.\n\n4) In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4)\n\n5) In Section 5.3, more details should be given on the data generating process for the synthetic data experiments. \n\n6) Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600). This should be explained and discussed.\n\nAll in all, I think that the paper provides a nice contribution to the art of volatility modelling. In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482710066475, "id": "ICLR.cc/2017/conference/-/paper192/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper192/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper192/AnonReviewer4", "ICLR.cc/2017/conference/paper192/AnonReviewer2", "ICLR.cc/2017/conference/paper192/AnonReviewer3"], "reply": {"forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482710066475}}}, {"tddate": null, "tmdate": 1481902631307, "tcdate": 1481902631307, "number": 2, "id": "S1149YZNl", "invitation": "ICLR.cc/2017/conference/-/paper192/official/review", "forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "signatures": ["ICLR.cc/2017/conference/paper192/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper192/AnonReviewer2"], "content": {"title": "Lack of smoothing for learning and lack of novelty", "rating": "5: Marginally below acceptance threshold", "review": "\nThe authors propose a recurrent neural network approach for constructing a\nstochastic volatility model for financial time series. They introduce an\ninference network based on a recurrent neural network that computes the\napproximation to the posterior distribution for the latent variables given the\npast data. This variational approximation is used to maximize the marginal\nlikelihood in order to learn the parameters of the model. The proposed method\nis validated in experiments with synthetic and real-world time series, showing\nto outperform parametric GARCH models and a Gaussian process volatility model.\n\nQuality:\n\nThe method proposed seems technically correct, with the exception that in\nequation (19) the inference model is doing filtering and not smoothing, in the\nsense that the posterior for z_t' only depends on those other z_t and x_t\nvalues with t<t', but in the true posterior p(Z|X) each z_t depends on all the\nX. This means the proposed learning method is inefficient. The reference below\nshows how to perform smoothing too and the results in that paper show indeed that\nsmoothing produces better results for learning the model.\n\nSequential Neural Models with Stochastic Layers Fraccaro, Marco and S\\o nderby,\nS\\o ren Kaae and Paquet, Ulrich and Winther, Ole In NIPS 2016.\n\nIt is not clear if the method proposed in the above reference would perform better \njust because of using smoothing when learning the model parameters.\n\nClarity:\n\nThe paper is clearly written and easy to read.\n\nFor the results on real-world data, in Table 1, how is the NLL computed? Is the\naverage NLL across the 162 time series?\n\nOriginality:\n\nThe method proposed is not very original. Previous work has already used the\nvariational approach with the reparametrization trick to learn recurrent neural\nnetworks with stochastic units (see the reference above). It seems that the main\ncontribution of the authors is to apply this type of techniques to the problem\nof modeling financial time series.\n\nSignificance:\n\nThe results shown are significant, the method proposed by the authors\noutperforms previous approaches. However, there is a huge amount of techniques\navailable for modeling financial time-series. The number of GARCH variants is\nprobably close to hundreds, each one claiming to be better than the others.\nThis makes difficult to quantify how important the results are.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482710066475, "id": "ICLR.cc/2017/conference/-/paper192/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper192/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper192/AnonReviewer4", "ICLR.cc/2017/conference/paper192/AnonReviewer2", "ICLR.cc/2017/conference/paper192/AnonReviewer3"], "reply": {"forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482710066475}}}, {"tddate": null, "tmdate": 1481727569420, "tcdate": 1481727569414, "number": 1, "id": "rytIA0A7e", "invitation": "ICLR.cc/2017/conference/-/paper192/official/review", "forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "signatures": ["ICLR.cc/2017/conference/paper192/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper192/AnonReviewer4"], "content": {"title": "An interesting application of RNN generative model to financial data", "rating": "5: Marginally below acceptance threshold", "review": "Thank you for an interesting read.\n\nI found the application of VRNN type generative model to financial data very promising. But since I don't have enough background knowledge to judge whether the performance gap is significant or not, I wouldn't recommend acceptance at this stage. \n\nTo me, the biggest issue for this paper is that I'm not sure if the paper contains significant novelty. The RNN-VAE combination has been around for more than a year and this paper does not propose significant changes to it. Maybe this paper fits better to an application targeting conference, rather than ICLR. But I'm not exactly sure about ICLR's acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482710066475, "id": "ICLR.cc/2017/conference/-/paper192/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper192/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper192/AnonReviewer4", "ICLR.cc/2017/conference/paper192/AnonReviewer2", "ICLR.cc/2017/conference/paper192/AnonReviewer3"], "reply": {"forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482710066475}}}, {"tddate": null, "tmdate": 1481563466514, "tcdate": 1481563466507, "number": 3, "id": "HJGIpI3Qx", "invitation": "ICLR.cc/2017/conference/-/paper192/public/comment", "forum": "B1IzH7cxl", "replyto": "rkPV4kDmg", "signatures": ["~Rui_Luo1"], "readers": ["everyone"], "writers": ["~Rui_Luo1"], "content": {"title": "Answers to the questions", "comment": "Dear Reviewer,\n\nWe are very glad to have your comment, where you have mentioned 3 issues of concern: 1) heavy-tailed distributions fit better than Gaussian in financial modelling; 2) you think the limited size of dataset and the plain structure of financial time series prevents neural networks from learning features; 3) you have concerns about the evaluation procedure and are wondering how we are doing this.\n\nWe agree with you on the first issue that some studies show that heavy-tailed distributions have better fitness to financial time series. We choose Gaussian rather than some heavy-tailed distribution to model the residual for the sake of simplicity. This allows us to model volatility in a principled yet simple way. That said, the higher moments beyond the first and the second can be later incorporated in a neural network, but the discussion beyond the scope of the paper and we leave this for future work.\n\nAs for the second issue, we would not agree as much as the first one. Indeed, the size of dataset we exploited (daily market prices) is limited comparing with those in CV (eg ImageNet). However, we can still learn critical information via neural networks as long as we set an appropriate dimension of networks or introduce some proper prior knowledge. In fact, the neural networks are essentially nothing more than a family of parameterised function approximators with gradient-based optimisation methods. To extract useful information, we would look for the optimal size of parameters by adjusting the dimensions to match the size of dataset and imposing proper constraints that incorporate our prior knowledge. On the other hand, we would say that the structure of financial time series is not ``little'' at all, on the contrary, it can be extremely complicated, as a consequence of the immense interacting factors within financial market. The reason econometricians utilise linear models like GARCH is not that the relations among factors are linear, but that those linear models provide with a good approximation to the latent, probably non-linear dynamics, where the time complexity of exact inference is too high to afford or no closed-form solutions can be obtained. Our proposed model is trying to improve the accuracy (ie. provide better approximation) on limited dataset. In section 4.4, we discussed the link between our model and GARCH, proving that GARCH is merely a special case of our model. It can be seen as a non-linear generalisation of linear model. In the revision of the paper, we have added additional experiments on widely used GARCH and its variants as well as a recent Gaussian processes models (using sampling-based learning algorithm) as baselines, namely GARCH(1,1), EGARCH(1,1), GJR-GARCH(1,1,1) and GPVOL [2]. The result shows that our model has the best predictive performance.\n\nIn the third issue, you've mentioned the sliding window approach. Actually, we have been using a sliding window of size 20 (by time-delay embedding) upon observations in the nearest past to make a one-step prediction in the future. The window then slides one step forward and the predictive procedure repeats so forth, as you described. The thing is we will not retrain the model on the test set (with ~500 points), but just use it to make predictions. The parameters are trained on the training set and held fixed afterwards. Notably, the baselines are retrained at each time step and our model still outperforms among all models.\n\n\nWe would sincerely ask the reviewer to read the sections of formulation, evaluation and comparison one more time to get more technical details.\n\nThank you for your valuable time and advice.\n\n\nBest regards,\nThe authors\n\n\n[1] Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. \"Stochastic backpropagation and approximate inference in deep generative models.\" arXiv preprint arXiv:1401.4082 (2014).\n[2] Wu, Yue, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, and Zoubin Ghahramani. \"Gaussian process volatility model.\" Advances in Neural Information Processing Systems. 2014."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287692776, "id": "ICLR.cc/2017/conference/-/paper192/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1IzH7cxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper192/reviewers", "ICLR.cc/2017/conference/paper192/areachairs"], "cdate": 1485287692776}}}, {"tddate": null, "tmdate": 1481306486492, "tcdate": 1481306486487, "number": 2, "id": "r1AdZ_uQl", "invitation": "ICLR.cc/2017/conference/-/paper192/public/comment", "forum": "B1IzH7cxl", "replyto": "rkxCjOszx", "signatures": ["~Rui_Luo1"], "readers": ["everyone"], "writers": ["~Rui_Luo1"], "content": {"title": "Additional experiment is added in the recent revision", "comment": "Dear Reviewer,\n\nWe have updated the paper with additional experiment on GPVOL, EGARCH, GJR-GARCH models.\nWe found out that the performance of EGARCH and GJR-GARCH are comparable with the standard GARCH while GPVOL performs poorly on synthetic dataset.\nThe GPVOL in our experiment is almost identical with that in the original paper (same parameter/hyperparameter, slightly modified procedure to fit our setting).\n\n\nBest regards,\nAuthors"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287692776, "id": "ICLR.cc/2017/conference/-/paper192/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1IzH7cxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper192/reviewers", "ICLR.cc/2017/conference/paper192/areachairs"], "cdate": 1485287692776}}}, {"tddate": null, "tmdate": 1481204804738, "tcdate": 1481204783132, "number": 2, "id": "rkPV4kDmg", "invitation": "ICLR.cc/2017/conference/-/paper192/pre-review/question", "forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "signatures": ["ICLR.cc/2017/conference/paper192/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper192/AnonReviewer2"], "content": {"title": "Lack of heavy tails in the 1 step-ahead predictive distribution of GARCH and evaluation approach", "question": "One well known limitation of GARCH is that its predictive distribution for the next time step is Gaussian. Empirically, it has been shown that when fitting this model to financial data, the residuals are still heavy-tailed. To solve this problem, there are many variants of GARCH that that include heavy-tailed distributions for the noise, instead of Gaussians. Stochastic volatility models have also heavy-tailed predictive distributions. The authors should compare with stronger baselines to really quantify the gains produced by their method with respect to previous work.\n\nIt seems that using recurrent neural networks in this setting is not very well justified. The amount of training data used is not very large (1500 data points) and also financial data has very little structure. Therefore, it is unlikely that the proposed neural networks are able to learn any feature representations that would justify them as an alternative to other modeling approaches such as Gaussian processes for example or other simpler models such as linear models (e.g. GARCH).\n\nThe evaluation process is also a bit strange. The authors evaluate their methods in a window with the last 500 data points. However, the most common approach for evaluating the predictive performance of models for financial time series is to use a sliding window approach in which the model is used to predict only the next point output of the training window. After this, the training window is moved one step forward, the model is retrained and the process repeats. By doing this, the model is always updated with the most up to date data. Why is that the authors do not use this evaluation approach?\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481204783670, "id": "ICLR.cc/2017/conference/-/paper192/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper192/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper192/AnonReviewer4", "ICLR.cc/2017/conference/paper192/AnonReviewer2"], "reply": {"forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481204783670}}}, {"tddate": null, "tmdate": 1481078566121, "tcdate": 1481078566114, "number": 1, "id": "HJAmDeBme", "invitation": "ICLR.cc/2017/conference/-/paper192/public/comment", "forum": "B1IzH7cxl", "replyto": "rkxCjOszx", "signatures": ["~Rui_Luo1"], "readers": ["everyone"], "writers": ["~Rui_Luo1"], "content": {"title": "Additional experiment and comparison will be presented shortly.", "comment": "Dear Reviewer,\n\nWe are very glad to read your message, it is indeed helpful for this topic!\nAs it is a completely different approach -- Gaussian processes other than neural networks -- to volatility modelling, we have been working on the additional experiments in the past few days using the recently proposed method GPVOL which you've mentioned. We have already got some promising result and hence have confidence in our newly proposed NSVM. Once comprehensive experiment is done, we will provide an updated version of the paper with additional experiment result and comparison.\n\nThank you very much for your time and valuable advice.\n\n\nBest regards,\nAuthors"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287692776, "id": "ICLR.cc/2017/conference/-/paper192/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1IzH7cxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper192/reviewers", "ICLR.cc/2017/conference/paper192/areachairs"], "cdate": 1485287692776}}}, {"tddate": null, "tmdate": 1480457160378, "tcdate": 1480457160374, "number": 1, "id": "rkxCjOszx", "invitation": "ICLR.cc/2017/conference/-/paper192/pre-review/question", "forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "signatures": ["ICLR.cc/2017/conference/paper192/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper192/AnonReviewer4"], "content": {"title": "comparison to recent methods", "question": "Thank you for an interesting read.\n\nThough I'm not very familiar with machine learning applications in Finance, I do notice that the GARCH method you were comparing with has been published around 30 years. Have you ever considered comparing with some recent machine learning methods for volatility, e.g. see the following paper which tested a Gaussian process?\n\nhttps://papers.nips.cc/paper/5376-gaussian-process-volatility-model.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Neural Stochastic Volatility Model", "abstract": "In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones.\nOur focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.", "pdf": "/pdf/04c83a73829a02e59e4813d7e5376a918d1d724f.pdf", "TL;DR": "A novel integration of statistical models with recurrent neural networks providing a new way of formulating volatility models.", "paperhash": "luo|a_neural_stochastic_volatility_model", "keywords": ["Deep learning", "Supervised Learning"], "conflicts": ["cs.ucl.ac.uk", "apex.sjtu.edu.cn"], "authors": ["Rui Luo", "Xiaojun Xu", "Weinan Zhang", "Jun Wang"], "authorids": ["r.luo@cs.ucl.ac.uk", "xuxj@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "j.wang@cs.ucl.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481204783670, "id": "ICLR.cc/2017/conference/-/paper192/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper192/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper192/AnonReviewer4", "ICLR.cc/2017/conference/paper192/AnonReviewer2"], "reply": {"forum": "B1IzH7cxl", "replyto": "B1IzH7cxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper192/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481204783670}}}], "count": 11}