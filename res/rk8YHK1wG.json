{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124427756, "tcdate": 1518470525734, "number": 281, "cdate": 1518470525734, "id": "rk8YHK1wG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rk8YHK1wG", "signatures": ["~Tharun_Medini1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "A4C: Anticipatory Asynchronous Advantage Actor-Critic", "abstract": "We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.  This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. We propose a method that squeezes more gradients from the same number of episodes and thereby achieves higher scores and converges faster. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on popular Atari Games.", "paperhash": "medini|a4c_anticipatory_asynchronous_advantage_actorcritic", "keywords": ["Reinforcement Learning", "A3C", "Actor Critic"], "_bibtex": "@misc{\n  medini2018a4c:,\n  title={A4C: Anticipatory Asynchronous Advantage Actor-Critic},\n  author={Tharun Medini and Xun Luan and Anshumali Shrivastava},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8YHK1wG}\n}", "authorids": ["trm3@rice.edu", "xun.luan@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Xun Luan", "Anshumali Shrivastava"], "TL;DR": "We propose to augment action space in A3C algorithm and extracting more gradients from episodes played, thereby achieving higher scores and faster convergence than the state-of-the-art GA3C.", "pdf": "/pdf/b62a7518282158295340b7943db9e511c76676f1.pdf"}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582969713, "tcdate": 1520005375981, "number": 1, "cdate": 1520005375981, "id": "SyO-bxPdz", "invitation": "ICLR.cc/2018/Workshop/-/Paper281/Official_Review", "forum": "rk8YHK1wG", "replyto": "rk8YHK1wG", "signatures": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer2"], "content": {"title": "A potentially promising RL approach, worth investigating further", "rating": "6: Marginally above acceptance threshold", "review": "This paper suggests to extend the action set in A3C with *sequences* of actions, so as to make the model predict the effect of up to k consecutive actions. It is found (on 4 Atari games) that using multiple gradient updates (one for each subsequence of size  <= k in the trajectory) saturates quickly, and better results are obtained by switching during training to a single update associated to the sequence actually  chosen by the agent.\n\nThis seems to me like an idea worth investigating, which is why I\u2019m voting for its acceptance. However this is clearly extremely preliminary work and I feel that a lot more needs to be done to better understand its potential and limitations. Including for instance:\n- More runs on more Atari games (3 runs on 4 games is clearly not enough)\n- Application to DQN (note that Q-learning equations are given but A3C is based on a V critic, not a Q one)\n- A much more in-depth investigation of why DU works so poorly (actually worse than IU even at the start on 3/4 games) and why switching to IU makes it improve that much that fast\n- A better switching strategy than manual observation \n- Trying to take advantage of the structure of sequences of actions, instead of considering them as totally independent: for instance we should have Q(s, ab) <= Q(s, a). Also it seems to me that to behave optimally the agent should only use basic actions (even if predicting the outcome of sequences of actions may be beneficial for training purpose)\n- Application to longer sequences than k=2 (may require a different approach to scale)\n- Links / comparisons with hierarchical RL techniques\n\nI would also like to ask if in IU the \u00ab sum of rewards \u00bb is actually the \u00ab sum of discounted rewards \u00bb? It should be, otherwise longer sequences would benefit from the lack of discount, at the expense of shorter ones.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A4C: Anticipatory Asynchronous Advantage Actor-Critic", "abstract": "We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.  This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. We propose a method that squeezes more gradients from the same number of episodes and thereby achieves higher scores and converges faster. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on popular Atari Games.", "paperhash": "medini|a4c_anticipatory_asynchronous_advantage_actorcritic", "keywords": ["Reinforcement Learning", "A3C", "Actor Critic"], "_bibtex": "@misc{\n  medini2018a4c:,\n  title={A4C: Anticipatory Asynchronous Advantage Actor-Critic},\n  author={Tharun Medini and Xun Luan and Anshumali Shrivastava},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8YHK1wG}\n}", "authorids": ["trm3@rice.edu", "xun.luan@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Xun Luan", "Anshumali Shrivastava"], "TL;DR": "We propose to augment action space in A3C algorithm and extracting more gradients from episodes played, thereby achieving higher scores and faster convergence than the state-of-the-art GA3C.", "pdf": "/pdf/b62a7518282158295340b7943db9e511c76676f1.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582969482, "id": "ICLR.cc/2018/Workshop/-/Paper281/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper281/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper281/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper281/AnonReviewer1"], "reply": {"forum": "rk8YHK1wG", "replyto": "rk8YHK1wG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper281/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper281/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582969482}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582930799, "tcdate": 1520328952917, "number": 2, "cdate": 1520328952917, "id": "rJ---k2_G", "invitation": "ICLR.cc/2018/Workshop/-/Paper281/Official_Review", "forum": "rk8YHK1wG", "replyto": "rk8YHK1wG", "signatures": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer3"], "content": {"title": "I liked the paper but it lacks of a proper bibliography", "rating": "7: Good paper, accept", "review": "This article study the idea of forcing an RL policy to predict/anticipate a sequence of action instead of a single action at each time step. According to the authors the practical benefit of this approach is to speedup learning by getting more gradient from the same number of training episodes.\nBy applying this multi-step forecasting idea to \"Asynchronous Advantage Actor Critic\" (A3C), the authors obtain a new algorithm called A4C (the new A comes from \"Anticipatory\"). Three updating strategies are tested: dependent (some would say \"recursive\") updating, independent updating and a combination of both.\nThe experiments on four Atari games corroborates the assumptions that A4C is improving from (G)A3C.\nAs expected this all-possible-options planning does not scale when the action space is too large.\n\nIt was a pleasure to read this (short/workshop/experimental) paper but it definitely calls for a deeper bibliographic work: for instance this notion of \"Anticipatory\" actions planning was already extensively studied under the name of \"options planning\" (See \"Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning\" by Sutton et al.). In the forecasting literature the key words are \"multi-step forecast\".\n\nMinor remark: the experiments on Atari games are great, but a few well-chosen experiments on simple MDPs could provide some insights on this anticipatory/options behavior.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A4C: Anticipatory Asynchronous Advantage Actor-Critic", "abstract": "We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.  This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. We propose a method that squeezes more gradients from the same number of episodes and thereby achieves higher scores and converges faster. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on popular Atari Games.", "paperhash": "medini|a4c_anticipatory_asynchronous_advantage_actorcritic", "keywords": ["Reinforcement Learning", "A3C", "Actor Critic"], "_bibtex": "@misc{\n  medini2018a4c:,\n  title={A4C: Anticipatory Asynchronous Advantage Actor-Critic},\n  author={Tharun Medini and Xun Luan and Anshumali Shrivastava},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8YHK1wG}\n}", "authorids": ["trm3@rice.edu", "xun.luan@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Xun Luan", "Anshumali Shrivastava"], "TL;DR": "We propose to augment action space in A3C algorithm and extracting more gradients from episodes played, thereby achieving higher scores and faster convergence than the state-of-the-art GA3C.", "pdf": "/pdf/b62a7518282158295340b7943db9e511c76676f1.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582969482, "id": "ICLR.cc/2018/Workshop/-/Paper281/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper281/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper281/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper281/AnonReviewer1"], "reply": {"forum": "rk8YHK1wG", "replyto": "rk8YHK1wG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper281/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper281/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582969482}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582800391, "tcdate": 1520623915448, "number": 3, "cdate": 1520623915448, "id": "B174ZvlKG", "invitation": "ICLR.cc/2018/Workshop/-/Paper281/Official_Review", "forum": "rk8YHK1wG", "replyto": "rk8YHK1wG", "signatures": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer1"], "content": {"title": "A mix of n-step returns and options ends in unclear experimental results.", "rating": "3: Clear rejection", "review": "This paper seems to present a version of n-step returns along with predefined options that are simple series of base actions.  There is no mention of n-step returns, options, or the bias-variance tradeoff of using n-step returns vs. 1-step returns.  Additionally, the authors claim to have better performance than A3C, but chose to switch between an n-step return and a 1-step return at a specific point in training, that is chosen once a training run has already been performed in entirety against a certain environment.  This means that to train a policy using the proposed algorithm, one needs to perform one full (or almost full) run, subjectively choose a switching point, and then re-run training form scratch.  Given this fact, and that the authors don't position any of their work relative to options & n-step returns, I don't believe this paper is sufficiently thorough to warrant presenting at the ICLR 2018 workshop track.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A4C: Anticipatory Asynchronous Advantage Actor-Critic", "abstract": "We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.  This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. We propose a method that squeezes more gradients from the same number of episodes and thereby achieves higher scores and converges faster. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on popular Atari Games.", "paperhash": "medini|a4c_anticipatory_asynchronous_advantage_actorcritic", "keywords": ["Reinforcement Learning", "A3C", "Actor Critic"], "_bibtex": "@misc{\n  medini2018a4c:,\n  title={A4C: Anticipatory Asynchronous Advantage Actor-Critic},\n  author={Tharun Medini and Xun Luan and Anshumali Shrivastava},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8YHK1wG}\n}", "authorids": ["trm3@rice.edu", "xun.luan@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Xun Luan", "Anshumali Shrivastava"], "TL;DR": "We propose to augment action space in A3C algorithm and extracting more gradients from episodes played, thereby achieving higher scores and faster convergence than the state-of-the-art GA3C.", "pdf": "/pdf/b62a7518282158295340b7943db9e511c76676f1.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582969482, "id": "ICLR.cc/2018/Workshop/-/Paper281/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper281/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper281/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper281/AnonReviewer1"], "reply": {"forum": "rk8YHK1wG", "replyto": "rk8YHK1wG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper281/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper281/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582969482}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573577726, "tcdate": 1521573577726, "number": 147, "cdate": 1521573577385, "id": "BJGR0RAFz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rk8YHK1wG", "replyto": "rk8YHK1wG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A4C: Anticipatory Asynchronous Advantage Actor-Critic", "abstract": "We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.  This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. We propose a method that squeezes more gradients from the same number of episodes and thereby achieves higher scores and converges faster. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on popular Atari Games.", "paperhash": "medini|a4c_anticipatory_asynchronous_advantage_actorcritic", "keywords": ["Reinforcement Learning", "A3C", "Actor Critic"], "_bibtex": "@misc{\n  medini2018a4c:,\n  title={A4C: Anticipatory Asynchronous Advantage Actor-Critic},\n  author={Tharun Medini and Xun Luan and Anshumali Shrivastava},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8YHK1wG}\n}", "authorids": ["trm3@rice.edu", "xun.luan@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Xun Luan", "Anshumali Shrivastava"], "TL;DR": "We propose to augment action space in A3C algorithm and extracting more gradients from episodes played, thereby achieving higher scores and faster convergence than the state-of-the-art GA3C.", "pdf": "/pdf/b62a7518282158295340b7943db9e511c76676f1.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1520624179274, "tcdate": 1520624179274, "number": 1, "cdate": 1520624179274, "id": "HJjEGwetz", "invitation": "ICLR.cc/2018/Workshop/-/Paper281/Official_Comment", "forum": "rk8YHK1wG", "replyto": "B174ZvlKG", "signatures": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper281/AnonReviewer1"], "content": {"title": "Followup", "comment": "If you would like to continue advancing in this direction, please have a look at chapters 7, 12 & 17.2 in Rich Sutton's 2nd edition RL book: http://incompleteideas.net/book/bookdraft2018feb28.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A4C: Anticipatory Asynchronous Advantage Actor-Critic", "abstract": "We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.  This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. We propose a method that squeezes more gradients from the same number of episodes and thereby achieves higher scores and converges faster. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on popular Atari Games.", "paperhash": "medini|a4c_anticipatory_asynchronous_advantage_actorcritic", "keywords": ["Reinforcement Learning", "A3C", "Actor Critic"], "_bibtex": "@misc{\n  medini2018a4c:,\n  title={A4C: Anticipatory Asynchronous Advantage Actor-Critic},\n  author={Tharun Medini and Xun Luan and Anshumali Shrivastava},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8YHK1wG}\n}", "authorids": ["trm3@rice.edu", "xun.luan@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Xun Luan", "Anshumali Shrivastava"], "TL;DR": "We propose to augment action space in A3C algorithm and extracting more gradients from episodes played, thereby achieving higher scores and faster convergence than the state-of-the-art GA3C.", "pdf": "/pdf/b62a7518282158295340b7943db9e511c76676f1.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222446322, "id": "ICLR.cc/2018/Workshop/-/Paper281/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rk8YHK1wG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper281/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper281/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper281/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper281/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper281/Reviewers", "ICLR.cc/2018/Workshop/Paper281/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222446322}}}, {"tddate": null, "ddate": null, "tmdate": 1518975685412, "tcdate": 1518975544770, "number": 2, "cdate": 1518975544770, "id": "HybScNDwM", "invitation": "ICLR.cc/2018/Workshop/-/Paper281/Public_Comment", "forum": "rk8YHK1wG", "replyto": "BJiWHiHPz", "signatures": ["~Tharun_Medini1"], "readers": ["everyone"], "writers": ["~Tharun_Medini1"], "content": {"title": "I've sent the updated file", "comment": "I've emailed the fixed paper with the subject 'A4C - Corrected version of our workshop paper' to iclr2018.programchairs@gmail.com. Thank you for the consideration."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A4C: Anticipatory Asynchronous Advantage Actor-Critic", "abstract": "We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.  This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. We propose a method that squeezes more gradients from the same number of episodes and thereby achieves higher scores and converges faster. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on popular Atari Games.", "paperhash": "medini|a4c_anticipatory_asynchronous_advantage_actorcritic", "keywords": ["Reinforcement Learning", "A3C", "Actor Critic"], "_bibtex": "@misc{\n  medini2018a4c:,\n  title={A4C: Anticipatory Asynchronous Advantage Actor-Critic},\n  author={Tharun Medini and Xun Luan and Anshumali Shrivastava},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8YHK1wG}\n}", "authorids": ["trm3@rice.edu", "xun.luan@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Xun Luan", "Anshumali Shrivastava"], "TL;DR": "We propose to augment action space in A3C algorithm and extracting more gradients from episodes played, thereby achieving higher scores and faster convergence than the state-of-the-art GA3C.", "pdf": "/pdf/b62a7518282158295340b7943db9e511c76676f1.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712623659, "id": "ICLR.cc/2018/Workshop/-/Paper281/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper281/Reviewers"], "reply": {"replyto": null, "forum": "rk8YHK1wG", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712623659}}}, {"tddate": null, "ddate": null, "tmdate": 1518871811095, "tcdate": 1518871811095, "number": 1, "cdate": 1518871811095, "id": "BJiWHiHPz", "invitation": "ICLR.cc/2018/Workshop/-/Paper281/Public_Comment", "forum": "rk8YHK1wG", "replyto": "rk8YHK1wG", "signatures": ["~Oriol_Vinyals1"], "readers": ["everyone"], "writers": ["~Oriol_Vinyals1"], "content": {"title": "Please Fix Length", "comment": "Your paper violates by a few lines the 3 page limit (see https://iclr.cc/Conferences/2018/CallForWorkshops). Please send us a fixed version of your PDF at iclr2018.programchairs@gmail.com by the end of Monday, February 19th, or else we will reject your paper.\n\nThanks,\nICLR2018 Program Chairs"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A4C: Anticipatory Asynchronous Advantage Actor-Critic", "abstract": "We propose to extend existing deep reinforcement learning  (Deep RL) algorithms by allowing them to additionally choose sequences of actions as a part of their policy.  This modification forces the network to anticipate the reward of action sequences, which, as we show, improves the exploration leading to better convergence. We propose a method that squeezes more gradients from the same number of episodes and thereby achieves higher scores and converges faster. Our proposal is simple, flexible, and can be easily incorporated into any Deep RL framework. We show the power of our scheme by consistently outperforming the state-of-the-art GA3C algorithm on popular Atari Games.", "paperhash": "medini|a4c_anticipatory_asynchronous_advantage_actorcritic", "keywords": ["Reinforcement Learning", "A3C", "Actor Critic"], "_bibtex": "@misc{\n  medini2018a4c:,\n  title={A4C: Anticipatory Asynchronous Advantage Actor-Critic},\n  author={Tharun Medini and Xun Luan and Anshumali Shrivastava},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8YHK1wG}\n}", "authorids": ["trm3@rice.edu", "xun.luan@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Xun Luan", "Anshumali Shrivastava"], "TL;DR": "We propose to augment action space in A3C algorithm and extracting more gradients from episodes played, thereby achieving higher scores and faster convergence than the state-of-the-art GA3C.", "pdf": "/pdf/b62a7518282158295340b7943db9e511c76676f1.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712623659, "id": "ICLR.cc/2018/Workshop/-/Paper281/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper281/Reviewers"], "reply": {"replyto": null, "forum": "rk8YHK1wG", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712623659}}}], "count": 8}