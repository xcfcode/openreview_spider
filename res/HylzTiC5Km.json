{"notes": [{"id": "HylzTiC5Km", "original": "SkeQga3cKX", "number": 780, "cdate": 1538087865657, "ddate": null, "tcdate": 1538087865657, "tmdate": 1550798427190, "tddate": null, "forum": "HylzTiC5Km", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1l8yJLoyV", "original": null, "number": 1, "cdate": 1544408797880, "ddate": null, "tcdate": 1544408797880, "tmdate": 1545354520359, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Meta_Review", "content": {"metareview": "All reviewers recommend acceptance, with two reviewers in agreement that the results represent a significant advance for autoregressive generative models. The AC concurs.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Oral)", "title": "metareview: significant progress on autoregressive models for image generation"}, "signatures": ["ICLR.cc/2019/Conference/Paper780/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper780/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353090456, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353090456}}}, {"id": "Bkx8AurWeN", "original": null, "number": 4, "cdate": 1544800461896, "ddate": null, "tcdate": 1544800461896, "tmdate": 1544800461896, "tddate": null, "forum": "HylzTiC5Km", "replyto": "Bke6VkQAY7", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Public_Comment", "content": {"comment": "https://arxiv.org/abs/1109.4389 seems to be another relevant reference for AR models using multiple scales", "title": "Related work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311754424, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylzTiC5Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311754424}}}, {"id": "rJgX0gv814", "original": null, "number": 3, "cdate": 1544085707436, "ddate": null, "tcdate": 1544085707436, "tmdate": 1544085851499, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Public_Comment", "content": {"comment": "Dear authors:\n\nThank you for your really interesting and impressive ideas. The idea is really amazing and experimental results are sound. Generating 256x256 imagenet images in the auto-regressive manner is really difficult and your paper gives a really solid solution.\n\nHowever, about this paper, I have a concern about the depth-upscaling part. In your experimental results, the bits/dim of SPN and SPN+ depth-upscaling is of no difference for most datasets, and sometiems the SPN+depth-upscaling even performs poorly compared to simply SPN. However, with the depth-upscaling, the sampling time is doubled: every dimension should be sampled twice compared SPN. Can you give more explanations on the benefits of depth-upscaling? Do we really need it given the really impressive results of SPN?\n\nAnyway, this is a really solid paper and congratulations.", "title": "A concern about depth-upscaling"}, "signatures": ["~Kun_Xu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Kun_Xu1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311754424, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylzTiC5Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311754424}}}, {"id": "SJeStxgqRm", "original": null, "number": 12, "cdate": 1543270524957, "ddate": null, "tcdate": 1543270524957, "tmdate": 1543270524957, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "New revision is now uploaded", "comment": "To our reviewers,\n\nPlease find our latest revision uploaded. We believe it addresses most of the comments including:\n- Detailing the number of parameters for each architecture (Table 4)\n- Clarifying exactly how the slice embedder conditions the decoder\n- Clarifying depth-upscaling with SPN\n- Including information about the use of TPU (Appendix C)\n- Supplying details about the nature of temperature adjustments during sampling\n- Adding references\n- Various writing improvements\n\nWe are also currently running our setup for the 64x64 and 256x256 samples with the intent to include them shortly in our revision.\n\nWe kindly thank our reviewers for their insightful comments which have substantially improved the exposition."}, "signatures": ["ICLR.cc/2019/Conference/Paper780/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "HJgrrcCO2X", "original": null, "number": 1, "cdate": 1541102140526, "ddate": null, "tcdate": 1541102140526, "tmdate": 1542821789731, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Review", "content": {"title": "A new version of PixelCNN-based model for HQ images", "review": "General:\nThe paper tackles a problem of learning long-range dependencies in images in order to obtain high fidelity images. The authors propose to use a specific architecture that utilizes three main components: (i) a decoder for sliced small images, (ii) a size-upscaling decoder for large image generation, (iii) a depth-upscaling decoder for generating high-res image. The main idea of the approach is slicing a high-res original image and a new factorization of the joint distribution over pixels. In this model various well-known blocks are used like 1D Transformer and Gated PixelCNN. The obtained results are impressive, the generated images are large and contain realistic details.\n\nIn my opinion the paper would be interesting for the ICLR audience.\n\nPros:\n+ The paper is very technical but well-written.\n+ The obtained results constitute new state-of-the-art on HQ image datasets.\n+ Modeling long-range dependencies among pixels is definitely one of the most important topics in image modeling. The proposed approach is a very interesting step towards this direction.\n\nCons:\n- The authors claim that the proposed approach is more memory efficient than other methods. However, I wonder how many parameters the proposed approach requires comparing to others. It would be highly beneficial to have an additional column in Table 1 that would contain number of parameters for each model.\n- All samples are take either at an extremely high temperature (i.e., 0.99) or at the temperature equal 1. How do the samples look for smaller temperatures? Sampling at very high temperature is a nice trick for generating nicely looking images, however, it could hide typical problems of generative models (e.g., see Rezende & Viola, \u201cTaming VAEs\u201d, 2018).\n\n--REVISION--\nI would like to thank the authors for their response. I highly appreciate their clear explanation of both issues raised by me. I am especially thankful for the second point (about the temperature) because indeed I interpreted it as in the GLOW paper. Since both my concerns have been answered, I decided to raise the final score (+2).", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper780/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Review", "cdate": 1542234378828, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335800782, "tmdate": 1552335800782, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1gOyezcaX", "original": null, "number": 7, "cdate": 1542229984069, "ddate": null, "tcdate": 1542229984069, "tmdate": 1542244124263, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HJgrrcCO2X", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "reply to AnonReviewer3", "comment": "Thank you for your comments. \n\n--\n- The authors claim that the proposed approach is more memory efficient than other methods. However, I wonder how many parameters the proposed approach requires comparing to others. It would be highly beneficial to have an additional column in Table 1 that would contain number of parameters for each model.\n--\n\nAs discussed with AnonReviewer2, we will include a table with the number of parameters for each model. Briefly, the models in the paper have between ~50M params and ~650M params in the most extreme case of full multidimensional upscaling on ImageNet 128.\n\nIn the case of 256x256 CelebA-HQ, we use a total of ~100M parameters to produce the depth-upscaled 8bit samples in Figure 5 and ~50M parameters to produce the 5bit samples in Figure 7. Compare this to Glow [1], whose blog post [2] indicates that up to 200M parameters are used for 5bit Celeb-A. Thus we have a ~4x reduction in the number of parameters vs Glow, with decisively improved likelihoods (see Table 3); I think this should address your concern about parameter-efficiency. We also note that autoregressive (and other) models are highly compressible at little to no loss (see e.g. [3]), which makes the absolute number of parameters only an initial, rough measure of parameter efficiency.\n\n--\n- All samples are take either at an extremely high temperature (i.e., 0.99) or at the temperature equal 1. How do the samples look for smaller temperatures? Sampling at very high temperature is a nice trick for generating nicely looking images, however, it could hide typical problems of generative models (e.g., see Rezende & Viola, \u201cTaming VAEs\u201d, 2018).\n--\n\nI believe there is a misunderstanding here. What we call temperature is a division on the logits of the softmax output distribution. Temperature 1.0 in our case means that the distribution of the trained model is used exactly as predicted by the model, with no adjustments or tweaks during sampling time.  *Reducing* the temperature (less than 1.0) is what can hide problems, because it artificially reduces the entropy in the distribution parameterized by the model during sampling time. \n\nAs we sample at temperatures of 0.95, 0.99, and 1.0 in the paper, we respectively *slightly*, *barely*, and *do-not-at-all* reduce the entropy in the model's distribution. Thus this concern does not apply and we are actually being comparatively transparent about our model\u2019s samples (note that Glow shows its best samples at temperature 0.7, but that \u201ctemperature\u201d has a different operational meaning in that case).\n\n[1] - Kingma et al. https://arxiv.org/abs/1807.03039\n[2] - https://blog.openai.com/glow/\n[3] - Kalchbrenner et al. https://arxiv.org/abs/1802.08435"}, "signatures": ["ICLR.cc/2019/Conference/Paper780/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "H1llqKX56m", "original": null, "number": 9, "cdate": 1542236551959, "ddate": null, "tcdate": 1542236551959, "tmdate": 1542236551959, "tddate": null, "forum": "HylzTiC5Km", "replyto": "r1xv8KGcp7", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "thanks for the clarification!", "comment": "Thanks for the clarification. Agreed a separate release isn't needed. It wasn't clear if you were using the same split as Reed et al doesn't talk about it. "}, "signatures": ["ICLR.cc/2019/Conference/Paper780/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "r1xv8KGcp7", "original": null, "number": 8, "cdate": 1542232399149, "ddate": null, "tcdate": 1542232399149, "tmdate": 1542232399149, "tddate": null, "forum": "HylzTiC5Km", "replyto": "rkeURDYKpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "re: More details on the 128x128 and 256x256 Imagenet benchmarks ", "comment": "Thanks for your thoroughness, AnonReviewer2.\n\nThe ImageNet dataset we use for 128x128 and 256x256 generation is the standard ILSVRC [1] benchmark used by classification models. We report final numbers on the official validation set consisting of 50k examples. We hold out 10k examples from the official training set for cross-validation, and train on the remaining 1271167 points.\n\nI don\u2019t believe a separate release is necessary here as the data is freely available [2] and our downsampling scheme is easily reproducible (simply tf.resize_area). \n\nWe can be more explicit about this split in the experiment section.\n\n[1] - Russakovsky et al. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.\n[2] - http://www.image-net.org/challenges/LSVRC/2014/"}, "signatures": ["ICLR.cc/2019/Conference/Paper780/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "H1lNzuZ9a7", "original": null, "number": 6, "cdate": 1542227979628, "ddate": null, "tcdate": 1542227979628, "tmdate": 1542228010113, "tddate": null, "forum": "HylzTiC5Km", "replyto": "S1gjDP-927", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "Reply to AnonReviewer2 (2/2)", "comment": "--\n4. Can you clarify how you condition the self-attention + Gated PixelCNN block on the previous slice embedding you get out of the above convnet? There are two embeddings passed in if I understand correctly: (1) All previous slices, (2) Tiled meta-position of current slice.  It is not clear to me how the conditioning is done for the transformer pixelcnn on this auxiliary embedding. The way you condition matters a lot for good performance, so it would be helpful for people to replicate your results if you provide all details. \n--\n\nThe output of the slice embedder -- that receives as input all previous slices and the tiled meta-position of the current slice -- is concatenated channel-wise with the 2D-reshaped output of the masked 1D transformer (which in turn receives as input only the current target slice). The resulting concatenated tensor conditions the PixelCNN decoder like $s$ in equation (5) of the Conditional PixelCNN paper [2]. I.e. the tensor $s$ maps, via 1x1 convolutions, to units which bias the masked convolution output for each layer in PixelCNN. The number of hidden units in this pathway is what is referred to as \"decoder residual channels\" in Appendix B. We will add this description to Section 3.2\n\n--\n5. I also don't understand the depth upscaling architecture completely. Could you provide a diagram clarifying how the conditioning is done there given that you have access to all pixels' salient bits now and not just meta-positions prior to this slice? \n--\n\nThe SPN which models the low-bit-depth image is identical to the exposition in section 3.2, except that the data it operates on has only 3 bits of depth. As we mention in section 3.4, the depth-upscaling SPN achieves its conditioning by concatenating (again channelwise) the full low-bit-depth image, organised into its constituent slices, to the rest of the slice-embedder's inputs. So no matter which target slice is being modelled (for the finest 5 bits), all slices of the 3bit data can be seen by the slice embedder when it produces context for the fine bits of a target slice. We will further clarify it and see how to add a diagram for this.\n\n--\n6. It is really cool that you don't lose out in bits/dim after depth upscaling that much. If you take Grayscale PixelCNN (pointed out in the anonymous comment), the bits/dim isn't as good as PixelCNN though samples are more structured. There is 0.04 b.p.d  difference in 256x256, but no difference in 128x128. Would be nice to explain this when you add the citation.\n--\n\nThanks for the observation, we will note this. The ordering in the SPN, considering both subscaling and upscaling, is indeed quite different from the vanilla ordering and it\u2019s nice to see that the NLL values are negligibly affected.\n\n--\n7. The architecture in the Appendix can be improved. It is hard to understand the notations. What are residual channels, attention channels, attention ffn layer, \"parameter attention\", conv channels? \n--\n\nThanks for bringing this to our attention. We will add the figures/explanations discussed and reference this hyperparameter table so that it's all clear. The attention parameters listed are configurable hyperparameters of the open source Transformer implementation in tensor2tensor [3] on github.\n\n\n[1] - Kalchbrenner et al. https://arxiv.org/abs/1802.08435\n[2] - Oord et al. https://arxiv.org/abs/1606.05328 \n[3] - https://github.com/tensorflow/tensor2tensor\n\nAnd we'll fix that typo too."}, "signatures": ["ICLR.cc/2019/Conference/Paper780/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "Bkgl4P-cpm", "original": null, "number": 5, "cdate": 1542227752180, "ddate": null, "tcdate": 1542227752180, "tmdate": 1542228000512, "tddate": null, "forum": "HylzTiC5Km", "replyto": "S1gjDP-927", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "reply to AnonReviewer2 (1/2)", "comment": "Thanks for your thorough review. Addressing your comments will improve the paper.\n\n--\n-1. Can you point out the total number of parameters in the models?\n--\n\nDepending on the dataset, each SPN network has between ~50M (CelebA) and ~250M parameters (ImageNet 128/256). ImageNet64 uses ~150M weights. With depth upscaling, two separate SPNs with non-shared weights model P(3bit) and P(rest given 3bit) respectively, doubling the number of parameters. With explicit size upscaling for ImageNet128, there is a third network (decoder-only) with ~150M parameters which generates the first 3 bits of the first slice. So the maximal number of parameters used to generate a sample in the paper is full multidimensional upscaling on ImageNet 128, where the total parameter count reaches ~650M. We will include the number of parameters for each model in the table, as requested.\n\n--\n-1. Also would be good to know what hardware accelerators were used. The batch sizes mentioned in the Appendix (2048 for 256x256 Imagenet) are too big and needs TPUs? If TPU pods, which version (how many cores)?\n--\n\nTo reach batch size 2048 we used 256 TPUv3 cores. We will clarify this in the paper.\n\n--\n0. I would really like to know the sampling times. The model still generates the image pixel by pixel. Would be good to have a number for future papers to reference this.\n--\n \nOur current implementation performs only naive sampling, where the outputs of the decoder are recomputed for all positions in a slice to generate each sample. This is convenient, but time-consuming and allows to only rarely inspect the samples coming from our model. The techniques for speeding-up AR inference - such as caching of states, low-level custom implementation, sparsification and multi-output generation [1] - are equally applicable to SPNs and would make sampling reasonably fast; on the order of a handful of seconds for a 256 x 256 x 3 image.\n\n--\n1. Any reason why 256x256 Imagenet samples are not included in the paper? Given that you did show 256x256 CelebA samples, sampling time can't be an issue for you to not show Imagenet 256x256. So, it would be nice to include them. I don't think any paper so far has shown good 256x256 unconditional samples. So showing this will make the paper even stronger.\n--\n\nThanks! We\u2019ll aim to adding 64 x 64 and 256 x 256 samples in our revision.\n\n--\n2. Until now I have seen no good 64x64 Imagenet samples from a density model. PixelRNN samples are funky (colorful but no global structure). So I am curious if this model can get that. It may be the case that it doesn't, given that subscale ordering didn't really help on 32x32.  It would be nice to see both 5-bit and 8-bit, and for 8-bit, both the versions: with and without depth upscaling.\n--\n\nThe 64x64 samples look much better with SPNs. We will aim at including some of the variants that you ask for in our revision.\n\n--\n3. I didn't quite understand the architecture in slice encoding (Sec 3.2).  Especially the part about using a residual block convnet to encode the previous slices with padding, and to preserve relative meta-position of the slices. The part I get is that you concatenate the 32x32 slices along the channel dimension, with padded slices. I also get that padding is necessary to have the same channel dimension for any intermediate slice. Not sure if I see the whole point of preserving ordering. Isn't it just normal padding -> space to depth in a structured block-wise fashion? \n--\n\nIt\u2019s like a meta-convolution: the relative ordering ensures that slices are embedded with weights that depend on the relative 2d distance to the slice that is being generated. Suppose we are predicting the target slice at meta-position (i,j), so that previous slices in the 2d ordering are presented to the slice embedder. For any previous slice (m,n), the weights applied to it are a function of the offset (i-m,j-n), as opposed to their absolute positions (m,n). We will add this clarification to the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper780/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "B1eI3IJ5pQ", "original": null, "number": 4, "cdate": 1542219437588, "ddate": null, "tcdate": 1542219437588, "tmdate": 1542219437588, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HkxN569T2X", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "reply to AnonReviewer1", "comment": "Thank you for the detailed feedback. In the next revision, we will make height, width, and channel indices in equation 1 explicit and make a thorough sweep over the rest of the equations to check for any other undefined parameters. \n\nWe will ensure that all figures are referenced, and in the correct order."}, "signatures": ["ICLR.cc/2019/Conference/Paper780/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "rkeURDYKpQ", "original": null, "number": 3, "cdate": 1542195150207, "ddate": null, "tcdate": 1542195150207, "tmdate": 1542195150207, "tddate": null, "forum": "HylzTiC5Km", "replyto": "S1gjDP-927", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "More details on the 128x128 and 256x256 Imagenet benchmarks", "comment": "Request the authors to provide details for the train/val split for Imagenet 128x128 and Imagenet 256x256 density estimation benchmarks. I wasn't able to find the details in Reed et al (https://arxiv.org/pdf/1703.03664.pdf). Would be ideal if the authors released the splits as done for 32x32 and 64x64 from PixelRNN in http://image-net.org/small/download.php to encourage and be useful for more people to push on this benchmark."}, "signatures": ["ICLR.cc/2019/Conference/Paper780/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "S1gjDP-927", "original": null, "number": 2, "cdate": 1541179235040, "ddate": null, "tcdate": 1541179235040, "tmdate": 1541555435168, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Review", "content": {"title": "Solid paper, excellent execution, very important advance in density modeling", "review": "Summary: \nThis paper addresses an important problem in density estimation which is to scale the generation to high fidelity images. Till now, there have been no good density modeling results on large images when taken into account large datasets like Imagenet (there have been encouraging results like with Glow, but on 5-bit color intensities and simpler datasets like CelebA). This paper is the first to successfully show convincing Imagenet samples with 128x128 resolution for a likelihood density model, which is hard even for a GAN (only one GAN paper (SAGAN) prior to this conference has managed to show unconditional 128x128 Imagenet samples). The ideas in this paper to pick an ordering scheme at subsampled slices uniformly interleaved in the image and condition slice generation in an autoregressive way is very likely to be adopted/adapted to more high fidelity density modeling like videos. Another important idea in this paper is to do depth upscaling, focusing on salient color intensity bits first (first 3 bits per color channel) before generating the remaining bits. The color intensity dependency structure is also neat: The non-salient bits per channel are conditioned on all previously generated color bits (for all spatial locations). Overall, I think this paper is a huge advance in density modeling, deserves an oral presentation and deserves as much credit as BigGAN, probably more, given that it is doing unconditional generation. \n\nDetails:\nMajor:\n-1. Can you point out the total number of parameters in the models? Also would be good to know what hardware accelerators were used. The batch sizes mentioned in the Appendix (2048 for 256x256 Imagenet) are too big and needs TPUs? If TPU pods, which version (how many cores)? If not, I am curious to know how many GPUs were used.\n0. I would really like to know the sampling times. The model still generates the image pixel by pixel. Would be good to have a number for future papers to reference this.\n1. Any reason why 256x256 Imagenet samples are not included in the paper? Given that you did show 256x256 CelebA samples, sampling time can't be an issue for you to not show Imagenet 256x256. So, it would be nice to include them. I don't think any paper so far has shown good 256x256 unconditional samples. So showing this will make the paper even stronger.\n2. Until now I have seen no good 64x64 Imagenet samples from a density model. PixelRNN samples are funky (colorful but no global structure). So I am curious if this model can get that. It may be the case that it doesn't, given that subscale ordering didn't really help on 32x32.  It would be nice to see both 5-bit and 8-bit, and for 8-bit, both the versions: with and without depth upscaling.\n3. I didn't quite understand the architecture in slice encoding (Sec 3.2).  Especially the part about using a residual block convnet to encode the previous slices with padding, and to preserve relative meta-position of the slices. The part I get is that you concatenate the 32x32 slices along the channel dimension, with padded slices. I also get that padding is necessary to have the same channel dimension for any intermediate slice. Not sure if I see the whole point of preserving ordering. Isn't it just normal padding -> space to depth in a structured block-wise fashion? \n4. Can you clarify how you condition the self-attention + Gated PixelCNN block on the previous slice embedding you get out of the above convnet? There are two embeddings passed in if I understand correctly: (1) All previous slices, (2) Tiled meta-position of current slice.  It is not clear to me how the conditioning is done for the transformer pixelcnn on this auxiliary embedding. The way you condition matters a lot for good performance, so it would be helpful for people to replicate your results if you provide all details. \n5. I also don't understand the depth upscaling architecture completely. Could you provide a diagram clarifying how the conditioning is done there given that you have access to all pixels' salient bits now and not just meta-positions prior to this slice? \n6. It is really cool that you don't lose out in bits/dim after depth upscaling that much. If you take Grayscale PixelCNN (pointed out in the anonymous comment), the bits/dim isn't as good as PixelCNN though samples are more structured. There is 0.04 b.p.d  difference in 256x256, but no difference in 128x128. Would be nice to explain this when you add the citation.\n7. The architecture in the Appendix can be improved. It is hard to understand the notations. What are residual channels, attention channels, attention ffn layer, \"parameter attention\", conv channels? \n\nMinor: \nTypo: unpredented --> unprecedented ", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper780/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Review", "cdate": 1542234378828, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335800782, "tmdate": 1552335800782, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkxN569T2X", "original": null, "number": 3, "cdate": 1541414283569, "ddate": null, "tcdate": 1541414283569, "tmdate": 1541533697093, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Review", "content": {"title": "Sound incremental advance", "review": "Authors propose a decoder arquitecture model named Subscale Pixel Network. It is meant to generate overall images as image slice sequences with memory and computation economy by using a Multidimensional Upscaling method.\nThe paper is fairly well written and structured, and it seems technically sound.\nExperiments are convincing.\nSome minor issues:\nFigure 2 is not referenced anywhere in the main text.\nFigure 5 is referenced in the main text after figure 6.\nEven if intuitively understandable, all parameters in equations should be explicitly described (e.g., h,w,H,W in eq.1)", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper780/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Review", "cdate": 1542234378828, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335800782, "tmdate": 1552335800782, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bke6VkQAY7", "original": null, "number": 1, "cdate": 1538301749004, "ddate": null, "tcdate": 1538301749004, "tmdate": 1538301749004, "tddate": null, "forum": "HylzTiC5Km", "replyto": "SJgqbZTaYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "content": {"title": "Thanks for the reference. Low resolution (small height/width) modeling goes back to Multi-Scale PixelRNN.", "comment": "Thanks for the reference - we will add the citation in the context of depth upscaling. Size upscaling in AR models goes back to at least the PixelRNN paper (van den Oord et al, 2016, see Multi-Scale section). \n\nSome differences: \n- Depth upscaling here is done by taking the most significant bits of each channel separately, as opposed to globally across the three channels as in Grayscale PixelCNN.\n- Multidimensional Upscaling used here combines both size and depth upscaling.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper780/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608644, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylzTiC5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper780/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper780/Authors|ICLR.cc/2019/Conference/Paper780/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608644}}}, {"id": "SJgqbZTaYQ", "original": null, "number": 1, "cdate": 1538277633936, "ddate": null, "tcdate": 1538277633936, "tmdate": 1538277633936, "tddate": null, "forum": "HylzTiC5Km", "replyto": "HylzTiC5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper780/Public_Comment", "content": {"comment": "https://arxiv.org/pdf/1612.08185 also proposed both low-resolution and sub-pixel color modelling.", "title": "Great results but very relevant work discussion missing?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper780/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING", "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark\nfor testing the performance of image decoders. Autoregressive image models\nhave been able to generate small images unconditionally, but the extension of\nthese methods to large images where fidelity can be more readily assessed has\nremained an open problem. Among the major challenges are the capacity to encode\nthe vast previous context and the sheer difficulty of learning a distribution that\npreserves both global semantic coherence and exactness of detail. To address the\nformer challenge, we propose the Subscale Pixel Network (SPN), a conditional\ndecoder architecture that generates an image as a sequence of image slices of equal\nsize. The SPN compactly captures image-wide spatial dependencies and requires a\nfraction of the memory and the computation. To address the latter challenge, we\npropose to use multidimensional upscaling to grow an image in both size and depth\nvia intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the\nunconditional generation of CelebAHQ of size 256 and of ImageNet from size 32\nto 128. We achieve state-of-the-art likelihood results in multiple settings, set up\nnew benchmark results in previously unexplored settings and are able to generate\nvery high fidelity large scale samples on the basis of both datasets.", "keywords": [], "authorids": ["jmenick@google.com", "nalk@google.com"], "authors": ["Jacob Menick", "Nal Kalchbrenner"], "TL;DR": "We show that autoregressive models can generate high fidelity images. ", "pdf": "/pdf/5425f9e6312c689aaf7331bc19de3aa2e0e87267.pdf", "paperhash": "menick|generating_high_fidelity_images_with_subscale_pixel_networks_and_multidimensional_upscaling", "_bibtex": "@inproceedings{\nmenick2018generating,\ntitle={{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},\nauthor={Jacob Menick and Nal Kalchbrenner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylzTiC5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper780/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311754424, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HylzTiC5Km", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper780/Authors", "ICLR.cc/2019/Conference/Paper780/Reviewers", "ICLR.cc/2019/Conference/Paper780/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311754424}}}], "count": 17}