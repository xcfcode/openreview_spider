{"notes": [{"id": "SyeyF0VtDr", "original": "BJlBpb_ODS", "number": 1232, "cdate": 1569439350828, "ddate": null, "tcdate": 1569439350828, "tmdate": 1577168241334, "tddate": null, "forum": "SyeyF0VtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xACNwEN81N", "original": null, "number": 1, "cdate": 1576798718120, "ddate": null, "tcdate": 1576798718120, "tmdate": 1576800918444, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a recurrent and autorgressive architecture to model temporal knowledge graphs and perform multi-time-step inference in the form of future link prediction. However, the reviewers feel that the papers are more of a straight application of current techniques. Furthermore, a better presentation of the experimental section will also help improve the paper.  ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720756, "tmdate": 1576800271647, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Decision"}}}, {"id": "SkeU4A5UjB", "original": null, "number": 5, "cdate": 1573461549915, "ddate": null, "tcdate": 1573461549915, "tmdate": 1573791210526, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "SkeXaQV0cr", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Response to reviewer 3 [1/3]", "comment": "Thank you very much for your helpful feedback and valuable comments! We appreciate the questions you raised regarding the missing related work/baselines and analysis on model components. We answer each question in the response below and have revised our draft to incorporate the comments. In particular, we have added comparison with the suggested baselines in the appendix (Section D) and explain our method\u2019s advantages over them. We hope the reviewer can check our response and new experiments, and consider changing the evaluation of our paper!\n\n-------------------------------\nQ1: Comparison with other existing discrete-time approaches that focus on representation learning over evolving graphs  [1,2,3,4,5].\n\nA1: Thanks for the suggestion! The main reasons why we didn\u2019t include detailed discussion on these recent work on learning representation for dynamic graphs (with discrete time stamps) are: (1) They study \u201csingle-relational\u201d (homogeneous) graphs while our work focuses on \u201cmultiple-relational\u201d knowledge graphs; and (2) these methods are designed to predict future relationship \u201cin one step\u201d (i.e., for t+1), while our method aims for multi-step prediction (i.e., for t+1, t+2, t+3, \u2026, t+k). We will expand our related work section to include a more detailed discussion on these methods.\n\nRegarding (1), one can change the GCN in [1] into a R-GCN to deal with multi-relational graphs but it is non-trivial to modify [2-4] to extend to multi-relational graphs. We added comparison to a new baseline, EvolveRGCN, by replacing GCN in Evolve-GCN [1] by a R-GCN, and also compare with DynGEM [2], dyngraph2vec [3], DynTriad [4] by ignoring the relation type information on the edges (i.e., changing the knowledge graph to single-relational graph). Also, we add relation embeddings for tNodeEmbed [5] so it can be applied to multi-relational graphs. Here, we briefly summarize the results on the ICEWS18 dataset as follows. More detailed results (over three datasets, all metrics) can be found in Section D.3 in the Appendix of the updated draft.\n\nMethod\t\t        | MRR  | Hits@1 | Hits@3 | Hits@10\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\nEvolveRGCN\t\t| 18.50 | 16.38    | 19.19    | 22.09\nDynGEM\t\t| 0.00   | 0            | 0           | 0\ndyngraph2vecAE\t| 1.88   | 1.77      | 1.99      | 2.02\nDynTriad\t\t| 3.48   | 0           | 3.55      | 11.47\ntNodeEmbed\t| 8.32   | 3.19      | 9.74      | 17.47\nRE-Net\t\t        | 42.93 | 36.19    | 45.47    | 55.80\n\nWe can see that RE-Net outperform these five (modified) baselines by a large margin. In fact, these single-relational, one-step prediction models cannot outperform some strong static graph baselines like ConvE. We conjecture that the methods are not specifically designed for multi-relational graphs, and thus they are not capable of handling multiple relations, which leads to degradation of their performances.\n\nRegarding (2), the suggested methods may be tweaked to predicted G_{t+2} if their models can be re-trained over G_{1, .., t, t+1}. However, such re-training is time consuming and relies on \u201cpredicted labels\u201d G_{t+1}. Due to limited time for rebuttal, we leave the in-depth analysis of this problem to future work.\n\n[1] Evolve-GCN: Evolving Graph Convolutional Networks for Dynamic Graphs, Pareja et. al.\n[2] DynGEM: Deep embedding method for dynamic graphs, Goyal et. al.\n[3] dyngraph2vec: Capturing network dynamics using dynamic graph representation learning, Goyal et. al.\n[4] Dynamic Network Embedding by Modeling Triadic Closure Process, Zhou et. al.\n[5] Node Embedding over Temporal Graphs, Singer et. al.\n\n-------------------------------\nQ2: Regarding \u201cThe baselines tested by the authors are either support static graphs, supports interpolation or supports continuous time data.\u201d\n\nA2: Thanks for the suggestion on strengthening our experiments. In our prior submission, we modified DyRep [1] (into DyRep+MLP) and GCRN [2] (into R-GCRN+MLP) as baselines, which deal with extrapolation, discrete-time, and multi-relational temporal graph prediction, same as the setting of our focus. The experiments show that RE-Net significantly outperforms these methods. We will include more discussion on other recent related work as mentioned in Q1. We have updated our draft to include other methods as discussed in our response A1.  \n\n[1] Dyrep: Learning representations over dynamic graphs, Trivedi et al. \n[2] Structured Sequence Modeling with Graph Convolutional Recurrent Networks, Seo et al.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "HJl_y3iTFS", "original": null, "number": 2, "cdate": 1571826656429, "ddate": null, "tcdate": 1571826656429, "tmdate": 1573749116673, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper presents the Re-Net model which sequentially generates a temporal knowledge graph (TKG) in an autoregressive fashion by taking both global and local information into account.\nThe generation of TKG is motivated via a joint distribution problem which is then parametrized by the usage of a recurrent event encoder.\nIn addition to past information, the encoder aggregates local as well as global information for which the authors propose three different aggregation schemes build upon the works of, e.g., attentive pooling and the RGCN model.\nIn an in-depth-evaluation study, the performance of the proposed model is evaluated on five different datasets on which it consistently improves upon the state-of-the-art.\nAn ablation study shows the benefits of all proposed features of Re-Net, e.g., the usage of more sophisticated aggregation schemes, the impact of using global information, and the number of RGCN layers.\n\nAs far as I know, the proposed method is a novel and clever (though not ground-breaking) contribution to the field of performing global structure inference over TGKs.\nThe paper is well-written but is partially becoming a little hard to comprehend due to its overloaded notation, e.g., $h_t(s)$ vs. $h_s^l$ and $N(s)_t$ vs. $N_t^{(s)}$ vs. $N_t^{(s,r)}$, and could be improved by a more rigorous formulation, e.g., for $N(s)_t$ or $c_s$ (which should also depend on r).\n\n1. Re-Net evolves the embeddings of entities and performs predictions via negative log likelihood. Hence, the model seems to be limited to predict events between entities which have been already seen during training and does not generalize to unseen entities. In addition, by applying a softmax classifier your model does not seem to be able to scale to large knowledge graphs? Are those observations correct and how could they be resolved?\n\n2. As far as I understood, the formulation of $N^{(s,r)}$ is not needed for defining the mean and attentive pooling aggregators since you are aggregating information independent of the relation type. However, the current formulation could confuse readers (including me).\n\n3. Algorithm 1 could be made more clear since the sampled number of M subjects does not get mentioned again. I guess the top-k triples are picked across all M samples and not individually? In addition, the sampling of subjects should relate to Equation 5 instead of Equation 4.\n\n4. In Figure 3 it is not clear why the accuracy does not decrease with temporal distance from the training examples. It would be helpful to interpret and clarify the results in more detail.\n\n5. I was not able to fully comprehend your complexity analysis. For example, it is not clear what $|E|$ means (I guess the maximum number of triples in a time step?). In addition, it seems that you are still dependent on computing node embeddings for all entities in your graph, even if you only report runtimes for computing a single example. In my opinion, there is a $L \\cdot |E|$ term missing in your complexity analysis for computing RGCN across the whole graph. Please clarify!\n\n6. The results of using the attentive aggregation scheme should be included into Tables 1 and 2.\n\n7. Since Figure 5c signalizes that Re-Net can effectively leverage larger receptive field sizes, how does it perform when increasing the number of layers further?\n\n------------------------\nUpdate after the rebuttal:\n\nI would like to thank the authors for answering my questions and clarifying several issues. The raised questions were not critical for my overall rating, which remains unchanged (6: Weak Accept).", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575881708614, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Reviewers"], "noninvitees": [], "tcdate": 1570237740380, "tmdate": 1575881708627, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Review"}}}, {"id": "BJeY0s9LsB", "original": null, "number": 2, "cdate": 1573460944636, "ddate": null, "tcdate": 1573460944636, "tmdate": 1573620884707, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "HJl_y3iTFS", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Response to reviewer 2 [1/2]", "comment": "We thank for positive feedback and thoughtful suggestions. We tried to resolve the issues and incorporated the comments in our updated draft. We also revised writing about notations and complexity analysis, etc, and updated experiments in Section 4.\n\n-------------------------------\nQ1: Regarding \u201cThe paper is a little hard to comprehend due to its overloaded notations.\u201d\n\nA1: Thanks for suggesting notations! We have updated the notations in the paper. We mostly used the same notations since we think current notations are intuitive. We tried to remove the confusion. The definition of each notation is as follows: \nN_t^(s) : neighbors of s at time t.\nN_t^(s,r) : neighbors of s under relation r at time t.\nh_t(s) : a history vector of s at time t.\nh_s^(l) : a hidden representation of RGCN (needs update).\nNote that h_t(s) and h_s^(l) are different notations. We use different bold faces to differentiate them.\n\n-------------------------------\nQ2: Regarding \u201cThe model seems to be limited to predict events between entities which have been already seen during training and does not generalize to unseen entities.\u201d\n\nA2: Thanks for the great question. As the reviewer said, RE-Net does not generalize to unseen entities in the current setting. This is because the entity attributes are not given in the dataset. However, If we have entity attributes for training and test sets, RE-Net can generalize to unseen entities by using entity attributes. For example, embeddings for each node will be [attributes] * W. W is a trainable matrix. In the current setting, We defined embeddings as [one-hot vector] * W. By using one-hot vector, parameters for unseen entities are not learned during training.\n\n-------------------------------\nQ3: Regarding \u201cBy applying a softmax classifier your model does not seem to be able to scale to large knowledge graphs.\u201d\n\nA3: Yes, I agree that a softmax classifier does not scale. To deal with huge graphs, we can adopt a sigmoid function with a negative sample strategy instead of a softmax function. Then, it can scale to large knowledge graphs.\n\n-------------------------------\nQ4: Regarding \u201cThe formulation of is not needed for defining the mean and attentive pooling aggregators since you are aggregating information independent of the relation type.\u201d \n\nA4: Thanks for the comment! The reviewer commented that the formulation is not needed for defining the mean and attentive pooling aggregators since RE-Net aggregates information independent of the relation type. However, mean and attentive pooling aggregators adopt a different strategy from a multi-relational aggregator. Since the mean and attentive pooling aggregator does not deal with multiple relations, they only aggregate neighbors under the same and fixed relation. For example, if we want to get P(o|s,r,N), then the aggregator only collects neighbors under relation r. We added the description in Section 3.2 of the paper.\n\n-------------------------------\nQ5: The top-k triples are picked across all M samples and not individually? And Equation 4 should be Equation 4 in Algorithm 1.\n\nA5: Thanks for pointing out the typo! As the reviewer said, top-k triples are picked across all M samples. We have updated writing in Section 3.3. We also have fixed the typo about Equation 4. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "rklhFy2PiS", "original": null, "number": 9, "cdate": 1573531523889, "ddate": null, "tcdate": 1573531523889, "tmdate": 1573531523889, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "rJxg-r9pFB", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Response to reviewer 1 [1/3]", "comment": "Thank you very much for the helpful feedback and comments! \n\nWe did our best to address the raised questions and incorporated the comments in our updated draft. In particular, we have included case study on concrete examples in Section E of the Appendix to help understand how our method work, and also updated the writing of complexity analysis in Section 3.3 to clarify things.\n\nBelow please find our detailed responses to the questions in your review. We sincerely hope you could check our response (especially Q1-Q4), and consider changing the evaluation on our paper!\n\n-------------------------------\nQ1: Regarding \u201cIs this the first paper to apply autoregressive to knowledge graphs?\u201d and our technical contributions.\n\nYes. To the best of our knowledge, this is the *first work* to formulate the structure inference (prediction) problem for temporal, multi-relational (knowledge) graphs *in an autoregressive fashion*\u2014-i.e., aiming to model and predict the occurrences (events) of entities and their relationships in the next (future) time stamp based on priorly observed events. \n\nOur autoregressive problem formulation is new and technically challenging *particularly in the context of multi-relational (knowledge) graph* because: (1) one needs to consider the interdependence between different entities (e.g., some events involving the same entities are more likely to happen together than others) rather than modeling the event sequence for each entity, e.g., [(e, r1, o1; t1), (e, r2, o2; t2), (e, r3, o3; t3), \u2026.], in an independent manner; and (2) one needs to carefully model the probability of a triple (event) given the history, i.e., P(s, r, o | G_<t), which requires both *effective and tractable* solution to deal with multi-relational data\u2014-e.g., directly parametrizing P(s, r, o | G_<t) will result in a huge output space that is unscalable and difficult to learn.\n\nRegarding challenge (1) above, we propose to impose conditional independence over the events at the same time stamp (i.e., Eq (1) in the paper), where events in G_t are mutually independent given the previous events G_t-m:t-1. We also define the joint probability distribution of an event based on past events and the involved entities (i.e., Eq (2) in the paper). This ensures that the prediction of a new event will be made based on events of all the entities in previous time stamps (i.e., to model the interdependence). The formulation of this joint probability distribution is new and has not been studied in previous work.\n\nRegarding challenge (2), we propose our recurrent event encoder (Eqs. (3)-(8) in the paper) to parametrize the probability function for P(s, r, o | G_<t). In particular, multi-relational data has a much larger search space due to its complex nature, i.e., |entities|-by-|relations|-by-|entities|. Thus, we need a careful design of the encoder to ensure the *tractability*. We have put significant efforts on exploring the parameterization design, and finally reached to the current formulation of p(s|G_<t), P(r | s, G_<t) and P(o | s, r, G_<t). \n\nTo our knowledge, the most related work in terms of autoregressive modeling of graphs is GraphRNN [1]. However, we would like to point out that GraphRNN focuses on a rather different problem setting: their input is a set of static, single-relational (homogeneous) graphs from the same distribution and their goal is to learn a generative model for this family of graphs (i.e., the data distribution) so as to generate more graphs of the same kind. The main challenge resolved in GraphRNN is how to deal with permutation-invariance of static graphs and how to learn the generative model from many graph examples, which are different from our focus in this paper. We updated our related work section to stress on this.\n\nWe make sure these points are well stressed in the revised version.\n\n[1] Graphrnn: Generating realistic graphs with deep auto-regressive models, You et al., ICML 2018"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "Bkl8wJhDir", "original": null, "number": 8, "cdate": 1573531485850, "ddate": null, "tcdate": 1573531485850, "tmdate": 1573531485850, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "rJxg-r9pFB", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Response to reviewer 1 [2/3]", "comment": "Q2: From related work, the answer to \u201cIs this the first paper to apply autoregressive to knowledge graphs?\u201d is no. Can the author clarify more on the following sentence? \u2014>  \"In contrast, our proposed method, RE-NET, augments a RNN with message passing procedure between entity neighborhood to encode temporal dependency between (concurrent) events (i.e., entity interactions), instead of using the RNN to memorize historical information about the node representations.\"\n\nA2: Sorry for the confusion caused by the above sentence in our related work! We meant to say that the usage of RNN (i.e., the idea of autoregressive modeling) is rather different from previous work which also leverage RNNs in their model architectures, such as EvolveGCN [1] and GCRN [2]. \n\nRNNs in our work are used to implement the recurrent event encoder (Eqs. (3)-(8) in the paper), which aims to parameterize the joint probability distribution for P(s, r, o | G_<t), i.e., modeling the sequence of graph snapshots {G_t} (events) in an autoregressive fashion. \n\nIn contrast, prior work adopt RNN to either memorize and update the states of node embeddings that are dynamically evolving (e.g., in GCRN [2]), or memorize and update the GCN model parameters for different time stamps (e.g., in EvolveGCN [1]). While our work aims to leverage autoregressive modeling to formulate the structure inference problem for temporal KGs, previous work has not applied RNNs to predict event sequence and use RNNs in rather different ways.\n\nWe have updated our related work section to clarify this question. Thank you!\n\n[1] Evolve-GCN: Evolving Graph Convolutional Networks for Dynamic Graphs, Pareja et. al.\n[2] Structured Sequence Modeling with Graph Convolutional Recurrent Networks, Seo et al.\n\n-------------------------------\nQ3: Regarding \u201cThis paper is more like an application paper. From the point of view on Machine learning / deep learning, there is not much insight from it.\u201d\n\nA3: We very much appreciate the reviewer\u2019s comments. Designing generative models for sequential, event data is an important topic in machine learning. Compared with the problems studied in existing work (e.g., graph generation, text generation), our problem is more challenging, as (1) the data instance at each time step is a multi-relational graph, (2) the structure of which is very complicated, and modeling such highly complicated data is nontrivial. \n\nRegarding (1), our problem on multi-relational graphs (knowledge graphs) is more challenging than on homogeneous graphs since knowledge graphs have multi-relational edges. This leads to a huge search space ( (# of relations) times bigger) and needs more sophisticated modeling. To tackle this problem, we design the joint probability of a knowledge graph as discussed in A1.\n\nRegarding (2), the structure of multi-relational graphs has its complex nature. Modeling such a multi-relational graph in an autoregressive manner requires big contributions to effectively include the structure of multi-relational graphs as discussed in A1. To tackle this, we propose our recurrent event encoder to parametrize the probability function for P(s, r, o | G_<t) with aggregators.\n\nFurthermore, our paper is the first work that proposes an autoregressive-based generative model for this problem. In this sense, our work still makes many contributions to the machine learning field.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "B1ltHknvjH", "original": null, "number": 7, "cdate": 1573531456782, "ddate": null, "tcdate": 1573531456782, "tmdate": 1573531456782, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "rJxg-r9pFB", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Response to reviewer 1 [3/3]", "comment": "Q4: Regarding \u201cThe paper lacks detailed analysis.\u201d\n\nA4: Thank you for pointing out this place to improve! First, we would like to point out that in prior submission we conducted analysis and ablation studies on model components and model variants (in Section 4.3 and 4.4) to help readers understand which parts of the proposed model are useful and how important they are. \n\nMoreover, with limited rebuttal time, we added case study about RE-Net\u2019s predictions in Section E (Appendix). RE-Net\u2019s predictions depend on interaction histories. The histories can be categorized into three cases: (1) consistent interactions with an object, (2) a specific temporal pattern, and (3) irrelevant history. In the first case, history has interactions with the same objects but with different relations. In the second case, history shows a specific temporal pattern on relations such as ( s, Arrest, o) -> ( s, Use force, o). In the third case, there are no relevant interaction histories to the query. RE-Net learns (1) and (2) cases, so it achieves good performances. However, RE-Net cannot predict answers given irrelevant history (case (3)). Please refer to Section E for details.\n\n-------------------------------\nQ5: Regarding \u201cThe paper give complexity of this algorithm but no comments about how it compare with other method and how practical it is.\u201d\n\nA5: We appreciate the reviewer\u2019s careful comments. We have updated time complexity for generating one graph in Section 3.3. The time complexity is linear to the number of entities and relations. Other methods do not generate graphs and do not perform multi-step predictions, whereas our method do with Algorithm 1. Thus we couldn\u2019t compare our time complexity with other methods. RE-Net have a limitation on generating huge graphs which contain billions of nodes. We left the work of designing an efficient generative model of TKGs as future work. \n\n-------------------------------\nQ6: What is the RNN structure?\n\nA6: Thanks for pointing out and sorry for the confusion! We adopt Gated Recurrent Units as our RNN structure in the experiments. The definition of RNN was given in appendix A in previous submission. We also added a short description of the RNN structure in Section 3.1.\n\n-------------------------------\nQ7: For the aggregator, what is the detailed formulation of h_o^0? \n\nA7: Thanks for pointing out the confusion about the initial hidden representation! We set the initial hidden representations of each node (h_o^0) in the RGCN aggregator as trainable embedding vectors of each node (e_o). We added details on this in section 3.2 of the updated paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "SkesRSBPsS", "original": null, "number": 6, "cdate": 1573504467514, "ddate": null, "tcdate": 1573504467514, "tmdate": 1573505526530, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Paper revision", "comment": "We would like to thank all the reviewers for their thoughtful and valuable comments. We have carefully revised the paper to address the comments from reviewers, and also added additional experiments to support our method. The major updates of the draft are listed as follows:\n\n- Added more related works in Section 2, as suggested by reviewers 1 and 3.\n- Updated complexity analysis in Section 3.3, as suggested by reviewers 1, 2, and 3\n- Added experiments of RE-Net with an attentive aggregator in Table 1,2, and 4, and other baselines in Section D, as suggested by reviewers 2 and 3\n- Added case study of RE-Net in Section E, as suggested by reviewer 1.\n- Improved clarity of Sections 3 and 4 to reflect the questions raised by reviewers.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "HJeUTp9UoB", "original": null, "number": 4, "cdate": 1573461438261, "ddate": null, "tcdate": 1573461438261, "tmdate": 1573462859979, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "SkeXaQV0cr", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Response to reviewer 3 [2/3]", "comment": "Q3: Regarding \u201cHowever, the aggregation techniques are not novel contributions but augmentation to the RNN architecture. Hence it is important to show how augmenting aggregation module with other baselines and the above mentioned discrete baselines would affect the performance of these baselines.\u201d\n\nA3: Thank you for making a good point to strengthen our experiment. We agree that the aggregation methods adopted in our model are not new and have been used in prior work. However, we would like to stress that the integration of two complementary modules (1) autoregressive modeling of the temporal aspect of the data (i.e., event sequences) and (2) aggregation over the local structural neighborhood (i.e., concurrent events within a time window that form a sub-graph) is novel and is one of our contributions. Such an integration of the two mutually enhancing modules has not been well understood. However, we argue that integrating aggregation module with static methods (e.g., ConvE) or temporal baselines that are also exploring structural similarity/consistency (e.g., DynTriad, DynGem, and TA-TransE/DistMult) may not bring significant gains, as the aggregation module also exploits the structural information in the local neighborhood. \n\nWith limited response time, we conduct experiments on ConvE+RGCN over the ICEWS18 and summarize the results as follows. ConvE+R-GCN shows better performances than R-GCN and worse performances than ConvE, which implies that the aggregation technique is not helpful to ConvE.\n\nMethod\t\t        | MRR  | Hits@1 | Hits@3 | Hits@10\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\nConvE+RGCN\t| 24.59 | 17.77    | 27.19    | 37.41\nRGCN\t\t\t| 23.19 | 16.36    | 25.34    | 36.48\nConvE\t\t\t| 36.67 | 28.51    | 39.80    | 50.69\nRE-Net\t\t        | 42.93 | 36.19    | 45.47    | 55.80\n\n-------------------------------\nQ4: Regarding \u201cResults of RE-Net with an attentive aggregator are missing.\u201d\n\nA4: Thank you for asking additional comparison to help validate our claims. We have updated tables 1, 2 & 4 to include an additional model variant, \u201cRE-Net with attentive aggregator\u201d. It shows improvements over the mean aggregator, which implies that giving different attention weights to each neighbor helps predictions. In particular, results on ICEWS18 are summarized as follows.\n\nMethod\t\t                | MRR  | Hits@1 | Hits@3 | Hits@10\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\nRE-Net w. mean agg.\t| 40.70 | 34.24    | 43.27    | 53.65\nRE-Net w. attn agg.\t| 40.96 | 34.57    | 44.08    | 54.32\nRE-Net\t\t                | 42.93 | 36.19    | 45.47    | 55.80\n\n-------------------------------\nQ5: Can the authors explain why the global vector H_t is not playing a significant role? \n\nA5: Thanks for the great question. H_t does not play a significant role in performances since most important information comes from local neighborhood. For example, most interactions occur between local neighborhood not distant neighbors. Global information H_t serves as a complement of local neighborhood and give information about distant neighbors.\n\n-------------------------------\nQ6: What aggregation is used to compute H_t?\n\nA6: Thanks for pointing out and sorry for the confusion. We use the multi-relational aggregator to compute H_t. After going through multi-relational aggregator, each node has h_t. We max-pool all h_t and this will become H_t. We have updated writing of \u201cRecurrent Event Encoder\u201d in Section 3.1. \n\n-------------------------------\nQ7: Algorithm 1 is not very clearly explained. When the authors mention that they only use one sample, does that mean a single subject is sampled at each time point t'?\n\nA7: Thanks for pointing out the confusion about sampling process. We sample one graph in Algorithm 1. To get one graph at each time step, we first sample M number of subjects and pick top-k triples which are from Equation 2. Then, we have k triples which build a temporal knowledge graph at time t. We have updated writing at the end of \u201cMulti-step Inference over Time\u201d in Section 3.3.\n\n-------------------------------\nQ8: Regarding \u201cInference computation complexity is concerning. There is a strict limitation on scalability of inference module.\u201d\n\nA8: This is a great question. As the reviewer said, RE-Net has some limitation on scalability. The time complexity for generating one graph is linear to the number of entities and relations. We have updated time complexity in Section 3.3. If the graph has billions of nodes, we need a more efficient version of RE-Net. We didn\u2019t proposed the efficient version. We leave this for future work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "rJx4zp5Isr", "original": null, "number": 3, "cdate": 1573461260428, "ddate": null, "tcdate": 1573461260428, "tmdate": 1573462807097, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "SkeXaQV0cr", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Response to reviewer 3 [3/3]", "comment": "Q9: Regarding \u201cIt is not clear what is the difference between RE-NET and RE-NET w. GT.\u201d\n\nA9: As the reviewer said, RE-Net w. GT does not update history (or generate a graph) since it already has ground truth history (\u201cVariant of RE-Net\u201d in Section 4.1). Thus it does not need to use Algorithm 1. In this case, \\hat{G}_{t+1:t+\\Delta t -1} is known from ground truth history. For inference, it uses equation (3) which is a probability for o_t given s, r, and history. Since RE-Net does not know ground truth, it needs to generate history of triples (or a graph) which is described in Algorithm 1. We have updated writing in Section 4.1.\n\n-------------------------------\nQ10: Regarding \u201cThe time span expansion for WIKI and YAGO is very unnatural and it is not clear if these experiments provide any value.\u201d\n\nA10: Thanks for the great question. WIKI and YAGO datasets are different from ICEWS and GDELT. TA-DistMult [1] and HyTE [2] also used these two datasets. WIKI and YAGO are not event-based but have time-ranged facts. In other words, WIKI and YAGO datasets have temporally associated facts (s,r,o,t1,t2) where t1 means a starting time and t2 means an ending time. We should convert them into an event-based setting (s,r,o,t1), (s,r,o,t1+1), ..., (s,r,o,t2) to do multi-step inference, since RE-Net takes sequences of triples and predicts interactions in a discrete manner. I agree that this conversion can be unnatural. We leave the sophisticated modeling for future work. Thanks for the sharp question!\n\n[1] Learning Sequence Encoders for Temporal Knowledge Graph Completion, Garc\u00eda-Dur\u00e1n et al.\n[2] HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding, Dasgupta et al.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "S1le2i5LoH", "original": null, "number": 1, "cdate": 1573460904142, "ddate": null, "tcdate": 1573460904142, "tmdate": 1573460904142, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "HJl_y3iTFS", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment", "content": {"title": "Response to reviewer 2 [2/2]", "comment": "Q6: Regarding \u201cIn Figure 3 it is not clear why the accuracy does not decrease with temporal distance from the training examples.\u201d\n\nA6: The reviewer pointed out that in Figure 3 it is not clear why the accuracy does not decrease with temporal distance from the training examples. The reason why the performances fluctuated is because testing entities are not always the same entities at each time.  We can verify this from ConvE\u2019s performances over time.  ConvE is a static method which is not affected by temporal distances. ConvE\u2019s accuracy does not decrease in the Figure. In other words, testing entities are different at each time step, thus leading to fluctuation. Instead, we have to focus on the difference between RE-Net and ConvE methods. The difference gets smaller as time increases. We added this in Section 4.2.\n\n-------------------------------\nQ7: Regarding \u201cI was not able to fully comprehend your complexity analysis.  There is a term L * |E| missing in your complexity analysis for computing RGCN across the whole graph\u201d\n\nA7: Sorry for the confusion. We have updated complexity analysis for generating one graph in Section 3.3. We provide a more detailed analysis. As reviewer said, |E| means the maximum number of triples in a time step. We have added L*|E| term which is needed to compute P(s|G_t-m:t-1). The time complexity of RE-Net is linear to |E|, number of entities and relations. Thanks for pointing out!\n\n-------------------------------\nQ8: Regarding \u201cThe results of using the attentive aggregation scheme should be included into Tables 1 and 2.\u201d\n\nA8: We have updated tables 1, 2, and 4 to include RE-Net with an attentive aggregator. It shows improvements over the mean aggregator, which implies that giving different attention weights to each neighbor helps predictions.\n\nSummarized results on ICEWS18 are as follows:\n\nMethod\t\t                | MRR  | Hits@1 | Hits@3 | Hits@10\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\nRE-Net w. mean agg.\t| 40.70 | 34.24    | 43.27    | 53.65\nRE-Net w. attn agg.\t| 40.96 | 34.57    | 44.08    | 54.32\nRE-Net\t\t                | 42.93 | 36.19    | 45.47    | 55.80\n\n-------------------------------\nQ9: How does it perform when increasing the number of layers further in Figure 5c?\n\nA9: We have added a result of the 3-layered model. It underperforms 2-layered model. We conjecture that the bigger parameter space leads to overfitting.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyeyF0VtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1232/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1232/Authors|ICLR.cc/2020/Conference/Paper1232/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159187, "tmdate": 1576860550662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Authors", "ICLR.cc/2020/Conference/Paper1232/Reviewers", "ICLR.cc/2020/Conference/Paper1232/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Comment"}}}, {"id": "rJxg-r9pFB", "original": null, "number": 1, "cdate": 1571820791566, "ddate": null, "tcdate": 1571820791566, "tmdate": 1572972495639, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper properly applied several technique from RNN and graph neural networks to model dynamically-evolving, multi-relational graph data. There are two key component: a RNN to encode temporal information from the past event sequences, and a neighborhood aggregator collects the information from the neighbor nodes. The contribution on RNN part is design the loss and parameterizes the tuple of the graph. The contribution of the second part was adapting Multi-Relational Aggregator to this network. The paper is well-written. Although I'm familiar with the dataset, the analysis and comparison seems thorough. \n\nI'm leaning to reject or give borderline for this paper because (1) This paper is more like an application paper. Although the two component is carefully designed, the are more like direct application. I'm not challenge this paper is not good for the target task. But from the point of view on Machine learning / deep learning, there is not much insight from it. The technical difficult was more from how to make existing technique to fit this new problem.  This \"new\" problem seems more fit to data mining conference. (2) The experiments give tons of number but it lack of detailed analysis, like specific win/loss case of this model. As a more application-side paper, these concrete example can help the reader understand why this design outperform others. For example, it can show what the attention weights look like, and compare to the proposed aggregator. \n\nSome questions:\n[This question is directly related to my decision] Does this the first paper to apply autoregressive to knowledge graph? from related work, the answer is no. Can the author clarify more on this sentence? \n\n\"In contrast, our proposed method, RE-NET, augments a RNN with message passing procedure between entity neighborhood to encode temporal dependency between (concurrent) events (i.e., entity interactions), instead of using the RNN to memorize historical information about\nthe node representations.\"\n\nThe paper give complexity of this algorithm but no comments about how it compare with other method and how practical it is.\n\nIt lacks of some details for the model:\n(1) what is the RNN structure? \n(2) For the aggregator, what is the detailed formulation of h_o^0? \n "}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575881708614, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Reviewers"], "noninvitees": [], "tcdate": 1570237740380, "tmdate": 1575881708627, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Review"}}}, {"id": "SkeXaQV0cr", "original": null, "number": 3, "cdate": 1572910010973, "ddate": null, "tcdate": 1572910010973, "tmdate": 1572972495548, "tddate": null, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "invitation": "ICLR.cc/2020/Conference/Paper1232/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a recurrent and autorgressive architecture to model temporal knowledge graphs and perform multi-time-step inference in the form of future link prediction. Specifically, given a historical sequence of graphs at discrete time points, the authors build sequential probabilistic approach to infer the next graph using joint over all previous graphs factorized into conditional distributions of subject, relation and the objects. The model is parameterized by a recurrent architecture that employs a multi-step aggregation to capture information within the graph at particular time step. The authors also propose a sequential approach to perform multi-step inference. The proposed method is evaluated on the task of future link prediction across several baselines, both static and dynamic, and ablation analysis is provided to measure the effect of each component in the architecture.\n\nThe authors propose to model temporal knowledge graphs with the key contribution being the sequential inference and augmentation of RNN with multi-step aggregation. The paper is well written in most parts and provides adequate details with some exceptions. I appreciate the extended ablation analysis as it helps to segregate the effect of each component very clearly. However, there are several major concerns which makes this paper weaker:\n\n- The paper approaches temporal knowledge graphs in discrete-time fashion where multiple events/edges are available at each time step. While this is intuitive, the authors fail to position the paper in light of various existing discrete-time approaches that focus on representation learning over evolving graphs [1,2,3,4,5]. Related work mentions [1] learns evolving representations but all these methods can do future link prediction and hence this is a big miss for the paper. A discussion and comparison with these approaches is certainly required as most of static and dynamic baselines currently compared also focus on learning representations, hence that is not a valid argument to miss comparison. \n\n- The baselines tested by the authors are either support static graphs, supports interpolation or supports continuous time data. However, as the authors explicitly propose a discrete time model starting from Section 3, it is important to perform experiments on atleast few of the discrete time baselines to demonstrate the efficacy of the proposed method. For instance, authors can augment relation as extra feature or use their encoders and optimization function to perform experiments e.g. Evolve-GCN  only require to replace GCN with R-GCN.\n\n- From the ablation it is clear that aggregation is the most important component as without it, the performance drops much closer to ConvE which is a static baseline and significantly worse than other RE-Net variants. However, the aggregation techniques are not novel contributions but augmentation to the RNN architecture. Hence it is important to show how augmenting aggregation module with other baselines (for instance, ConvE and TA-DistMult)) and the above mentioned discrete baselines would affect the performance of these baselines. \n\n- While the authors describe attentive Pooling Aggregator, the experiments only show mean aggregator and multi-step one. Is there a reason Attentive pooling is not used for any experiments? \n-It appears that global vector H_t is not playing significant role based on ablation study. Can the authors explain why that si the case? Also, what aggregation is used to compute H_t? Is it sum over all previous h_t's?\n\n- Algorithm 1 is not very clearly explained. When the authors mention that they only use one sample, does that mean a single subject is sampled at each time point t'? If so, how do you ensure the coverage is good across subjects in the newly generated graph? I admit I am not clear on this and would recommend the authors to elaborate in response and also in the paper. Also, the inference computation complexity is concerning. While it seems fine for the provided dataset, most real-world graphs have billion of nodes and I all of E, L and D would be larger for such graphs. This seems to put a strict limitation on scalability of inference module. \n\n- It is not clear what is the difference between RE-NET and RE-NET w. GT. Could the authors elaborate this more? It seems the authors do not update history when they perform RE-NET w/o multi-step. However, in the RE-NET w. GT, where is the ground truth history used in Algorithm 1?\n\n- The time span expansion for WIKI and YAGO is very unnatural and it is not clear if these experiments provide any value. For instance, can the authors show that in multi-step inference scheme, they can actually predict events at multiple time points corresponding to time span events in actual dataset? As multiple triplets can appear at consecutive time points, the current modification just makes them equivalent which doesn't seem correct. \n\nI am willing to revisit my score if the above concerns are appropriately addressed and requested experiments are provided.\n\n[1] Evolve-GCN: Evolving Graph Convolutional Networks for Dynamic Graphs, Pareja et. al.\n[2] DynGEM: Deep embedding method for dynamic graphs, Goyal et. al.\n[3] dyngraph2vec: Capturing network dynamics using dynamic graph representation learning, Goyal et. al.\n[4] Dynamic Network Embedding by Modeling Triadic Closure Process, Zhou et. al.\n[5] Node Embedding over Temporal Graphs, Singer et. al."}, "signatures": ["ICLR.cc/2020/Conference/Paper1232/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1232/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph", "authors": ["Woojeong Jin", "He Jiang", "Meng Qu", "Tong Chen", "Changlin Zhang", "Pedro\u00a0Szekely", "Xiang Ren"], "authorids": ["woojeong.jin@usc.edu", "jian567@usc.edu", "meng.qu@umontreal.ca", "tongc2@andrew.cmu.edu", "changlin.zhang@usc.edu", "pszekely@isi.edu", "xiangren@usc.edu"], "keywords": ["Temporal Knowledge Graphs", "Representation Learning", "Graph Sequence Inference", "Knowledge Graph Completion"], "TL;DR": "We propose an autoregressive model to infer graph structures on temporal knowledge graphs.", "abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on \ufb01ve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.", "pdf": "/pdf/076ffbc995d7dcde4d6b4289ece94ac94fdf0e53.pdf", "code": "https://github.com/fhuiewwwjklfu2iy43wtqe/jkqehwf2783fhasvfv", "paperhash": "jin|recurrent_event_network_global_structure_inference_over_temporal_knowledge_graph", "original_pdf": "/attachment/9b9159565b45713cd6c0a77e6d33bdea8e1b55be.pdf", "_bibtex": "@misc{\njin2020recurrent,\ntitle={Recurrent Event Network : Global Structure Inference Over Temporal Knowledge Graph},\nauthor={Woojeong Jin and He Jiang and Meng Qu and Tong Chen and Changlin Zhang and Pedro~Szekely and Xiang Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=SyeyF0VtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyeyF0VtDr", "replyto": "SyeyF0VtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1232/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575881708614, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1232/Reviewers"], "noninvitees": [], "tcdate": 1570237740380, "tmdate": 1575881708627, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1232/-/Official_Review"}}}], "count": 14}