{"notes": [{"id": "S1lVniC5Y7", "original": "B1xgz165tm", "number": 704, "cdate": 1538087852416, "ddate": null, "tcdate": 1538087852416, "tmdate": 1545355388726, "tddate": null, "forum": "S1lVniC5Y7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "From Nodes to Networks: Evolving Recurrent Neural Networks", "abstract": "Gated recurrent networks such as those composed of Long Short-Term Memory\n(LSTM) nodes have recently been used to improve state of the art in many sequential\nprocessing tasks such as speech recognition and machine translation. However,\nthe basic structure of the LSTM node is essentially the same as when it was\nfirst conceived 25 years ago. Recently, evolutionary and reinforcement learning\nmechanisms have been employed to create new variations of this structure. This\npaper proposes a new method, evolution of a tree-based encoding of the gated\nmemory nodes, and shows that it makes it possible to explore new variations more\neffectively than other methods. The method discovers nodes with multiple recurrent\npaths and multiple memory cells, which lead to significant improvement in the\nstandard language modeling benchmark task. Remarkably, this node did not perform\nwell in another task, music modeling, but it was possible to evolve a different\nnode that did, demonstrating that the approach discovers customized structure for\neach task. The paper also shows how the search process can be speeded up by\ntraining an LSTM network to estimate performance of candidate structures, and\nby encouraging exploration of novel solutions. Thus, evolutionary design of complex\nneural network structures promises to improve performance of deep learning\narchitectures beyond human ability to do so.", "keywords": ["Recurrent neural networks", "evolutionary algorithms", "genetic programming"], "authorids": ["aditya@cs.utexas.edu", "jasonzliang@utexas.edu", "risto@cs.utexas.edu"], "authors": ["Aditya Rawal", "Jason Liang", "Risto Miikkulainen"], "TL;DR": "Genetic programming to evolve new recurrent nodes for language and music. Uses a LSTM model to predict the performance of the recurrent node. ", "pdf": "/pdf/5fa938d04617bb6f945d2558a6b1a7a17bd9264a.pdf", "paperhash": "rawal|from_nodes_to_networks_evolving_recurrent_neural_networks", "_bibtex": "@misc{\nrawal2019from,\ntitle={From Nodes to Networks: Evolving Recurrent Neural Networks},\nauthor={Aditya Rawal and Jason Liang and Risto Miikkulainen},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lVniC5Y7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1Wpco0el4", "original": null, "number": 1, "cdate": 1544772501008, "ddate": null, "tcdate": 1544772501008, "tmdate": 1545354521945, "tddate": null, "forum": "S1lVniC5Y7", "replyto": "S1lVniC5Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper704/Meta_Review", "content": {"metareview": "In this work, the authors explore using genetic programming to search over network architectures. The reviewers noted that the proposed approach is simple and fast. However, the reviewers expressed concerns about the experimental validation (e.g., experiments were conducted on small tasks; issues with comparisons (cf. feedback from Reviewer2)), and the fact that the method were not compared against various baseline methods related to architecture search. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Work could be strengthened with additional experimental validation"}, "signatures": ["ICLR.cc/2019/Conference/Paper704/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper704/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Nodes to Networks: Evolving Recurrent Neural Networks", "abstract": "Gated recurrent networks such as those composed of Long Short-Term Memory\n(LSTM) nodes have recently been used to improve state of the art in many sequential\nprocessing tasks such as speech recognition and machine translation. However,\nthe basic structure of the LSTM node is essentially the same as when it was\nfirst conceived 25 years ago. Recently, evolutionary and reinforcement learning\nmechanisms have been employed to create new variations of this structure. This\npaper proposes a new method, evolution of a tree-based encoding of the gated\nmemory nodes, and shows that it makes it possible to explore new variations more\neffectively than other methods. The method discovers nodes with multiple recurrent\npaths and multiple memory cells, which lead to significant improvement in the\nstandard language modeling benchmark task. Remarkably, this node did not perform\nwell in another task, music modeling, but it was possible to evolve a different\nnode that did, demonstrating that the approach discovers customized structure for\neach task. The paper also shows how the search process can be speeded up by\ntraining an LSTM network to estimate performance of candidate structures, and\nby encouraging exploration of novel solutions. Thus, evolutionary design of complex\nneural network structures promises to improve performance of deep learning\narchitectures beyond human ability to do so.", "keywords": ["Recurrent neural networks", "evolutionary algorithms", "genetic programming"], "authorids": ["aditya@cs.utexas.edu", "jasonzliang@utexas.edu", "risto@cs.utexas.edu"], "authors": ["Aditya Rawal", "Jason Liang", "Risto Miikkulainen"], "TL;DR": "Genetic programming to evolve new recurrent nodes for language and music. Uses a LSTM model to predict the performance of the recurrent node. ", "pdf": "/pdf/5fa938d04617bb6f945d2558a6b1a7a17bd9264a.pdf", "paperhash": "rawal|from_nodes_to_networks_evolving_recurrent_neural_networks", "_bibtex": "@misc{\nrawal2019from,\ntitle={From Nodes to Networks: Evolving Recurrent Neural Networks},\nauthor={Aditya Rawal and Jason Liang and Risto Miikkulainen},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lVniC5Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper704/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353118446, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lVniC5Y7", "replyto": "S1lVniC5Y7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper704/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper704/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper704/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353118446}}}, {"id": "HkgjYAaj3Q", "original": null, "number": 3, "cdate": 1541295746628, "ddate": null, "tcdate": 1541295746628, "tmdate": 1541533758767, "tddate": null, "forum": "S1lVniC5Y7", "replyto": "S1lVniC5Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper704/Official_Review", "content": {"title": "An interesting idea but experiments and writeup need improvement. ", "review": "A genetic algorithm is used to do an evolutionary architecture search to find better tree-like architectures with multiple memory cells and recurrent paths. To speed up search, an LSTM based seq2seq framework is also developed that can predict the final performance of the child model based on partial training results.\n\nThe algorithms and intuitions based on novelty search are interesting and there are improvements over baseline NAS model with the same architecture search space. \n\nAlthough, the experiments are not compared against latest architectures and best results. For example on PTB, there are new architectures such as those created by ENAS that result in much lower perplexity than best reported in Table 1, for the same parameter size. While you have mentioned ENAS in the related work, the lack of a comparison makes it hard to evaluate the true benefit if this work compared with existing literature. \n\nThere is no clear abolition study for the Meta-LSTM idea. Figure 4 provides some insights but it'd be good if some experiments were done to show clear wins over baseline methods that do not employ performance prediction.\n\nThere are many typos and missing reference in the paper that needs to be fixed.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper704/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Nodes to Networks: Evolving Recurrent Neural Networks", "abstract": "Gated recurrent networks such as those composed of Long Short-Term Memory\n(LSTM) nodes have recently been used to improve state of the art in many sequential\nprocessing tasks such as speech recognition and machine translation. However,\nthe basic structure of the LSTM node is essentially the same as when it was\nfirst conceived 25 years ago. Recently, evolutionary and reinforcement learning\nmechanisms have been employed to create new variations of this structure. This\npaper proposes a new method, evolution of a tree-based encoding of the gated\nmemory nodes, and shows that it makes it possible to explore new variations more\neffectively than other methods. The method discovers nodes with multiple recurrent\npaths and multiple memory cells, which lead to significant improvement in the\nstandard language modeling benchmark task. Remarkably, this node did not perform\nwell in another task, music modeling, but it was possible to evolve a different\nnode that did, demonstrating that the approach discovers customized structure for\neach task. The paper also shows how the search process can be speeded up by\ntraining an LSTM network to estimate performance of candidate structures, and\nby encouraging exploration of novel solutions. Thus, evolutionary design of complex\nneural network structures promises to improve performance of deep learning\narchitectures beyond human ability to do so.", "keywords": ["Recurrent neural networks", "evolutionary algorithms", "genetic programming"], "authorids": ["aditya@cs.utexas.edu", "jasonzliang@utexas.edu", "risto@cs.utexas.edu"], "authors": ["Aditya Rawal", "Jason Liang", "Risto Miikkulainen"], "TL;DR": "Genetic programming to evolve new recurrent nodes for language and music. Uses a LSTM model to predict the performance of the recurrent node. ", "pdf": "/pdf/5fa938d04617bb6f945d2558a6b1a7a17bd9264a.pdf", "paperhash": "rawal|from_nodes_to_networks_evolving_recurrent_neural_networks", "_bibtex": "@misc{\nrawal2019from,\ntitle={From Nodes to Networks: Evolving Recurrent Neural Networks},\nauthor={Aditya Rawal and Jason Liang and Risto Miikkulainen},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lVniC5Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper704/Official_Review", "cdate": 1542234398816, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lVniC5Y7", "replyto": "S1lVniC5Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper704/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335784560, "tmdate": 1552335784560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper704/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HylpXeaq3Q", "original": null, "number": 2, "cdate": 1541226533417, "ddate": null, "tcdate": 1541226533417, "tmdate": 1541533758490, "tddate": null, "forum": "S1lVniC5Y7", "replyto": "S1lVniC5Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper704/Official_Review", "content": {"title": "Interesting but not enough ", "review": "This paper explores evolutionary optimization for LSTM architecture search. To better explore the search space, authors used tree-based encoding and Genetic Programing (GP) with homologous crossover, tree distance metric, etc.  The search process is pretty simple and fast. However, there is a lack of experiments and analysis to show the effectiveness of the search algorithm and of the architecture founded by the approach. \n\nRemarks:\nThe contents provided in this paper is not enough to be convinced that this is a better approach for RNN architecture search and for sequence modeling tasks. \nThis paper requires more comparisons and analysis.\n\nExperiments on Penn Tree Bank\n - The dataset on both experiments are pretty small to know the effect of the new architecture they found. More experiments on larger datasets e.g., wikitext-2 will be needed. \n - In the paper \"On the state of the art of evaluation in neural language models\", Melis et al., 2018 reported improvement using classic LSTM over other variations of LSTM. They intensively compared the performance of classic LSTM, NAS, and RHN (Recurrent Highway Network) as authors did. Melis et al. reported LSTM (with depth 1) can already achieve a test perplexity of 59.6 with 10M parameters and 59.5 with 24M parameters.\n- Could you analyze a new finding of the LSTM architecture compared to the classic LSTM and NAS? Figure 5 and 6 are not very clear how are their final architectures different and the important/useful nodes changes for different tasks?\n- Recently, there are a number of architecture search algorithms introduced, but there is only one comparison in this direction (Zoph&Le16). It is important to compare this approach with other architecture search methods.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper704/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Nodes to Networks: Evolving Recurrent Neural Networks", "abstract": "Gated recurrent networks such as those composed of Long Short-Term Memory\n(LSTM) nodes have recently been used to improve state of the art in many sequential\nprocessing tasks such as speech recognition and machine translation. However,\nthe basic structure of the LSTM node is essentially the same as when it was\nfirst conceived 25 years ago. Recently, evolutionary and reinforcement learning\nmechanisms have been employed to create new variations of this structure. This\npaper proposes a new method, evolution of a tree-based encoding of the gated\nmemory nodes, and shows that it makes it possible to explore new variations more\neffectively than other methods. The method discovers nodes with multiple recurrent\npaths and multiple memory cells, which lead to significant improvement in the\nstandard language modeling benchmark task. Remarkably, this node did not perform\nwell in another task, music modeling, but it was possible to evolve a different\nnode that did, demonstrating that the approach discovers customized structure for\neach task. The paper also shows how the search process can be speeded up by\ntraining an LSTM network to estimate performance of candidate structures, and\nby encouraging exploration of novel solutions. Thus, evolutionary design of complex\nneural network structures promises to improve performance of deep learning\narchitectures beyond human ability to do so.", "keywords": ["Recurrent neural networks", "evolutionary algorithms", "genetic programming"], "authorids": ["aditya@cs.utexas.edu", "jasonzliang@utexas.edu", "risto@cs.utexas.edu"], "authors": ["Aditya Rawal", "Jason Liang", "Risto Miikkulainen"], "TL;DR": "Genetic programming to evolve new recurrent nodes for language and music. Uses a LSTM model to predict the performance of the recurrent node. ", "pdf": "/pdf/5fa938d04617bb6f945d2558a6b1a7a17bd9264a.pdf", "paperhash": "rawal|from_nodes_to_networks_evolving_recurrent_neural_networks", "_bibtex": "@misc{\nrawal2019from,\ntitle={From Nodes to Networks: Evolving Recurrent Neural Networks},\nauthor={Aditya Rawal and Jason Liang and Risto Miikkulainen},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lVniC5Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper704/Official_Review", "cdate": 1542234398816, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lVniC5Y7", "replyto": "S1lVniC5Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper704/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335784560, "tmdate": 1552335784560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper704/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1ge9p1bhX", "original": null, "number": 1, "cdate": 1540582792258, "ddate": null, "tcdate": 1540582792258, "tmdate": 1541533758290, "tddate": null, "forum": "S1lVniC5Y7", "replyto": "S1lVniC5Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper704/Official_Review", "content": {"title": "Few contributions to architecture search, limited comparison to relevant work", "review": "The authors apply (tree-based) genetic programming (GP) to RNN search, or more specifically RNNs with memory cells, with the foremost example of this being the LSTM. GP provide a structured search that seems appropriate for designing NN modules, and has previously been applied successfully to evolving CNNs. However, the authors fail to mention that (tree-based) GP has been applied to evolving RNN topologies as far back as 2 decades ago, with even multiple cells in a single RNN unit [1]. The selection of more advanced techniques is good though - use of Modi for allowing multiple outputs, and neat-GP for more effective search (though a reference to the \"hall of fame\" [2] is lacking).\n\nThe authors claim that their method finds more complex, better performing structures than NAS, but allow their method to find architectures with more depth (max 15 vs. the max 10 of NAS), so this is an unfair comparison. It may be the case that GP scales better than the RL-based NAS method, but this is an unfair comparison as the max depth of NAS is not in principle limited to 10.\n\nThe second contribution of allowing heterogeneity in the layers of the network is rather minimal, but OK. Certainly, GP probably would have an advantage when searching at this level, as compared to other methods (like NAS). Performance prediction in architecture search has been done before, as noted by the authors (but see also [3]), so the particular form of training an LSTM on partial validation curves is also a minor contribution. Thirdly, concepts of archives have been in use for a long time [2], and the comparison to novelty search, which optimises for a hand-engineered novelty criteria, reaches beyond what is necessary. There are methods based on archives, such as MAP-Elites [4], which would make for a fairer comparison. However, I realise that novelty search is better known in the wider ML community, so from that perspective it is reasonable to keep this comparison in as well.\n\nFinally, it is not surprising that GP applied to searching for an architecture for one task does not transfer well to another task - this is not specific to GP but ML methods in general, or more specifically any priors used and the training/testing scheme. That said, prior work has explicitly discussed problems with generalisation in GP [5].\n\n[1] Esparcia-Alcazar, A. I., & Sharman, K. (1997). Evolving recurrent neural network architectures by genetic programming. Genetic Programming, 89-94.\n[2] Rosin, C. D., & Belew, R. K. (1995, July). Methods for Competitive Co-Evolution: Finding Opponents Worth Beating. In ICGA (pp. 373-381).\n[3] Zhou, Y., & Diamos, G. (2018). Neural Architect: A Multi-objective Neural Architecture Search with Performance Prediction. In SysML.\n[4] Mouret, J. B., & Clune, J. (2015). Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909.\n[5] Kushchu, I. (2002). An evaluation of evolutionary generalisation in genetic programming. Artificial Intelligence Review, 18(1), 3-14.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Nodes to Networks: Evolving Recurrent Neural Networks", "abstract": "Gated recurrent networks such as those composed of Long Short-Term Memory\n(LSTM) nodes have recently been used to improve state of the art in many sequential\nprocessing tasks such as speech recognition and machine translation. However,\nthe basic structure of the LSTM node is essentially the same as when it was\nfirst conceived 25 years ago. Recently, evolutionary and reinforcement learning\nmechanisms have been employed to create new variations of this structure. This\npaper proposes a new method, evolution of a tree-based encoding of the gated\nmemory nodes, and shows that it makes it possible to explore new variations more\neffectively than other methods. The method discovers nodes with multiple recurrent\npaths and multiple memory cells, which lead to significant improvement in the\nstandard language modeling benchmark task. Remarkably, this node did not perform\nwell in another task, music modeling, but it was possible to evolve a different\nnode that did, demonstrating that the approach discovers customized structure for\neach task. The paper also shows how the search process can be speeded up by\ntraining an LSTM network to estimate performance of candidate structures, and\nby encouraging exploration of novel solutions. Thus, evolutionary design of complex\nneural network structures promises to improve performance of deep learning\narchitectures beyond human ability to do so.", "keywords": ["Recurrent neural networks", "evolutionary algorithms", "genetic programming"], "authorids": ["aditya@cs.utexas.edu", "jasonzliang@utexas.edu", "risto@cs.utexas.edu"], "authors": ["Aditya Rawal", "Jason Liang", "Risto Miikkulainen"], "TL;DR": "Genetic programming to evolve new recurrent nodes for language and music. Uses a LSTM model to predict the performance of the recurrent node. ", "pdf": "/pdf/5fa938d04617bb6f945d2558a6b1a7a17bd9264a.pdf", "paperhash": "rawal|from_nodes_to_networks_evolving_recurrent_neural_networks", "_bibtex": "@misc{\nrawal2019from,\ntitle={From Nodes to Networks: Evolving Recurrent Neural Networks},\nauthor={Aditya Rawal and Jason Liang and Risto Miikkulainen},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lVniC5Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper704/Official_Review", "cdate": 1542234398816, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lVniC5Y7", "replyto": "S1lVniC5Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper704/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335784560, "tmdate": 1552335784560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper704/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}