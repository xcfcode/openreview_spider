{"notes": [{"id": "kic8cng35wX", "original": "5Bz0ZoVNzvw", "number": 167, "cdate": 1601308027273, "ddate": null, "tcdate": 1601308027273, "tmdate": 1614985705532, "tddate": null, "forum": "kic8cng35wX", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BCezA2BoqE", "original": null, "number": 1, "cdate": 1610040438182, "ddate": null, "tcdate": 1610040438182, "tmdate": 1610474038976, "tddate": null, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "In line with recent work in the NAS literature, the authors consider a weak NAS performance strategy to filter out bad architectures and narrow down the exploration to the most promising region of the search space. The authors propose to estimate weak predictors progressively by learning a series of weak predictors that can connect towards the best architectures. The authors provided a number of additional experiments during rebuttal, addressing most of the reviewers' comments convincingly and further showing the strong performance of their method. However, the authors should relate their work to Bayesian optimization, which comes in many flavors, and black-box optimization techniques in general as their work shows a number of similarities, but is less principled."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"forum": "kic8cng35wX", "replyto": "kic8cng35wX", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040438169, "tmdate": 1610474038960, "id": "ICLR.cc/2021/Conference/Paper167/-/Decision"}}}, {"id": "_Dw94RpI7SE", "original": null, "number": 4, "cdate": 1603932823342, "ddate": null, "tcdate": 1603932823342, "tmdate": 1606798158542, "tddate": null, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Review", "content": {"title": "Interesting approach", "review": "Summary of contribution: The authors propose an interesting approach to address the sample-efficiency issue in Neural Architecture Search (NAS). Compared to other existing predictor based methods, the approach distinguishes itself by progressive shrinking the search space. The paper correctly identifies the sampling is an important aspect in using a predictor based NAS method;\n\nThe writing is clear and easy to follow. The author provided an explanation of how their algorithm works, evaluate the algorithm on both NASBench-101 and NASBench-201, and show their methods work in practice on both\n\nSome critiques:\n\n1. Questions on predictor methods:\n\nI understand the NAS community has accepted many predictor based paper in major conferences, but it seems NAS is re-making the wheel in derivative-free optimizations, e.g. Bayesian Optimization, Evolutionary Algorithm, and MCTS. The predictor is essentially the surrogate model in Bayesian Optimizations, however, the current predictor methods simply ignore the acquisition function in BO that makes the trade-off between exploration and exploitation. These predictor paper in NAS basically suggest the acquisition functions are not necessary, and we can achieve great performance without that (e.g. on NASBench, I will come to this later). Apparently this predictor trend in NAS community is against the decades of development of Bayesian Optimization. If the authors believe the development in BO community is wrong, please justify that using acquisition is indeed not important, i.e. exploration is not necessary, with extensive experiments.\n\nIf the new experiment can persuade me, I believe this paper can be top 1% paper, and I will argue for accepting it.\n\nHere I'd like to list a set of predictor based paper that I have seen in the NAS community: \n\n[1] Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS, NeurIPS-2020 \n\n[2] Brp-nas: Prediction-based nas using gcns, NeurIPS-2020 \n\n[3] Neural predictor for neural architecture search, ECCV-2020\n\nI understand these paper have been accepted but do not necessarily mean these approaches expand the frontier of knowledge. \n\n2. Evaluations on NASBench. \n\nThe reason why predictor works well on NASBench simply because it can predict every architectures. The largest dataset has 4.2*10^5 architectures, it won't be hard to predict them all. However, I believe the original NASBench paper has set baselines for us to compare (though currently published paper fail to follow these baselines and setup). I strongly encourage the authors to take look at the following repository, to see how original NASBench-101 setup the comparisons. I admit the design of NASBench might have some glitch in evaluations, but it is important to follow the same standard.\nhttps://github.com/automl/nas_benchmarks\n\n3. Evaluations on ImageNet. \n\nGetting a good results on CIFAR10 and ImageNet can be tricky. Given the current ImageNet accuracy is 76.4@597 MFLOPS, while the SoTA top1 accuracy for a 600 MFLOPS model is 80.8. I understand these models use different search spaces, but the results on CIFAR-10 is missing. There are too many tricks to hack the network accuracy on ImageNet. Therefore, it will be better to judge NASNet search space on CIFAR-10.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper167/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148951, "tmdate": 1606915783258, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper167/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Review"}}}, {"id": "7zWxevh-hjn", "original": null, "number": 8, "cdate": 1606121354352, "ddate": null, "tcdate": 1606121354352, "tmdate": 1606282931271, "tddate": null, "forum": "kic8cng35wX", "replyto": "kUFXa0Os92c", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to Reviewer3 (part1/2)", "comment": "We appreciate your time and efforts in reviewing our paper!\n\n*Q1: Incremental improvement to the neural predictor. It gets a small performance improvement by learning in a maybe better subspace.*\n\nOur contribution in nontrivial compared with existing neural predictors since these tend to model the whole architecture space. In contrast, our key insight is that \"We not do need to model the whole architecture space, if the goal of NAS is finding the best architecture(s) in the search space\", which then motivated the proposed weak predictors. We believe that it is **important** to share this observation to the community. \nBased on this key observation, we are the first to propose to progressively learn a series of weak predictors to efficiently sample a path to the best architectures. As shown in Figure 3 of the main paper, the probability of sampling better architectures keeps increasing, thus in each iteration we only need to sample a few architectures, which largely improves the efficiency.\n\nIn the experiment, we also demonstrate that it is crucial for predictor-based NAS to learn better subspace. As shown in Table 1, to achieve the same Test Accuracy to existing predictor based NAS SOTA methods (e.g., Neural Predictor), the samples required in our method can be ~5.71X less. \n\nTable 1: Comparison to SOTA on NAS-Bench-101 by limiting the total amount of queries\n\n| Method   |      \\#Queries      |  Test Acc.(\\%) |  Std(\\%) |  Test Regret(\\%) |\n|----------|:-------------:|------:|------:|------:|\n| Random Search | 2000 | 93.64 | 0.25 | 0.68\n| NAO  | 2000 | 93.90 | 0.03 | 0.42\n| Reg Evolution  | 2000 | 93.96 | 0.05 | 0.36\n| Neural Predictor  | **2000** | **94.04** | **0.05** | **0.28**\n| Ours | **350** | **94.04** | **0.13** | **0.28**\n\n*Q2: Is it effective to simply choose TopK models? As the author said, the learning space is not convex thus TopK samples may contain many local-optima and make the learning of predictor difficult even failure (all the predicted Accs fall within a small range).*\n\nThis is a really insightful question. In our method, we sample a fix amount (10/100) of models in Top-K (K=100/1000) models instead of choosing all of Top K models to alleviate the effect of noisy (local-optima) samples. In the following Table 2, We did an ablation study to know whether the sampling is indispensable. We set the budget to 2000 Queries. For the experiment without sampling, we sample 100 samples out of Top 100/1000 models for 20 iterations. For the experiment with sampling, we sample 100 sample out of Top 1000 models for 20 iterations. We find that the latter (w/ sampling) achieves slightly better performance with lower variance between runs than the former (w/o sampling).\n\nTable 2: Comparison to without/with sampling in TopK models on NAS-Bench-101 over 50 runs.\n\n| Method   |      \\#Queries      |  Test Acc.(\\%) |  Std(\\%) |  Test Regret(\\%) |\n|:---------|:-------------:|------:|------:|------:|\n| Ours (Top 100, w/o sampling) | 2000 | 94.1739 | 0.038 | 0.1461 |\n| Ours (Sample 100 out of Top 1000, w/ sampling) | 2000 | 94.2203 | 0.028 | 0.0997 |\n\n*Q3: In Fig. 1, why is there a sudden increase for the strong predictor (at near 15625 samples)*\n\nIn Figure 1 of our paper, the sudden increase at the end indicates that there is a gap for strong predictor to find the optimal architecture, since a single strong predictor cannot extrapolate well on unseen well-performing architectures. This motivates us to use a progressive line of weak predictors to bridge this gap, which helps us connect a path to the best architecture.\n\n*Q4: What if the search space is huge, like 10^20 in many SOTA NAS search space, rather than the small models in NAS-Benchs, the proposed method seems inefficient in this case. Will it still work?*\n\nYes, our method works well for a larger search space. Table 4 of our paper shows our result on NASNet ImageNet search space with 10^25 architectures. Instead of evaluating the performance of all 10^25 architectures at every iteration, we only need to sample 1000 models per iteration for evaluation and re-ranking. Based on this setting, we can achieve SOTA NASNet search space performance of 76.4\\% with 597 MFLOPs budget without bells and whistles."}, "signatures": ["ICLR.cc/2021/Conference/Paper167/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kic8cng35wX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper167/Authors|ICLR.cc/2021/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873909, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment"}}}, {"id": "R-dJ2-8VDay", "original": null, "number": 5, "cdate": 1606121008464, "ddate": null, "tcdate": 1606121008464, "tmdate": 1606243752609, "tddate": null, "forum": "kic8cng35wX", "replyto": "_Dw94RpI7SE", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to Reviewer1 (part2/2)", "comment": "*Q2: Our method does not follow the same guideline used in NASBench-101?*\n\nWe exactly follow the guideline described in the Section 2.6 and Supplement Section 6 of the NASBench-101 [4] paper\n\n- \"Repeatedly query the dataset at $(A, E_{stop})$ pairs to train our NAS predictor, where A is an architecture in the search space and $E_{stop}$ is an allowed number of epochs ($E_{stop} \\in \\{4, 12, 36, 108\\}$). Each query does a look-up using a random trial index, drawn uniformly at random from $\\{1, 2, 3\\}$, to simulate the stochasticity of SGD training.\"\n\nIn our paper, we followed the exact setting. For each sample fed into our weak predictors, we queried a random trial index in the look-up using $E_{stop}$=108 epoch.\n\n- \"Perform many runs of the various NAS algorithms.\"  \n\nIn Table 1/2/3 and Figure 4/5/6/7 of our paper, We performed 250 runs of various NAS algorithms and reported the mean and variance.\n\n- \"Plot performance as a function of estimated wall clock time and/or number of function evaluations. This allows judging the performance of algorithms under different resource constraints.\"  \n\nIn Table 1/2/3/4 and Figure 4/5/6/7 of our paper, We plotted performance as a function of number of evaluations calls (number of samples). \n\n- \"Do not use test set error during the architecture search process. In particular, the choice of the best architecture found up to each time step can only be based on the training and validation sets. The test error can only be used for offline evaluation once the search runs are complete.\"  \n\nIn our paper, we did not use test error until search is completed, details are described in Section 3.1.\n\nIn summary, we think we have strictly followed the guideline in NASBench-101.\n\n*Q3: There are too many tricks to hack the accuracy on ImageNet, request evaluations on NASNet search space with CIFAR-10*\n\nIn our ImageNet experiments, we followed standard Imagenet training [5], and **did not use any tricks** (e.g., MixUp, ZeroGamma, NoBiasDecay, DropPath, DropPath, EMA, etc). Based on the standard training, we still achieve SoTA performance on ImageNet NASNet search space with 23.6% Top-1 Error, shown in Table 4 in the paper. We will release the code upon acceptance.\n\nWe further evaluate different methods on the CIFAR-10 dataset. Here, all methods including ours do not use Cutout for fair comparison, since Cutout is known as a useful trick to impact the performance on CIFAR-10. As shown in the Table 1, our method obtains the Top-1 Error of 3.42% at the cost of 4.3M Params, which is better than most of the SoTA methods. Although it is marginally worse than AmoebaNet-A and PNAS, our search time is **12,600X and 900X faster** than theirs respectively.\n\nTable 1. Compared to SOTA results on CIFAR-10 by using NASNet search space\n\n| Method         | \\#Queries | Error (\\%) |  # Params |  GPU Days |\n|----------      |:-----------:|------:|------:|------:|\n| Random-WS      | -    | 3.92 | 3.9M | 0.25\n| AmoebaNet-A    | 20000| 3.34 | 3.2M | 3150.00\n| PNAS           | -    | 3.41 | 3.2M | 225.00\n| ENAS           | -    | 3.54 | 4.6M | 0.45\n| NAO (Weight Sharing)         | -    | 3.53 | 2.5M | 0.30\n| Ours (Weight Sharing)        | -    | 3.42 | 4.3M | 0.25\n\n[4] \"Nas-bench-101: Towards reproducible neural architecture search.\" ICML 2019.  \n[5] https://github.com/pytorch/examples/tree/master/imagenet"}, "signatures": ["ICLR.cc/2021/Conference/Paper167/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kic8cng35wX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper167/Authors|ICLR.cc/2021/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873909, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment"}}}, {"id": "TX_NeiEu-Jq", "original": null, "number": 6, "cdate": 1606121044741, "ddate": null, "tcdate": 1606121044741, "tmdate": 1606243688525, "tddate": null, "forum": "kic8cng35wX", "replyto": "_Dw94RpI7SE", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to Reviewer1 (part1/2)", "comment": "We appreciate your time and efforts in reviewing our paper!\n\n*Q1: Predictor-based NAS is re-making the wheel in derivative-free optimizations, such as Bayesian Optimization. The NAS predictor is essentially the surrogate model in Bayesian Optimizations, but it does not have an acquisition function thus lacks exploration of new architectures*\n\n* \"Connection to Bayesian Optimization.\"\n\nInterpreting our Weak Predictor NAS as Bayesian Optimization (BO) is an interesting angle, but we respectfully suggest that might not represent the fully accurate picture. Please kindly allow us to explain:\n\n- Our current approach does not rely on any explicit uncertainty-based modeling such as Gaussian Process, mainly motivated by reducing the search cost (uncertainty modeling is heavier). However, our approach is not free of exploration \u2013 just that we have a different, simpler build-in exploration mechanism.\n\n- Specifically, at each iteration, instead of choosing all of Top K models by the predictor, we sample a fix amount N models in Top-K to explore new architectures in a probabilistic manner. Hence, an exploitation-exploration trade-off can be performed here, by adjusting the K to N ratio, and by varying the sampling strategy (follow uniform, linear or exponential distribution). Moreover, note that the weak predictor's fitting ability also gradually grows along with the search process, which implicitly introduces another source of possible \"exploration\" - and how well the predictor is fit can implicitly calibrate the balance between exploration and exploitation too. More details please see below.\n\nWe find our simplified exploration in this current setting works well for this application, presumably due to the specially structured NAS search space in current use. Specifically:\n\n- Overall, NAS space has a highly non-uniform distribution of architectures, whose density is also heterogenous (although a precise definition of architecture distribution or density hinges on the concrete definition of distance). That is because the candidate architectures were created from varying operators (often causing similar performers), to varying width or depth (often causing bigger performance gaps). Therefore, the architectures display highly clustered distribution (as can be visualized by histogram on NAS-Bench-101/201 in Figure 8 of Supplementary); and the best performers are often gathered close to each other. This is also our work\u2019s underlying assumption: we can progressively connect the line from the initialization to the finest subspace, where the best architecture resides.\n\n- Due to the non-unform or heterogenous assumption, the demand for the exploitation-exploration trade-off is also heterogenous across different search stages. At the beginning stage, more exploration will be needed to identify the promising direction towards the finer space. That is naturally achieved because the weak predictor at the initial stage only roughly fits the whole space (in other words, it CANNOT fit too well by then, and we intentionally make so). As we keep zooming-in to the good-performing subspace, the weak predictor is also gradually refined and better fit, therefore making stronger exploitation. Eventually, our weak NAS predictor is made to adaptively balance the exploration and exploitation throughout the search process, which echos the findings by many prior works such as [1][2][3].\n\nTo summarize in brief: (1) our method does not take a precise BO form nor was it motivated that day, but it follows a similar spirit; (2) we use a different and simpler exploration mechanism, due to the specially structured NAS space; (3) we actually benefit from adaptively balancing exploration and exploitation along with search, in an implicit way.\n\nWe are happy to include the above discussions into our revised paper. We sincerely appreciate the reviewer for his/her very insightful comments.\n\n[1] \"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search.\" AAAI 2019.  \n[2] \"Sample-efficient neural architecture search by learning action space.\" arXiv 2019.  \n[3] \"Revisiting Neural Architecture Search.\" arXiv 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper167/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kic8cng35wX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper167/Authors|ICLR.cc/2021/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873909, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment"}}}, {"id": "UOkWY9-2J2", "original": null, "number": 4, "cdate": 1606115949658, "ddate": null, "tcdate": 1606115949658, "tmdate": 1606121612386, "tddate": null, "forum": "kic8cng35wX", "replyto": "xptymNNtxpa", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "We appreciate your time and efforts in reviewing our paper!\n\n*Q1: Our method does not take into account of memory and computational cost constrains.*\n\nWe can easily integrate above two constraints into the sampling stage. Instead of sampling from the whole search space, we only sample new architectures that meet specific memory and FLOPs requirements.\n\n*Q2: Comparison with Differentiable-NAS (Gradient-based NAS) and DSNAS in terms of efficiency.*\n\nIt's not intuitive to directly compare non-differentiable (sample-based) NAS with differentiable (gradient-based) NAS since these are two different streams in the NAS literatures. The latter usually has worse performance but is more efficient since they do not need to train each architecture from scratch. To give the readers a boarder context, we added the comparison with SOTA differentiable NAS[1][2][3][4] in the following Table 5. In addition, we include the results of DSNAS [5]. As we can see in Table 5, our method and sample-based NAS achieve higher performance than gradient-based NAS methods. In terms of efficiency, since we only use the accuracy derived from the weight-sharing supernet to train our NAS predictor, our weak NAS predictor still has a comparable efficiency to gradient-based NAS.\n\nTable 1: Comparison results to more SOTA methods on ImageNet by using NASNet search space.\n\n| Model | Methods|\\#Queries|Top-1 Err.(\\%)|Top-5 Err.(\\%)|Params(M)|FLOPs(M)|GPU Days|\n|----|:----:|----:|----:|----:|----:|----:|----:|\n| MobileNetV2  | Manual | -  | 25.3 |  -  | 6.9 | 585 | - |\n| ShuffletNetV2 | Manual | - | 25.1 |  -  | 5.0 | 591 | - |\n| EfficientNet-B0 | Manual | - | 23.7 | 6.8 | 5.3 | 390 | - |\n| SNAS[1] | Gradient-based | - | 27.3 | 9.2 | 4.3 | 522 | 1.5 |\n| DARTS[2] | Gradient-based | - | 26.9 | 9.0 | 4.9 | 595 | 4.0 |\n| P-DARTS[3] | Gradient-based | - | 24.4 | 7.4 | 4.9 | 557 | 0.3 |\n| PC-DARTS[4] | Gradient-based | - | 24.2 | 7.3 | 5.3 | 597 | 3.8 |\n| DS-NAS[5] | Gradient-based | - | 24.2 | 7.3 | 5.3 | 597 | 10.4 |\n| NASNet-A | Sample-based | 20000 | 26.0 | 8.4 | 5.3 | 564 | 2000 |\n| AmoebaNet-A  | Sample-based | 10000 | 25.5 | 8.0 | 5.1 | 555 | 3150 |\n| PNAS         | Sample-based | 1160 | 25.8 | 8.1 | 5.1 | 588 | 200 |\n| NAO  | Sample-based | 1000 | 24.5 | 7.8 | 6.5 | 590 | 200 |\n| Ours | Sample-based + Weight Sharing| 1000 | **23.6** | **6.8** | **5.9** | 597 | 1.08 |\n\n*Q3: The setup and hardware used to evaluate the approach is not reported*\n\nWe conduct all experiment on Nvidia Tesla P100 GPUs with 16GB VRAM.\n\n*Q4: Typos and grammatical mistakes*\n\nThanks for pointing out, we will correct them in the camera-ready manuscript.\n\n[1] Xie, Sirui, et al. \"SNAS: stochastic neural architecture search.\" arXiv preprint arXiv:1812.09926 (2018).  \n[2] Liu, Hanxiao, et al. \"Darts: Differentiable architecture search.\" arXiv preprint arXiv:1806.09055 (2018).  \n[3] Chen, Xin, et al. \"Progressive differentiable architecture search: Bridging the depth gap between search and evaluation.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.  \n[4] Xu, Yuhui, et al. \"Pc-darts: Partial channel connections for memory-efficient differentiable architecture search.\" arXiv preprint arXiv:1907.05737 (2019).  \n[5] Hu, Shoukang, et al. \"DSNAS: Direct Neural Architecture Search without Parameter Retraining.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper167/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kic8cng35wX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper167/Authors|ICLR.cc/2021/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873909, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment"}}}, {"id": "_LIKbQt2Jg1", "original": null, "number": 7, "cdate": 1606121335802, "ddate": null, "tcdate": 1606121335802, "tmdate": 1606121335802, "tddate": null, "forum": "kic8cng35wX", "replyto": "kUFXa0Os92c", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to Reviewer3 (part2/2)", "comment": "*Q5: What's the difference and advantages between the proposed model, and choosing the top 10\\% of the models and training a single predictor?*\n\nWe followed the setting and conducted the experiment in Table 3. We randomly sampled the Top 10%/20%/50% or all sampled models to train a single predictor, and we found all of these ways performed worse than our method. It demonstrates that training a single predictor is difficult and sub-optimal because the NAS search space is very complex. It also evidences our key idea of using a series of weak predictors is not trivial at all.\n\nTable 3: Comparison to the simple baselines by training a single predictor with TopX% sample on NAS-Bench-101 over 50 runs\n\n| Method   |      \\#Queries      |  Test Acc.(\\%) |  Std(\\%) |  Test Regret(\\%) |\n|----------|:-------------:|------:|------:|------:|\n| Random Search | 2000 | 93.64 | 0.25 | 0.68\n| NAO  | 2000 | 93.90 | 0.03 | 0.42\n| Reg Evolution  | 2000 | 93.96 | 0.05 | 0.36\n| Neural Predictor  | 2000 | 94.04 | 0.05 | 0.28\n| Ours (Single Predictor) (Top 10%) | 2000 | 93.65 | 0.025 | 0.67\n| Ours (Single Predictor) (Top 20%) | 2000 | 93.67 | 0.031 | 0.65\n| Ours (Single Predictor) (Top 50%) | 2000 | 93.67 | 0.023 | 0.65\n| Ours (Single Predictor) (All) | 2000 | 93.92 | 0.082 | 0.04\n| Ours (20 Predictors) | 2000 | 94.22 | 0.028 | 0.10"}, "signatures": ["ICLR.cc/2021/Conference/Paper167/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kic8cng35wX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper167/Authors|ICLR.cc/2021/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873909, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment"}}}, {"id": "Liejlv5umSA", "original": null, "number": 3, "cdate": 1606115585963, "ddate": null, "tcdate": 1606115585963, "tmdate": 1606115585963, "tddate": null, "forum": "kic8cng35wX", "replyto": "flTv2z2U-bl", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "*Q1: Comparison with DARTS*\n\nSample-based NAS (our method) and gradient-based NAS (DARTS) are two different streams in the NAS field, the latter usually has worse performance and but is more efficient since they do not need to train each architecture from scratch. To give the reader a boarder context, we added the comparison with SoTA methods in Differentiable NAS[1][2][3][4] in the following Table 1. Additionally, we also cite the result of DSNAS[5]. As we can see, our method and sample-based NAS usually achieve higher performance than gradient-based NAS methods. Moreover, regarding the efficiency, since we only use the accuracy derived from weight-sharing supernet to train our NAS predictor, our weak NAS predictor has a comparable efficiency to gradient-based NAS.\n\nTable 1: Compared to SOTA results on ImageNet using NASNet search space.\n\n| Model | Methods|\\#Queries|Top1 Err.(\\%)|Top5 Err.(\\%)|Params(M)|FLOPs(M)|GPU Days|\n|----|:----:|----:|----:|----:|----:|----:|----:|\n| MobileNetV2  | Manual | -  | 25.3 |  -  | 6.9 | 585 | - |\n| ShuffletNetV2 | Manual | - | 25.1 |  -  | 5.0 | 591 | - |\n| EfficientNet-B0 | Manual | - |   23.7 | 6.8 | 5.3 | 390 | - |\n| SNAS[1] | Gradient-based | - | 27.3 | 9.2 | 4.3 | 522 | 1.5 |\n| DARTS[2] | Gradient-based | - | 26.9 | 9.0 | 4.9 | 595 | 4.0 |\n| P-DARTS[3] | Gradient-based | - | 24.4 | 7.4 | 4.9 | 557 | 0.3 |\n| PC-DARTS[4] | Gradient-based | - | 24.2 | 7.3 | 5.3 | 597 | 3.8 |\n| DS-NAS[5] | Gradient-based | - | 24.2 | 7.3 | 5.3 | 597 | 10.4 |\n| NASNet-A | Sample-based | 20000 | 26.0 | 8.4 | 5.3 | 564 | 2000 |\n| AmoebaNet-A  | Sample-based | 10000 | 25.5 | 8.0 | 5.1 | 555 | 3150 |\n| PNAS         | Sample-based | 1160 | 25.8 | 8.1 | 5.1 | 588 | 200 |\n| NAO  | Sample-based | 1000 | 24.5 | 7.8 | 6.5 | 590 | 200 |\n| Ours | Sample-based + Weight Sharing| 1000 | **23.6** | **6.8** | **5.9** | 597 | 1.08 |\n\n*Q2: For each query, do we need to train the model till convergence?*\n\nIn all our experiment, we need to train each model till convergence.\nHowever, we found that our method still works if each query is trained for fewer epochs. As we can see in the following Table 2, training for 180 epochs can achieve similar performance as a fully trained model (200 epoch), but if we further decrease the training to 150/160 epochs, it will hurt the final performance dramatically due to the bad ranking of early stopping models.\n\nTable 2: Ablation on Training Epoch on NASBench-201 over 50 runs (CIFAR100)\n\n| Method   |      \\#Queries      |  Test Acc.(\\%) |  Std(\\%) |\n|----------|:-------------:|:------:|:------:|\n| Ours (150 Epochs) | 200 | 72.57 (-0.86) | 0.48 |\n| Ours (160 Epochs) | 200 | 72.34 (-1.09) | 0.42 |\n| Ours (170 Epochs) | 200 | 72.67 (-0.76) | 0.38 |\n| Ours (180 Epochs) | 200 | 73.35 (-0.08) | 0.15 |\n| Ours (190 Epochs) | 200 | 73.39 (-0.04) | 0.13 |\n| Ours (200 Epochs) (Fully-Trained) | 200 | 73.43 (Baseline) | 0.12 |\n\n*Q3: How much time it takes to yield a good model (i.e., time for 1000 queries)?*\n\nIn the ImageNet experiment, we only use accuracies derived from the supernet. In particular, we only evaluate a sample (i.e., a sub-network in the supernet) on a small subset of the test set, which takes around 6-7s. In total, evaluating 1000 samples takes around 1.6 hrs.\n\n[1] Xie, Sirui, et al. \"SNAS: stochastic neural architecture search.\" arXiv preprint arXiv:1812.09926 (2018).  \n[2] Liu, Hanxiao, et al. \"Darts: Differentiable architecture search.\" arXiv preprint arXiv:1806.09055 (2018).  \n[3] Chen, Xin, et al. \"Progressive differentiable architecture search: Bridging the depth gap between search and evaluation.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.  \n[4] Xu, Yuhui, et al. \"Pc-darts: Partial channel connections for memory-efficient differentiable architecture search.\" arXiv preprint arXiv:1907.05737 (2019).  \n[5] Hu, Shoukang, et al. \"DSNAS: Direct Neural Architecture Search without Parameter Retraining.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper167/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kic8cng35wX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper167/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper167/Authors|ICLR.cc/2021/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873909, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Comment"}}}, {"id": "kUFXa0Os92c", "original": null, "number": 1, "cdate": 1603568110335, "ddate": null, "tcdate": 1603568110335, "tmdate": 1605024749193, "tddate": null, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Review", "content": {"title": "This work is more like an incremental improvement to the neural predictor. It gets a small performance improvement by learning in a better subspace.", "review": "Pros:\n- A progressive method for neural predictor-based NAS is proposed, and it shows better performance than previous methods.\n- Experiments on the NAS-bench-101 and NAS-bench-201, as well as on the NASNet search space to validate the effectiveness of the method.\n- The proposed method is conceptually simple and efficient.\n\nCons:\n- This work is more like an incremental improvement to the neural predictor. It gets a small performance improvement by learning in a maybe better subspace.\n- Is it effective to simply choose TopK models? As the author said, the learning space is not convex thus TopK samples may contain many local-optima and make the learning of predictor difficult even failure (all the predicted Accs fall within a small range).\n- In Fig. 1, why is there a sudden increase for the strong predictor (at near 15625 samples.)\n- What if the search space is huge, like 10^20 in many SOTA NAS search space, rather than the small models in NAS-Benchs, the proposed method seems inefficient in this case. Will it still work?\n- What's the difference and advantages between the proposed model, and choosing the top 10% of the models and training a single predictor?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper167/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148951, "tmdate": 1606915783258, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper167/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Review"}}}, {"id": "xptymNNtxpa", "original": null, "number": 3, "cdate": 1603836501265, "ddate": null, "tcdate": 1603836501265, "tmdate": 1605024749130, "tddate": null, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Review", "content": {"title": "This paper deals with a common bottleneck in neural search approches which is the prediction of performance of the potential sampled architectures. It proposes to learn weak predictors on a limited sample rather than model the performance over the whole architecture space. The experiments demonstrates that the proposed methods can reach a top performing architecture with fewer samples. ", "review": "Overall this work moves into the right direction of trying to improve the performance of predictors. \n\nPros:\n\n- The paper provides experiments on different datasets, and evaluates different predictors to validate the approach. \n\n- The approach seems to have a significant speedup on the search time, but I would also like to see the results in terms of GPU days which is a common metric. \n\nCons:\n\n- In a space where there is rapid progress and accuracy alone is not a practical metric for real-world applications since neural networks are used widely today from data centers to mobile devices. More effort should be towards this multi-objective problems considering also memory and computational cost. In this respect I find this a weakness of the paper in that it does not address these factors which would make the problem much more interesting, even though the proposed approach is appealing. \n\n- A broader comparison should be made with DARTs approaches that have been shown to be generally faster. Also how does the approach compare with DSNAS: Direct Neural Architecture Search without Parameter Retraining in terms of efficiency?\n\n- The setup and hardware used to evaluate the approach is not reported. \n\n- Please correct for typos and grammatical mistakes, e.g., end of page 2 \"the the loss\", page 5 \"our method can performs\"\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper167/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148951, "tmdate": 1606915783258, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper167/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Review"}}}, {"id": "flTv2z2U-bl", "original": null, "number": 2, "cdate": 1603684029188, "ddate": null, "tcdate": 1603684029188, "tmdate": 1605024749067, "tddate": null, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "invitation": "ICLR.cc/2021/Conference/Paper167/-/Official_Review", "content": {"title": "An interesting and inspirational paper", "review": "The paper proposes an idea of jointly optimizing the sampling policy and predictor in NAS. With this method, we avoid to train a predictor which performs well on the whole search space and search a model with less queries.\n\n### advantages\nThe authors propose that training a good accuracy predictor for models in the whole search space is difficult. Thus, the paper focuses on a predictor that works in a small range, then optimizes the sampling policy as well as the predictor to yield a good structure.\n\nCompared with some evolution-based and RL-based methods, the algorithm observes a better model with less queries.\n\nTotally speaking, the idea is very interesting and it does ease the search process. \n\n### Weaknesses\nA weakness of the paper is that it does not compare with differentiable methods such as DARTS. It is well known that differentiable methods are nearly the fastest methods now and yield not bad results.\n\nAs the results show, the algorithm finds a good model on ImageNet with 1000 queries, which seems to take more time than differentiable methods.\n\n### Question\n1. For each query, do we need to train the model till convergence?\n2. How much time it takes to yield a good model (i.e., time for 1000 queries)?\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper167/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper167/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weak NAS Predictor Is All You Need", "authorids": ["~Junru_Wu2", "~Xiyang_Dai2", "~Dongdong_Chen1", "~Yinpeng_Chen1", "~Mengchen_Liu2", "~Ye_Yu2", "~Zhangyang_Wang1", "~Zicheng_Liu1", "~Mei_Chen2", "~Lu_Yuan1"], "authors": ["Junru Wu", "Xiyang Dai", "Dongdong Chen", "Yinpeng Chen", "Mengchen Liu", "Ye Yu", "Zhangyang Wang", "Zicheng Liu", "Mei Chen", "Lu Yuan"], "keywords": ["performance predictor", "neural architecture search"], "abstract": "Neural Architecture Search (NAS) finds the best network architecture by exploring the architecture-to-performance manifold. It often trains and evaluates a large amount of architectures, causing tremendous computation cost. Recent predictor-based NAS approaches attempt to solve this problem with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Existing predictors attempt to model the performance distribution over the whole architecture space, which could be too challenging given limited samples. Instead, we envision that this ambitious goal may not be necessary if the final aim is to find the best architecture. We present a novel framework to estimate weak predictors progressively. Rather than expecting a single strong predictor to model the whole space, we seek a progressive line of weak predictors that can connect a path to the best architecture, thus greatly simplifying the learning task of each predictor. It is based on the key property of the predictors that their probabilities of sampling better architectures will keep increasing. We thus only sample a few well-performed architectures guided by the predictive model, to estimate another better weak predictor. By this coarse-to-fine iteration, the ranking of sampling space is refined gradually, which helps find the optimal architectures eventually. Experiments demonstrate that our method costs fewer samples to find the top-performance architectures on NAS-Bench-101 and NAS-Bench-201, and it achieves the state-of-the-art ImageNet performance on the NASNet search space.", "one-sentence_summary": "We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|weak_nas_predictor_is_all_you_need", "pdf": "/pdf/c85896e7f611529de5599d18c080791a16e600a7.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tYavBqz0m", "_bibtex": "@misc{\nwu2021weak,\ntitle={Weak {\\{}NAS{\\}} Predictor Is All You Need},\nauthor={Junru Wu and Xiyang Dai and Dongdong Chen and Yinpeng Chen and Mengchen Liu and Ye Yu and Zhangyang Wang and Zicheng Liu and Mei Chen and Lu Yuan},\nyear={2021},\nurl={https://openreview.net/forum?id=kic8cng35wX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kic8cng35wX", "replyto": "kic8cng35wX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148951, "tmdate": 1606915783258, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper167/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper167/-/Official_Review"}}}], "count": 12}