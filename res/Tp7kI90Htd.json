{"notes": [{"id": "Tp7kI90Htd", "original": "p4qsaIoVsSz", "number": 146, "cdate": 1601308024992, "ddate": null, "tcdate": 1601308024992, "tmdate": 1615982772601, "tddate": null, "forum": "Tp7kI90Htd", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WcDEMY3vSu6", "original": null, "number": 1, "cdate": 1610040427313, "ddate": null, "tcdate": 1610040427313, "tmdate": 1610474026928, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "This paper has received four positive reviews. The main intellectual contribution of the paper is the introduction of a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning across neurons and even across animals. The reviewers commented on the technical strength of the paper. At the same time, the main contribution remains relatively incremental from a technical standpoint, and while the approach may be of value to future work, the impact of the current study on neuroscience (which is the target here) is quite limited. Nonetheless, there seems to be sufficient enthusiasm from the reviewers to recommend this paper be accepted."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040427300, "tmdate": 1610474026911, "id": "ICLR.cc/2021/Conference/Paper146/-/Decision"}}}, {"id": "Z-56obrxYg", "original": null, "number": 16, "cdate": 1606165549594, "ddate": null, "tcdate": 1606165549594, "tmdate": 1606165846341, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "BvTAyPeetGw", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment", "content": {"title": "Changes to the paper addressing your suggestions ", "comment": "Thank you for your positive feedback and suggestions on how to further improve it. We agree with you on both your issues:\n\n1. Thank you for pointing us towards this interesting paper for an unbiased estimation of FVE. We included an equivalent figure to our Fig. 5 with this measure in the appendix of the updated paper. In short: our data passes the threshold for a stable computation of the unbiased FVE and the relative ordering of all the curves are preserved, corroborating our results. Note though, that the calculation of the unbiased FVE assumes that the data is variance stabilized. However, in order to properly do this, we would have needed to retrain the models on the variance stabilized data which we had not done during the experiments and could not add during the constrained time of this rebuttal period. For the sake of the comparison, however, we still added the comparison with this metric for our most important results to the appendix. \n\n2. We are aware of the shortcomings of calcium imaging and it might be that multi-spike events which are detected as single spikes could pose a problem. Note, however that we use \u201cdeconvolved\u201d calcium signals. Also note that we predict  the responses over a large time window of 500ms. Even if a multi-spike even is detected as a single large spike, the effect of that might be cancelled out by this integration. \nWhile electrical recordings are more precise in the temporal dimension, they currently come at the expense of much fewer neurons being recorded simultaneously. Our method takes advantage of having the recordings of many neurons by sharing the computations in the core network between these neurons. It thus benefits from the ability of calcium imaging to yield a massive amount of neurons. For a different project, we are currently investigating whether transferring data-driven cores can also work for electrophysiological recordings of less neurons, but the results were not finished at the time of the rebuttal...\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Paper146/Reviewers", "ICLR.cc/2021/Conference/Paper146/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tp7kI90Htd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper146/Authors|ICLR.cc/2021/Conference/Paper146/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874135, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment"}}}, {"id": "gfNaT2yfPBY", "original": null, "number": 2, "cdate": 1603898926056, "ddate": null, "tcdate": 1603898926056, "tmdate": 1606157883210, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Review", "content": {"title": "Review for \"Generalization in data-driven models of primary visual cortex\"", "review": "The authors adopt a data-driven approach to neural system identification. They train a neural network consisting of a \"core\" and a \"readout\" in an end-to-end fashion to learn stimulus (visual inputs) -- response (single neuron activity) pairs. Since the core is shared across neurons, these stimulus-response pairs can be learnt in a massively parallel manner. In particular, they propose a novel readout mechanism that is parameter efficient and drives the core to learn better and generalizable features of the visual inputs. They find that their representations are more suited to predict neural responses in the mouse visual cortex when compared to representations derived from task-driven learning, especially in the context of transfer to previously unseen animals. Lastly, they also observe that the combination of their core+readout is more sample efficient than other naive alternatives.\n\nPros:\nOne of the major positives about this paper is the presented dataset. It seems to be relatively large and well-curated. This can certaily support several follow-up studies.\n\nThe authors identify that \"global\" use of features (i.e. the full hXwXc representational tensor) in the readout is a wasteful strategy (in terms of learned parameters per neuron) and instead adopt a local approach where they only select specific feature columns per neuron to drive the readout. Though this is of minor technical novelty, this constraint forces the core to learn appropriate representations while allowing the entire module to be more data efficient, given the big reduction in the number of free parameters.\n\nThe sample efficiency studies are neat and informative. The dissociations gleaned from diff-core/best-readout vs best-core/diff-readout scenarios are useful. Though it needs more work to fully justify this claim, their demonstration that transferred representations seem to be more effective than direct training is surprising and interesting.\n\nCons:\nThough this study is certainly valuable, the manuscript needs several clarifications before it can be publication-ready.\n\n(i) The authors seek to develop better core representations indirectly by controlling the readout mechanism. This is fine, but there is little justification as to why they chose the current \"core\" architecture. This choice contains arbitary decisions (such as including depth-separable convolutions) that are not justified. Was there a systematic procedure behind a search that led to this architecture? Were other non-standard architecures tested?\n\n(ii) Figure 2 currently seems to be adding very little value and needs to be improved. Given that the proposed readout mechanism was a major contribution of this paper, the authors could have used the Fig. 2 space to visually depict this readout procedure, on top of the readout position network. The arrow to a neuron is also a bit misleading.\n\n(iii) One of my main concerns is with respect to the liberal use of the term \"generalization\". The authors repeatedly state that train-val-test splits were based on neurons and not images. This, coupled with the fact that their readouts leverage retinotopy, it is surprising that the authors never discuss the spatial segregation of the \"held-out\" neurons (say H) from the neurons in the training set (say T). If most H neurons were spatially proximate to T neurons, then this amounts to an \"interpolation\" regime for the network as opposed to \"extrapolation\". If my understanding here is wrong, could the authors please clarify why?\n\n(iv) The authors report that transfered \"core\" representations work better than direct-training in their generalization experiments. This result is surprising and needs to be more strongly justified computationally. Is it possible that a sub-optimal training regime was used for direct-training? Is this anyhow related to issue (iii) raised above?\n\n(v) The authors report that task-driven cores (such as VGG-16 pretrained on imagenet) perform badly in generalizaing across animals. Is this due to impoverished data regimes? Or are there more systemic issues? Also, VGG-16 isn't the best ventral stream model that best fits neural data. Do the authors think that this claim would hold for more recent task-driven systems, like CORnet-S for example.\n\n(vi) Though not necessary for this manuscript per se, it would be helpful to test the usefulness of the generalizable core representations presented here for visual tasks supported by early visual areas. Perhaps some commentary on this would be nice.\n\nMinor:\n\"Code for the analyses and the weights of the best generalizing representation will be shared in the final version of the paper\". The authors do not commit to making the dataset public. Is this oversight or intentional?\n\nhow sensitive to number neurons in a scan?\n\nClarity: (Fig. 4 caption) \"a fully trained core\": I think the authors are referring to a core trained on all available data, which is different from \"fully training\" a network as this alludes to loss saturation. Also language like \"a sub-optimal\" core is vague and misleading.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149422, "tmdate": 1606915780138, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper146/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Review"}}}, {"id": "D3DKskzm8j", "original": null, "number": 1, "cdate": 1603855048593, "ddate": null, "tcdate": 1603855048593, "tmdate": 1606054171603, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Review", "content": {"title": "Interesting findings", "review": "The paper presents an experimental study on predicting the responses of mice V1 neurons with computational models. The paper advances a few contributions:\n\n1. Confirm that task-driven models based on object recognition, are outperformed by data-driven models for predicting single neuron responses.\n2. Show that training a shared model of neural responses on data from several animals and several neurons leads to models that transfer well to data from new neurons and new animals.\n3. Introduce a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning.\n\nI think this paper is interesting and it should be presented at ICLR. I am not an expert in this specific sub-field so I am not qualified to make suggestions or evaluate the experimental design. I will leave here a few suggestions that I hope you will consider for a camera ready version.\n\n1. Is the claim that, in mice, task-driven models are outperformed by data-driven models fully justified? I have no trouble believing that object recognition is not the right task for mice, but there is a fair chance we just haven't found a good task yet, and that we might find it in the future. If this is correct, it might be worth mentioning in the introduction or discussion.\n\n2. I think the Introduction could do a better job of anticipating the implications of this study, similarly, the discussion is a bit dry and does little more than just repeating the results. As I mentioned, I am not especially familiar with the literature in this particular subfield, so I had a hard time imagining what I should learn about visual systems in general from your study. What can we do with this new information?\n\n3. Would it be possible to visualize the receptive fields you learn? Maybe some unit maximization technique could be sufficient. I think it would be cool to see a few.\n\n4. It might make sense to move the training regimes for direct training, within- and across-animal transfer to the method section and explain the data splitting, training and evaluation after those have been introduced. The reader will know why you are designing your datasets and training a certain way, which might make it easier to follow.\n\nThank you again for sharing these cool ideas and results, I hope my suggestions help.\nAll the best!", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149422, "tmdate": 1606915780138, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper146/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Review"}}}, {"id": "CmZG0TiEErV", "original": null, "number": 11, "cdate": 1605784433689, "ddate": null, "tcdate": 1605784433689, "tmdate": 1605784515308, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "gfNaT2yfPBY", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment", "content": {"title": "Changes to the paper addressing your suggestions", "comment": "We have now uploaded a new version of the paper containing the following changes according to your suggestions:\n\n1. In order to come up with the core architecture, we had done extensive searches among architecture choices both in this as well as in other projects. We now added this information to the section \u201cNetworks and Training\u201d.\n2. Thanks for this feedback. We have now adjusted Figure 2 to contain the whole readout procedure, not just the readout position network. We also replaced the arrow to the neuron with a stylized spike train. \n3. Please refer to the discussion in the previous responses to you.\n4. Please refer to the discussion in the previous responses to you.\n5. As noted by Cadena and colleagues 2019, whose results are consistent with ours, one reason why task driven networks like VGG16 trained on imagenet perform poorly on mouse V1 prediction could be that image classification might not be the right task for mouse visual cortex. There might be tasks which are better suited for this purpose in the future (see also Reviewer 4) but until then our network provides a predictive alternative. We do not think that the poor performance of image-classification task-driven approaches is due to our choice of network (VGG16). VGG16 actually outperforms CORnet-S on V1 (0.294 for CORnet-S vs. 0.355 for VGG16 on brain-score.org) Comparing our data-driven network against a large number of pre-trained architectures would be interesting, but is beyond the scope of the current work. We changed our discussion to make clear that we do not want to imply that all task-driven representation must necessarily be suboptimal in mouse visual cortex.  \n6. We added a comment and a reference on this in the discussion section.\n\nMinor: Concerning the publication of the datasets and model, please refer to our response to the Area Chair.\n\nClarity: We re-phrased the respective paragraphs.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Paper146/Reviewers", "ICLR.cc/2021/Conference/Paper146/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tp7kI90Htd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper146/Authors|ICLR.cc/2021/Conference/Paper146/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874135, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment"}}}, {"id": "hvXG4-2CQo", "original": null, "number": 8, "cdate": 1605533544136, "ddate": null, "tcdate": 1605533544136, "tmdate": 1605533544136, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "hr_mX9fylTe", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment", "content": {"title": "Availability of datasets and model weights", "comment": "Thanks for your comment. The networks in our manuscript are trained on 13 two photon scans recorded by our experimental co-authors. As you note, the datasets are very valuable, scientifically and because they take a lot of effort and funding to record. Since this manuscript was not the main reason why these datasets were recorded, we cannot share all datasets (yet), to give the experimental scientists who recorded them enough time to finish their analyses and publish their work which will include a large two-photon data release. \n\nHowever, we also see the need for reproducibility. For this reason we want to find a middle ground between these interests. We will share the evaluation dataset (one scan, used to test in figure 5). We actually have made that set public already (but not shared the location in the paper yet). We will make a note of this and the online location in the final version (to not break anonymity). In addition, we will make the network trained on 11 scans (fig 5) publicly available. With that, people can reproduce the key parts of figure 5 and parts of the data are available for further research. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tp7kI90Htd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper146/Authors|ICLR.cc/2021/Conference/Paper146/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874135, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment"}}}, {"id": "wnBOs_Vr-R9", "original": null, "number": 7, "cdate": 1605533388477, "ddate": null, "tcdate": 1605533388477, "tmdate": 1605533388477, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "2rqJntKVgwx", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment", "content": {"title": "Generalization between neurons", "comment": "Response\nThanks for your quick response. Re (iv): We\u2019ll bring out this detail to be more salient in the next iteration of this figure. \n\nRegarding your other comment: The transfer between animals clearly shows that it cannot be due to spatial interpolation among similarly tuned neurons in close spatial proximity, since it\u2019s still there even when the neurons are from two different animals. In addition, in contrast to orientation columns in primates, features of cortical neurons in mice are locally more diverse (sometimes referred to as \u201csalt and pepper\u201d). Even if that were the case though, our network does not provide a mechanism that allows two neurons in the same scan to share information about the feature weight vector in the readout.\n\nThe reason why we think that training on many neurons helps is the same reason why transfer learning works with imagenet. Pretraining the convolutional part with many neurons helps to identify good basis functions (representations) such that learning a readout on similar problems becomes more data efficient/feasible. An illustration of this idea would be the following: assume that the entirety of  V1 would be made up of simple cells with Gabor filters. Since we use a convolutional core, a feature learned at one spatial position is available on all other spatial positions. This enables the model to learn shared features among neurons regardless of their receptive field position. So with enough neurons, we would eventually identify the basis set of all relevant Gabors (a kind of dictionary if you want). At this point, learning new ones should be easier. \n\nThis reasoning is consistent with figure 4, where the best core (convolutional part) is trained on 3597 neurons and 17596 images. Following the logic above, we think that this identifies a good set of convolutional features (core), such that merely learning a new readout for 1000 neurons on top of the frozen core (yellow curve) is more data efficient. Thus, the learning curves rise quicker compared to directly training the entire network on 1000 neurons (brown curve). \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tp7kI90Htd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper146/Authors|ICLR.cc/2021/Conference/Paper146/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874135, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment"}}}, {"id": "hr_mX9fylTe", "original": null, "number": 5, "cdate": 1605291740313, "ddate": null, "tcdate": 1605291740313, "tmdate": 1605291740313, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "iKMrWE3_bi", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment", "content": {"title": "availability of the dataset", "comment": "Hi there -- R2 lists the dataset as one of the major positives about this paper. I glanced quickly at the paper and could not find any clear statement about releasing the data. Could you please clarify whether the data will be released or not? Thanks"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tp7kI90Htd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper146/Authors|ICLR.cc/2021/Conference/Paper146/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874135, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment"}}}, {"id": "2rqJntKVgwx", "original": null, "number": 4, "cdate": 1605290169968, "ddate": null, "tcdate": 1605290169968, "tmdate": 1605290169968, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "iKMrWE3_bi", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment", "content": {"title": "Are neurons here truly I.I.D?", "comment": "Thanks for the clarification. I do better understand the train/test protocol better now. Off the bat, I do agree that transfer across animals is a good demonstration. Perhaps my question can be paraphrased. Your transfer argument (within the same animal) to a held-out set of neurons points towards \"more neurons in training\" leads to better performance on \"neurons in test set\". It is unclear to me as to what is the knowledge that is being transferred if it does not support feature weight interpolation (directly or indirectly). If two neurons are spatially proximate, and one responds to an input in a particular manner, then you can reasonably guess that the other one would respond in a similar manner. This was the rationale behind my original comment. The neurons are clearly not independently and identically distributed as is the case for typical train/test examples in ML applications. Clarifying how you think this dependency structure is exploited by your system can help.\n\nRe: (iv) Thanks for outlining this. The significance of blue (1-S) and purple (direct) being close to each other is pretty easy to miss in Figure 5. Perhaps the authors should make this more obvious. "}, "signatures": ["ICLR.cc/2021/Conference/Paper146/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tp7kI90Htd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper146/Authors|ICLR.cc/2021/Conference/Paper146/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874135, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment"}}}, {"id": "iKMrWE3_bi", "original": null, "number": 3, "cdate": 1605173082037, "ddate": null, "tcdate": 1605173082037, "tmdate": 1605173116307, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "gfNaT2yfPBY", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment", "content": {"title": "Clarifications for points (iii) and (iv)", "comment": "Thank you very much for your detailed feedback! We are positive that we can address all your points in the rebuttal. We just briefly wanted to clarify a few items regarding your points (iii) and (iv). We feel there might have been a misunderstanding and we want to make sure we understand your point correctly so we can address it adequately. We would kindly ask you to comment whether this addresses your points or clarify your concerns for us:\n\nRegarding (iii):\n - We are not sure whether we succeeded at describing the train-, val- and test-splits properly. While it is true that we split our neurons into a train set (for the core) and a test set, our final results that we report are always evaluated on a separated test-set of images. Specifically, our training scheme consists of the three following steps: 1) train core & readout on neuron set T and training image set; 2) freeze the core, train a new readout on neuron set H and training image set; 3) report score in neurons H and test image set. By that we test how well the core generalizes/transfers to novel neurons\n\n- The information we get from retinotopy concerns solely the readout positions of the neurons, not their feature weights. The feature weights of different neurons are still learned separately even if the neurons are spatially close in the cortex. In that sense, the readout feature weights are not interpolated. \n\n- Finally, our main figure (Fig. 5) shows generalization across animals. This means that neurons from set T and H are from two different animals and could not possibly suffer from any effect of spatial proximity. \n\nRegarding (iv):\n- The results showing that a transfer core can outperform direct-training are very promising indeed. Please note though that while the direct-training regime has access to the test neurons themselves (again, the final performance is always reported on a separate test set of images), the core from the transfer regime has access to a greater number of neurons (from 11 datasets in the orange line, Fig. 5) and images (up to 4 different training stimulus sets). As you pointed out correctly, direct training should outperform transfer training if the data size is comparable, which is exactly what we find (Fig 5, purple line). You can also see that performance of direct training is not saturated yet. If it were possible to record more responses from the same neurons, we would expect direct training to reach the performance of the transfer regime eventually. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tp7kI90Htd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper146/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper146/Authors|ICLR.cc/2021/Conference/Paper146/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874135, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Comment"}}}, {"id": "v0OBKdMoHsl", "original": null, "number": 3, "cdate": 1604089910746, "ddate": null, "tcdate": 1604089910746, "tmdate": 1605024753950, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Review", "content": {"title": "Innovative state-of-the-art predictor of mouse visual responses", "review": "Summarize what the paper claims to contribute. \nThe paper introduces a deep-network-based approach to regression of responses to natural stimuli in mouse primary visual cortex. There is closely related work in the literature, but this paper achieves very good performance, partly through a new way of accounting for neurons\u2019 receptive-field positions. The paper also provides a helpful analysis of prediction performance versus numbers of images and neurons used to train the model, and shows that the already excellent performance is not saturated with respect to the number of images. The work also shows that features learned by a core network generalize well across different mice. \n\nList strong and weak points of the paper.\nStrong points: \n-\tEmpirical modelling of neural responses has a long tradition, and the results in this paper are state-of-the-art. \n-\tThorough and insightful positioning in the recent literature. \n-\tExpert execution in terms of details of the technical work. \n-\tThe method of parameterizing the receptive field location is well-motivated and effective. \n-\tThe analyses are interesting and provide useful insights. \n\nWeak points: \nI wouldn\u2019t characterize any part of the paper as weak, but here are some minor suggestions to further strengthen it: \n-\tSay more about how the model can be used, or what insights might arise from it (there is only a short comment on inception loops). \n-\tSay more about limitations as a model of neural responses, particularly with respect to dynamics. While the method is impressive with respect to short-time-window responses, system identification methods have long been used to study temporal responses. I think a short comment on this scope limitation would help to further contextualize the paper. \n-\tAn additional way to contextualize the results might be relative to the total number of neurons in L2/3 of VISp (I believe ~200K). Does this number have any significance relative to the dimension of the core-network output, or the number of recorded neurons? \n-\tConsider adding a sentence on ethics oversight regarding the animal experiments. \n-\tConsider adding a few further details of the experiments. \n\nClearly state your recommendation (accept or reject) with one or two key reasons for this choice. \nI recommend that the paper be accepted. The paper addresses a long-standing problem very well. It introduces a new method that is well justified and effective. Overall, the performance is impressive, and the analyses are well done. \n\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. \nWhat are the kernel sizes in the core?\nWhich hyperparameters are adjusted in the hyperparameter search?\n\nProvide additional feedback with the aim to improve the paper. \nI was confused by the following sentences: \n\u201c\u2026 both readouts assume that the receptive field of each neuron is the same across features\u201d \n\u201c\u2026 readout has c + 7 parameters per neuron \u2026\u201d (I only see c+6.)\n\u201cFig 5 for the factorized readout \u2026\u201d (I didn\u2019t get it until reading it four times and looking for these results in Figure 5 twice.) \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149422, "tmdate": 1606915780138, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper146/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Review"}}}, {"id": "BvTAyPeetGw", "original": null, "number": 4, "cdate": 1604335286637, "ddate": null, "tcdate": 1604335286637, "tmdate": 1605024753891, "tddate": null, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "invitation": "ICLR.cc/2021/Conference/Paper146/-/Official_Review", "content": {"title": "Predicting V1 responses with less training data", "review": "The authors train a neural net to predict responses of mouse V1 L2/3 neurons to visual stimulation. The NN has a \"core\" that is shared between all neurons, and a neuron-specific readout. They train the core on multiple animals and find that it can generalize well: it can be used in a new animal and (with sufficient training of the readouts) achieve high performance. They also use a neat approach of constraining the readout weights (receptive field location) using the known retinotopy of V1. Finally, they show that their network outperforms task-trained ones at predicting V1 responses.\n\nThis is nice work overall. I have a few suggestions:\n\n1) It might be worth considering other measures of performance, different from the normalized correlation coefficient. Recent work shows that this measure can have unintended bias, being substantially affected by trial-to-trial variability.\nSee \"The unbiased estimation of the fraction of variance explained by a model\" from Pospisal and Bair (https://doi.org/10.1101/2020.10.30.361253) for details, and a potential solution.\n\n2) 2-photon imaging can have issues at detecting single spikes (see this preprint, for example: Relationship between spiking activity and simultaneously recorded fluorescence signals in transgenic mice expressing GCaMP6,  https://doi.org/10.1101/788802). So the neural dataset could in principle show more multi-spike events than single-spike ones, or have other issues. This is inevitable of course with calcium imaging, but it makes the problematic to compare with previous work that used electrical recordings. E.g., I don't think it is possible to prove better performance for this work than the prior ones, because of this difference in recording methods. A good follow-up work should try this method on electrical recordings from (say) monkey, and compare with performance from the Cadena, Yamins, Kindel, Klindt, Batty, etc. studies.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper146/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper146/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization in data-driven models of primary visual cortex", "authorids": ["~Konstantin-Klemens_Lurz1", "mohammad.bashiri@uni-tuebingen.de", "konstantin-friedrich.willeke@uni-tuebingen.de", "akshay-kumar.jagadish@student.uni-tuebingen.de", "eric.wang2@bcm.edu", "~Edgar_Y._Walker1", "~Santiago_A_Cadena1", "taliah.muhammad@bcm.edu", "~Erick_Cobos1", "~Andreas_S._Tolias1", "~Alexander_S_Ecker1", "~Fabian_H._Sinz1"], "authors": ["Konstantin-Klemens Lurz", "Mohammad Bashiri", "Konstantin Willeke", "Akshay Jagadish", "Eric Wang", "Edgar Y. Walker", "Santiago A Cadena", "Taliah Muhammad", "Erick Cobos", "Andreas S. Tolias", "Alexander S Ecker", "Fabian H. Sinz"], "keywords": ["neuroscience", "cognitive science", "multitask learning", "transfer learning", "representation learning", "network architecture", "computational biology", "visual perception"], "abstract": "Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron\u2019s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.", "one-sentence_summary": "We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lurz|generalization_in_datadriven_models_of_primary_visual_cortex", "supplementary_material": "/attachment/9782e8f743bf7dcdba3df6881471a1f8ebfcd9d9.zip", "pdf": "/pdf/a10fba1a4a77e56503923a67cf4e95e82d6f9b59.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlurz2021generalization,\ntitle={Generalization in data-driven models of primary visual cortex},\nauthor={Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Tp7kI90Htd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Tp7kI90Htd", "replyto": "Tp7kI90Htd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149422, "tmdate": 1606915780138, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper146/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper146/-/Official_Review"}}}], "count": 13}