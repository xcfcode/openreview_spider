{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457631018177, "tcdate": 1457631018177, "id": "3QxzglN5nsp7y9wltPD0", "invitation": "ICLR.cc/2016/workshop/-/paper/109/review/10", "forum": "OM0jjYW3BHp57ZJjtNEO", "replyto": "OM0jjYW3BHp57ZJjtNEO", "signatures": ["ICLR.cc/2016/workshop/paper/109/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/109/reviewer/10"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "Interesting idea. Although not totally novel as pointed out in other comments, perhaps this new form of scale preservation is better or more efficient, although the results from what appears to be a *single experiment*, on MNIST, makes it hard to pass much judgement.\n\nIntuitively, since rescaling is a hard constraint, gradient descent might have quite some trouble adjusting towards the convergence; which might be why in the end the unnormalized model gets a better test score.\n\nThe reasoning is interesting but it is not backed up by enough empirical evidence.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Scale Normalization", "abstract": "One of the difficulties of training deep neural networks is caused by improper scaling between layers.  These scaling issues introduce exploding / gradient problems, and have typically been addressed by careful variance-preserving initialization.  We consider this problem as one of preserving scale, rather than preserving variance.  This leads to a simple method of scale-normalizing weight layers, which ensures that scale is approximately maintained between layers.  Our method of scale-preservation ensures that forward propagation is impacted minimally, while backward passes maintain gradient scales.  Preliminary experiments show that scale normalization effectively speeds up learning, without introducing additional hyperparameters or parameters. ", "pdf": "http://arxiv.org/pdf/1604.07796v1", "paperhash": "lo|scale_normalization", "conflicts": ["cs.umb.edu"], "authors": ["Henry Z Lo", "Kevin Amaral", "Wei Ding"], "authorids": ["henryzlo@cs.umb.edu", "kevin.m.amaral@gmail.com", "ding@cs.umb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579951369, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579951369, "id": "ICLR.cc/2016/workshop/-/paper/109/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "OM0jjYW3BHp57ZJjtNEO", "replyto": "OM0jjYW3BHp57ZJjtNEO", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/109/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457629946962, "tcdate": 1457629946962, "id": "P7Vnw4z9NfKvjNORtJjO", "invitation": "ICLR.cc/2016/workshop/-/paper/109/review/12", "forum": "OM0jjYW3BHp57ZJjtNEO", "replyto": "OM0jjYW3BHp57ZJjtNEO", "signatures": ["ICLR.cc/2016/workshop/paper/109/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/109/reviewer/12"], "content": {"title": "scale normalization, not thorough enough", "rating": "5: Marginally below acceptance threshold", "review": "Appears to be an unthorough set of experiments attempting to keep network weight matrices length preserving. There have been many papers along these lines e.g. arxiv.org/abs/1602.07714 or indeed explicitly length-preserving weights like in arxiv.org/abs/1511.06464\n\nIn any case, this looks like very little work on what could be a promising idea. It's just hard to tell from so little data.\n\nI'm not sure what the standards are for the workshop papers, I guess they are lower than the conference's by definition. Perhaps this is acceptable.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Scale Normalization", "abstract": "One of the difficulties of training deep neural networks is caused by improper scaling between layers.  These scaling issues introduce exploding / gradient problems, and have typically been addressed by careful variance-preserving initialization.  We consider this problem as one of preserving scale, rather than preserving variance.  This leads to a simple method of scale-normalizing weight layers, which ensures that scale is approximately maintained between layers.  Our method of scale-preservation ensures that forward propagation is impacted minimally, while backward passes maintain gradient scales.  Preliminary experiments show that scale normalization effectively speeds up learning, without introducing additional hyperparameters or parameters. ", "pdf": "http://arxiv.org/pdf/1604.07796v1", "paperhash": "lo|scale_normalization", "conflicts": ["cs.umb.edu"], "authors": ["Henry Z Lo", "Kevin Amaral", "Wei Ding"], "authorids": ["henryzlo@cs.umb.edu", "kevin.m.amaral@gmail.com", "ding@cs.umb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579950263, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579950263, "id": "ICLR.cc/2016/workshop/-/paper/109/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "OM0jjYW3BHp57ZJjtNEO", "replyto": "OM0jjYW3BHp57ZJjtNEO", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/109/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457607123405, "tcdate": 1457607123405, "id": "0YrwmZ2pJTGJ7gK5tRW1", "invitation": "ICLR.cc/2016/workshop/-/paper/109/review/11", "forum": "OM0jjYW3BHp57ZJjtNEO", "replyto": "OM0jjYW3BHp57ZJjtNEO", "signatures": ["ICLR.cc/2016/workshop/paper/109/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/109/reviewer/11"], "content": {"title": "Review for Scale Normalization", "rating": "6: Marginally above acceptance threshold", "review": "A few points about the idea:\n (a) Depending on the dataset, and minibatch size, E(s) but not be very stable. This you can see in the peaks of the training loss. E(s) should really be over the dataset. If that is expensive than maybe computing a moving average (even if W changes from step to step, it should not drastically change). Regardless, the instability introduced by this normalization scheme is somewhat worrisome.\n (b) The fact that hurts on the test set, leading to lower score is also a bit worrisome. IMHO what is going one is that the algorithm is to greedy, overfitting the current minibatch early on training, making me think again about E(s) not being estimated properly. Regardless, this points that some important detail is missing from its current form.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Scale Normalization", "abstract": "One of the difficulties of training deep neural networks is caused by improper scaling between layers.  These scaling issues introduce exploding / gradient problems, and have typically been addressed by careful variance-preserving initialization.  We consider this problem as one of preserving scale, rather than preserving variance.  This leads to a simple method of scale-normalizing weight layers, which ensures that scale is approximately maintained between layers.  Our method of scale-preservation ensures that forward propagation is impacted minimally, while backward passes maintain gradient scales.  Preliminary experiments show that scale normalization effectively speeds up learning, without introducing additional hyperparameters or parameters. ", "pdf": "http://arxiv.org/pdf/1604.07796v1", "paperhash": "lo|scale_normalization", "conflicts": ["cs.umb.edu"], "authors": ["Henry Z Lo", "Kevin Amaral", "Wei Ding"], "authorids": ["henryzlo@cs.umb.edu", "kevin.m.amaral@gmail.com", "ding@cs.umb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579950700, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579950700, "id": "ICLR.cc/2016/workshop/-/paper/109/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "OM0jjYW3BHp57ZJjtNEO", "replyto": "OM0jjYW3BHp57ZJjtNEO", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/109/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455816870099, "tcdate": 1455816870099, "id": "OM0jjYW3BHp57ZJjtNEO", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "OM0jjYW3BHp57ZJjtNEO", "signatures": ["~Henry_Z_Lo1"], "readers": ["everyone"], "writers": ["~Henry_Z_Lo1"], "content": {"CMT_id": "", "title": "Scale Normalization", "abstract": "One of the difficulties of training deep neural networks is caused by improper scaling between layers.  These scaling issues introduce exploding / gradient problems, and have typically been addressed by careful variance-preserving initialization.  We consider this problem as one of preserving scale, rather than preserving variance.  This leads to a simple method of scale-normalizing weight layers, which ensures that scale is approximately maintained between layers.  Our method of scale-preservation ensures that forward propagation is impacted minimally, while backward passes maintain gradient scales.  Preliminary experiments show that scale normalization effectively speeds up learning, without introducing additional hyperparameters or parameters. ", "pdf": "http://arxiv.org/pdf/1604.07796v1", "paperhash": "lo|scale_normalization", "conflicts": ["cs.umb.edu"], "authors": ["Henry Z Lo", "Kevin Amaral", "Wei Ding"], "authorids": ["henryzlo@cs.umb.edu", "kevin.m.amaral@gmail.com", "ding@cs.umb.edu"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}