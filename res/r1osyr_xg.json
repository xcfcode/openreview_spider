{"notes": [{"tddate": null, "ddate": null, "tmdate": 1511854205120, "tcdate": 1511854205120, "number": 2, "cdate": 1511854205120, "content": {"question": " "}, "id": "r1BYgq9gz", "invitation": "ICLR.cc/2017/conference/-/paper56/pre-review/question", "forum": "r1osyr_xg", "replyto": "r1osyr_xg", "signatures": ["ICLR.cc/2017/conference/paper56/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper56/AnonReviewer3"], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1511854205724, "id": "ICLR.cc/2017/conference/-/paper56/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper56/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper56/AnonReviewer1", "ICLR.cc/2017/conference/paper56/AnonReviewer3"], "reply": {"forum": "r1osyr_xg", "replyto": "r1osyr_xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1511854205724}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396329824, "tcdate": 1486396329824, "number": 1, "id": "S1zhjfUdg", "invitation": "ICLR.cc/2017/conference/-/paper56/acceptance", "forum": "r1osyr_xg", "replyto": "r1osyr_xg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers agree that the paper's clarity and experimental evaluation can be improved."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396330361, "id": "ICLR.cc/2017/conference/-/paper56/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1osyr_xg", "replyto": "r1osyr_xg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396330361}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484816905848, "tcdate": 1478148003291, "number": 56, "id": "r1osyr_xg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1osyr_xg", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "content": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484812238245, "tcdate": 1484812238245, "number": 10, "id": "SkLCkg0Ix", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "H1Dkgh-Eg", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "Some more revisions", "comment": "Hello. Thank you again for your reviews.\n\nWe have made some additional revisions as our final version before the decision.\n\n1. We realized that the benchmarks are testing different aspects of the vectors. We do not think the inconsistency for best parameters is an issue now. We put the changes of the benchmark scores under different vector dimensions from section 3.2 to 3.3 as it is about the effect of vector dimensions. We also revised the title of section 3.2 from \"Issues about benchmarks\" to \"Benchmarks.\"\n2. We used our proposed model instead of fastText to test the margin of errors for the benchmarks and revised section 3.2.\n3. We also revised the discussion on the parameters and gave a more detailed discussion in section 3.\n4. We revised the y labels in the figures to keep the same float format.\n5. We add an explanation of the transpose mark for Equation (5) in section 2.4.\n6. We add the missed introduction of Figure 2 in section 3.3.\nWe recheck the misspellings and grammar mistakes.\n\nThank you again for spending the time to read our papers and the helpful advice."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1484812214003, "tcdate": 1484812214003, "number": 9, "id": "SyC2ke0Lg", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "ryjNRqSVg", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "Some more revisions", "comment": "Hello. Thank you again for your reviews.\n\nWe have made some additional revisions as our final version before the decision.\n\n1. We realized that the benchmarks are testing different aspects of the vectors. We do not think the inconsistency for best parameters is an issue now. We put the changes of the benchmark scores under different vector dimensions from section 3.2 to 3.3 as it is about the effect of vector dimensions. We also revised the title of section 3.2 from \"Issues about benchmarks\" to \"Benchmarks.\"\n2. We used our proposed model instead of fastText to test the margin of errors for the benchmarks and revised section 3.2.\n3. We also revised the discussion on the parameters and gave a more detailed discussion in section 3.\n4. We revised the y labels in the figures to keep the same float format.\n5. We add an explanation of the transpose mark for Equation (5) in section 2.4.\n6. We add the missed introduction of Figure 2 in section 3.3.\nWe recheck the misspellings and grammar mistakes.\n\nThank you again for spending the time to read our papers and the helpful advice."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1484812132004, "tcdate": 1484812132004, "number": 8, "id": "rJnPkl0Ul", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "H1GGwg8Ve", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "Some more revisions", "comment": "Hello. Thank you again for your reviews.\n\nWe have made some additional revisions as our final version before the decision.\n\n1. We realized that the benchmarks are testing different aspects of the vectors. We do not think the inconsistency for best parameters is an issue now. We put the changes of the benchmark scores under different vector dimensions from section 3.2 to 3.3 as it is about the effect of vector dimensions. We also revised the title of section 3.2 from \"Issues about benchmarks\" to \"Benchmarks.\"\n2. We used our proposed model instead of fastText to test the margin of errors for the benchmarks and revised section 3.2.\n3. We also revised the discussion on the parameters and gave a more detailed discussion in section 3.\n4. We revised the y labels in the figures to keep the same float format.\n5. We add an explanation of the transpose mark for Equation (5) in section 2.4.\n6. We add the missed introduction of Figure 2 in section 3.3.\nWe recheck the misspellings and grammar mistakes.\n\nThank you again for spending the time to read our papers and the helpful advice."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1484380416740, "tcdate": 1484380416740, "number": 7, "id": "B1KZKLPUl", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "H1Dkgh-Eg", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "List of Revisions", "comment": "Thank you again for your kindly reviews.\n\nWe have revised our paper.\n\nFor the first question how well the proposed method achieves the goal, we added the comparison with simpler control function to evaluate the effectiveness of our approach in section 3.5. We also evaluate the methods using more benchmarks. Besides, the benchmarks' margins of errors are tested in section 3.2; they are used to analyze whether the experimental results indicate the proposed method performs better than the prior works.\n\nFor the second question, we explored the effects of the corpus, paraphrase sets, the dimension of word vectors, the context window size and the number of negative samples. We show the results and give discussions in section 3.3, 3,4 and 3.5.\n\nPlease check the full list of revisions here:\n1)\u3000We rewrote the abstract and tried to avoid misleading utterances.\n2)\u3000We rewrote the parts that may mislead or confuse readers in section 1. \n3)\u3000We added lists of contributions and sections at the end of section 1.\n4)\u3000We deleted the \"Related Works\" section and put the introduction about them in section 1.\n5)\u3000We rewrote section 2  to make it more logical and easier to read. We introduce about the ideas of fuzzy paraphrases in a separated subsection in section 2.1 now.\n6)\u3000We replaced the figure of the architecture with a process flow chart in section 2, which is easier to understand.\n7)\u3000We moved the discussion about a control function considering all context from section 2 to section 5 as the future work.\n8)\u3000We performed some additional experiments to explore the effects of paraphrase types, parameters, and the control function. We discuss them in the new section 3 \"Model Exploration.\"\n9)\u3000We added experiments about the benchmarks to see how they can evaluate the performance. We tested their margins of error when we repeat learning under the same conditions. We provide the results and discussion in section 3.2.\n10)\u3000We used an additional large corpus -- ukWaC for exploration about the effects of different corpus sizes to show the effects of the corpus size more clearly. We describe the experiments and discuss the results in section 3.5.\n11)\u3000 We added comparison with the situation that f(x)=1 (regards every paraphrase equally) and f(x)=0 (equivalent to CBOW) in  section 3.5.\n12)\u3000 We redid the comparative experiments for comparison with the prior works using the same corpus and benchmarks to make it fairer. We describe the experiment method and discuss the results in section 4.\n13)\u3000 We deleted the discussion of processing speed because it is less related to the major contributions and the paper is longer.\n14)\u3000We rewrote the conclusion in section 5. We avoided the misleading utterances such as \"we figure out.\" \n\nThank you again for your review and kindly advice."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1484373493873, "tcdate": 1484373493873, "number": 6, "id": "SyAgC4v8l", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "ryjNRqSVg", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "List of Revisions", "comment": "Thank you again for your comments.\n\nWe apologize again for the poor English in the old version. We wrote the most of the paper. I hope the current form is easier to read and understand.\n\nWe have added more benchmarks in evaluation experiments. We also thank for your advice that recommends papers in RepEval 2016 for us. Inspired by the articles, we tested the benchmarks themselves and their margins of error when the same experiment repeats. Benefitting from the results, we analyze the other experimental results more fairly now. We put the findings about the benchmarks in section 3.2 \"Issues about Benchmarks\".\n\nWe list the major contributions at the end of section 1 \"Introduction\", rewrote the discussion in section \"Conclusion & The Future Works\" to clarify the motivations and contributions.\n\nThe full list of revisions is here:\n\n1)\u3000We rewrote the abstract and tried to avoid misleading utterances.\n2)\u3000We rewrote the parts that may mislead or confuse readers in section 1. \n3)\u3000We added lists of contributions and sections at the end of section 1.\n4)\u3000We deleted the \"Related Works\" section and put the introduction about them in section 1.\n5)\u3000We rewrote section 2  to make it more logical and easier to read. We introduce about the ideas of fuzzy paraphrases in a separated subsection in section 2.1 now.\n6)\u3000We replaced the figure of the architecture with a process flow chart in section 2, which is easier to understand.\n7)\u3000We moved the discussion about a control function considering all context from section 2 to section 5 as the future work.\n8)\u3000We performed some additional experiments to explore the effects of paraphrase types, parameters, and the control function. We discuss them in the new section 3 \"Model Exploration.\"\n9)\u3000We added experiments about the benchmarks to see how they can evaluate the performance. We tested their margins of error when we repeat learning under the same conditions. We provide the results and discussion in section 3.2.\n10)\u3000We used an additional large corpus -- ukWaC for exploration about the effects of different corpus sizes to show the effects of the corpus size more clearly. We describe the experiments and discuss the results in section 3.5.\n11)\u3000 We added comparison with the situation that f(x)=1 (regards every paraphrase equally) and f(x)=0 (equivalent to CBOW) in  section 3.5.\n12)\u3000 We redid the comparative experiments for comparison with the prior works using the same corpus and benchmarks to make it fairer. We describe the experiment method and discuss the results in section 4.\n13)\u3000 We deleted the discussion of processing speed because it is less related to the major contributions and the paper is longer.\n14)\u3000We rewrote the conclusion in section 5. We avoided the misleading utterances such as \"we figure out.\" \n\nWe thank for your review and apologize again for our poor English and the confusing utterances in the old version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1484370680541, "tcdate": 1484370007767, "number": 5, "id": "BJePxNv8l", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "H1GGwg8Ve", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "List of Revision", "comment": "Thank you again for your comments.\n\nWe have revised the paper. \n\n1)For the first point, \"long sections are very difficult to understand due to the unconventional sentence structure,\" we have rewritten most parts of the paper. We use shorter paragraphs now and more lists now.\n2)For the second point, \"the tables need better and more descriptive labels,\" we have reformed the table of the comparative experiments with the prior works (now Table 3). We label each prior works with reference rather than abbreviation now.\n3)For the third point about the inconclusive results, we have given more results under different benchmarks now. We also test the benchmarks themselves. We test their margin of errors when the experiments repeat and use the findings in the discussion of the other experimental results.\n4)For the absence of \"Enriched CBOW\", we put it in the old version but now remove it because it is not an approach using lexicons to improve word vectors, a work in a different direction and less related to our works. Comparing with it is confusing. We also remove the results under GloVe for the same reason.\n5)For the fifth point of the misleading utterances, we are sorry for them and have rewritten or removed the misleading sentences.\n\nPlease check the full list of revisions:\n\n1)\u3000We rewrote the abstract and tried to avoid misleading utterances.\n2)\u3000We rewrote the parts that may mislead or confuse readers in section 1. \n3)\u3000We added lists of contributions and sections at the end of section 1.\n4)\u3000We deleted the \"Related Works\" section and put the introduction about them in section 1.\n5)\u3000We rewrote section 2  to make it more logical and easier to read. We introduce about the ideas of fuzzy paraphrases in a separated subsection in section 2.1 now.\n6)\u3000We replaced the figure of the architecture with a process flow chart in section 2, which is easier to understand.\n7)\u3000We moved the discussion about a control function considering all context from section 2 to section 5 as the future work.\n8)\u3000We performed some additional experiments to explore the effects of paraphrase types, parameters, and the control function. We discuss them in the new section 3 \"Model Exploration.\"\n9)\u3000We added experiments about the benchmarks to see how they can evaluate the performance. We tested their margins of error when we repeat learning under the same conditions. We provide the results and discussion in section 3.2.\n10)\u3000We used an additional large corpus -- ukWaC for exploration about the effects of different corpus sizes to show the effects of the corpus size more clearly. We describe the experiments and discuss the results in section 3.5.\n11)\u3000 We added comparison with the situation that f(x)=1 (regards every paraphrase equally) and f(x)=0 (equivalent to CBOW) in  section 3.5.\n12)\u3000 We redid the comparative experiments for comparison with the prior works using the same corpus and benchmarks to make it fairer. We describe the experiment method and discuss the results in section 4.\n13)\u3000 We deleted the discussion of processing speed because it is less related to the major contributions and the paper is longer.\n14)\u3000We rewrote the conclusion in section 5. We avoided the misleading utterances such as \"we figure out.\" \n\nWe thank again for your review and apologize again for our poor English and the misleading sentences in the old version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1482396329133, "tcdate": 1482396329133, "number": 4, "id": "BkW2MGtVl", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "H1Dkgh-Eg", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "Thank you for your careful review, kindly comments and valuable advice.", "comment": "Thank you for your careful review, kindly comments and valuable advice.\n\nWe are glad that you like our idea. According to the issues you mentioned and your valuable advice, we will perform additional experiments add detailed and further discussions:\n\n1. \"how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms\"\n\nThank you for the comment. We apologize for the unclear discussion of the motivations and contributions in the paper.\nWe think the major contribution of this work is to propose an idea that alleviates the bad effects of polysemous words and improve the performance, but not involves word sense vectors and word disambiguation. Word disambiguation is therefore not necessary when we use the estimated word embeddings in other works.\nWe will revise the paper and give statements and discussions about the motivations and contributions.\n\n2. \"I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal.\"\n\nThank for your valuable comments and advice.\nAs you say, it is interesting and we are going to investigate different choices of function f and compare the results.\n\n3. \"the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work\"\n\nThank you for your kindly comments.\nWe think the word analogy is not enough to distinguish the proposed method from prior works. We are going to use more benchmarks including WordSim353 and SimLex999. After we collect the results, we will add them in the paper and give you a revision list.\n\n4. \"it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \"\n\nThank you for your kindly comments and advice.\nWe will use more benchmarks including WordSim353 and SimLex999 to evaluate our method. We will also compare the results at different situations including different corpora, window sizes, vector size, numbers of samples, versions of the paraphrase database.\nAfter we collect the results, we will revise the paper and report them. We will also post a revision list for you.\n\nThank you again for your kindly comments and valuable advice.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1482393571618, "tcdate": 1482393571618, "number": 3, "id": "BJnyO-KEx", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "ryjNRqSVg", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "Thank you for your comments and the issues pointed out", "comment": "Thank you for your comments and the issues pointed out.\n\nWe are sorry for our poor English. We will recheck the paper and try our best to make it clearer and easier to read and understand.\n\nThank you very much for pointing out the issues of Google's analogy dataset. We will use the other datasets including WordSim353 and SimLex999 to perform additional evaluations.\n\nThe motivations of this work are:\n1. We noticed that the previous works (that use lexicons to improve word embeddings) do not improve the accuracy in the semantic part of Google's analogy dataset. \nAs you mentioned, the Google's analogy dataset has many issues. But we think there are unsolved issues in the previous works too. We think one of them is the issue of polysemous words.\n2. We find that although there are works in word disambiguation for learning word embeddings, they result in one vector per sense. \nIf we want to use such word embeddings in other works, we need to do word disambiguation first. The additional word disambiguation may bring additional error.\n3. We think it is contributing to propose an idea that alleviates the bad effects of polysemous words, without estimating one vector per sense. \nIt is easier to use in other works when one vector per word because it is not necessary to do word disambiguation. \nIf we can alleviate the bad effects of polysemous words and keep one vector per word at the same time without word disambiguation. We think it is contributing.\n\nSorry for our poor English confused you and thank you for your comments again."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1482392224522, "tcdate": 1482392207717, "number": 2, "id": "Bk_qMZtEl", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "H1GGwg8Ve", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "Thank you for your comments and valuable advice", "comment": "Thank you for your comments and valuable advice.\n\nAs you say, there are confusing and misleading statements in the paper, and the experiment results are not convincing enough.\n\nWe will rewrite the confusing and misleading parts. We will also perform further evaluations, compare the results under different parameters and difference f function.\n\nAfter we get all done, we will post the revised version and the revision list.\n\n1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.\n\nWe are sorry for our poor English. We will recheck the paper and make it clearer to understand.\n\n2) The tables need better and more descriptive labels.\n\nThank you for your advice. We will rewrite the labels and reform the tables.\n\n3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?\n\nThank you for pointing out the issue.\nBecause the negative sampling and the f function in our methods are stochastic, we think it is reasonable that the proposed method is less efficient when we use a smaller corpus. But as you say, the experiments are not convincing enough and inconclusive. We are going to perform additional experiments using the other benchmarks like WordSim353 and SimLex999. We are also going to repeat the experiments several times to alleviate the randomness.\n\n4) Why was \"Enriched CBOW\" not included in the analogy task?\n\nWe are sorry that our tables are confusing but we report \"Enriched CBOW\" in Table 2. We will rewrite the labels and reform the tables to make them look less similar and less confusing.\n\n5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn't been enough work on this. That feels a little misleading.\n\nWe are sorry for our poor English and the misleading statements. \n\nWe do not intend to say the proposed method is the first work of using lexicons or word disambiguation. \nWe want to express that our work  has the following features at the same time:\n1. Use lexicons to improve the word embeddings\n2. Alleviate the bad effects of polysemous words without additional word disambiguation\n3. Keep one vector for one word. So when we use the vector table, we do not need to do word disambiguation first.\nWe know that we are not first in using lexicons, nor working in word disambiguation for word embeddings, nor generate one vector per word. \nWe propose a new idea that alleviates the bad effects of polysemous words, keeping one vector per word without additional word disambiguation.\n\nBut as you say, some statements are misleading. We will rewrite the misleading parts.\n\nAfter we revise the paper, we will post the revision list.\nThank you very much again for your kindly comments and valuable advice."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1482192650322, "tcdate": 1482192650322, "number": 3, "id": "H1GGwg8Ve", "invitation": "ICLR.cc/2017/conference/-/paper56/official/review", "forum": "r1osyr_xg", "replyto": "r1osyr_xg", "signatures": ["ICLR.cc/2017/conference/paper56/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper56/AnonReviewer2"], "content": {"title": "Good idea but not enough to convince", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.\n\nOn balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.\n\nDetailed/minor points below:\n\n1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.\n2) The tables need better and more descriptive labels.\n3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?\n4) Why was \"Enriched CBOW\" not included in the analogy task?\n5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn't been enough work on this. That feels a little misleading.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512713258, "id": "ICLR.cc/2017/conference/-/paper56/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper56/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper56/AnonReviewer1", "ICLR.cc/2017/conference/paper56/AnonReviewer3", "ICLR.cc/2017/conference/paper56/AnonReviewer2"], "reply": {"forum": "r1osyr_xg", "replyto": "r1osyr_xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512713258}}}, {"tddate": null, "tmdate": 1482169906865, "tcdate": 1482169906865, "number": 2, "id": "ryjNRqSVg", "invitation": "ICLR.cc/2017/conference/-/paper56/official/review", "forum": "r1osyr_xg", "replyto": "r1osyr_xg", "signatures": ["ICLR.cc/2017/conference/paper56/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper56/AnonReviewer3"], "content": {"title": "Difficult to understand", "rating": "3: Clear rejection", "review": "This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.\n\nI think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.\n\nIn addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).\n\nFinally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512713258, "id": "ICLR.cc/2017/conference/-/paper56/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper56/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper56/AnonReviewer1", "ICLR.cc/2017/conference/paper56/AnonReviewer3", "ICLR.cc/2017/conference/paper56/AnonReviewer2"], "reply": {"forum": "r1osyr_xg", "replyto": "r1osyr_xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512713258}}}, {"tddate": null, "tmdate": 1481912287126, "tcdate": 1481912287126, "number": 1, "id": "H1Dkgh-Eg", "invitation": "ICLR.cc/2017/conference/-/paper56/official/review", "forum": "r1osyr_xg", "replyto": "r1osyr_xg", "signatures": ["ICLR.cc/2017/conference/paper56/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper56/AnonReviewer1"], "content": {"title": "Interesting idea, unconvincing experiments", "rating": "6: Marginally above acceptance threshold", "review": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512713258, "id": "ICLR.cc/2017/conference/-/paper56/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper56/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper56/AnonReviewer1", "ICLR.cc/2017/conference/paper56/AnonReviewer3", "ICLR.cc/2017/conference/paper56/AnonReviewer2"], "reply": {"forum": "r1osyr_xg", "replyto": "r1osyr_xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512713258}}}, {"tddate": null, "tmdate": 1480745420811, "tcdate": 1480745420807, "number": 1, "id": "rkrAWJxQx", "invitation": "ICLR.cc/2017/conference/-/paper56/public/comment", "forum": "r1osyr_xg", "replyto": "rJh7twyQe", "signatures": ["~Yuanzhi_Ke1"], "readers": ["everyone"], "writers": ["~Yuanzhi_Ke1"], "content": {"title": "Answer to the question about semantic vs syntactic analogy performance", "comment": "Dear AnonReviewer1:\n\nHello. Thanks for your comment.\n\nEach question in the semantic part of the word analogical reasoning task contains two pairs of words sharing the same semantic relationship, for example, (\"boy\", \"girl\") and (\"brother, sister\"). Each question in the syntactic part contains two pairs of words sharing the syntactic relationship, for example, (\"amazing\", \"amazingly\") and (\"calm\", \"calmly\").\n\nThe JointReps by Bollegala et al. minimizes the distance of the target word and one of its context words in the vector space if they are paraphrases in the lexicon. For polysemous words, it leads to overfitting to a wrong vector of a wrong sense. Because it is an issue about representing the semantic relationship, it deteriorates the accuracy for the semantic part and much less for the syntactic part.\n\nOur method alleviates the issue about overfitting to the wrong sense. Therefore, we get an improvement in the semantic part.\n\nAs the previous works using lexicons report improvement in the syntactic sense, we can see that the paraphrases in the lexicons improve the accuracy in syntactic sense.\n\nHowever, our method drops some of the paraphrases in learning the word embeddings. That is one of the reasons that our method is less efficient in improving the accuracy of the syntactic part.\n\nBesides, the reported results of JointReps are achieved using a different corpus called UkWaC. It is another possible reason of the difference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287747396, "id": "ICLR.cc/2017/conference/-/paper56/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1osyr_xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper56/reviewers", "ICLR.cc/2017/conference/paper56/areachairs"], "cdate": 1485287747396}}}, {"tddate": null, "tmdate": 1480714532191, "tcdate": 1480714532186, "number": 1, "id": "rJh7twyQe", "invitation": "ICLR.cc/2017/conference/-/paper56/pre-review/question", "forum": "r1osyr_xg", "replyto": "r1osyr_xg", "signatures": ["ICLR.cc/2017/conference/paper56/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper56/AnonReviewer1"], "content": {"title": "Semantic vs syntactic analogy performance", "question": "Compared to JointREPs, your method achieves slightly better overall accuracy on the analogy task, but it performs much better on the semantic category than it does on the syntactic one. Do you have an explanation for this imbalance?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. \nIn this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.", "pdf": "/pdf/28e7b976b8ca9d854ce0c725c74051773df282a9.pdf", "TL;DR": "We propose a novel idea to address polysemy problem by annotating paraphrases with a degree of reliability like a member of a fuzzy set.", "paperhash": "ke|fuzzy_paraphrases_in_learning_word_representations_with_a_lexicon", "conflicts": ["keio.ac.jp"], "keywords": ["Natural language processing", "Unsupervised Learning"], "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "authorids": ["enshika8811.a6@keio.jp", "hagiwara@keio.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1511854205724, "id": "ICLR.cc/2017/conference/-/paper56/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper56/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper56/AnonReviewer1", "ICLR.cc/2017/conference/paper56/AnonReviewer3"], "reply": {"forum": "r1osyr_xg", "replyto": "r1osyr_xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper56/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1511854205724}}}], "count": 17}