{"notes": [{"id": "C70cp4Cn32", "original": "m-eNDIPpcoj", "number": 1804, "cdate": 1601308199045, "ddate": null, "tcdate": 1601308199045, "tmdate": 1615928280327, "tddate": null, "forum": "C70cp4Cn32", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zB_ggx-rr8W", "original": null, "number": 1, "cdate": 1610040394051, "ddate": null, "tcdate": 1610040394051, "tmdate": 1610473988851, "tddate": null, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper studies a hierarchical or multi-level version of local SGD, extending earlier work by (Wang & Joshi, 2018), (Lin et al, 2018) and  (Jiang et al. 2019) among others. It gives novel convergence rates in relevant settings, such as by allowing different workers to take different numbers of local steps within a given time interval. The current analysis is restricted to the IID data case, but still insightful, and might serve as a useful building block for follow-up research in the future.\nSmaller concerns remained that the presented multi-level results cannot exactly recover local SGD as a special case. Nevertheless the consensus remained that the overall contributions and relevance of the paper remain above the bar. In the discussion phase, several concerns were clarified and additional deep learning experiments have been added to the paper, which is appreciated."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040394037, "tmdate": 1610473988833, "id": "ICLR.cc/2021/Conference/Paper1804/-/Decision"}}}, {"id": "SGTfo51KHWE", "original": null, "number": 1, "cdate": 1603784965859, "ddate": null, "tcdate": 1603784965859, "tmdate": 1606784627104, "tddate": null, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Review", "content": {"title": "Solid theoretical analysis but limited empirical validations", "review": "## Summary\nThis paper proposes a new variant of local SGD algorithm to make it be more realistic. In particular, (1) it allows workers to perform different number of local steps, depending on their computational resources; (2) workers are organized in a multi-level structure. Workers connected to one central hub can synchronize frequently and hubs are communicated in an infrequent and decentralized manor.\n\nThe authors provide convergence analysis under non-convex, iid data partition settings, and conduct preliminary experiments to validate their theoretical findings.\n\n## Pros\n1. This paper is easy to follow and very well-written.\n2. It makes a non-trivial extension of (Wang & Joshi, 2018). By introducing probability of taking local updates, the framework allows different workers to take different local steps within a given time interval. This is a realistic setting typically ignored by related literature.\n3. The analysis of the proposed algorithm is not trivial. It is nice to see how the authors model the complex algorithm in a simple way, although the formulation is roughly the same as (Wang & Joshi 2018).\n\n## Cons\n1. It seems that the results of MTL-SGD cannot recover local SGD? In particular, the additional error terms (the last two terms in (13)) increase with $q^2 \\tau^2$. However, in local SGD, the additional error terms increases linearly with $\\tau$, as shown in (Wang & Joshi, 2018). I didn't find any discussions on this discrepancy.\n2. The experimental results are too limited. Especially, it is hard to see the advantages of MTL-SGD over other two baselines. I encourage the authors to redefine the x-axis in experiments to time slots. Within one time slot, each worker has a probability $p_i$ to perform one local step. In local SGD, in order to finish one round of $\\tau$ local updates, the time slots required for one worker is $\\tau/p_i$, and hence, the time slots used for one round is $\\max_i \\tau/p_i$. However, in MTL-SGD, the time slots used per round can be exactly $\\tau$ by allowing workers to have different number of local steps. By doing this, MTL-SGD might have much faster convergence than local SGD, in terms of loss versus time slots.\n\n## Post-rebuttal\nI've read the authors' response and other reviewers' comments. The response and the updated version clarify my concerns. So I slightly increase my score.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110363, "tmdate": 1606915770843, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1804/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Review"}}}, {"id": "u8OE2Xf1CH", "original": null, "number": 10, "cdate": 1606246482230, "ddate": null, "tcdate": 1606246482230, "tmdate": 1606246482230, "tddate": null, "forum": "C70cp4Cn32", "replyto": "uFCroXsQjCf", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "re: post-rebuttal", "comment": "Thank you for taking your time to review our revision. We have submitted another revision to address your new comments.\n\n\u201cWhat is the difference between \"\u03c4 time slots\" and \"\u03c4 gradient steps\", regarding Figure 6? It would be great if the authors can include more simulation details, or release the source code.\u201d\n\nWe have added a few sentences to the second to last paragraph of Section 6 on page 9 to better explain the difference between time slots and gradient steps. In every time slot, each worker will take a gradient step with a probability $p_i$. When $p_i = 1$ for a worker $i$, the number of gradient steps taken will match the number of time slots $T$. Otherwise, the number of gradient steps taken will be $T \\cdot p_i$ in expectation. \n\nWe will work towards preparing our code for release on Github.\n\n\u201cIs it possible to study the impact of different \u03c4q values for the proposed MLL-SGD?\u201d\n\nDue to time constraints, we were unable to explore more values of $\\tau q$, though we agree it would be interesting to explore in future work\n\nWe also note that when reviewing the experiments we noticed a minor plotting error. The changes to the plots are negligible, but for completeness we have updated Figure 6 in the new submission. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "hOZLVglJbmT", "original": null, "number": 4, "cdate": 1603986294211, "ddate": null, "tcdate": 1603986294211, "tmdate": 1606216858065, "tddate": null, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Review", "content": {"title": "initial review", "review": "The paper extends the idea of hierarchical local SGD by extending the top hierarchy level to decentralized communication. The multi-level local SGD is achieved by the edge devices (performing all-reduce per \\tau local edge steps in terms of the corresponding hub) and hubs (gossip averaging per q local hub steps with the neighboring hubs).\n\n### pros\n* the paper considers a probabilistic form of the local edge update, making the whole system formulation more realistic.\n* A convergence analysis is given for the provided system formulation.\n\n### cons\n1. the formulated problem can be seen as a decentralized optimization problem with specifically designed time-varying communication topology and local edge update steps. It is better to compare the derived rate with the existing rate (e.g. in [1]) for the case of $p_i=1$, otherwise, it is unclear the tightness of the derived rate.\n2. the claim 'if the gradient of F has a large Lipschitz constant, the step size in the algorithm is large' looks strange to me; normally we have stepsize <= 1/L.\n3. the motivation example of the system is formed by edges devices and data-center hubs. however, in both theoretical analysis and numerical results, only the iid data case is been investigated. It somehow contrasts motivation.\n4. the numerical evaluation setup might be unfair. \n    1. the evaluated decentralized communication over the hubs is unclear to me and why it is a manually generated graph with $\\zeta=0.74$.\n    2. only the learning curves w.r.t. update step k are visualized. due to the extra gossip averaging steps over hubs, the current observations (i.e. improved convergence speed over local SGD) are reasonable but it is hard to identify the exact benefits of the proposed algorithm. is it possible to use a communication model to simulate some important metrics, e.g. time-to-target-loss? is it possible to include the numerical results of hierarchical local SGD e.g. in Lin et al (i.e. All-reduce is performed for hubs) as an extra baseline?\n5. The provided numerical results are limited to the toy problems and may not be sufficient to justify the effectiveness of the proposed algorithm, due to the existing large amount of local SGD work (either empirical or theoretical).\n\n### reference\n1. A Unified Theory of Decentralized SGD with Changing Topology and Local Updates, ICML 2020.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110363, "tmdate": 1606915770843, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1804/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Review"}}}, {"id": "uFCroXsQjCf", "original": null, "number": 9, "cdate": 1606216837095, "ddate": null, "tcdate": 1606216837095, "tmdate": 1606216837095, "tddate": null, "forum": "C70cp4Cn32", "replyto": "hOZLVglJbmT", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "post-rebuttal", "comment": "Thank you for the authors' responses and significant revision. The responses have addressed most of my concerns thus I will increase my score from 5 to 6. However, the authors are encouraged to further address the following points:\n1. what is the difference between \"$\\tau$ time slots\" and \"$\\tau$ gradient steps\", regarding Figure 6? It would be great if the authors can include more simulation details, or release the source code.\n2. Is it possible to study the impact of different $\\tau q$ values for the proposed MLL-SGD?\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "Z4WFQzgZGX5", "original": null, "number": 4, "cdate": 1606055234846, "ddate": null, "tcdate": 1606055234846, "tmdate": 1606055609424, "tddate": null, "forum": "C70cp4Cn32", "replyto": "hOZLVglJbmT", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (1/2)", "comment": "Thank you again for the thorough review. We have submitted a revision that addresses your comments.\n\n\"The formulated problem can be seen as a decentralized optimization problem with specifically designed time-varying communication topology and local edge update steps. It is better to compare the derived rate with the existing rate (e.g. in [1]) for the case of pi=1, otherwise, it is unclear the tightness of the derived rate.\"\n\nIn thoroughly examining the bound in [1], we agree that there is discrepancy with our bound. Specifically, there is an extra factor of $\\tau$. This discrepancy arises due to the formulation of $g_i$, namely, that $\\mathbb{E}[g_i] = p_i \\nabla F(x)$.  Because of this, there are cross terms in the expressions in equations in 156 and 173 that do not cancel out. Thus, we needed to use a more conservative analysis at these steps on the proof. This is the reason that plugging in a value of $p_i=1$ is not enough to recover the same bound as in [1]. We note that a reference [2] shows a similar discrepancy to the bound in [1] as our own results.\nWe have added a paragraph discussing this question at the end of Section 5 on page 7 before Corollary 1 and in Appendix section B.4 on page 35.\n\nReferences\n\n[1] A Unified Theory of Decentralized SGD with Changing Topology and Local Updates, ICML 2020.\n\n[2] J. Wang and G. Joshi. Cooperative sgd: A unified framework for the design and analysis of\ncommunication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.\n\n\"the claim 'if the gradient of F has a large Lipschitz constant, the step size in the algorithm is large' looks strange to me; normally we have stepsize <= 1/L.\"\n\nWe understand that the construction of this sentence is confusing since $\\eta$ and $L$ are typically inversely proportional, and we have removed this sentence.\n\n\"the motivation example of the system is formed by edges devices and data-center hubs. however, in both theoretical analysis and numerical results, only the iid data case is been investigated. It somehow contrasts motivation.\"\n\nWe agree that this assumption that the data distribution is IID may not apply to all scenarios. However, we believe our work lays an important foundation for convergence analysis in multi-level communication networks and where workers have heterogeneous operating rates. We believe that extending this analysis to data that is non-IID is an important open question as we state in the conclusion. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "5uUZXacd0a9", "original": null, "number": 8, "cdate": 1606055579624, "ddate": null, "tcdate": 1606055579624, "tmdate": 1606055579624, "tddate": null, "forum": "C70cp4Cn32", "replyto": "SGTfo51KHWE", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you again for the thorough review. We have submitted a revision that addresses your comments.\n\n\"It seems that the results of MTL-SGD cannot recover local SGD? In particular, the additional error terms (the last two terms in (13)) increase with q2\u03c42. However, in local SGD, the additional error terms increases linearly with \u03c4, as shown in (Wang & Joshi, 2018). I didn't find any discussions on this discrepancy.\"\n\nWe agree there is a discrepancy, and we have added a paragraph discussing this question at the end of Section 5 on page 7 before Corollary 1 and in Appendix section B.4 on page 35. This discrepancy arises due to the formulation of $g_i$, namely, that $\\mathbb{E}[g_i] = p_i \\nabla F(x)$.  Because of this, there are cross terms in the expressions in equations in 156 and 173 that do not cancel out. Thus, we needed to use a more conservative analysis at these steps on the proof. This is the reason that plugging in a value of $p_i=1$ is not enough to recover the same bound as in Cooperative SGD.\n\n\"The experimental results are too limited. Especially, it is hard to see the advantages of MTL-SGD over other two baselines. I encourage the authors to redefine the x-axis in experiments to time slots. Within one time slot, each worker has a probability pi to perform one local step. In local SGD, in order to finish one round of \u03c4 local updates, the time slots required for one worker is \u03c4/pi, and hence, the time slots used for one round is maxi \u03c4/pi. However, in MTL-SGD, the time slots used per round can be exactly \u03c4 by allowing workers to have different number of local steps. By doing this, MTL-SGD might have much faster convergence than local SGD, in terms of loss versus time slots.\"\n\nWe thank you for the insightful suggestion as to how to better evaluate MLL-SGD. We have added a new experiment at the end of Section 6 where we compare Local SGD and MLL-SGD with the x-axis measured in time slots as suggested. In every time slot, each worker takes a gradient step with a probability $p_i$. MLL-SGD will wait $\\tau$ time slots before averaging, regardless of the number of gradient steps taken, while Local SGD waits for all workers to take $\\tau$ gradient steps. This experiment shows that MLL-SGD offers a benefit in terms of convergence time. \nWe have also reworked the experiments section as a whole to better illustrate the benefits of the hierarchical model in MLL-SGD with $q>1$, while also exploring the effect of parameters in the algorithm. In addition to the experiments with the CNN, we have included results for CIFAR-10 with ResNet-18 for all experimental settings. The experiments using logistic regression on MNIST have been deferred to Appendix A.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "2XT--3Ro7e8", "original": null, "number": 7, "cdate": 1606055495171, "ddate": null, "tcdate": 1606055495171, "tmdate": 1606055495171, "tddate": null, "forum": "C70cp4Cn32", "replyto": "h2kaDsr4b0y", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you again for the thorough review. We have submitted a revision that addresses your comments.\n\n\"My concern about this paper is that it seems to me the algorithm has a similar motivation with grouping SGD:\nW. Jiang et al., \"A Novel Stochastic Gradient Descent Algorithm Based on Grouping over Heterogeneous Cluster Systems for Distributed Deep Learning,\" 2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), Larnaca, Cyprus, 2019, pp. 391-398, doi: 10.1109/CCGRID.2019.00053.\nI suggesting also comparing with it in the experiments. My first thought is that this paper should be faster than grouping-SGD when the network between hubs is not good (and that benefit comes from the fact that communication between hubs in this paper is decentralized).\"\n\nWe thank you for bringing this paper to our attention. We have added a discussion of Grouping-SGD to the related work section (Section 2 on page 2). After reading about Grouping-SGD in detail, we feel the motivation does differ in a significant way. The goal of the paper by Jiang et al. is to design clusters where workers of similar operating rates or computation power are grouped together. In MLL-SGD, our goal is to design an algorithm that performs well when a communication network structure is imposed a priori. Workers may be clustered due to geographic location, communication constraints, or business policies. The system model also differs as the parameters being trained are vertically (layerwise) partitioned across the parameter servers in Grouping-SGD. Each hub in MLL-SGD, however, is in charge of the full model and updates the entire model in specific training rounds. For these reasons, we feel these algorithms are not comparable. However, we have revised experiments in Section 6 to better illustrate the benefits of MLL-SGD over Local SGD and Hierarchical Local SGD.\n\n\"The hub in this paper does not seem necessary to me also, since it is used to average all local workers' model. Why not just do an AllReduce on all local workers and have one of the worker responsible for averaging with other groups? That should be easier to implement and more efficient.\"\n\nThe approach you describe is indeed another way to implement the proposed algorithm. Our analysis is applicable to this approach without modification. We utilize the hub-and-spoke model in each sub-network because this is a standard model in Federated Learning. In typical Federated Learning settings, it is not common for the workers to communicate with each other, hence we have chosen to formulate our problem setting in this specific way. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "HmTp9OMgU4t", "original": null, "number": 6, "cdate": 1606055388403, "ddate": null, "tcdate": 1606055388403, "tmdate": 1606055388403, "tddate": null, "forum": "C70cp4Cn32", "replyto": "i7VPprTwIh", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you again for the thorough review. We have submitted a revision that addresses your comments.\n\n\"Theorem 1 only considers constant step size and shows a (at least) constant upper bound for MLL-SGD. A theorem that involves decreasing step size and a upper bound that decreasing to zero can be more favorable.\"\n\nWe agree that analysis of MLL-SGD with a decreasing step size MLL-SGD is an important research question, however, it is not straightforward to extend our analysis to this setting. We have instead added analysis for the case when the convergence rate is constant but is proportional to $1/\\sqrt{K}$. This analysis is captured in Corollary 1 at the end of Section 5 on page 7. This result shows that MLL-SGD has the same asymptotic convergence rate as Local SGD and Hierarchical SGD.  \n\n\"Eq. (2) and (3). Though here the authors claim they take a probabilistic approach, but actually they treat tau(i)  as constants, where they should be random variables. Can you explain what would be a counterpart of Theorem 1 if  tau(i) are random variables?\"\n\nWe realize the discussion of $\\tau_i$ as it relates to $p_i$ may not have been clear. While each $\\tau_i$ is constant, the number of local steps a worker takes per global iteration is not, it is a random variable with expected value $tau_i$. We have clarified this definition in the first paragraph of Section 4 on page 4. This model captures a scenario where worker computation rate is relatively stable for the duration of training. We believe that a model where $tau_i$ can change over time is an interesting subject for future work.\n\n\"The proof mainly follows (Wang & Joshi, 2018) and the technical contribution is limited. With that being said, I appreciate the formal theory for MLL-SGD in the setting of hierarchic networks.\"\n\nWe thank the reviewer for appreciating the formal theory this paper has on hierarchical networks. We agree the proof follows a similar format to that of (Wang & Joshi, 2018), however we believe that the technical contribution is non-trivial. The addition of heterogeneous worker rates, weighted worker contributions, and multi-level network creates many key differences and difficulties that were not necessary to face in the proof of Cooperative SGD. \n\n\"Weak experiments. It can be great if there are some experiments that involve training models in real hierarchic networks.\"\n\nWe understand the importance of testing algorithms in realistic settings. We use synthetic networks so we can evaluate different parameters in isolation. However, we have expanded the experiments in Section 6 to include results with CIFAR-10 and ResNet-18, a more complex dataset and model to address the concern. We also have added a set of experiments to evaluate the convergence time of MLL-SGD against Local SGD which gives an indication of how the algorithms compare in real networks. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "KWf8HQCWhi", "original": null, "number": 5, "cdate": 1606055266589, "ddate": null, "tcdate": 1606055266589, "tmdate": 1606055266589, "tddate": null, "forum": "C70cp4Cn32", "replyto": "Z4WFQzgZGX5", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (2/2) ", "comment": "\"the numerical evaluation setup might be unfair.\nthe evaluated decentralized communication over the hubs is unclear to me and why it is a manually generated graph with \u03b6=0.74\"\n\nWe realize the choice of this hub network graph was not well-explained. A complete graph will have $\\zeta=0$, and a path graph (worst-case connected graph) has a $\\zeta$ on the order of $1-\\frac{1}{N}$. We selected that hub graph with zeta = 0.74 as a happy medium. However, we have revised our experiments and no longer use this graph in the section. We use a complete graph between hubs when comparing MLL-SGD with Local SGD, since MLL-SGD and Local SGD are equivalent when there is a complete hub graph and $q=1$. In the first experiment shown in Figure 1, we increase the value of $q$ in MLL-SGD while keeping $q\\tau$ the same to show its benefit over Local SGD.  \n\n\"only the learning curves w.r.t. update step k are visualized. due to the extra gossip averaging steps over hubs, the current observations (i.e. improved convergence speed over local SGD) are reasonable but it is hard to identify the exact benefits of the proposed algorithm. is it possible to use a communication model to simulate some important metrics, e.g. time-to-target-loss? is it possible to include the numerical results of hierarchical local SGD e.g. in Lin et al (i.e. All-reduce is performed for hubs) as an extra baseline?\"\n\nWe appreciate the suggestion to explore the convergence speed in the experiments. We have added a new experiment at the end of Section 6 in Figure 4 where we compare Hierarchical Local SGD and MLL-SGD with the x-axis measured in \u201ctime slots.\u201d In every time slot, each worker will take a gradient step with a probability $p_i$. MLL-SGD will wait $\\tau$ time slots before averaging, regardless of the number of gradient steps taken, while Hierarchical Local SGD waits for all workers to take $\\tau$ gradient steps. This allows us to compare the progress of each algorithm over time. Our results show a benefit to MLL-SGD over Hierarchical Local SGD in terms of convergence time.\n\n\"The provided numerical results are limited to the toy problems and may not be sufficient to justify the effectiveness of the proposed algorithm, due to the existing large amount of local SGD work (either empirical or theoretical).\"\n\nWe understand your concern and have completely reworked the experiments section (Section 6). We now include results using a CNN with EMNIST, as well as ResNet-18 with CIFAR-10 for all of the experimental settings. Experiments using logistic regression on MNIST have been deferred to the Appendix A. We have also added experiments to explicitly study the convergence time, as described above. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "taBb5iBEebY", "original": null, "number": 3, "cdate": 1605398243258, "ddate": null, "tcdate": 1605398243258, "tmdate": 1605398243258, "tddate": null, "forum": "C70cp4Cn32", "replyto": "CNnEKd3QGib", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "clarify \"toy problems\"", "comment": "The paper only evaluated the logistic regression model on MNIST and (a simple) CNN model on EMNIST; these empirical results are not sufficient to justify the trade-off (communication v.s. training/test performance) of the proposed multi-level local SGD.\n\nIt is suggested to include some results for training standard neural networks (e.g. ResNet) on relative challenge datasets (e.g. CIFAR)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "CNnEKd3QGib", "original": null, "number": 2, "cdate": 1605384214142, "ddate": null, "tcdate": 1605384214142, "tmdate": 1605384214142, "tddate": null, "forum": "C70cp4Cn32", "replyto": "hOZLVglJbmT", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment", "content": {"title": "Clarification question", "comment": "Thank you for your thorough review. We plan on addressing all your concerns in a revision of the paper. We had a question of clarification pertaining to your final point. By \u201ctoy problems\u201d are you referring to the model and the dataset used?  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "C70cp4Cn32", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1804/Authors|ICLR.cc/2021/Conference/Paper1804/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Comment"}}}, {"id": "h2kaDsr4b0y", "original": null, "number": 2, "cdate": 1603951320119, "ddate": null, "tcdate": 1603951320119, "tmdate": 1605024354096, "tddate": null, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Review", "content": {"title": "Unsure about how this compares to grouping SGD", "review": "The paper proposed a multi level SGD algorithm, where workers are assigned to different groups, and each group averages local workers' model. \n\nThe idea is a natural extension on existing SGD algorithms (and should be beneficial in heterogeneous networks). The proof should not be hard based on existing asynchronous/decentralized SGD proof. My concern about this paper is that it seems to me the algorithm has a similar motivation with grouping SGD:\n\nW. Jiang et al., \"A Novel Stochastic Gradient Descent Algorithm Based on Grouping over Heterogeneous Cluster Systems for Distributed Deep Learning,\" 2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), Larnaca, Cyprus, 2019, pp. 391-398, doi: 10.1109/CCGRID.2019.00053.\n\nwhich is a straightforward extension of AllReduce SGD. I suggesting also comparing with it in the experiments. My first thought is that this paper should be faster than grouping-SGD when the network between hubs is not good (and that benefit comes from the fact that communication between hubs in this paper is decentralized).\n\nThe hub in this paper does not seem necessary to me also, since it is used to average all local workers' model. Why not just do an AllReduce on all local workers and have one of the worker responsible for averaging with other groups? That should be easier to implement and more efficient.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110363, "tmdate": 1606915770843, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1804/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Review"}}}, {"id": "i7VPprTwIh", "original": null, "number": 3, "cdate": 1603953002104, "ddate": null, "tcdate": 1603953002104, "tmdate": 1605024354033, "tddate": null, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "invitation": "ICLR.cc/2021/Conference/Paper1804/-/Official_Review", "content": {"title": "This paper proposes MLL-SGD for training models in hierarchic networks", "review": "This paper extends (Wang & Joshi, 2018) and proposes MLL-SGD for training models in hierarchic networks, where the network consists multiple sub-networks, and each sub-network contains multiple workers. In the level of sub-networks, models can be averaged. In the level of workers, the local copies of models can be averaged within a sub-network; however, workers cannot communicate directly with those from a different sub-networks. In such setting, MLL-SGD is proved to enjoy certain convergence property.\n\n\n# Pros\nHierarchic networks are a common object in practice, but current SGD algorithms fail to cover it due to the hierarchic communication restrictions. Though MLL-SGD itself is a quite na\u00efve extension of existing local SGD methods for handling hierarchic networks, the convergence theory is not a trivial work. I think this paper can serve as a ground work for developing more efficient algorithms for training models in hierarchic networks.\n\n\n# Cons\n- Theorem 1 only considers constant step size and shows a (at least) constant upper bound for MLL-SGD. A theorem that involves decreasing step size and a upper bound that decreasing to zero can be more favorable.\n\n- Eq. (2) and (3). Though here the authors claim they take a probabilistic approach, but actually they treat $tau^(i)$ as constants, where they should be random variables. Can you explain what would be a counterpart of Theorem 1 if $tau^(i)$ are random variables?\n\n- The proof mainly follows (Wang & Joshi, 2018) and the technical contribution is limited. With that being said, I appreciate the formal theory for MLL-SGD in the setting of hierarchic networks.\n\n- Weak experiments. It can be great if there are some experiments that involve training models in real hierarchic networks.\n\n\nIn sum, I think this work is OK but not very significant. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1804/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1804/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks", "authorids": ["~Timothy_Castiglia1", "dasa2@rpi.edu", "~Stacy_Patterson1"], "authors": ["Timothy Castiglia", "Anirban Das", "Stacy Patterson"], "keywords": ["Machine Learning", "Stochastic Gradient Descent", "Federated Learning", "Hierarchical Networks", "Distributed", "Heterogeneous", "Convergence Analysis"], "abstract": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.", "one-sentence_summary": "We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.", "pdf": "/pdf/8dbde4c960f1598d19e7201058a77a1224b4a939.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castiglia|multilevel_local_sgd_distributed_sgd_for_heterogeneous_hierarchical_networks", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncastiglia2021multilevel,\ntitle={Multi-Level Local {\\{}SGD{\\}}: Distributed {\\{}SGD{\\}} for Heterogeneous Hierarchical Networks},\nauthor={Timothy Castiglia and Anirban Das and Stacy Patterson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=C70cp4Cn32}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "C70cp4Cn32", "replyto": "C70cp4Cn32", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1804/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110363, "tmdate": 1606915770843, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1804/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1804/-/Official_Review"}}}], "count": 15}