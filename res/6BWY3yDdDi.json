{"notes": [{"id": "6BWY3yDdDi", "original": "sBQRFRbxkVF", "number": 2463, "cdate": 1601308272054, "ddate": null, "tcdate": 1601308272054, "tmdate": 1614985764591, "tddate": null, "forum": "6BWY3yDdDi", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xJss0QWXzZm", "original": null, "number": 1, "cdate": 1610040369324, "ddate": null, "tcdate": 1610040369324, "tmdate": 1610473960522, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All reviewers agree that this paper is interesting, but needs improvement in order to be suitable for a highly competitive venue such as ICLR. Reviewer 3 is especially incisive and detailed, but other reviewers make similar points."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040369309, "tmdate": 1610473960502, "id": "ICLR.cc/2021/Conference/Paper2463/-/Decision"}}}, {"id": "HsMP679crn", "original": null, "number": 4, "cdate": 1604507007486, "ddate": null, "tcdate": 1604507007486, "tmdate": 1606807207506, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Official_Review", "content": {"title": "LSH for negative sampling, as heuristic for top-k softmax. Implementation on multi-threaded CPU. Does not beat sampled softmax on GPU, gives no accuracy or approximation guarantees.", "review": "### Summary\nThe paper proposes a heuristic version of top-k negative sampling which is computationally effective. The main toolbox for this heuristic is locality sensitive hashing. The contribution is mainly algorithmic with an implementation. Experimental results support the effectiveness of the heuristic.\n\n### Strengths\n+ The use of LSH in machine learning applications is very promising, and this paper takes this trend to yet another direction where it can make a difference.\n+ The approach is simple to describe and implement.\n\n### Weaknesses\n- There are several claims that are mostly marketing, namely \u201cadaptivity\u201d and \u201cdistribution awareness\u201d. Yes, the sampling depends on the updated weights. But exactly how and in what manner that relates to, say, approximating the true objective, are not treated. The best intuition we can get is by thinking of the new method as a heuristic to top-k negative sampling, leading to believe that maybe some of the latter\u2019s statistical/optimization properties are inherited. But this relationship is not made at all.\n- The Theorems are mostly ornamental, they do not add anything new or relevant.\n- It\u2019s true that the proposed algorithm is amenable to a CPU implementation. But the CPU on which the experiments are run is a behemoth (28 core, 224 thread). So it clearly still needs the added parallelism to be competitive.\n- The significance of the result are weakened when compared against one of the simplest alternatives, the sample softmax (orange curves in Figure 4). It is evident that in terms of reaching the vicinity of the ultimate accuracy, the sample softmax takes the same (if not less) time than the new method [the per-iteration plots are not very relevant, they demonstrate interim accuracy but have no bearing on ultimate accuracy and speed up].\nSo claims like \u201c[these alternatives] fail to demonstrate any training time improvements\u201d (page 2) are clearly false. Granted, sample softmax would be of order $O(\\log N)$, but in practice this clearly is not a handicap.\n- ~~For an application such as this, the details of LSH\u2019s choices are critical, yet the paper only glosses over them. For example, the similarity metrics should differ between the \u201cembedding\u201d version and the \u201clabel\u201d version, this is briefly alluded to in the experiments section without much explanation or references.~~ [Edit: thanks for clarifying this.]\n\n### Overall\nThe motivation and approach of the paper are very strong. But the results are not as strong as they are hyped up to be, since a simple alternative achieves the same practical accuracy/time benefits and no guarantees are given for the accuracy of the new heuristic sampling, not even in terms of approximation of another heuristic such as top-k. This means any potential adoption would be based merely on experimental evidence. The fact that the algorithm can be implemented on a (highly parallel) CPU is not a good enough selling point, especially when not pitted against an equivalent optimized CPU implementation of the simple alternative.\n\n_[Edit: The authors do not give any substantive feedback to my review, except for clarifying the hash choices. It is surprising that they object so vehemently to my intuitive description of their method as a heuristic to top-k, when they themselves write \"Our proposed negative sampling scheme is a proxy to topK-softmax. It selects the top-k classes via LSH [...]\". Also my reading of the sampled softmax is directly from their paper, showing a comparable accuracy-time tradeoff, but I was not refuted on this and instead was given other references claiming the inferiority of that method. I have updated my recommendation to reflect these shortcomings.]_\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2463/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2463/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095792, "tmdate": 1606915763843, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2463/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2463/-/Official_Review"}}}, {"id": "tzQPcaK895p", "original": null, "number": 1, "cdate": 1603779575655, "ddate": null, "tcdate": 1603779575655, "tmdate": 1606331658100, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Official_Review", "content": {"title": "Simple yet effective approach for efficient negative sampling", "review": "When the number of classes is very large, calculating softmax for classification (e.g., in backpropagation) is computationally costly. Approaches based on negative sampling have been used in literature to alleviate this problem. However, most of existing approaches are (argued to be) either inaccurate or computationally costly. This paper proposes to use the well-known LSH (locality sensitive hashing) method to address this problem. In particular, two variants, LSH label and LSH Embedding are showed to speed up the training in terms of time needed to converge compared with a number of baseline methods over three large scale datasets. \n\n+ The suggested approach is simple, making it potentially useful in practice. \n+ The methodological contribution of the paper is more or less an off-the-shelf use of LSH for negative sampling. This being said, the application of LSH in this context is (seemingly) new, and the two basic similarity measures are interesting.\n+ I would be cautious about calling the method \"A truly constant-time\" method. If we assume K and L are constant then yes; but theoretically in order to get good result we need to have larger values especially for L. Please elaborate on this.\n+ The two theorems offered in the paper are the theorems from the LSH literature. Since a proxy is being used for similarity (e.g., label or embedding) these do not translate into the final classification result. The authors can perhaps elaborate on this and also give clues on how large K and L should be set depending on the parameters of the dataset, etc.\n+ In intro we have \"We show that our technique is not only provable but ..\"; it is not clear what is exactly provable, and how it relates to the classification result. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2463/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2463/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095792, "tmdate": 1606915763843, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2463/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2463/-/Official_Review"}}}, {"id": "OKYsPaEpXDY", "original": null, "number": 8, "cdate": 1605910813729, "ddate": null, "tcdate": 1605910813729, "tmdate": 1605911044429, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "tzQPcaK895p", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "We thank the reviewer for their valuable comments.\n\n- The total cost of sampling is KL hash computations, followed by a constant number  (< 100) of bucket lookups. The KL hash computations can be done with fast hash functions in one read of the vector, so essentially the hashing cost is the same as input vector reading cost. The cost of aggregating a constant number (At most L) buckets is negligible compared to the cost of sorting N numbers (where N is in hundreds of thousands).  If this was not the case there is no way we could have achieved faster training time on CPU compared to Tensorflow packages which use the powerful V100.\nIn theory, the cost is O(KL) which is independent of L in practice. K is smaller than 10 and L is in few hundreds, so the computation is in few thousands, compared to hundreds of thousands which is the size of N. \n- Please refer to the \u201cintuition of LSH Label/Embedding\u201d and the theorems in section 2.1 of the paper. As stated in the paper, a desirable negative sampling should be dependent on the input and the parameters (which keep changing over time). When the network has not learned anything (initial phases of training), any random class is equally confusing, and hence random sampling is desirable. However, near convergence, as mentioned in the paper, the best negative class is the closest enough class (example stated in the paper is \"Adidas shoes\" vs. \"Nike shoes\"). No method in the literature has this adaptivity as the training progresses. Either they are static (sampled softmax) or only dependent on input (learning-based). However, none of them will change the distribution as the network learns. Our method is the first to have this adaptivity where the sampling cost is O(1). The collision probability (p) in theorem1 and theorem2 is with respect to the last layer's parameters, which will be random in the initial phases. Hence, our samples will be similar to random samples. However, as the network learns and figures out how to discriminate between different categories, similar classes based on last layers\u2019 weights are selected more frequently. \n-Results are sensitive to K due to the exponential growth of buckets, and the rule of thumb is K={8,9} for simhash hash function and K={5,6} for DWTA hash function. The results are not sensitive to choice of L. We only tried a few choices of K in all the experiments and it is mentioned in the paper. \n- Our technique is provably adaptive. We have modified this part in the paper. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2463/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6BWY3yDdDi", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2463/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2463/Authors|ICLR.cc/2021/Conference/Paper2463/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848095, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2463/-/Official_Comment"}}}, {"id": "XZAJ5Ksy8fm", "original": null, "number": 5, "cdate": 1605906092367, "ddate": null, "tcdate": 1605906092367, "tmdate": 1605909075432, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "HsMP679crn", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Official_Comment", "content": {"title": "Not a heuristic for top-k softamx, and outperforms sampled softmax on GPU", "comment": "We thank the reviewer for their valuable comments. \n- The exact objective for \"negative sampling\" is still a matter of debate. For instance, even the original Mikolov's paper [1] does not define any. However, as stated in the paper, a desirable negative sampling should be input dependent and dependent on the parameters (which keep changing over time). If we take the active sampling perspective [2,3], the most confusing class is an excellent negative sample. When the network has not learned anything (initial phases of training), any random class is equally confusing, and hence random sampling is desirable. However, near convergence, as mentioned in the paper, the best negative class is the closest enough class (example stated in the paper is \"Adidas shoes\" vs. \"Nike shoes\"). No method in the literature has this adaptivity as the training progresses. Either they are static (sampled softmax) or only dependent on input (learning-based). However, none of them will change the distribution as the network learns. Our method is provably, the first to have this adaptivity and where the sampling cost is O(1). If you look at Theorem 1 and Theorem 2, and the associated comments written in the paper, the collision probability (p) in those formulas is with respect to the last layer's parameters, which will be random in the initial phases. Hence, our samples will be similar to random samples. However, as the network learns and figures out how to discriminate between different categories, similar classes based on last layers\u2019 weights are selected more frequently.  The proposal is NOT similar to the top-k heuristic because it rarely samples the top-k. All it guarantees is that the most similar element has the highest chance of sampling in expectations. It is also not heuristic because we can exactly compute the sampling probabilities.\\\n We can write a discussion on this if needed, but we hope our proposal method's subtlety is appreciated. It is easy to miss that because there is nothing in the literature capable of achieving that in constant time.\n- Please see previous comments. The theorems give the precise sampling probability and are needed to prove that they are adaptive. Also, the formulas provide insight into how the sampling relates with the learned weights, and how the  change of weights over time changes the negative sampling distribution.  \n- Yes, our algorithm is data parallel and can utilize any number of cores, however taking into consideration that CPUs are much more economically viable than GPUs and moreover, they offer more flexibility in terms of memory, our algorithm amenability to CPUs is an advantage. \n- We believe that the reviewer\u2019s statement \u201cdoes not beat sampled softmax on GPU\u201d is not correct. Sampled softmax in general is a fast proxy to full softmax, however it is not a desirable proxy due to its low accuracy. Please look at [4,5,6,7],  sampled softmax has poor accuracy and convergence even though it is fast. Same is observed in our experiments. For example on Amazon670 sampled softmax barely reaches 34% while ours easily goes to 36%.  So the efficiency comes at the cost of accuracy with static sampling like sampled softmax (as explained  in the paper).\n- In our experiments the hash function utilized for each dataset is the same for both versions of LSH Labl and LSH Embedding. We used the standard Signed Random Projections and DTWA for the hash functions and have added a part for them in the appendix (A.2 and A.3).  Choice of LSH is an interesting discussion which is left to future work, given that LSH is a more than 2 decade old topic and there are a variety of options.  \n\n[1]: Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems. 2013.\\\n[2]: Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. \"Training region-based object detectors with online hard example mining.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\\\n[3]: P. F. Felzenszwalb, R. B. Girshick, D. McAllester and D. Ramanan, \"Object Detection with Discriminatively Trained Part-Based Models,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 9, pp. 1627-1645, Sept. 2010, doi: 10.1109/TPAMI.2009.167.\\\n[4] Rawat, Ankit Singh, et al. \"Sampled softmax with random fourier features.\" Advances in Neural Information Processing Systems. 2019.\\\n[5]Blanc, G. & Rendle, S.. (2018). Adaptive Sampled Softmax with Kernel Based Sampling.Proceedings of the 35th International Conference on Machine Learning, in PMLR80:590-599\\\n[6]Bamler, Robert, and Stephan Mandt. \"Extreme Classification via Adversarial Softmax Approximation.\" arXiv preprint arXiv:2002.06298 (ICLR 2020).\\\n[7]Y. Bengio and J. -S. Senecal. 2008. Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model. Trans. Neur. Netw. 19, 4 (April 2008)\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2463/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6BWY3yDdDi", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2463/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2463/Authors|ICLR.cc/2021/Conference/Paper2463/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848095, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2463/-/Official_Comment"}}}, {"id": "nACCKCyZrk", "original": null, "number": 7, "cdate": 1605909033685, "ddate": null, "tcdate": 1605909033685, "tmdate": 1605909033685, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "YcRFPgAaNI-", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "We thank the reviewer for the recognition of our work\u2019s subtlety and appreciate their constructive comment. We have polished the paper and uploaded the new version. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2463/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6BWY3yDdDi", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2463/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2463/Authors|ICLR.cc/2021/Conference/Paper2463/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848095, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2463/-/Official_Comment"}}}, {"id": "gFCdmley6up", "original": null, "number": 6, "cdate": 1605908600979, "ddate": null, "tcdate": 1605908600979, "tmdate": 1605908600979, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "Q2drg7xo13t", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Official_Comment", "content": {"title": "Clarification on some misunderstandings", "comment": "We thank the reviewer for their valuable comments.\n\nWe would like to clarify the misunderstanding here. Our algorithm is not about near neighbor search, it is about sampling via LSH to retrieve the most confusing labels for the classifier. There is no notion of neighbor here, it is just similarity and sampling probability. We request reviewer to please revisit Theorem 1 and 2 for the insight. LSH is known to be expensive for exact search because it requires a large number of hash tables. If we use LSH for search, it will be orders of magnitude slower. We instead use LSH for adaptive sampling, which is very efficient and sub-linear because retrieving only a few buckets suffices for adaptive sampling. \n\ni) Most negative sampling is concerned with efficiency and does not require calibration. We followed the standard strategy in the Mikolov\u2019s paper[1]\n\n[1]: Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems. 2013.\n\nii) Results are sensitive to K due to the exponential growth of buckets, and the rule of thumb is K={8,9} for simhash hash function and K={5,6} for DWTA hash function. The results are not sensitive to choice of L. We only tried a few choices of K in all the experiments and it is mentioned in the paper.\niii) We are not doing any precision tuning. The experiments with CPU use only CPU and experiments with GPU use standard Tensorflow packages. \n\ni) The aforementioned paper by  J. Huang et al. explores embedding-based search retrieval which is a different objective from our proposal. We propose a negative sampling framework which aims to approximate full softmax via label sampling. Hard negative sampling has been always known to be a significant approach for efficient training[1,2]. [3] argues that \u201chard\u201d negatives maintain a better signal-to-ratio in gradient and encourage efficient training, while \u201ceasy negatives\u201d retain lower signal-to-noise ratio due to the low correlation between input feature and sampled labels. \n\n[1]: Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. \"Training region-based object detectors with online hard example mining.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\\\n[2]: P. F. Felzenszwalb, R. B. Girshick, D. McAllester and D. Ramanan, \"Object Detection with Discriminatively Trained Part-Based Models,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 9, pp. 1627-1645, Sept. 2010, doi: 10.1109/TPAMI.2009.167.\\\n[3] Bamler, Robert, and Stephan Mandt. \"Extreme Classification via Adversarial Softmax Approximation.\" arXiv preprint arXiv:2002.06298 (ICLR 2020).\n\nii) The total cost of sampling is KL hash computations, followed by a constant number  (< 100) of bucket lookups. The KL hash computations with fast hash functions can be done in one read of the vector so essentially the hashing cost is the same as input vector reading cost. The cost of aggregating a constant number (At most L) buckets is negligible compared to the cost of sorting N numbers (where N is in hundred of thousands.).  If this was not the case there is no way we could have achieved faster training time on CPU compared to all Tensorflow packages which use the powerful V100. \n\nIn theory, the cost is O(KL) which is independent of L in practice. K is smaller than 10 and L is in few hundreds, so the computation is in few thousands, compared to the size of N. \n\niii) We have shown the complete accuracy climb of the two methods with both iteration and running time. Yes, they seem to work similarly on most datasets except text8.\n\nWe hope our clarifications on the misunderstandings will be taken into consideration and  our proposal method's subtlety is appreciated.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2463/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6BWY3yDdDi", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2463/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2463/Authors|ICLR.cc/2021/Conference/Paper2463/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848095, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2463/-/Official_Comment"}}}, {"id": "YcRFPgAaNI-", "original": null, "number": 2, "cdate": 1603863335872, "ddate": null, "tcdate": 1603863335872, "tmdate": 1605024205257, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Official_Review", "content": {"title": "Looks like a great paper, somewhat sloppy writing", "review": "The paper studies extreme classification, where the number of classes is very large.\n\nIn that case, previous work came up with a technique to subsample bad classes for each data point to approximate cross-entropy or other used loss. Prior work used uniform samples, but this paper proposes an approach based on locality-sensitive hashing. One particular challenge that the authors need to overcome is updating LSH tables during the backpropagation (where parameters get updated).\n\nThe experiments are very convincing and show that CPU C++ implementation that uses new sampling can outperform GPU-based approach based on uniform sampling.\n\nOne comment is that the writing is fairly sloppy and could use additional polishing.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2463/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2463/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095792, "tmdate": 1606915763843, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2463/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2463/-/Official_Review"}}}, {"id": "Q2drg7xo13t", "original": null, "number": 3, "cdate": 1604263693545, "ddate": null, "tcdate": 1604263693545, "tmdate": 1605024205120, "tddate": null, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "invitation": "ICLR.cc/2021/Conference/Paper2463/-/Official_Review", "content": {"title": " I think this paper over-claim its contribution even though it proposes a tractable solution to extreme classification.", "review": "This paper proposes a neighbor search method for negative sampling in extreme classification. The core idea is to use hash tables to select neighbors of a query.\n\nI agree with the authors that:\ni) The proposed hashing method can quickly retrieve neighbors for arbitrary queries in nearly constant time.\nii) Selecting hash functions is important for potential gain.\n\nHowever, I think there are still missing pieces in this paper that need to clarify:\ni) How do you calibrate the negative sampling results?\nii) How do you tune K and L in your experiments. To my understanding, using either too large or too small K and L is suboptimal in your setup.\niii) You mentioned that you are using CPU or computation. But the CPU and GPU precisions may contribute to the final result. Have you studied that in your experiments?\n\nThere are still some points that I disagree with:\ni) There is no consensus that whether \u201ceasy negatives\u201d or \u201chard negatives\u201d should be selected in extreme classifications. For a recent survey on this topic, see \u201cEmbedding-based retrieval in Facebook search\u201d by J. Huang et al. So I wonder if the hashing algorithm has the capacity to choose both easy and hard negatives?\n\nii) You claim you are using an O(1) algorithm independent of negative classes N. But this is simply not true and not reasonable because your K and L may need to increase when N grows in order to keep a constant collision probability.\n\niii) You mentioned you propose two schemes: \u201cLSH Embedding\u201d and \u201cLSH Label\u201d. But those two methods only differ in query representations. It would be interesting to compare these two carefully in experiments.\n\nOverall, I think this paper over-claim its contribution even though it proposes a tractable solution to extreme classification.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2463/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2463/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Truly Constant-time Distribution-aware Negative Sampling", "authorids": ["~Shabnam_Daghaghi1", "~Tharun_Medini1", "~Beidi_Chen1", "mengnan.zhao@rice.edu", "~Anshumali_Shrivastava1"], "authors": ["Shabnam Daghaghi", "Tharun Medini", "Beidi Chen", "Mengnan Zhao", "Anshumali Shrivastava"], "keywords": [], "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "daghaghi|a_truly_constanttime_distributionaware_negative_sampling", "one-sentence_summary": "We provide two LSH based hard-negative sampling strategies and an efficient C++implementation that outperforms Tensorflow-GPU on time while retaining precision.", "pdf": "/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7RApYWr4ch", "_bibtex": "@misc{\ndaghaghi2021a,\ntitle={A Truly Constant-time Distribution-aware Negative Sampling},\nauthor={Shabnam Daghaghi and Tharun Medini and Beidi Chen and Mengnan Zhao and Anshumali Shrivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=6BWY3yDdDi}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6BWY3yDdDi", "replyto": "6BWY3yDdDi", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2463/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095792, "tmdate": 1606915763843, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2463/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2463/-/Official_Review"}}}], "count": 10}