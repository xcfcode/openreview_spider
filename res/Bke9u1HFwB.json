{"notes": [{"id": "Bke9u1HFwB", "original": "ryxw8GAdvr", "number": 1814, "cdate": 1569439601901, "ddate": null, "tcdate": 1569439601901, "tmdate": 1577168255017, "tddate": null, "forum": "Bke9u1HFwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "H-ehLtvS7", "original": null, "number": 1, "cdate": 1576798733050, "ddate": null, "tcdate": 1576798733050, "tmdate": 1576800903378, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Decision", "content": {"decision": "Reject", "comment": "The paper makes broad claims, but the depth of the experiments is very limited to a narrow combination of algorithms.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726166, "tmdate": 1576800278235, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Decision"}}}, {"id": "SklzXhN3sS", "original": null, "number": 5, "cdate": 1573829657665, "ddate": null, "tcdate": 1573829657665, "tmdate": 1573829657665, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment", "content": {"title": "Paper has not been revised", "comment": "Unfortunately, we were not able to finish revising the paper in the given time and include additional evaluations. Thus, we'll proceed with the initial version of the manuscript.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke9u1HFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1814/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1814/Authors|ICLR.cc/2020/Conference/Paper1814/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150508, "tmdate": 1576860545320, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment"}}}, {"id": "BygXZnm2sr", "original": null, "number": 2, "cdate": 1573825530694, "ddate": null, "tcdate": 1573825530694, "tmdate": 1573827328269, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment", "content": {"title": "Response to the most common points raised by the reviewers", "comment": "We want to thank all reviewers for all the time spent on analyzing our papers and for the constructive feedback. It is very much appreciated. We'll try to summarize and address all the concerns below.\n\n\n1. The paper is framed as an analysis of MBRL methods but only compares to the dyna-style algorithms.\n\nWe aimed to show that recent advancements in sample efficiency of DRL (as benchmarked by the Atari domain) are solely due to using invalid baselines. I.e., they tend to use Rainbow/DQN in it's original or hypertuned form from Hessel et al. (2018) or Mnih et al. (2015). This form, however, did not focus on data efficiency but can be very easily modified to do so (as presented by OTRainbow). We chose the model-based dyna-like SimPLe as an example because it showed the most impressive results in that area. Nevertheless, we did not want to focus on MBRL but on sample-efficient RL as a whole. We agree that both the title and parts of the body are misleading. Simply when writing this paper, the MBRL algorithms were the main studies we were aware of that reported meaningful improvements over the Rainbow DQN. However, the same point would apply to MFRL for Atari. As an example, Lee et al. (2019) proposed a novel way of using experience replay for DQN that improves DQN's data efficiency. However, while doing so, it also increases the ratio r, without doing the same for the baseline to ensure a fair comparison.\n\nWe're in the process of experimentally comparing EBU and Overtrained version of DQN, and we will revise our paper to make our aims clearer. \n\n\n2. Other papers (Holland et al., 2018; van Hasselt et al., 2019) have already introduced similar ideas. \n\nWhile we agree it is often the case, there are some missing points that our work tries to cover:\n\na) Holland et al. (2018) have already shown that more training updates with the agent's existing experience results in higher sample efficiency of DQN. Nevertheless, Kaiser et al. (2019) mention that SimPLe outperforms the DynaDQN (and thus it outperforms replay-based methods). We clearly show that it is not the case.\n\nb) van Hasselt et al. (2019) is a concurrent study that focuses on a comparison of model-based and replay-based algorithms. While the conclusion is similar, our goal, first and foremost, is to underline the importance of using appropriate baselines when introducing more data-efficient algorithms, not to analyze differences between different types of methods. To adhere to our aim of establishing a fair evaluation of data efficiency, we will add the comparison between EBU and Overtrained DQN showing that the problem of wrong baselines is not only relevant to MBRL.\n\nd) Given the points above, we believe our study adds an essential aspect to the discourse in data-efficient RL. Especially, given that novel algorithms, although significant in other areas, are praised for their substantial improvements of data-efficiency, even though well established, existing methods are not worse. EBU and SimPLe being an example.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke9u1HFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1814/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1814/Authors|ICLR.cc/2020/Conference/Paper1814/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150508, "tmdate": 1576860545320, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment"}}}, {"id": "r1ll5A73sB", "original": null, "number": 3, "cdate": 1573826183948, "ddate": null, "tcdate": 1573826183948, "tmdate": 1573827316570, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "H1eNYZKRtS", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1", "comment": "Thank you for your thorough review and useful feedback. Please find our response to the two issues raised below:\n\n\nIssue 1:\nIn addition to comparing OTRainbow(100k) with SimPLe(100k), Figure 2 serves the analysis of the long term effects for each of the versions of Rainbow we introduced, similarly to what (Kaiser et al.) did in section 7.3. We do not aim to compare, e.g., OTRainbow(500k) with SimPLe(100k) because clearly, as you pointed out, it is an unfair comparison.\n\nHowever, given that we are currently extending our study to cover another recent advancement in the data-efficient reinforcement learning, analysis backed by this figure becomes less relevant. Therefore, as per your suggestion, we will delete it from the paper.\n\n\nIssue 2:\nWe addressed the second issue in the first point of our response to all reviewers. Please find the details there. \n\nIn short, as you have pointed out, the problem was in the framing of the paper. It aimed to focus on advancement in data efficiency for Atari (including MFRL for Atari), and not on MBRL as a whole."}, "signatures": ["ICLR.cc/2020/Conference/Paper1814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke9u1HFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1814/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1814/Authors|ICLR.cc/2020/Conference/Paper1814/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150508, "tmdate": 1576860545320, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment"}}}, {"id": "H1xRhf42sr", "original": null, "number": 4, "cdate": 1573827253523, "ddate": null, "tcdate": 1573827253523, "tmdate": 1573827303111, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "ByechtJ6tB", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment", "content": {"title": "Response to Official Blind Review #2", "comment": "Thank you for your thorough review and useful feedback. We identified two points raised in your comment.\n\n\nPoint 1: The paper is limited to image-space discrete RL tasks, and does not cover the wide range of other RL.\n\nWe partially addressed the second issue in the first point of our response to all reviewers. Please find the details there. \n\nIn short, the problem was in the framing of the paper. It aimed to underline the importance of using fair baselines when proposing improvements in data efficiency of reinforcement learning algorithms and not necessarily compare MFRL to MBRL. We mainly found this issue in the studies that focus on image-space discrete RL.\n\n\nPoint 2: The proposed algorithm is not fundamentally different from Rainbow. \n\nIndeed, that is the case. We did not intend to introduce a novel algorithm. Instead, our goal was to underline the importance of using appropriate baselines for data-efficient reinforcement learning by showing that already existing DQN-like algorithms can be much more data-efficient than it is often portrayed.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke9u1HFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1814/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1814/Authors|ICLR.cc/2020/Conference/Paper1814/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150508, "tmdate": 1576860545320, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment"}}}, {"id": "ByechtJ6tB", "original": null, "number": 1, "cdate": 1571776946004, "ddate": null, "tcdate": 1571776946004, "tmdate": 1572972420139, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?\n\nIn this paper, the authors revisit the model-free baselines used in recent model-based reinforcement learning algorithms. And after more careful tuning of the hyperparameters, the model-free baselines can obtain comparable and much better performance using the same number of samples.\n\nThe paper is well-written, and the experiments are well-designed to support the claim.\nHowever, the research contribution of the project is limited to image-space discrete RL tasks, and does not cover the wide-range other RL. In terms of the novelty, the proposed algorithm is not fundamentally different from Rainbow. \nTherefore I tend to vote for borderline for this paper and am willing to increase the scores if more improvement is updated.\n\nBesides, I would like to thank Mr. Ankesh Anand for mentioning the Hasselt et al. (2019) [1] paper, which is indeed very similar in terms of the topic discussed and the methods used to evaluate the algorithms. \nI have also read Hasselt et al. (2019) before this submission, but I think it would be fair to say the two papers are relatively concurrent.\n\nPotential improvement:\n- It will be great if the authors also extend the discussion to current reinforcement learning algorithms that are applied in continuous tasks from states. In [2], similar conclusion is observed in continuous control tasks, where SAC [3] / TD3 [4] perform substantially better than many of the state-of-the-art model-based baselines.\n\n[1] van Hasselt, Hado, Matteo Hessel, and John Aslanides. \"When to use parametric models in reinforcement learning?.\" arXiv preprint arXiv:1906.05243 (2019).\n[2] Wang, Tingwu & Bao, Xuchan & Clavera, Ignasi & Hoang, Jerrick & Wen, Yeming & Langlois, Eric & Zhang, Shunshi & Zhang, Guodong & Abbeel, Pieter & Ba, Jimmy. (2019). Benchmarking Model-Based Reinforcement Learning.\n[3] Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).\n[4] Fujimoto, Scott, Herke van Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" arXiv preprint arXiv:1802.09477 (2018).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1814/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1814/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575836906906, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1814/Reviewers"], "noninvitees": [], "tcdate": 1570237731909, "tmdate": 1575836906921, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Official_Review"}}}, {"id": "Sygk5-d0Kr", "original": null, "number": 2, "cdate": 1571877255338, "ddate": null, "tcdate": 1571877255338, "tmdate": 1572972420095, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents a data-efficient version of the Rainbow DQN by Hessel et al. (2018) that matches the performance of a recent state of the art model-based method by Kaiser et al. (2019) on several Atari games. Particularly, the paper empirically shows that a simple hyper-parameter tuning, in this case increasing the ratio of number of training steps to the environment interactions as well as decreasing epsilon-decay period, can result in significant improvements in sample efficiency of the Rainbow DQN agent. They show that their method (which requires significantly less computation) can outperform the model-based variant on half of the games tested, while performing worse on the rest. \n\nOverall, I believe that this paper is below the acceptance threshold due to lack of 1) novelty, 2) significance and 3) depth of analysis.\n\nThe observation that more training updates with the agent\u2019s existing experience results in  sample efficiency in DQN method has already been shown empirically by Holland et al. (2018) which has also been cited by this paper. Additionally, more recent work by Hasselt et al. (2019) which is not currently cited, explicitly addresses the same problem as this work by tuning the hyper-parameters of the Rainbow DQN to achieve significant sample efficiency, outperforming Kaiser et al. (2019) in 17 out of 26 Atari games tested. In addition, their work gave a rather detailed motivation and analysis of their findings, proposing and testing several hypotheses for how and when model-based methods could outperform replay-based model-free variants. \n\nIn comparison to prior work, the current paper has a more limited scope and significance. Hence, I believe more work would be needed to warrant acceptance.\n\nHessel et al., 2018: Rainbow: Combining Improvements in Deep Reinforcement Learning https://arxiv.org/abs/1710.02298\nKaiser et al., 2019: Model-Based Reinforcement Learning for Atari https://arxiv.org/abs/1903.00374\nHolland et al., 2018: The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces https://arxiv.org/abs/1806.01825\nHasselt et al., 2019: When to use parametric models in reinforcement learning? https://arxiv.org/abs/1906.05243\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1814/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1814/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575836906906, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1814/Reviewers"], "noninvitees": [], "tcdate": 1570237731909, "tmdate": 1575836906921, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Official_Review"}}}, {"id": "H1eNYZKRtS", "original": null, "number": 3, "cdate": 1571881340174, "ddate": null, "tcdate": 1571881340174, "tmdate": 1572972420048, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents an emprical study of how a properly tuned implementation of a model-free RL method can achieve data-efficiency similar to a state-of-the-art model-based method for the Atari domain. \n\nThe paper defines r as ratio of network updates to environment interactions to describe model-free and model-based methods, and hypothesizes that model-based methods are more data-efficient because of a higher ratio r. To test this hypothesis, the authors take Rainbow DQN (model-free) and modify it to increase its ratio r to be closer to that SiMPLe (model-based). Using the modified verison of Rainbow (OTRainbow), the authors replicate an experimental comparison with SiMPLe (Kaiser et al, 2019), showing that Rainbow DQN can be a harder baseline to beat than previously reported (Figure 1). This paper raises an important point about empirical claims without properly tuned baselines, when comparing model-based to model-free methods, identifying the amount of computation as a hyperparameter to tune for fairer comparisons.\n\nI recommend this paper to be accepted only if the following issues are addressed. The first is the presentation of the empirical results. In Figure 1, OTRainbow is compared against the reported results in (Kaiser et al, 2019), along with other baselines, when limiting the experience to 100k interactions. Then, in Figure 2, human normalized scores are reported for varying amounts of experience for the variants of Rainbow, and compared against SiMPLe with 100k interactions, with the claim that the authors couldn't run the method for longer experiences. Unless a comparison can be made with the same amounts of experience, I don't see how Figure 2 can be interpreted objectively. In any case, the results in Figure 1 and the appendix are useful for showing that the baselines used in prior works were not as strong as they could be.\n\nThe second has to do with the interpretation of the results. The paper chooses a single method class of model-based methods to do this comparison, namely dyna-style algorithms that use the model to generate new data. But models can also be used for value function estimation (Model Based Value Expansion) and reducing gradient variance(using pathwise derivatives). The paper is written as if the conclusions could be extended to model-based methods in general. Can we get the same conclusions on a different domain where other model-based methods have been successful; e.g. continuous control tasks? A way to improve the paper would be to make it clear from the beginning that these results are about Dyna-style algorithms in the Atari domain.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1814/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1814/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575836906906, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1814/Reviewers"], "noninvitees": [], "tcdate": 1570237731909, "tmdate": 1575836906921, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Official_Review"}}}, {"id": "HJevJKu2_r", "original": null, "number": 1, "cdate": 1570699486759, "ddate": null, "tcdate": 1570699486759, "tmdate": 1570699994949, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "H1xyE8kNdH", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment", "content": {"comment": "Thank you very much for your comment. Hasselt et. al (2019) indeed reaches a similar conclusion, thank you for pointing it out. We were not aware of this preprint before and the work was done fully concurrently. \n\nAlthough both studies achieve similar experimental outcomes, there are some important distinctions:\n\n1. The high-level contrast between Hasselt et al. (2019) and our paper is that the former discusses differences between replay and model-based methods and the latter, first and foremost, emphasizes the importance of appropriate baselines in any future work focused on sample efficient deep reinforcement learning (not only model-based). We can agree that the title and related work section can be slightly misleading in that regard. It was motivated by the fact that the model-based approaches were producing the best results at the time of writing. One example of non-model-based work where arguments from this paper would be applicable could be Su Young et al. (2019) that proposed a novel model-free algorithm. It is not included as related work because we were not aware of any existing preprint at the moment of submission and did not want to base the analysis solely on abstract. We plan to include these points once the revision period opens.\n\n2. Also, approaches to data-efficient hyperparameters of Rainbow DQN differ. Hasselt et. al (2019) increases the length of the multi-step update, whereas our paper increases the number of training steps per each data sample. Given that both studies employ simple and intuitive but different hyperparameter changes and do not exhaustively tune the algorithm, we believe that together they provide much stronger case behind using appropriate baselines showing that it does not take much to drastically improve the efficiency of existing methods.\n\nLee, Su Young, Sungik Choi, and Sae-Young Chung. \"Sample-efficient deep reinforcement learning via episodic backward update.\" To appear at NeurIPS '19 https://arxiv.org/abs/1805.12375\n\nvan Hasselt, Hado, Matteo Hessel, and John Aslanides. \"When to use parametric models in reinforcement learning?.\" To appear at NeurIPS '19. https://arxiv.org/abs/1906.05243", "title": "Thank you for sharing the preprint!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke9u1HFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1814/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1814/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1814/Authors|ICLR.cc/2020/Conference/Paper1814/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150508, "tmdate": 1576860545320, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Official_Comment"}}}, {"id": "H1xyE8kNdH", "original": null, "number": 1, "cdate": 1570137639027, "ddate": null, "tcdate": 1570137639027, "tmdate": 1570138397361, "tddate": null, "forum": "Bke9u1HFwB", "replyto": "Bke9u1HFwB", "invitation": "ICLR.cc/2020/Conference/Paper1814/-/Public_Comment", "content": {"comment": "Thanks for the thorough investigation of sample-efficiency of model-free methods in the low-data regime. It seems Hasselt et. al (2019) [1] also reach the same conclusion: a well tuned Rainbow outperforms SimPLE in the low-data regime. They go on to add scenarios where MBRL could be useful. \n\nIt's nice that multiple groups reached the same conclusion as it adds credibility to the baseline. I am not sure about ICLR rules regarding this, but there could be an argument to consider this as concurrent work. \n\n[1] van Hasselt, Hado, Matteo Hessel, and John Aslanides. \"When to use parametric models in reinforcement learning?.\" To appear at NeurIPS '19. https://arxiv.org/abs/1906.05243", "title": "Findings very similar to Hasselt et. al (2019)"}, "signatures": ["~Ankesh_Anand1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ankesh_Anand1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?", "authors": ["Kacper Piotr Kielak"], "authorids": ["k.kielak@bham.ac.uk"], "keywords": ["deep learning", "reinforcement learning", "data efficiency", "DQN", "Rainbow", "SimPLe"], "TL;DR": "Recent advancements in data-efficient model-based reinforcement learning are not any more data efficient than existing model-free approaches.", "abstract": "Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.", "pdf": "/pdf/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "paperhash": "kielak|do_recent_advancements_in_modelbased_deep_reinforcement_learning_really_improve_data_efficiency", "original_pdf": "/attachment/8b2b49acf4b1cc4124eb9a228de0ae68029c92bc.pdf", "_bibtex": "@misc{\nkielak2020do,\ntitle={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},\nauthor={Kacper Piotr Kielak},\nyear={2020},\nurl={https://openreview.net/forum?id=Bke9u1HFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bke9u1HFwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504189434, "tmdate": 1576860578687, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1814/Authors", "ICLR.cc/2020/Conference/Paper1814/Reviewers", "ICLR.cc/2020/Conference/Paper1814/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1814/-/Public_Comment"}}}], "count": 11}