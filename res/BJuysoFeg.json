{"notes": [{"id": "HJlDXc1ZrX", "original": null, "number": 9, "cdate": 1533241887259, "ddate": null, "tcdate": 1533241887259, "tmdate": 1533241887259, "tddate": null, "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "content": {"title": "This is an important research question needing more addresses", "comment": "Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work. \n\nThe work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.\n\n1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain?\n2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant. \n3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? "}, "signatures": ["~Jianbo_Ye1"], "readers": ["everyone"], "nonreaders": [""], "writers": ["~Jianbo_Ye1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396372299, "tcdate": 1486396372299, "number": 1, "id": "r12CjGLdl", "invitation": "ICLR.cc/2017/conference/-/paper126/acceptance", "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper performs domain adaptation using a very simple trick inspired by BatchNorm. The paper received below margin scores. The reviewers both, liked the simplicity of the approach, and at the same time felt that the contribution was too thin. Given the high bar of ICLR, this paper falls short."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396372866, "id": "ICLR.cc/2017/conference/-/paper126/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJuysoFeg", "replyto": "BJuysoFeg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396372866}}}, {"tddate": null, "tmdate": 1485197570322, "tcdate": 1481948233461, "number": 2, "id": "rJW8h4GEl", "invitation": "ICLR.cc/2017/conference/-/paper126/official/review", "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "signatures": ["ICLR.cc/2017/conference/paper126/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper126/AnonReviewer1"], "content": {"title": "Final review", "rating": "4: Ok but not good enough - rejection", "review": "Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.\n\nThis paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.\n\nOverall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:\n\n1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). \n\n2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512689434, "id": "ICLR.cc/2017/conference/-/paper126/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper126/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper126/AnonReviewer3", "ICLR.cc/2017/conference/paper126/AnonReviewer1", "ICLR.cc/2017/conference/paper126/AnonReviewer2"], "reply": {"forum": "BJuysoFeg", "replyto": "BJuysoFeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper126/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper126/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512689434}}}, {"tddate": null, "tmdate": 1483779742925, "tcdate": 1483779742925, "number": 8, "id": "ByDjAQCHl", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "forum": "BJuysoFeg", "replyto": "HyxKF3sre", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "Another related paper", "comment": "We find another related paper [a] to support the effectiveness of our AdaBN. In that paper, the authors also proposed a simple but effective method to learn multiple styles at the same time for neural style transfer. In our previous comment, we have shown that neural style transfer can be intrinsically considered as a domain adaptation problem, which shares the same core process: distribution alignment. In [a], authors share a similar idea with our AdaBN method: modulating BN layers for distribution alignment. Differently, [a] learns different slope and bias terms in BN layers for different 'domains' (if we consider a target style image is an individual domain). While our AdaBN uses the mean and variance statistics in BN layers for different domains. \n\nWe believe our AdaBN and [a] both exploit the functionality of BN layers for distribution alignment, which is then applied in domain adaptation and neural style transfer, respectively. At the same time, our paper \"Demystifying Neural Style Transfer\" also connects the neural style transfer and domain adaptation. We think these three papers together provide a more holistic and systematical perspective of these two fields and the connection with BN layers.\n\n\n[a] A Learned Representation For Artistic Style (https://openreview.net/forum?id=BJO-BuT1g)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "tmdate": 1483618679868, "tcdate": 1483618679868, "number": 7, "id": "HyxKF3sre", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "Additional explanation of our AdaBN method", "comment": "This work also inspired our another recent work \"Demystifying Neural Style Transfer\"(https://arxiv.org/abs/1701.01036), which explains why Gram matrix represents the artistic style in neural style transfer. In that work, we also exploit the idea in our AdaBN method: matching BN statistics to align distributions in different domains, for neural style transfer and achieves comparable results with more complicated methods such as MMD. We believe this demonstrates the effectiveness of our method in domain adaptation field."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "tmdate": 1482817086615, "tcdate": 1482817086615, "number": 6, "id": "rJvrRuyrg", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "Paper Updated", "comment": "According to the reviewers\u2019 helpful suggestions, we have revised and updated our paper. The main modifications are as follows:\n(1) We have updated the writing of our abstract and revised section 3.3 to make it clearer and merge it to the section 3.2.\n(2) We have removed the original section 4.3.1, and revised section \u201csensitivity to target domain size\u201d by adding the experimental results of using smaller number of images.\n(3) We have added a new analysis section \u201cAdaptation Effect for Different BN Layers\u201d in section 4.3.2.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "tmdate": 1482817001612, "tcdate": 1482817001612, "number": 5, "id": "ByGeAO1Se", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "About simplicity of our method.", "comment": "We do not think simplicity is our drawback of our method. On the contrary, we believe this is a great advantage of our method. Being technically simple does not mean no novelty, and it should never be a reason to reject a paper. In fact, Batch Normalization is a very simple method, while dropout is even simpler. These techniques have had huge impacts to the field despite the simplicity. As the Reviewer2 indicates, there are also prior simple but important methods in domain adaptation. (e.g. \u201cFrustratingly Easy Domain Adaptation\u201d and \u201cReturn of Frustratingly Easy Domain Adaptation\u201d) Further, to our best knowledge, there are no prior works to exploit Batch Normalization for domain adaptation.\n\nThe main contributions in our paper is that we propose a simple yet effective AdaBN method for domain adaptation by modulating the statistics in all BN layers, which outperforms the states-of-the-art methods. Furthermore, we demonstrate that our method is complementary with other existing methods. Thus, we think it is valuable for the researchers in the field.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1482816698663, "tcdate": 1478240992583, "number": 126, "id": "BJuysoFeg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJuysoFeg", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "content": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": ["Hk6dkJQFx"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482816476725, "tcdate": 1482816476725, "number": 4, "id": "HyrJnuJrx", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "forum": "BJuysoFeg", "replyto": "rkpVV6H4l", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "Response to Reviewer2", "comment": "Thanks for your insightful comments and suggestions of our work.\n1. As mentioned in the original BN paper: \u201cthe means and variances are fixed during inference.\u201d ( Ioffe & Szegedy, 2015). In practice, many methods \u201cfreeze the batch normalization parameters to those estimated during ImageNet pre-training\u201d (\u201cSpeed/accuracy trade-offs for modern convolutional object detectors\u201d) for detection and segmentation tasks. Thus, we think modulating BN statistics in the inference is far from been explored, especially for the domain adaptation task.\n\n2. About section 3.3 and section 4.3.1\nThanks for your suggestions. We have updated our paper.\n(1) We have revised section 3.3 to make it clearer and merge it to the section 3.2.\n(2) We have removed the original section 4.3.1. At the same time, we also added a new analysis section \u201cAdaptation Effect for Different BN Layers\u201d.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "tmdate": 1482816360476, "tcdate": 1482816360476, "number": 3, "id": "ryxusdJBe", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "forum": "BJuysoFeg", "replyto": "rJW8h4GEl", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "Response to Reviewer1", "comment": "Thanks for your insightful comments and suggestions of our work.\n1.  About writing. \nWe have updated the writing of abstract to make our idea clearer.\n\n2.  About experiments.\n(1) We agree with the reviewer that same base network should be used for a fair comparison. Thus, we tried our best to transfer other methods to the Inception-BN model. As shown in our answers to the pre-review questions, we have implemented SA, GFK, LSSA and CORAL to reproduce results based on Inception-BN. For other methods, we had not reproduced promising results. This may be due to some missing implementation details (some methods do not have public implementations) or the uniqueness of AlexNet (AlexNet has intrinsic different structure with Inception). Thus, we do not report these controversial results. \n\n(2) We respectfully disagree with the reviewer\u2019s suggestion of training a model from scratch. Currently, most computer vision tasks (including classification, detection and segmentation) use an pre-trained model, and it has significant impact on the performance. For our domain adaptation task (especially for Office dataset), the number of images is too small (e.g. 500 images for DSLR domain) compared to that of ImageNet dataset (1M images).  Training a DNN like AlexNet or Inception-BN with such amount images would suffer from severe over-fitting and significantly drop the performance. Thus, we think this comparison with a model trained from scratch is meaningless. It cannot manifest the difference of different methods. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "tmdate": 1482816268832, "tcdate": 1482816249758, "number": 2, "id": "S1MZj_yHl", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "forum": "BJuysoFeg", "replyto": "B1atIp-Ve", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "Response to Reviewer3", "comment": "Thanks for your insightful comments and suggestions of our work.\n1. About section 3.1\nThanks for your suggestion and we have updated our writing. However, we think aligning the distribution of training data is not just a side effect. It is the key way to achieve the purpose of BN which is to avoid the problem of vanishing gradients and help optimization. In original BN paper, the authors\u2019 motivation is to address the problem of \u201cinternal covariate shift\u201d, which means \u201cthe change in the distributions of layers\u2019 inputs\u201d. Thus, BN is proposed to \u201creduce internal covariate shift\u201d and make \u201cthe distribution of nonlinearity inputs more stable\u201d.\n\n(2) We also directly visualize the intermediate features with Inception-BN network instead of our BN features in Sec. 3.1. The figure can be seen at this link (https://s30.postimg.org/fdamc2l1t/a2d_feature_tsne.png). Red circles are features of samples from training domain (Amazon) while blue ones are testing features (DSLR). It blends much more than that in Figure 2. This demonstrates the statistics of BN layer indeed contain the traits of the data domain. The features of individual image cannot be separated directly in terms of different domains.\n\n2. About section 3.3 and section 4.3.1\nThanks for your suggestion. We have revised section 3.3 to make it clearer and merged it to the section 3.2.\nAccording to your and Reviewer2\u2019s suggestion, we remove the previous section 4.3.1. \n\n3. About section 4.3.2\nThanks for your suggestion. We have updated additional experimental results in the revised paper. \n(1) We have experiments with smaller number of samples and found that the performance will drop more (e.g. 0.652 with 16 samples, 0.661 with 32 samples.) We have updated the results in the section \u201cSensitivity to target domain size\u201d (section 4.3.1 now).\n(2) In the section \u201cAdaptation Effect for Different BN Layers\u201d (section 4.3.2 now), we add the detailed analysis of adaptation effect for different BN layers of our AdaBN method.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "tmdate": 1482180184207, "tcdate": 1482179637168, "number": 3, "id": "rkpVV6H4l", "invitation": "ICLR.cc/2017/conference/-/paper126/official/review", "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "signatures": ["ICLR.cc/2017/conference/paper126/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper126/AnonReviewer2"], "content": {"title": "trivially simple yet effective", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.\n\n\nPros:\n\nThe method is very simple and easy to understand and apply.\n\nThe experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.\n\nThe analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.\n\n\nCons:\n\nThere is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what\u2019s done in the Inception BN results in Table 1-2?)\n\nThe analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN.\n\nSection 3.3: it\u2019s not clear to me what point is being made here.\n\n\nOverall, there\u2019s not much novelty here, but it\u2019s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with \u201cFrustratingly Easy Domain Adaptation\u201d).  If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512689434, "id": "ICLR.cc/2017/conference/-/paper126/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper126/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper126/AnonReviewer3", "ICLR.cc/2017/conference/paper126/AnonReviewer1", "ICLR.cc/2017/conference/paper126/AnonReviewer2"], "reply": {"forum": "BJuysoFeg", "replyto": "BJuysoFeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper126/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper126/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512689434}}}, {"tddate": null, "tmdate": 1481918085057, "tcdate": 1481918085057, "number": 1, "id": "B1atIp-Ve", "invitation": "ICLR.cc/2017/conference/-/paper126/official/review", "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "signatures": ["ICLR.cc/2017/conference/paper126/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper126/AnonReviewer3"], "content": {"title": "An interesting paper that shows improvements, but I am not sure about its technical advantage", "rating": "5: Marginally below acceptance threshold", "review": "Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.\n\nDetailed comments:\n\nSection 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate.\n\nSection 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low)\n\nExperiments:\n\n- section 4.3.1 is not an accurate measure of the \"effectiveness\" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance.\n\n- section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single \"whitening\" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512689434, "id": "ICLR.cc/2017/conference/-/paper126/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper126/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper126/AnonReviewer3", "ICLR.cc/2017/conference/paper126/AnonReviewer1", "ICLR.cc/2017/conference/paper126/AnonReviewer2"], "reply": {"forum": "BJuysoFeg", "replyto": "BJuysoFeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper126/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper126/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512689434}}}, {"tddate": null, "tmdate": 1480820337262, "tcdate": 1480820337252, "number": 1, "id": "rJKOUbbQe", "invitation": "ICLR.cc/2017/conference/-/paper126/public/comment", "forum": "BJuysoFeg", "replyto": "SJjZpU1me", "signatures": ["~Yanghao_Li1"], "readers": ["everyone"], "writers": ["~Yanghao_Li1"], "content": {"title": "Answers to the questions of AnonReviewer1", "comment": "Thanks a lot for your questions. Our answers are shown as follows.\n1. In section 3.3, we make further discussions about two questions of our AdaBN after explaining the algorithm in section 3.1 and section 3.2. This section is not to explain more implementation details of the algorithm, but to discuss about some complementary thoughts of our method.\n     a) We explain why the simple translation and scaling operations in our AdaBN could approximate complex non-linear domain transfer function. This discussion demonstrates that our method has potentially strong transfer power despite of the simplicity.\n     b) We discuss about why we choose to transform neural response independently instead of decorrelating and then re-correlating the covariance matrix.\n\n2. In our knowledge, there are two unsupervised protocols for the Office dataset. \n      a) Use all source examples with labels and all target examples without labels in DAN (Long et al., 2015), CORAL (Sun et al., 2016) and RevGrad (Ganin & Lempitsky, 2015)\n      b) Subsample source examples (8 samples per category for webcam/dslr and 20 for amazon) in Saenko et al., 2010.\n      We use the first protocol in Table 1. In DAN (Long et al., 2015), CORAL (Sun et al., 2016) and RevGrad (Ganin & Lempitsky, 2015), the reproduced results are inconsistent with the same method (such as RevGrad) although these papers use the same protocol.\n      In our paper, the results of AlexNet, DDC and DAN are taken from the Table 1 in DAD (Long et al., 2015). The results of Deep CORAL are taken directly from its original paper. The results of RevGrad were taken from CORAL (Sun et al., 2016) before. We have updated the results of RevGrad from its original paper now.\n\n3. Since our method has to use deep models with BN layers, AlexNet is not suitable and we choose to use the Inception-BN model. For a fair comparison, we tried our best to transfer other methods to the Inception-BN model. We have implemented SA, GFK, LSSA and CORAL to reproduce results based on Inception-BN. \nHowever, it is not easy to transform from AlexNet to Inception-BN for other methods. There are two main reasons. First, some methods (e.g. DDC, Deep CORAL) do not have publicly available implementations. Second, AlexNet has 3 fully-connected layers, which is different with Inception-BN. However, some strategies in these methods are based on the structure of AlexNet. For example, DAN uses MK-MMD among the 3 fully connected layers. We tried to implement some of them based on Inception-BN but did not get promising results even worse than baseline method. This may be due to some missing implementation technologies or the unsuitableness of these methods for different network structures. Thus, we do not report these results for fair comparisons. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287716853, "id": "ICLR.cc/2017/conference/-/paper126/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJuysoFeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper126/reviewers", "ICLR.cc/2017/conference/paper126/areachairs"], "cdate": 1485287716853}}}, {"tddate": null, "tmdate": 1480711426846, "tcdate": 1480711426842, "number": 1, "id": "SJjZpU1me", "invitation": "ICLR.cc/2017/conference/-/paper126/pre-review/question", "forum": "BJuysoFeg", "replyto": "BJuysoFeg", "signatures": ["ICLR.cc/2017/conference/paper126/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper126/AnonReviewer1"], "content": {"title": "Questions", "question": "1. p. 5, 3.3, What is the purpose of the second paragraph? I'm not sure I understand the link between this one and the previous one.\n\n2. p. 6, Table 1, What is the evaluation protocol for the Office dataset? DAN results are for the transductive setting; RevGrad results differ from the ones in the ICML paper (also transductive setting)\n\n3. The authors are using Inception BN as a base model while the other competing methods are built on top of AlexNet. Could that happen that those methods would also benefit from having a stronger base model. Is it possible to conduct a fair comparison?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "pdf": "/pdf/5175fa7c53f00b8c602a918d3869e4f64cdde13c.pdf", "TL;DR": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "paperhash": "li|revisiting_batch_normalization_for_practical_domain_adaptation", "conflicts": ["pku.edu.cn"], "keywords": [], "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959448293, "id": "ICLR.cc/2017/conference/-/paper126/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper126/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper126/AnonReviewer1"], "reply": {"forum": "BJuysoFeg", "replyto": "BJuysoFeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper126/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper126/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959448293}}}], "count": 15}