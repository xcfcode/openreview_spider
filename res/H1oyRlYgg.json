{"notes": [{"id": "H1lP_11cdH", "original": null, "number": 9, "cdate": 1570529135173, "ddate": null, "tcdate": 1570529135173, "tmdate": 1570529135173, "tddate": null, "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "content": {"title": "Typo in Figure 5", "comment": "The values of epsilon in Figure 5 are set as: 1e-3 and 5e-3. \nBased on the previous experiments, and the fact that sharpness of 5e-3 values are less than sharpness of 1e-3 values, it is my understanding that 5e-3 should be changed to 5e-4. "}, "signatures": ["~Amir_H_Abdi1"], "readers": ["everyone"], "nonreaders": [""], "writers": ["~Amir_H_Abdi1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"id": "Syx--115Or", "original": null, "number": 8, "cdate": 1570529017153, "ddate": null, "tcdate": 1570529017153, "tmdate": 1570529017153, "tddate": null, "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "content": {"title": "Potential mistake in Appendix C", "comment": "In the \"PERFORMANCE MODEL\" appendix, f_l(p) is assumed to be 1.0; however, P (number of processors) is assumed to be smaller than B_l. Therefore, the following statement does not match the provided intuition: \"the parallel efficiency of the LB method, is 1.0. In other words, we assume that the LB method is perfectly scalable due to use of a large batch size\""}, "signatures": ["~Amir_H_Abdi1"], "readers": ["everyone"], "nonreaders": [""], "writers": ["~Amir_H_Abdi1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1486672493222, "tcdate": 1478196707275, "number": 76, "id": "H1oyRlYgg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "H1oyRlYgg", "signatures": ["~Nitish_Shirish_Keskar1"], "readers": ["everyone"], "content": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396341768, "tcdate": 1486396341768, "number": 1, "id": "rJAhifIul", "invitation": "ICLR.cc/2017/conference/-/paper76/acceptance", "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All reviews (including the public one) were extremely positive, and this sheds light on a universal engineering issue that arises in fitting non-convex models. I think the community will benefit a lot from the insights here.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396342299, "id": "ICLR.cc/2017/conference/-/paper76/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396342299}}}, {"tddate": null, "tmdate": 1482869025270, "tcdate": 1482869001651, "number": 7, "id": "SJMMtBgrg", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "forum": "H1oyRlYgg", "replyto": "HkYQOlGEl", "signatures": ["~Nitish_Shirish_Keskar1"], "readers": ["everyone"], "writers": ["~Nitish_Shirish_Keskar1"], "content": {"title": "Clarification regarding noise", "comment": "Thank you for your review. We experimented with additive random Gaussian noise (both in gradients and in iterates), noisy labels and noisy input-data. However, despite significant tuning of the hyperparameters of the random noise, we did not observe any consistent improvements in testing error.  Overall, our feeling is that this needs deeper investigation and that LB methods may need to be modified in a more fundamental way to achieve good generalization.\f"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"tddate": null, "tmdate": 1482868610936, "tcdate": 1482868610936, "number": 6, "id": "H1oYDrgSe", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "forum": "H1oyRlYgg", "replyto": "H1UEbWm4x", "signatures": ["~Nitish_Shirish_Keskar1"], "readers": ["everyone"], "writers": ["~Nitish_Shirish_Keskar1"], "content": {"title": "Clarifications", "comment": "Thank you Alex for your interesting questions. \nFrom our preliminary experiments, it seems that there is not significant benefit from reducing batch-sizes to a very small value. For instance, in C1 and C2 networks, a batch-size of 8 or 16 led to (statistically) similar values of testing accuracy as a batch size of 256.\n\nLB methods may very well benefit from noise injection, but we have not figured out how to do so successfully. We experimented with additive random Gaussian noise (both in gradients and in iterates), noisy labels and noisy input-data. However, despite significant tuning of the hyperparameters of the random noise, we did not observe any consistent improvements in testing error. \n\nAlso, we agree that the motivation for using smaller minibatches is related to the motivation for adversarial examples. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"tddate": null, "tmdate": 1482868538672, "tcdate": 1482868538672, "number": 5, "id": "Sy7BvSxrx", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "forum": "H1oyRlYgg", "replyto": "SJVFyZfNe", "signatures": ["~Nitish_Shirish_Keskar1"], "readers": ["everyone"], "writers": ["~Nitish_Shirish_Keskar1"], "content": {"title": "Thank you", "comment": "Thanks for your review and your suggestions on how our work could be used in further investigations."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"tddate": null, "tmdate": 1482868493056, "tcdate": 1482868493056, "number": 4, "id": "rkHGPBxHe", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "forum": "H1oyRlYgg", "replyto": "S1Eq3CUEl", "signatures": ["~Nitish_Shirish_Keskar1"], "readers": ["everyone"], "writers": ["~Nitish_Shirish_Keskar1"], "content": {"title": "Thank you for your review", "comment": "We thank you for your review and your kind assessment."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"tddate": null, "tmdate": 1482251403893, "tcdate": 1482251403893, "number": 3, "id": "S1Eq3CUEl", "invitation": "ICLR.cc/2017/conference/-/paper76/official/review", "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "signatures": ["ICLR.cc/2017/conference/paper76/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper76/AnonReviewer1"], "content": {"title": "Analysis of large batch training", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512705701, "id": "ICLR.cc/2017/conference/-/paper76/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper76/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper76/AnonReviewer3", "ICLR.cc/2017/conference/paper76/AnonReviewer2", "ICLR.cc/2017/conference/paper76/AnonReviewer1"], "reply": {"forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512705701}}}, {"tddate": null, "tmdate": 1481998638403, "tcdate": 1481998638403, "number": 3, "id": "H1UEbWm4x", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "signatures": ["~Alex_Lamb1"], "readers": ["everyone"], "writers": ["~Alex_Lamb1"], "content": {"title": "Good paper", "comment": "I think that this is a great empirical exploration of long-held folk wisdom in the Deep Learning community - that using larger minibatches makes generalization error worse.  \n\nThe paper does a good of explaining why phenomenon occurs, by analyzing the \"sharpness\" of the loss function for large-batch and small-batch trained models.  \n\nSome other connections that I think would be interesting to explore: \n  -If you use very small minibatches, does generalization get even better (perhaps at the expense of very slow training).  \n  -Can other forms of noise injection compensate for the use of a larger minibatch?  For example, if I inject increasing amounts of noise into the gradients or the parameters with larger batches, does this close the gap with small-batch training?  \n  -The motivation for using smaller minibatches here seems closely related to the motivation for adversarial examples (ensuring that loss is relatively flat in a large region around data points).  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"tddate": null, "tmdate": 1481932668415, "tcdate": 1481932668415, "number": 2, "id": "SJVFyZfNe", "invitation": "ICLR.cc/2017/conference/-/paper76/official/review", "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "signatures": ["ICLR.cc/2017/conference/paper76/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper76/AnonReviewer2"], "content": {"title": "Little novelty but valuable empirical evidence", "rating": "10: Top 5% of accepted papers, seminal paper", "review": "The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. \n\nPros and Cons:\nAlthough there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. \n\nSignificance:\nI think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.\n\nComments:\nEarlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.\n\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512705701, "id": "ICLR.cc/2017/conference/-/paper76/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper76/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper76/AnonReviewer3", "ICLR.cc/2017/conference/paper76/AnonReviewer2", "ICLR.cc/2017/conference/paper76/AnonReviewer1"], "reply": {"forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512705701}}}, {"tddate": null, "tmdate": 1481930784573, "tcdate": 1481930784573, "number": 1, "id": "HkYQOlGEl", "invitation": "ICLR.cc/2017/conference/-/paper76/official/review", "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "signatures": ["ICLR.cc/2017/conference/paper76/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper76/AnonReviewer3"], "content": {"title": "Good paper", "rating": "6: Marginally above acceptance threshold", "review": "I think that the paper is quite interesting and useful. \nIt might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512705701, "id": "ICLR.cc/2017/conference/-/paper76/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper76/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper76/AnonReviewer3", "ICLR.cc/2017/conference/paper76/AnonReviewer2", "ICLR.cc/2017/conference/paper76/AnonReviewer1"], "reply": {"forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512705701}}}, {"tddate": null, "tmdate": 1481138498066, "tcdate": 1481138498061, "number": 2, "id": "HkcS-1U7l", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "forum": "H1oyRlYgg", "replyto": "ByV60KkQg", "signatures": ["~Nitish_Shirish_Keskar1"], "readers": ["everyone"], "writers": ["~Nitish_Shirish_Keskar1"], "content": {"title": "Second-Order Optimality", "comment": "Agreed, we cannot guarantee that the solutions obtained by the SB and LB methods are indeed local minima of the problem (as we did not attempt to verify second-order optimality). To be cautious, we will use the terms \u201cLB solution\u201d and \u201cSB solution\u201d, as appropriate, in our updated manuscript."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"tddate": null, "tmdate": 1481132576287, "tcdate": 1481132576281, "number": 1, "id": "SJdX9TrQg", "invitation": "ICLR.cc/2017/conference/-/paper76/public/comment", "forum": "H1oyRlYgg", "replyto": "HJWN7OkXe", "signatures": ["~Nitish_Shirish_Keskar1"], "readers": ["everyone"], "writers": ["~Nitish_Shirish_Keskar1"], "content": {"title": "On The Sharpness Metric", "comment": "You are right, this metric is not scale invariant, and we will remove that statement from the paper in our next update. The inclusion of the \u201c1+\u201d in the denominator is of course to guard against the case when f(x) is zero, and as such it is useful for our purposes. Nevertheless, in response to your comment, we experiment with the denominator changed to f(x0) - f(x), where x0 is the initial point, and observed similar order-of-magnitude results, indicating that our numbers reported in the paper are not misleading."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739026, "id": "ICLR.cc/2017/conference/-/paper76/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1oyRlYgg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper76/reviewers", "ICLR.cc/2017/conference/paper76/areachairs"], "cdate": 1485287739026}}}, {"tddate": null, "tmdate": 1480724156029, "tcdate": 1480724156024, "number": 2, "id": "ByV60KkQg", "invitation": "ICLR.cc/2017/conference/-/paper76/pre-review/question", "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "signatures": ["ICLR.cc/2017/conference/paper76/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper76/AnonReviewer3"], "content": {"title": "local minima", "question": "true \"local minima\" or just \"local minima\" of one-dimensional slices of a unimodal (within the search boundary) nonconvex problem?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959477088, "id": "ICLR.cc/2017/conference/-/paper76/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper76/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper76/AnonReviewer2", "ICLR.cc/2017/conference/paper76/AnonReviewer3"], "reply": {"forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959477088}}}, {"tddate": null, "tmdate": 1480717097531, "tcdate": 1480717097525, "number": 1, "id": "HJWN7OkXe", "invitation": "ICLR.cc/2017/conference/-/paper76/pre-review/question", "forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "signatures": ["ICLR.cc/2017/conference/paper76/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper76/AnonReviewer2"], "content": {"title": "Contant \"1\" in equation (3)?", "question": "If I understand correctly, the paragraphs above Eq (3) claim the sharpness criterion is scale invariant. Does that happen when the denominator in Eq (3) is \"1+f(x)\". f(x) could have any scale/unit but \"1\" is a unitless constant. How the proposed criterion is scale invariant with \"1+f(x)\"? Specifically, if I replace the objective function f(x) with a scaled version c*f(x), does your criterion produces the same value?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "pdf": "/pdf/bd71807dc65ba0c6a814e403b33cc9666cb17d5b.pdf", "TL;DR": "We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization properties. ", "paperhash": "keskar|on_largebatch_training_for_deep_learning_generalization_gap_and_sharp_minima", "keywords": ["Deep learning", "Optimization"], "conflicts": ["northwestern.edu", "intel.com", "ibm.com"], "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "authorids": ["keskar.nitish@u.northwestern.edu", "dheevatsa.mudigere@intel.com", "j-nocedal@northwestern.edu", "mikhail.smelyanskiy@intel.com", "peter.tang@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959477088, "id": "ICLR.cc/2017/conference/-/paper76/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper76/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper76/AnonReviewer2", "ICLR.cc/2017/conference/paper76/AnonReviewer3"], "reply": {"forum": "H1oyRlYgg", "replyto": "H1oyRlYgg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper76/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959477088}}}], "count": 16}