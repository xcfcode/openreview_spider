{"notes": [{"id": "ryx0nnEKwH", "original": "BJxaUWAzDH", "number": 210, "cdate": 1569438902342, "ddate": null, "tcdate": 1569438902342, "tmdate": 1577168293176, "tddate": null, "forum": "ryx0nnEKwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Improving Batch Normalization with Skewness Reduction for Deep Neural Networks", "authors": ["Pak Lun Kevin Ding", "Sarah Martin", "Baoxin Li"], "authorids": ["kevinding@asu.edu", "samart44@asu.edu", "baoxin.li@asu.edu"], "keywords": ["Batch Normalization", "Deep Learning"], "TL;DR": "Reduce Skewness", "abstract": "Batch Normalization (BN) is a well-known technique used in training deep neural networks.\n    The main idea behind batch normalization is to normalize the features of the layers ($i.e.$, transforming them to have a mean equal to zero and a variance equal to one).\n    Such a procedure encourages the optimization landscape of the loss function to be smoother, and improve the learning of the networks for both speed and performance.\n    In this paper,\n    we demonstrate that the performance of the network can be improved,\n    if the distributions of the features of the output in the same layer are similar.\n    As normalizing based on mean and variance does not necessarily make the features to have the same distribution, we propose a new normalization scheme: Batch Normalization with Skewness Reduction (BNSR).\n    Comparing with other normalization approaches,\n    BNSR transforms not just only the mean and variance,\n    but also the skewness of the data.\n    By tackling this property of a distribution, we are able to make the output distributions of the layers to be further similar. The nonlinearity of BNSR may further improve the expressiveness of the underlying network.\n    Comparisons with other normalization schemes are tested on the CIFAR-100 and ImageNet datasets. Experimental results show that the proposed approach can outperform other state-of-the-arts that are not equipped with BNSR.", "pdf": "/pdf/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "paperhash": "ding|improving_batch_normalization_with_skewness_reduction_for_deep_neural_networks", "original_pdf": "/attachment/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "_bibtex": "@misc{\nding2020improving,\ntitle={Improving Batch Normalization with Skewness Reduction for Deep Neural Networks},\nauthor={Pak Lun Kevin Ding and Sarah Martin and Baoxin Li},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx0nnEKwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "y74Ry1hL2", "original": null, "number": 1, "cdate": 1576798690409, "ddate": null, "tcdate": 1576798690409, "tmdate": 1576800944799, "tddate": null, "forum": "ryx0nnEKwH", "replyto": "ryx0nnEKwH", "invitation": "ICLR.cc/2020/Conference/Paper210/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a novel mechanism to reduce the skewness of the activations. The paper evaluates their claims on the CIFAR-10 and Tiny Imagenet dataset. The reviewers found the scale of the experiments to be too limited to support the claims. Thus we recommend the paper be improved by considering larger datasets such as the full Imagenet. The paper should also better motivate the goal of reducing skewness.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Batch Normalization with Skewness Reduction for Deep Neural Networks", "authors": ["Pak Lun Kevin Ding", "Sarah Martin", "Baoxin Li"], "authorids": ["kevinding@asu.edu", "samart44@asu.edu", "baoxin.li@asu.edu"], "keywords": ["Batch Normalization", "Deep Learning"], "TL;DR": "Reduce Skewness", "abstract": "Batch Normalization (BN) is a well-known technique used in training deep neural networks.\n    The main idea behind batch normalization is to normalize the features of the layers ($i.e.$, transforming them to have a mean equal to zero and a variance equal to one).\n    Such a procedure encourages the optimization landscape of the loss function to be smoother, and improve the learning of the networks for both speed and performance.\n    In this paper,\n    we demonstrate that the performance of the network can be improved,\n    if the distributions of the features of the output in the same layer are similar.\n    As normalizing based on mean and variance does not necessarily make the features to have the same distribution, we propose a new normalization scheme: Batch Normalization with Skewness Reduction (BNSR).\n    Comparing with other normalization approaches,\n    BNSR transforms not just only the mean and variance,\n    but also the skewness of the data.\n    By tackling this property of a distribution, we are able to make the output distributions of the layers to be further similar. The nonlinearity of BNSR may further improve the expressiveness of the underlying network.\n    Comparisons with other normalization schemes are tested on the CIFAR-100 and ImageNet datasets. Experimental results show that the proposed approach can outperform other state-of-the-arts that are not equipped with BNSR.", "pdf": "/pdf/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "paperhash": "ding|improving_batch_normalization_with_skewness_reduction_for_deep_neural_networks", "original_pdf": "/attachment/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "_bibtex": "@misc{\nding2020improving,\ntitle={Improving Batch Normalization with Skewness Reduction for Deep Neural Networks},\nauthor={Pak Lun Kevin Ding and Sarah Martin and Baoxin Li},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx0nnEKwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryx0nnEKwH", "replyto": "ryx0nnEKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730125, "tmdate": 1576800282857, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper210/-/Decision"}}}, {"id": "HJlamBFoYr", "original": null, "number": 1, "cdate": 1571685669132, "ddate": null, "tcdate": 1571685669132, "tmdate": 1572972624814, "tddate": null, "forum": "ryx0nnEKwH", "replyto": "ryx0nnEKwH", "invitation": "ICLR.cc/2020/Conference/Paper210/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed to improve the regular batch normalization by reducing the skewness of the hidden features. To this end, the authors introduce a non-linear function to reduce the skewness. However, the analysis and experiments are too weak to support the authors' claim.\n\n1. The motivation is not clear. Why is it necessary to reduce the skewness? There is no practical or theoretical evidence to support it.\n\n2. To verify the proposed non-linear transformation can reduce skewness, it's better to visualize the learned feature distribution to confirm this point.\n\n3. Experiments with more datasets and networks are needed to evaluate the performance of the proposed method. "}, "signatures": ["ICLR.cc/2020/Conference/Paper210/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper210/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Batch Normalization with Skewness Reduction for Deep Neural Networks", "authors": ["Pak Lun Kevin Ding", "Sarah Martin", "Baoxin Li"], "authorids": ["kevinding@asu.edu", "samart44@asu.edu", "baoxin.li@asu.edu"], "keywords": ["Batch Normalization", "Deep Learning"], "TL;DR": "Reduce Skewness", "abstract": "Batch Normalization (BN) is a well-known technique used in training deep neural networks.\n    The main idea behind batch normalization is to normalize the features of the layers ($i.e.$, transforming them to have a mean equal to zero and a variance equal to one).\n    Such a procedure encourages the optimization landscape of the loss function to be smoother, and improve the learning of the networks for both speed and performance.\n    In this paper,\n    we demonstrate that the performance of the network can be improved,\n    if the distributions of the features of the output in the same layer are similar.\n    As normalizing based on mean and variance does not necessarily make the features to have the same distribution, we propose a new normalization scheme: Batch Normalization with Skewness Reduction (BNSR).\n    Comparing with other normalization approaches,\n    BNSR transforms not just only the mean and variance,\n    but also the skewness of the data.\n    By tackling this property of a distribution, we are able to make the output distributions of the layers to be further similar. The nonlinearity of BNSR may further improve the expressiveness of the underlying network.\n    Comparisons with other normalization schemes are tested on the CIFAR-100 and ImageNet datasets. Experimental results show that the proposed approach can outperform other state-of-the-arts that are not equipped with BNSR.", "pdf": "/pdf/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "paperhash": "ding|improving_batch_normalization_with_skewness_reduction_for_deep_neural_networks", "original_pdf": "/attachment/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "_bibtex": "@misc{\nding2020improving,\ntitle={Improving Batch Normalization with Skewness Reduction for Deep Neural Networks},\nauthor={Pak Lun Kevin Ding and Sarah Martin and Baoxin Li},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx0nnEKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryx0nnEKwH", "replyto": "ryx0nnEKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper210/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper210/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575571971756, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper210/Reviewers"], "noninvitees": [], "tcdate": 1570237755438, "tmdate": 1575571971769, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper210/-/Official_Review"}}}, {"id": "SylxYe7TtS", "original": null, "number": 2, "cdate": 1571790967927, "ddate": null, "tcdate": 1571790967927, "tmdate": 1572972624781, "tddate": null, "forum": "ryx0nnEKwH", "replyto": "ryx0nnEKwH", "invitation": "ICLR.cc/2020/Conference/Paper210/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper develops an improved Batch Normalization method, called BNSR. BNSR applies a nonlinear mapping to modify the skewness of features, which is believed to keep the features similar, speedup training procedure, and further improve the performance. \n\nI have several concerns:\n1.\tTo investigate the impact of the similarity of the feature distributions, the author proposes four settings, including BN and BNSR. However, the added noise of last three settings not only makes the feature dissimilar but also breaks the nature of zero mean and unit variance. This is still unclear whether the similar distribution of features make BNSR outperform BN.\n2.\tThe skewness is used to measure the asymmetry of the probability distribution, but not the similarity between two different distributions. Distributions with zero mean, unit variance, and near zero skewness could still be very different.\n3.\tBased on \u201cfor ... X with zero mean and unit variance, there is a high probability that ... lies in the interval (-1, 1)\u201d, the paper introduces \u03c6_p(x), where p > 1, to decrease the skewness of the feature map x. However, there are about 32% elements of X that their absolute values are larger than 1, for a standard normal distribution. Figure 6 & 7 also show that for real features in neural network, there are a significant number of elements that lie out of (-1, 1). Will this lead to instability during training? To better understand the effect of \u03c6_p(x), I think \u03c1 (the Pearson\u2019s second skewness coefficient), right before and after \u03c6_p(x), should be shown for each layer at several epochs.\n4.\tThe results on CIFAR-100 and Tiny ImageNet are not convincing enough in my opinion. Some further experiments on ImageNet with a reasonable baseline will makes the results more convincing.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper210/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper210/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Batch Normalization with Skewness Reduction for Deep Neural Networks", "authors": ["Pak Lun Kevin Ding", "Sarah Martin", "Baoxin Li"], "authorids": ["kevinding@asu.edu", "samart44@asu.edu", "baoxin.li@asu.edu"], "keywords": ["Batch Normalization", "Deep Learning"], "TL;DR": "Reduce Skewness", "abstract": "Batch Normalization (BN) is a well-known technique used in training deep neural networks.\n    The main idea behind batch normalization is to normalize the features of the layers ($i.e.$, transforming them to have a mean equal to zero and a variance equal to one).\n    Such a procedure encourages the optimization landscape of the loss function to be smoother, and improve the learning of the networks for both speed and performance.\n    In this paper,\n    we demonstrate that the performance of the network can be improved,\n    if the distributions of the features of the output in the same layer are similar.\n    As normalizing based on mean and variance does not necessarily make the features to have the same distribution, we propose a new normalization scheme: Batch Normalization with Skewness Reduction (BNSR).\n    Comparing with other normalization approaches,\n    BNSR transforms not just only the mean and variance,\n    but also the skewness of the data.\n    By tackling this property of a distribution, we are able to make the output distributions of the layers to be further similar. The nonlinearity of BNSR may further improve the expressiveness of the underlying network.\n    Comparisons with other normalization schemes are tested on the CIFAR-100 and ImageNet datasets. Experimental results show that the proposed approach can outperform other state-of-the-arts that are not equipped with BNSR.", "pdf": "/pdf/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "paperhash": "ding|improving_batch_normalization_with_skewness_reduction_for_deep_neural_networks", "original_pdf": "/attachment/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "_bibtex": "@misc{\nding2020improving,\ntitle={Improving Batch Normalization with Skewness Reduction for Deep Neural Networks},\nauthor={Pak Lun Kevin Ding and Sarah Martin and Baoxin Li},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx0nnEKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryx0nnEKwH", "replyto": "ryx0nnEKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper210/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper210/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575571971756, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper210/Reviewers"], "noninvitees": [], "tcdate": 1570237755438, "tmdate": 1575571971769, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper210/-/Official_Review"}}}, {"id": "H1l5qFkCKr", "original": null, "number": 3, "cdate": 1571842449988, "ddate": null, "tcdate": 1571842449988, "tmdate": 1572972624736, "tddate": null, "forum": "ryx0nnEKwH", "replyto": "ryx0nnEKwH", "invitation": "ICLR.cc/2020/Conference/Paper210/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to add an extra nonlinearity function in batch normalization, between the normalization and affine scaling. The nonlinearity is a power function x ** p for x >= 0 and - (-x) ** p for x < 0 (python pseudo-code), where p is a constant, and the authors propose to keep it fixed to 1.01. The intuition behind is reducing skewness of activations, and the modification is evaluated on CIFAR and tiny ImageNet datasets. The authors also experiment with the part at which to insert the nonlinearity.\n\nI propose reject mainly due to insufficient experimental evaluation. The authors choose small datasets on which trained networks have large variance, and report a single accuracy value for each network, so it is not possible to judge the effectiveness of the method. Regarding the skewness reduction itself, it is not very convincing too, because the authors use a very small value for p (1.01), and because after affine layer ReLU removes most of the negative part, so perhaps the negative part of the nonlinearity is not needed.\n\nI would suggest the following experiments to improve the paper:\n - try various values of p\n - try removing negative part of the nonlinearity\n - report mean+-std results of 5 runs for each experiment on small dataset\n - include experiments on full scale ImageNet\n - include more network architectures, or at least have more configurations of ResNet.\n\nWould be very helpful if authors included code in pytorch or tensorflow for the proposed modification. It is surprising that such a small addition increases epoch time from 86 s to 119 s.\n\nAlso, there are some minor grammar mistakes. There is an undefined figure reference on page 6, and it is not clear what figures 6 and 7 are supposed to show, since the colors of histograms are never explained."}, "signatures": ["ICLR.cc/2020/Conference/Paper210/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper210/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Batch Normalization with Skewness Reduction for Deep Neural Networks", "authors": ["Pak Lun Kevin Ding", "Sarah Martin", "Baoxin Li"], "authorids": ["kevinding@asu.edu", "samart44@asu.edu", "baoxin.li@asu.edu"], "keywords": ["Batch Normalization", "Deep Learning"], "TL;DR": "Reduce Skewness", "abstract": "Batch Normalization (BN) is a well-known technique used in training deep neural networks.\n    The main idea behind batch normalization is to normalize the features of the layers ($i.e.$, transforming them to have a mean equal to zero and a variance equal to one).\n    Such a procedure encourages the optimization landscape of the loss function to be smoother, and improve the learning of the networks for both speed and performance.\n    In this paper,\n    we demonstrate that the performance of the network can be improved,\n    if the distributions of the features of the output in the same layer are similar.\n    As normalizing based on mean and variance does not necessarily make the features to have the same distribution, we propose a new normalization scheme: Batch Normalization with Skewness Reduction (BNSR).\n    Comparing with other normalization approaches,\n    BNSR transforms not just only the mean and variance,\n    but also the skewness of the data.\n    By tackling this property of a distribution, we are able to make the output distributions of the layers to be further similar. The nonlinearity of BNSR may further improve the expressiveness of the underlying network.\n    Comparisons with other normalization schemes are tested on the CIFAR-100 and ImageNet datasets. Experimental results show that the proposed approach can outperform other state-of-the-arts that are not equipped with BNSR.", "pdf": "/pdf/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "paperhash": "ding|improving_batch_normalization_with_skewness_reduction_for_deep_neural_networks", "original_pdf": "/attachment/19798eb85fb898ed3e633f820877d89a242fee0e.pdf", "_bibtex": "@misc{\nding2020improving,\ntitle={Improving Batch Normalization with Skewness Reduction for Deep Neural Networks},\nauthor={Pak Lun Kevin Ding and Sarah Martin and Baoxin Li},\nyear={2020},\nurl={https://openreview.net/forum?id=ryx0nnEKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryx0nnEKwH", "replyto": "ryx0nnEKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper210/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper210/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575571971756, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper210/Reviewers"], "noninvitees": [], "tcdate": 1570237755438, "tmdate": 1575571971769, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper210/-/Official_Review"}}}], "count": 5}