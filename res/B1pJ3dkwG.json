{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124455641, "tcdate": 1518468069176, "number": 256, "cdate": 1518468069176, "id": "B1pJ3dkwG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "B1pJ3dkwG", "signatures": ["~Simon_Brodeur1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "HoME: a Household Multimodal Environment", "abstract": "We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.", "paperhash": "brodeur|home_a_household_multimodal_environment", "keywords": ["simulated environment", "virtual embodiment", "multimodality", "reinforcement learning", "gym", "language grounding"], "_bibtex": "@misc{\n  brodeur2018home:,\n  title={HoME: a Household Multimodal Environment},\n  author={Simon Brodeur and Ethan Perez and Ankesh Anand and Florian Golemo and Luca Celotti and Florian Strub and Jean Rouat and Hugo Larochelle and Aaron Courville},\n  year={2018},\n  url={https://openreview.net/forum?id=B1pJ3dkwG}\n}", "authorids": ["simon.brodeur@usherbrooke.ca", "ethanperez@rice.edu", "ankesh.anand@umontreal.ca", "florian.golemo@inria.fr", "luca.celotti@usherbrooke.ca", "florian.strub@inria.fr", "jean.rouat@usherbrooke.ca", "hugolarochelle@google.com", "aaron.courville@umontreal.ca"], "authors": ["Simon Brodeur", "Ethan Perez", "Ankesh Anand", "Florian Golemo", "Luca Celotti", "Florian Strub", "Jean Rouat", "Hugo Larochelle", "Aaron Courville"], "TL;DR": "HoME is an open-source and extensible platform for artificial agents to learn at large-scale from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context of thousands of simulated 3D household environments.", "pdf": "/pdf/911408eb0fe0e1272beaa110595bb7fdc2e92d32.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582946597, "tcdate": 1520246011122, "number": 1, "cdate": 1520246011122, "id": "H1Q-T5q_f", "invitation": "ICLR.cc/2018/Workshop/-/Paper256/Official_Review", "forum": "B1pJ3dkwG", "replyto": "B1pJ3dkwG", "signatures": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer3"], "content": {"title": "A useful addition for multimodal learning, although looks fairly preliminary", "rating": "6: Marginally above acceptance threshold", "review": "This describes HoME - a multimodal simulation environment containing RGB and depth, audio, and interaction with objects. This builds on SUNCG and adds the Panda3D rendering engine, EVERT acoustic engine, a simple semantic engine, and the Bullet physics engine. It's hard to tell from this how robust the system is, or whether the coverage of tests for homes/features is. It looks like fairly preliminary work.\n\nPros\n- multimodal simulation environment.\n\nCons\n- the github repo lists many todos\n- code documentation is almost entirely lacking.\n- no running examples", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HoME: a Household Multimodal Environment", "abstract": "We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.", "paperhash": "brodeur|home_a_household_multimodal_environment", "keywords": ["simulated environment", "virtual embodiment", "multimodality", "reinforcement learning", "gym", "language grounding"], "_bibtex": "@misc{\n  brodeur2018home:,\n  title={HoME: a Household Multimodal Environment},\n  author={Simon Brodeur and Ethan Perez and Ankesh Anand and Florian Golemo and Luca Celotti and Florian Strub and Jean Rouat and Hugo Larochelle and Aaron Courville},\n  year={2018},\n  url={https://openreview.net/forum?id=B1pJ3dkwG}\n}", "authorids": ["simon.brodeur@usherbrooke.ca", "ethanperez@rice.edu", "ankesh.anand@umontreal.ca", "florian.golemo@inria.fr", "luca.celotti@usherbrooke.ca", "florian.strub@inria.fr", "jean.rouat@usherbrooke.ca", "hugolarochelle@google.com", "aaron.courville@umontreal.ca"], "authors": ["Simon Brodeur", "Ethan Perez", "Ankesh Anand", "Florian Golemo", "Luca Celotti", "Florian Strub", "Jean Rouat", "Hugo Larochelle", "Aaron Courville"], "TL;DR": "HoME is an open-source and extensible platform for artificial agents to learn at large-scale from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context of thousands of simulated 3D household environments.", "pdf": "/pdf/911408eb0fe0e1272beaa110595bb7fdc2e92d32.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582946402, "id": "ICLR.cc/2018/Workshop/-/Paper256/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper256/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper256/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper256/AnonReviewer1"], "reply": {"forum": "B1pJ3dkwG", "replyto": "B1pJ3dkwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper256/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper256/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582946402}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582824544, "tcdate": 1520609813627, "number": 2, "cdate": 1520609813627, "id": "rJ0z9XgtM", "invitation": "ICLR.cc/2018/Workshop/-/Paper256/Official_Review", "forum": "B1pJ3dkwG", "replyto": "B1pJ3dkwG", "signatures": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer2"], "content": {"title": "A promising multimodal environment for building general AI that comes at the right time...", "rating": "7: Good paper, accept", "review": "The authors propose a new open-source environment to learn from multiple modalities (vision, language, audio, physics and interaction with other entities). They extended the SUNCG dataset (containing the main parts of the vision modality) by adding the interactive aspect (navigation, sound, language and physics). \nThe paper is well motivated, well written and well positioned compared to the state-of-the-art environments. The paper describes a work that makes a big step in the dataset-level toward a general Artificial Intelligence. Indeed, this environment allows a huge amount of task-possibilities. \n\nMajor concern:  \nThe workshop template (3 pages) is clearly not sufficient for a dataset paper which generally needs a lot of illustrations (e.g. an illustration of the dynamic aspect should be provided), data and annotation collection details (How do you get the provided short text description for each object?), a complete list of the elementary engine properties (e.g., complete list of material-absorption and reflection, atmospheric conditions, agent-object interactions, etc.), statistical details, etc.  Moreover, the related work and the associated Table 1 and Figure 2 (which are an important part of dataset-proposals), should not be located in supplementary material. The Applications section that highlights the potential of the proposed environment deserves a more detailed and structured list of tasks. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HoME: a Household Multimodal Environment", "abstract": "We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.", "paperhash": "brodeur|home_a_household_multimodal_environment", "keywords": ["simulated environment", "virtual embodiment", "multimodality", "reinforcement learning", "gym", "language grounding"], "_bibtex": "@misc{\n  brodeur2018home:,\n  title={HoME: a Household Multimodal Environment},\n  author={Simon Brodeur and Ethan Perez and Ankesh Anand and Florian Golemo and Luca Celotti and Florian Strub and Jean Rouat and Hugo Larochelle and Aaron Courville},\n  year={2018},\n  url={https://openreview.net/forum?id=B1pJ3dkwG}\n}", "authorids": ["simon.brodeur@usherbrooke.ca", "ethanperez@rice.edu", "ankesh.anand@umontreal.ca", "florian.golemo@inria.fr", "luca.celotti@usherbrooke.ca", "florian.strub@inria.fr", "jean.rouat@usherbrooke.ca", "hugolarochelle@google.com", "aaron.courville@umontreal.ca"], "authors": ["Simon Brodeur", "Ethan Perez", "Ankesh Anand", "Florian Golemo", "Luca Celotti", "Florian Strub", "Jean Rouat", "Hugo Larochelle", "Aaron Courville"], "TL;DR": "HoME is an open-source and extensible platform for artificial agents to learn at large-scale from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context of thousands of simulated 3D household environments.", "pdf": "/pdf/911408eb0fe0e1272beaa110595bb7fdc2e92d32.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582946402, "id": "ICLR.cc/2018/Workshop/-/Paper256/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper256/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper256/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper256/AnonReviewer1"], "reply": {"forum": "B1pJ3dkwG", "replyto": "B1pJ3dkwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper256/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper256/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582946402}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582789828, "tcdate": 1520629560779, "number": 3, "cdate": 1520629560779, "id": "S1ZBD_eFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper256/Official_Review", "forum": "B1pJ3dkwG", "replyto": "B1pJ3dkwG", "signatures": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer1"], "content": {"title": "Nice work", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper introduces HoME, an interactive environment for multi-modal learning. I think this is a good idea, the paper is well-written and fits well with current trends in the field. I only have a few minor comments:\n\n1. The references for prior work in \"multimodal learning\" are meagre, or even inappropriate: there has been a lot of work in multimodal learning before those papers (I get that the emphasis is on \"interactive\", but still).\n2. I really like the related work comparison in the appendix, but I missed it initially. Perhaps it's worth emphasizing more?\n3. It's interesting that the environment has an acoustic engine. Can you be more precise about what this would be used for exactly? How would it be useful e.g. for language grounding?\n4. It would be nice to be a bit more precise, i.e., can you not list exactly what actions are available?\n5. \"HoME may be extended to generate language instructions\" -- I think one of the big problems with other environments is that they have templated language, do you really think that this is a good idea? Perhaps change \"generate\" to \"obtain\", to allow e.g. human annotation, which is likely to be superior?\n6. I realize that space is limited, but it would be great to see some more motivation about why we would want to go in this direction (rather than, say, sticking to single modalities or sticking to passive fixed-dataset learning).", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HoME: a Household Multimodal Environment", "abstract": "We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.", "paperhash": "brodeur|home_a_household_multimodal_environment", "keywords": ["simulated environment", "virtual embodiment", "multimodality", "reinforcement learning", "gym", "language grounding"], "_bibtex": "@misc{\n  brodeur2018home:,\n  title={HoME: a Household Multimodal Environment},\n  author={Simon Brodeur and Ethan Perez and Ankesh Anand and Florian Golemo and Luca Celotti and Florian Strub and Jean Rouat and Hugo Larochelle and Aaron Courville},\n  year={2018},\n  url={https://openreview.net/forum?id=B1pJ3dkwG}\n}", "authorids": ["simon.brodeur@usherbrooke.ca", "ethanperez@rice.edu", "ankesh.anand@umontreal.ca", "florian.golemo@inria.fr", "luca.celotti@usherbrooke.ca", "florian.strub@inria.fr", "jean.rouat@usherbrooke.ca", "hugolarochelle@google.com", "aaron.courville@umontreal.ca"], "authors": ["Simon Brodeur", "Ethan Perez", "Ankesh Anand", "Florian Golemo", "Luca Celotti", "Florian Strub", "Jean Rouat", "Hugo Larochelle", "Aaron Courville"], "TL;DR": "HoME is an open-source and extensible platform for artificial agents to learn at large-scale from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context of thousands of simulated 3D household environments.", "pdf": "/pdf/911408eb0fe0e1272beaa110595bb7fdc2e92d32.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582946402, "id": "ICLR.cc/2018/Workshop/-/Paper256/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper256/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper256/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper256/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper256/AnonReviewer1"], "reply": {"forum": "B1pJ3dkwG", "replyto": "B1pJ3dkwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper256/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper256/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582946402}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573552773, "tcdate": 1521573552773, "number": 42, "cdate": 1521573552428, "id": "SJt3RCAKz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "B1pJ3dkwG", "replyto": "B1pJ3dkwG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HoME: a Household Multimodal Environment", "abstract": "We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.", "paperhash": "brodeur|home_a_household_multimodal_environment", "keywords": ["simulated environment", "virtual embodiment", "multimodality", "reinforcement learning", "gym", "language grounding"], "_bibtex": "@misc{\n  brodeur2018home:,\n  title={HoME: a Household Multimodal Environment},\n  author={Simon Brodeur and Ethan Perez and Ankesh Anand and Florian Golemo and Luca Celotti and Florian Strub and Jean Rouat and Hugo Larochelle and Aaron Courville},\n  year={2018},\n  url={https://openreview.net/forum?id=B1pJ3dkwG}\n}", "authorids": ["simon.brodeur@usherbrooke.ca", "ethanperez@rice.edu", "ankesh.anand@umontreal.ca", "florian.golemo@inria.fr", "luca.celotti@usherbrooke.ca", "florian.strub@inria.fr", "jean.rouat@usherbrooke.ca", "hugolarochelle@google.com", "aaron.courville@umontreal.ca"], "authors": ["Simon Brodeur", "Ethan Perez", "Ankesh Anand", "Florian Golemo", "Luca Celotti", "Florian Strub", "Jean Rouat", "Hugo Larochelle", "Aaron Courville"], "TL;DR": "HoME is an open-source and extensible platform for artificial agents to learn at large-scale from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context of thousands of simulated 3D household environments.", "pdf": "/pdf/911408eb0fe0e1272beaa110595bb7fdc2e92d32.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}