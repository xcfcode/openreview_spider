{"notes": [{"id": "GjqcL-v0J2A", "original": "La_ddD2_Op", "number": 1055, "cdate": 1601308118795, "ddate": null, "tcdate": 1601308118795, "tmdate": 1614985746883, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "GRIPTGvBfFM", "original": null, "number": 1, "cdate": 1610040389236, "ddate": null, "tcdate": 1610040389236, "tmdate": 1610473983293, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper introduces and analyses a method to train a population of VAEs with mixed continuous (referred to as \"style\") and discrete (referred to as \"labels\") latent-variables. The population is trained under the constraint that inferred discrete latent variables  to be the same for all models.\nThe paper also investigates a data augmentation mechanism inspired by  (Antoniou et al., 2017).\nThe presentation is overall clear and the idea is interesting, although the language of \"agents\" is not standard in generative model literature and is a bit confusing. The experiments also show very good clustering results of the proposed method.\nUnfortunately the pipeline was determined to be quite complex while the motivation for its design choices were unclear. This, combined with multiple concerns about the experimental validation, led to a reject decision.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040389223, "tmdate": 1610473983276, "id": "ICLR.cc/2021/Conference/Paper1055/-/Decision"}}}, {"id": "dXg_NYYmK-y", "original": null, "number": 1, "cdate": 1603715138348, "ddate": null, "tcdate": 1603715138348, "tmdate": 1606800045775, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Review", "content": {"title": "Official review 4 on the borderline", "review": "====================================================================================================\n\nSummary : \n\nThe paper proposed the new disentanglement approach based on the \"wisdom of the crowd\". First, the proposed method enforces the consensus on the categorical assignments from the different agents. Each agent receives a similar image that is generated from the data augmentation method (based on the VAE-GAN technique).  The encoder of each agent first estimates the categorical distribution and estimates continuous variables from the categorical data and original image. The proposed method focuses on disentangling categorical information. The evaluation also focuses on categorical data and the proposed method outperforms the other single-agent baselines.  \n\n===================================================================================================\n\nReason for score: \n\nOverall, I'm on the borderline between acceptance and rejection. The proposed method is persuasive, but the ablation study is required to improve this paper. I will finalize my decision after reading the reviews from the other reviewers and authors' responses to my concerns.\n\n====================================================================================================\n\nStrong points: \n\n(1) I was impressed by the idea to generate a similar image with the same class label through techniques developed from VAE-GAN since this idea reverts the unsupervised setting into a weakly-supervised setting. In this regard, I have a concern (See cons). \n\n(2) I like the proposed multi-agent approach to increase the robustness of the classification label. Maybe this method largely reduces the variance from the different seeds.\n\n====================================================================================================\n\nCons :\n\nThe idea which leverages a similar image with the same class label is quite similar in the disentanglement under weakly supervised setting as in [1]. Therefore, type-preserving augmentation seems critical in the performance of the proposed method. In this sense, I believe the ablation study on the augmentation method is required to persuade readers and to provide guidelines for future research. (e.g. Adding gaussian noise, Mirror image)\n\nEven though the disentanglement score cannot be evaluated without supervision on the labels, I recommend authors to compare the disentanglement score with other baselines(Joint VAE, Cascade VAE), 'I(s;c)', and etc. \n\n=============================================================================================================\n\nAfter Rebuttal : \n\nSorry for the delay. I checked the comments of the other reviewers, responses, and the revised version. The authors address my concerns in the revised version. I vote for marginal acceptance. \n\n[1] Weakly-Supervised Disentanglement Without Compromises, ICML 2020", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128178, "tmdate": 1606915769375, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1055/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Review"}}}, {"id": "zgBJel_PzDy", "original": null, "number": 10, "cdate": 1606263320838, "ddate": null, "tcdate": 1606263320838, "tmdate": 1606263320838, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment", "content": {"title": "manuscript and supplement are updated", "comment": "We would like to thank all the reviewers for their valuable and constructive comments. To find our latest modifications, please see the newest manuscript and supplement files."}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GjqcL-v0J2A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1055/Authors|ICLR.cc/2021/Conference/Paper1055/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864207, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment"}}}, {"id": "8UiLNAIe0GW", "original": null, "number": 9, "cdate": 1606263023660, "ddate": null, "tcdate": 1606263023660, "tmdate": 1606263023660, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "61Q3i8hMi8p", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment", "content": {"title": "Further question(s)", "comment": "We thank the reviewer for updating the score. If there are other concerns/questions, we would be happy to address them. We are quite excited about our work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GjqcL-v0J2A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1055/Authors|ICLR.cc/2021/Conference/Paper1055/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864207, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment"}}}, {"id": "61Q3i8hMi8p", "original": null, "number": 8, "cdate": 1606255517384, "ddate": null, "tcdate": 1606255517384, "tmdate": 1606255517384, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "0T0OeBEbes5", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment", "content": {"title": "Updated Review", "comment": "Thanks for your response and the updated version of the paper. I've increased my score to 6."}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GjqcL-v0J2A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1055/Authors|ICLR.cc/2021/Conference/Paper1055/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864207, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment"}}}, {"id": "6U-U2CdfW4", "original": null, "number": 4, "cdate": 1604318339332, "ddate": null, "tcdate": 1604318339332, "tmdate": 1606255360319, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Review", "content": {"title": "Interesting work, but some concerns with the current presentation", "review": "This work presents a new approach to handling categorical latent variables in VAEs. The method has two key components: a *multi-agent* architecture in which categorical assignments are generated through consensus across multiple VAE models and a data-augmentation method which allows each model (termed *agent*) to be trained on a perturbed version of the original data. The authors provide some theoretical justification for their approach and evaluate on two benchm`ark data-sets and one real-world application.\n\n**Strengths**\n1. The method presented seems to be quite novel, with various technical contributions required to jointly train the separate VAE models in a way that they didn't collapse to a single model.\n2. The evaluation on MNIST and dSprites provide empirical evidence that this method out-performs a number of baseline methods. To my knowledge, the methods selected are good candidates for being SOTA methods for VAEs with discrete latent variables.\n3. The application to the scRNA-seq provides further evidence that the method works well on real-world data.\n\nHowever, I have some concerns with the presentation of the paper as is. If the authors can address these, I am happy to increase my score.\n\n**Questions / Concerns**\n1. While I did not have time to check all the proofs in detail, I wasn't convinced by the proof of Proposition 1 in Appendix A. A few specific things on which I would appreciate clarification from the authors:\n\na) It seems like the proof (as stated) would follow if all the agents were identical, but this doesn't match what one would expect. Can the authors make the impact of using different agents more explicit in the assumptions and explain how this leads to the conclusion?\n\nb) The proof assumes the augmented samples are generated from p(x | \\phi = n) but the method generates augmented samples as augmentations of a specific training example x_n. Can the authors explain why they believe this isn't an issue to the relevance of their result?\n\nc) Going from (6) to (7) appears to assume that the probability for each augmented sample is the same. We would not expect this to be the case for a single agent or multiple agents. Can the authors explain the basis for this step?\n\n2. I believe there are a number of different lines of research which could be considered as relevant related work which the authors have missed. \n\na) Boosting Variational Inference [1,2] describes techniques which use a mixture of variational distributions within the VI setup to provide a better approximation to the posterior. While the method presented here is not strictly a boosting method, it is quite possible that the gains seen are a result of being able to better approximate the true posterior by allowing the model to fit multiple inference networks. \n\nb) Consensus clustering aggregates the results of a clustering algorithm over multiple initialisations [3]. This often gives an improvement over the output of a single instance of a clustering algorithm and can be applied post-hoc to any algorithm that outputs cluster assignment. For example, a consensus version of CascadeVAE could be considered as an additional method in the experiments.\n\nc) Co-training is a method for training classifiers on multi-view data such that they predict same labels for co-occurring patterns in each view. This was originally presented by Blum and Mitchell [4] as an approach for semi-supervised learning and was subsequently applied to unsupervised learning by Kumar and Daume III [5]. The key idea in methods that leverage co-tr`aining is to use consensus across the different models, which feels similar to the consensus constraint in equation (3). As well as being relevant related work, the theoretical perpsective of co-training might also be useful to address some of the concerns stated about the theoretical result in this paper.\n\n3. I'm not sure how informative Figure 3 is. It's to be expected that the best performance will come when the dimension of the latent variable is equal to the true number of classes, but this will not be available in the fully-unsupervised case where the true classes are unknown. It would be useful if model selection using AMP was shown to lead to the correct number of classes for MNIST, but it wasn't clear to me if this could be inferred from the plot. If this is what the authors intended to show, they should add some additional text to explain why this is the case.\n\n4. In the experiments on MNIST, the authors compare to the case where m=10, S=10 (Table 3 of Jeong and Song). But Jeong and Song reported higher accuracy (with lower variance) for  the case with m=4, S=10. Why did the authors choose to compare against the lower performing of the two configurations studied in Jeong and Song?  \n\n5. It wasn't 100% clear to me whether the model uses fully independent VAEs (i.e. separate parameters for encoder and decoder in each model), but I believe this is the case. It would be informative to compare against the case where the decoder network has the same parameters in each model. I suspect the gain is primarily due to the fact that we are aggregating over different encoders / inference networks and this additional experiment would make this clear. \n\n**References**\n1. Guo et al. (2016). Boosting Variational Inference\n2. Locatello et al. (2018). Boosting Black Box Variational Inference\n3. Monti et al. (2003). Consensus Clustering: A Resampling-Based Method for Class Discovery and Visualization of Gene Expression Microarray Data\n4. Blum and Mitchell (1998). Combining labeled and unlabeled data with co-training\n5. Kumar and Daume III (2011). A co-training approach for multi-view spectral clustering", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128178, "tmdate": 1606915769375, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1055/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Review"}}}, {"id": "Rso6hyLaOx7", "original": null, "number": 7, "cdate": 1605925831062, "ddate": null, "tcdate": 1605925831062, "tmdate": 1605934167594, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "DmRG_JR7jow", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "We now revised our manuscript and added a Related Work section, which discusses papers beyond those already cited in Introduction, including the paper suggested by this reviewer. \n- We have included additional quantitative and qualitative evaluation for the data augmentation (Experiments section and Section. Our analysis on MNIST suggests that the augmenter achieves ~96.5% type-consistency. Briefly, we imported a state-of-the-art classifier for MNIST achieving ~99.5% classification accuracy. We passed the augmented test samples to this classifier to obtain the result reported above. We revised our manuscript to include this new analysis. Moreover, to demonstrate the performance of the data augmentation for the non-image dataset, we reported the augmentation results for the single cell data set including the genes expression profiles (revised supplement, Section I). \n\n- To address the reviewer\u2019s comment on the hyperparameters of the proposed cpl-mixVAE model, we would like to point out: \n\n(i) Requiring a fine parameter tuning process is one of multiple reasons for the alternative methods in falling short. Results across different datasets demonstrate some of those issues, e.g. mode collapse. We observed that alternative methods depended strongly on parameters tuning. One of the main challenges of methods like JointVAE is their reliance on channel capacity parameters, which need to be dynamically adjusted throughout training in a nontrivial way, and independently adjusted for each discrete and continuous variable, while there could be dependency between those factors. JointVAE requires three parameters just for each latent space, i.e. minimum capacity, maximum capacity, and the number of iterations to change the capacity. In contrast, the proposed cpl-mixVAE only requires a coupling factor (lambda) that regularizes the cooperation across agents. For all datasets, we suggested 1=<lambda<=10. In the data augmentation implementation, we also used the same gamma value across all datasets, without any additional fine tuning.  \nWe now added a new section in the supplement(Section H), in which we investigated the sensitivity of the cpl-mixVAE performance to lambda, in comparison with JointVAE with four critical parameters. Our results show that while cpl-mixVAE's performance remains robust within a window covering two orders of magnitude, JointVAE is sensitive to changes to the channel capacity. \n\n(ii) The temperature parameter (tau) is not an additional parameter in our framework. It is a parameter of the Gumbel-softmax distribution which is part of both alternative methods, JointVAE and CascadeVAE. In all our experiments, we used the same temperature values suggested by the previous studies, without any tuning. The study by Jang et al., 2016 discusses the impact of tau on the discrete probability estimation.  \n\n- The proposed multi-agent framework assumes only that the agents agree in expectation on the probabilities, and not on a point-by-point basis: the agents have identical architectures, they are i.i.d. initialized, and our training method is completely symmetric over the agents. To avoid further confusion, we have revised the proof of Prop. 1 and clarified these aspects in both main text and the supplement. (Please also see a similar question by Reviewer3) \n\n- The approximation used in the ELBO derivation for the joint discrete distribution, is using the Midpoint rule.  We now added new analysis bounding the error in calculating joint categorical distribution using the Midpoint rule. To show the quality of the ELBO-approximation, we explicitly formulated the error term and showed the error bound on the joint probability is a function of a constant value, independent of the variational inference optimization parameters. \n\n- Type-preserving augmentation is not helpful for 1-agent framework, because, (i) we are not using data augmentation to increase the training sample size; (ii) our augmentation method is ~96.5% type-preserving. Therefore, utilizing augmented samples is helpful only when there are multiple agents cooperating to make a decision. This is demonstrated by JointVAE performing slightly worse when it uses augmented samples as well.  \nOn a related note, we have now added new analysis with CascadeVAE on the scRNAseq dataset (Fig. 5), which shows CascadeVAE performs significantly worse than our method, while it is ~70 times slower than our method on the same GPU. This can be attributed to both the complexity of the dataset and the strong reliance of CascadeVAE  assumption on a uniform categorical prior (Lemma 1 in the CascadeVAE paper). \n\n- We now discuss the ML-VAE approach in the new Related Work section. The ML-VAE method proposed by Bouchacourt et al. is a weakly supervised disentanglement method, in which class label information is used for grouping samples. In contrast, here we focus on the fully unsupervised representation learning problem and without relying on any class label information. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GjqcL-v0J2A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1055/Authors|ICLR.cc/2021/Conference/Paper1055/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864207, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment"}}}, {"id": "yYtFiaO0sC", "original": null, "number": 6, "cdate": 1605924113238, "ddate": null, "tcdate": 1605924113238, "tmdate": 1605934023432, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "dXg_NYYmK-y", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment", "content": {"title": "Response to reviewer 4", "comment": "We revised the manuscript and included a Related Work section, where we cite and discuss the paper suggested by the referee. \n\n-To address the reviewer\u2019s comment on ablation studies on the augmenter: \n\nWe have included more assessment for the proposed data augmentation for both image and non-image datasets (Experiments section and the supplement of the revised manuscript, Section I). Our quantitative study on the MNIST dataset reveals that the augmenter achieves ~96.5% type-consistency. Please also see a similar question by AnonReviewer1. \n\nWe also added new theoretical analysis on the consequences of the augmenter potentially under-exploring the variability within a cluster. Briefly, the Remark after the proof of Proposition 1 shows that the multi-agent framework will still be useful in this scenario, except that each agent will contribute less than before to improving the confidence score. \n\nIn addition, we would like to bring the reviewer\u2019s attention to two experimental studies in the Table 1: \n         (i) evaluating a single agent model performance using the proposed data augmentation approach, in which we showed that adding the augmenter only slightly degrades the performance of JointVAE (JointVAE++ in Table 1). This is due to the 3.5% performance gap. (mentioned above) \n         (ii) We have also reported a study on the cpl-mixVAE model by replacing the proposed VAE-GAN-like augmenter with an augmenter that applies a set of conventional image augmentations including affine transformation to the MNIST image (reported as cpl-mixVAE^a in Table 1). This study shows that this affine augmenter performs slightly worse than the proposed VAE-GAN-like augmenter. \n\n \n-Per the reviewer\u2019s request, we have now added the disentanglement score to Table 2, for the dSprite dataset. To have a fair comparison with the alternative methods, we used the original disentanglement score reported for CascadeVAE and JointVAE. Briefly, our method jointly achieved high accuracy and disentanglement scores. On the other hand, we would like to point out that in calculation of this score (suggested by Kim et al., 2018), it is assumed that all latent factors are independent (disentangled). However, in the proposed cpl-mixVAE model, the continuous factor can depend on the discrete random variable to represent class-dependent variabilities. Since the dSprites is a simulated dataset, in which the continuous factors like scale, rotation, etc., are independent of the geometrical shape (class label), the model can infer q(s|c,x) as q(s|x). Therefore, we can compute the disentanglement score over all factors, the same as cascadeVAE. But for real-world applications, the continuous factor often depends on class label and the suggested disentanglement score is not a proper measure to assess the mixture representation. For instance, the scRNAseq experiment shows that the continuous factors depend strongly on the categorical variable (Fig. 6), and our method significantly outperforms cascadeVAE on this complicated, highly imbalanced problem (updated Fig. 5).  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GjqcL-v0J2A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1055/Authors|ICLR.cc/2021/Conference/Paper1055/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864207, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment"}}}, {"id": "0T0OeBEbes5", "original": null, "number": 4, "cdate": 1605922755076, "ddate": null, "tcdate": 1605922755076, "tmdate": 1605933944743, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "6U-U2CdfW4", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "1) We thank the reviewer for drawing our attention to the proof of Prop. 1. We found typos in the proof, in our original submission. We have now corrected these typos, and revised the notations, presentation of the proof, and the main text surrounding Prop. 1. \n\n(a) The proposed multi-agent framework introduced in Prop. 1 assumes only that the autoencoder agents are identical in expectation, and not on a point-by-point basis: the agents have identical architectures, they are i.i.d. initialized, and our training method is completely symmetric over the agents. As suggested by the reviewer, we edited the proof to make the connection between the proposition and the implementation more precise and explicit (Eqs. 1,2,6 and surrounding text in the proof). We also explicitly define the multi-agent framework in the revised main text (Definition 1). \nFinally, we would like to add that Prop. 1 is not meant to capture all the details of the implementation closely, rather to motivate the general multi-agent framework. (Please also see a similar question by Reviewer1) \n\n(b) Our framework assumes that the augmentations \u2018of a specific training example x_n\u2019 are type-preserving. Therefore, the proposition/proof and the framework agree with each other in this regard. We have now explicitly tested, on MNIST, the validity of the type-preserving assumption, and found that our augmenter is ~96.5% type-preserving. We added this new result to the main text.  \nIf the reviewer is questioning the under-exploration scenario, in which augmented samples are concentrated around the given sample, the proof follows in the same way except the augmented samples are no longer conditionally independent. This means that each augmented sample adds less than before to the confidence score. Yet, the same argument shows that there will be an A for which the claims of the proposition are satisfied. We have added a Remark after the proof summarizing this discussion. \n\n(c) Going from Eq. 6 to Eq. 7 in the original submission, we do not assume the probabilities are the same. Rather, the equality is on the expectations of those probabilities. Specifically, we assume E_x[p(x_1|phi)] = E_x[p(x_A|phi)], which is justified when x_1, \u2026 x_A ~ p(x|phi).\n\n2) We thank the reviewer for these suggestions. Indeed, we found them relevant to our work and we now cite these papers in an additional Section called Related work (in the updated manuscript) explaining how they relate to and differ from our work. \nRegarding consensus clustering, while it provides an interesting aspect as discussed in the new Related work section, it is highly unlikely to improve results when only 2 agents are used, because consensus clustering suggests a voting-like scheme. More importantly, we note that a novelty of our approach is to seek consensus online, at the time of training, not after the agents converge to their individual decisions. This influences the inference of the continuous factors because they depend on the categorical variable, which can only be addressed in a joint learning framework. \n\n3) We thank the reviewer for pointing out this issue and apologize for not being clear about Fig. 3. The main goal of Fig. 3 is to study the performance of cpl-mixVAE for different cardinalities of the categorical variable, |c|. In the figure, we report two measures; ACC (categorical assignment accuracy) and AMP (average of maximum posterior of categories), where each of them represents different aspects of the cpl-mixVAE's performance in encoding the discrete variabilities. The main reason for reporting the AMP measure in this figure is to show that (i) for |c|<10, cpl-mixVAE utilizes all categories, without suffering from collapse, and (ii) for |c|>10, it does not allocate unneeded categories in the interest of high categorical assignment accuracy.  \nAs the reviewer correctly mentioned, since both measures utilize the class label, they cannot be used for finding the true number of clusters. To fix this point, we have updated Fig. 3 and its description to clarify the purpose of those plots. \n\n4) Using a 10-dim continuous variable is quite standard in mixture representation learning for the MNIST dataset. JointVAE (Dupont, 2018), InfoGAN (Chen et al., 2016), and ML-VAE (Bouchacourt et al., 2018) all use 10-dim continuous variables. Indeed, the cascadeVAE paper also chose m=10 to study continuous variability and latent traversals on MNIST. They only claim that m=4 results in a higher clustering accuracy in isolation. It is, however, key to consider both discrete and continuous factors jointly, which is the main focus of our work. \n\n5) We thank the reviewer for this suggestion. We have now performed this experiment, and revised Table 1 to report it, i.e. cpl-mixVAE*. Briefly, sharing parameters across agents produced competitive results. Yet, they were slightly worse than those of fully independent agents, which allow for better exploration of the parameter space. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GjqcL-v0J2A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1055/Authors|ICLR.cc/2021/Conference/Paper1055/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864207, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment"}}}, {"id": "DFnV_ajJdJK", "original": null, "number": 3, "cdate": 1605920852570, "ddate": null, "tcdate": 1605920852570, "tmdate": 1605932987433, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment", "content": {"title": "General response ", "comment": "The reviews were thorough and constructive. There was a clear consensus that our method is \u201cquite novel\u201d and \u201cpersuasive\u201d, and our ideas are \u201ccompelling\u201d, \u201csignificant\u201d, \u201cimpressive\u201d, and \u201coriginal\u201d. The reviews did not identify common weaknesses. Instead, each reviewer questioned different aspects of our work and asked for clarifications/experiments. \nWe believe that there is a mismatch between the qualitative consensus assessment and the quantitative scores \u2013 multiple reviewers explicitly mentioned that they\u2019d be happy to increase their scores if they find our responses satisfactory. \n\nBelow, we address the concerns raised by the reviewers. We respectfully hope that these clarifications and analyses will warrant substantially higher scores. \n\nOur approach achieves strong results on multiple datasets, outperforming state-of-the-art baselines. Yet, beyond quantitative results, we think that the conceptual advance is a key merit of our approach: we demonstrate online consensus building jointly with dissection of learning interpretable continuous factors, without assuming any supervision. We demonstrate that this approach can be implemented in a fully neural architecture, without calling external optimization algorithms. We have now updated our manuscript to clarify the presentation and add further analyses, as suggested by the reviewers. \n\nSummary of main updates: \n1) clarification and improvement of the presentation for Proposition 1\n2) additional theoretical results on type-preserving augmentation (updated supplement, Remark 1) \n3) a new section on related work (updated manuscript, Section 2)\n4) quantitative and additional qualitative analyses on the type-preserving augmenter \n5) a new result on the single cell RNA-seq dataset with cascadeVAE (updated manuscript, Fig. 5) to show that it performs significantly worse than the proposed cpl-mixVAE model\n6) a new study on the robustness against hyperparameter tuning (updated supplement, Fig. 3) \n7) providing the error bound for ELBO approximation (updated supplement, Section D) "}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GjqcL-v0J2A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1055/Authors|ICLR.cc/2021/Conference/Paper1055/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864207, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment"}}}, {"id": "drYP5EJxiW", "original": null, "number": 5, "cdate": 1605923680571, "ddate": null, "tcdate": 1605923680571, "tmdate": 1605927818915, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "cDhEfdZCS3", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "We would like to begin by addressing a potential confusion about the main objective of the paper: we have proposed a general framework for the joint representation learning problem, whose aim is to learn interpretable variational factors. Our results demonstrate the ability to build consensus clustering in an online fashion jointly with dissection of interpretable continuous factors. In contrast, it is not clear how the approach suggested by the reviewer (see below for a detailed discussion on \u201cDeepcluster\u201d and alternating optimization) is immediately applicable to this problem. \n\n- We would like to bring the reviewer's attention to a key point that although the proposed cpl-mixVAE framework is using a type-preserving data augmentation module and multiple independent autoencoders, the complexity of the entire framework in terms of optimization run time, number of hyperparameters and required prior knowledge/assumption is less than that of alternative methods. For instance,   \n(a) the training run time for CascadeVAE method is roughly 70 times longer (on the same GPU hardware) than our method due to its external optimization, based on the RNAseq experiment.   \n(b) For each dataset, JointVAE requires the tuning of hyperparameters (which are sensitive) which is computationally expensive and challenging in practice.\n\n- To support the practicality of our method on multiple applications with different complexities, we showcased its performance not only on benchmark datasets including MNIST and dSprites, but also on a highly non-uniform single cell dataset with ~100 clusters (Fig. 5 and 6 in the main manuscript). We note that none of the benchmark datasets, e.g. MNIST, CelebA, dSprites, have more than 10 clusters, nor inherent class imbalances.  \nIn the revised manuscript, we have updated Fig. 5, which now shows both CascadeVAE and JointVAE performing significantly worse than our method. Moreover, we added an additional section in the supplement (Section H in the updated manuscript) to demonstrate the necessity of parameter tuning for JointVAE.  \n\n- In the second point under \u2018Cons\u2019, the reviewer mentioned \u2018the DeepClustering paper\u2019. While a web search for \u2018DeepClustering\u2019 or \u2018Deep Clustering\u2019 produces multiple potential papers, including the highly popular paper \u2018Deep Clustering for Unsupervised Learning of Visual Features\u2019 by Caron et al, to the best of our understanding, the reviewer is referring to \u2018DeepCluster: A General Clustering Framework based on Deep Learning\u2019 by Tian et al., 2017. \n\nTo address the reviewer\u2019s comment: \n(i) the proposed \u201cDeepCluster\u201d approached by Tian et al. is a clustering algorithm and cannot be utilized for the joint mixture representation learning problem, which is our main focus in this study.   \n(ii) cascadeVAE\u2019s approach is a similar approach to what the reviewer suggested, which uses alternating optimization. The study on the RNAseq data shows it performs significantly slowly and significantly worse (updated Fig. 5). \nWhile trying to use ADMM sounds like an interesting future direction, it is not immediately applicable to our model because of our joint continuous factor dissection and consensus building goals.  \n(iii) We cited Tian et al. , 2017 in the new Related Work section in a comparative way.  \n\n- To address the third comment and prevent further confusion, we have revised the manuscript and explicitly defined the \u201cmulti-agent autoencoder framework\u201d (Definition 1 in the revised manuscript) by introducing autoencoding units as \u2018agents\u2019. In this framework, since autoencoding agents are independent machines which are cooperating with each other during training only and only through the cost function, we called it a \u201cmulti-agent framework\u201d. \n\n- To address the minor comments: \nThe reported results in Fig. 2 were obtained from one of the agents of the cpl-mixVAE framework with two autoencoding agents, where each agent mixture representation is parameterized with 10-dimensional continuous and 10-dimensional categorical variables. We have revised the figure caption and included the missing information. \n\nWe have revised Fig. 1 to provide a better illustration for our framework. \n\nWe respectfully hope that we were able to address any confusions, and the reviewer will revise their initial score in light of these clarifications.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "GjqcL-v0J2A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1055/Authors|ICLR.cc/2021/Conference/Paper1055/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864207, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Comment"}}}, {"id": "DmRG_JR7jow", "original": null, "number": 2, "cdate": 1603817175410, "ddate": null, "tcdate": 1603817175410, "tmdate": 1605024541810, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Review", "content": {"title": "The authors propose a framework to learn a mixture distribution (a distribution which consists of continuous and categorical variables) using a coupled variational autoencoder framework. The different autoencoders are trained using augmented samples which are generated using a VAE-GAN framework. The idea of using coupled autoencoders is compelling and it makes sense and is well-known that data augmentation increases performance - given the \u2018new\u2019 data is good enough. ", "review": "Quality\nThe paper proposes good ideas that are compelling and make sense. Using multiple agents and data augmentation seems directions worth pursuing to improve mixture representation learning.\nThe experiments are mostly clear and well-motivated. Unfortunately, the related work is limited and could be improved and extended. For the evaluation of the data augmentation step, there is only a qualitative evaluation in Figure 2. A more in-depth analysis regarding the type-consistency would strengthen this part of the pipeline.\nIn section 2, the authors say that previous work falls short in efficiently learning mixture distributions due to hyper-parameter tuning or additional cost due to the optimization procedure. In the proposed work, there are some (e.g. coupling hyper-parameter lambda, temperature tau or gamma in data augmentation) new hyper parameters introduced as well. This somehow contradicts the motivation of the method and it is not evident why the proposed method is more efficient. Additionally, ablation studies on critical hyper-parameters like lambda or the temperature parameter would be interesting to see and strengthen the paper. Without further evidence, the claim of a more efficient method is not really justified in my opinion.\n\nClarity\nIn general, the paper is well and clearly written. A related work section is missing which makes it more difficult to position the proposed work with respect to previous work. The authors motivate the representation learning part but sometimes use the term clustering. In my opinion, this weakens the motivation and goal of the paper.\n\nOriginality\nThe paper presents a novel view on mixture representation learning using data augmentation which itself is not a new idea nor do they use a new method to augment the data. The multi-agent view on mixture representation learning is a new idea (to my knowledge, but not aware of all related work) making the paper original.\n\nSignificance\nThe ideas of the paper are significant and worth pursuing.\n\nQuestions to the authors\n-Proof of Proposition 1, equation 7: I do not fully understand where the factor A is coming from in eq. 7. Isn\u2019t this assuming that all agents agree on p(x_a | phi = m)? Is this valid? Thanks for explaining this a bit more in detail.\n-Is your final objective still a valid ELBO? In the proof in the appendix you use approximations to derive the objective, but not only bounds. So I am not sure if it is still an ELBO.\n-In the experiments (Table 1), why is only jointVAE used with augmented data and not cascadeVAE which seems to perform better in its vanilla version? \n\nFurther Comments\n-The work has limited comparison to previous work. Bouchacourt et al.\u2019s \u201cMulti-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations\u201d presents a similar idea of using content/class and style/state spaces (only with continuous variables). A comparison to this work showing the potential benefit of using a discrete class space would improve the quality of the paper. \n-I am happy to upgrade my score if the authors address my concerns.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128178, "tmdate": 1606915769375, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1055/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Review"}}}, {"id": "cDhEfdZCS3", "original": null, "number": 3, "cdate": 1603830780494, "ddate": null, "tcdate": 1603830780494, "tmdate": 1605024541744, "tddate": null, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "invitation": "ICLR.cc/2021/Conference/Paper1055/-/Official_Review", "content": {"title": "OK submission but lack of  motivation and proposed model is computationally expensive", "review": "This paper proposed a multi-agent VAE model that combines multiple copies of VAEs with coupling constraints to improve its latent representation learning (by encouraging discrete variable consistency). The experiments show that the proposed model outperforms other discrete&continuous VAE models in terms of clustering ACC. The experiment is conducted on MNIST and scRNA-seq datasets.\n\nOverall\n===\nI think this is a good submission in terms of describing its methodology. However, it is hard to justify its motivation as the model is over-complex, and the same task could be achieved by other approaches such as deep clustering. It is better if the authors could include such discussions in the paper. \n\nPros\n===\n1. This paper gives sufficient justification on why we need multiple copies of estimators to reach better estimation. \n2. The method description given in Section 3.1 is quite clear as the equations are self-explainable. \n3. Results shown in table 2 indicate significant performance improvement.\n\n\nCons\n===\n1. The approach proposed itself is very complex comparing to other generative model architecture. It is hard to justify if the model can generalize to other tasks other than simple, low-dimensional tasks. Consider maintaining multiple VAEs graphs in memory; it appears hard to take advantage of this work. Is the cost of deploying this model worth the performance improvement (as Table 1 shows limited improvement)? Also, to train such a model, the user also needs to produce a type-preserving augmentation, which is very costly for clustering.\n\n2. The proposed model jointly optimizes the main objective (as Equation 3) and also optimizes the relaxed equivalent constraints at the same time (as Equation 6 last component). Is there any justification why not alternative optimization but joint? As the DeepClustering paper mentioned, alternative optimization is better than joint in their case. Not sure if there will be a similar observation here. Isn't it comparable to the DeepClustering paper if the goal is to do the clustering and interpretation?\n\n3. The introduction of this paper gives me a hard time to follow as the terminology used is uncommon in generative model literature (single-agent, multi-agent). I was confused as it appears to be an RL paper at the beginning.\n\n\nMinors\n===\n1. Which style state shown in Figure 2 (b-e) if you have multiple agents? The description said those are four style/state dimensions, but there are two agents in the experiment setting, and each of them has 10 style/state dimensions. Please be more precise. \n2. Figure 1 (b-c) is not quite informative, and they are too close to each other. It is better to provide a more intuitive demonstrative figure. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1055/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1055/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixture Representation Learning with Coupled Autoencoding Agents", "authorids": ["~Yeganeh_Marghi1", "~Rohan_Gala1", "~Uygar_S\u00fcmb\u00fcl2"], "authors": ["Yeganeh Marghi", "Rohan Gala", "Uygar S\u00fcmb\u00fcl"], "keywords": ["Multi-agent network", "representation learning", "collective decision making", "type-preserving data augmentation"], "abstract": "Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.", "one-sentence_summary": "We propose a multi-agent variational framework to jointly infer discrete and continuous factors through collective decision making.", "pdf": "/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "marghi|mixture_representation_learning_with_coupled_autoencoding_agents", "supplementary_material": "/attachment/1541087e36fd63ba402ae87622d890420217edd2.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=vf_koE9GAV", "_bibtex": "@misc{\nmarghi2021mixture,\ntitle={Mixture Representation Learning with Coupled Autoencoding Agents},\nauthor={Yeganeh Marghi and Rohan Gala and Uygar S{\\\"u}mb{\\\"u}l},\nyear={2021},\nurl={https://openreview.net/forum?id=GjqcL-v0J2A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "GjqcL-v0J2A", "replyto": "GjqcL-v0J2A", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1055/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128178, "tmdate": 1606915769375, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1055/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1055/-/Official_Review"}}}], "count": 14}