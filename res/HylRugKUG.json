{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582854109, "tcdate": 1520570502962, "number": 1, "cdate": 1520570502962, "id": "BJyce9kYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper32/Official_Review", "forum": "HylRugKUG", "replyto": "HylRugKUG", "signatures": ["ICLR.cc/2018/Workshop/Paper32/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper32/AnonReviewer2"], "content": {"title": "useful gradient calculation mechanism generalises to second order but lacks algorithm and experiments", "rating": "5: Marginally below acceptance threshold", "review": "This paper shows the outer-product structure in gradient, which has been reported by previous works, specifically the \"sufficient-factor\" work in the context of first-order gradient.  The authors show that similar structure exists in the second order gradient. \n\nHowever, it is unclear how this structure can be applied to algorithms that speedups the update. I would encourage the author to come up with such algorithm and along with appropriate experiments to show the applicability of the method. \n\nOne potential drawback of such approach is that the factored representation relies on small batch size. While second order method usually requires bigger batch-sizes.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Outer Product Structure of Neural Network Derivatives", "abstract": "Training methods for neural networks are primarily variants on stochastic gradient descent. Techniques that use (approximate) second-order information are rarely used because of the computational cost and noise associated with those approaches in deep learning contexts. We can show that feedforward and recurrent neural networks exhibit an outer product derivative structure but that convolutional neural networks do not. This structure makes it possible to use higher-order  information without needing approximations or significantly increasing computational cost.", "pdf": "/pdf/12fadc054c0b3188848331beebc44a31ee0812aa.pdf", "TL;DR": "We can show that feedforward and recurrent neural networks exhibit an outer product derivative structure, and this makes it possible to use higher-order information without needing approximations or significantly increasing computational cost.", "paperhash": "bakker|the_outer_product_structure_of_neural_network_derivatives", "keywords": ["Deep Learning", "Training Methods", "Gradient Calculations"], "authors": ["Craig Bakker", "Michael J. Henry", "Nathan O. Hodas"], "authorids": ["craig.bakker@pnnl.gov", "michael.j.henry@pnnl.gov", "nathan.hodas@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582853921, "id": "ICLR.cc/2018/Workshop/-/Paper32/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper32/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper32/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper32/AnonReviewer1"], "reply": {"forum": "HylRugKUG", "replyto": "HylRugKUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper32/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper32/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582853921}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582599987, "tcdate": 1520977160761, "number": 2, "cdate": 1520977160761, "id": "ByWGBpSKM", "invitation": "ICLR.cc/2018/Workshop/-/Paper32/Official_Review", "forum": "HylRugKUG", "replyto": "HylRugKUG", "signatures": ["ICLR.cc/2018/Workshop/Paper32/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper32/AnonReviewer1"], "content": {"title": "Interesting, though opaque paper", "rating": "6: Marginally above acceptance threshold", "review": "The paper is well-written and well-motivated, but the central claim, \"[feed-forward and recurrent neural networks] exhibit an outer product derivative structure\", is not spelled out in sufficient mathematical clarity -- nor is the contrasting claim, that convolutional networks do not possess this structure. No experiments or specific applications of the finding are described in detail. In my view this would nonetheless be an interesting workshop presentation.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Outer Product Structure of Neural Network Derivatives", "abstract": "Training methods for neural networks are primarily variants on stochastic gradient descent. Techniques that use (approximate) second-order information are rarely used because of the computational cost and noise associated with those approaches in deep learning contexts. We can show that feedforward and recurrent neural networks exhibit an outer product derivative structure but that convolutional neural networks do not. This structure makes it possible to use higher-order  information without needing approximations or significantly increasing computational cost.", "pdf": "/pdf/12fadc054c0b3188848331beebc44a31ee0812aa.pdf", "TL;DR": "We can show that feedforward and recurrent neural networks exhibit an outer product derivative structure, and this makes it possible to use higher-order information without needing approximations or significantly increasing computational cost.", "paperhash": "bakker|the_outer_product_structure_of_neural_network_derivatives", "keywords": ["Deep Learning", "Training Methods", "Gradient Calculations"], "authors": ["Craig Bakker", "Michael J. Henry", "Nathan O. Hodas"], "authorids": ["craig.bakker@pnnl.gov", "michael.j.henry@pnnl.gov", "nathan.hodas@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582853921, "id": "ICLR.cc/2018/Workshop/-/Paper32/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper32/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper32/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper32/AnonReviewer1"], "reply": {"forum": "HylRugKUG", "replyto": "HylRugKUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper32/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper32/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582853921}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573578637, "tcdate": 1521573578637, "number": 151, "cdate": 1521573578291, "id": "SJXC00CFz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HylRugKUG", "replyto": "HylRugKUG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Outer Product Structure of Neural Network Derivatives", "abstract": "Training methods for neural networks are primarily variants on stochastic gradient descent. Techniques that use (approximate) second-order information are rarely used because of the computational cost and noise associated with those approaches in deep learning contexts. We can show that feedforward and recurrent neural networks exhibit an outer product derivative structure but that convolutional neural networks do not. This structure makes it possible to use higher-order  information without needing approximations or significantly increasing computational cost.", "pdf": "/pdf/12fadc054c0b3188848331beebc44a31ee0812aa.pdf", "TL;DR": "We can show that feedforward and recurrent neural networks exhibit an outer product derivative structure, and this makes it possible to use higher-order information without needing approximations or significantly increasing computational cost.", "paperhash": "bakker|the_outer_product_structure_of_neural_network_derivatives", "keywords": ["Deep Learning", "Training Methods", "Gradient Calculations"], "authors": ["Craig Bakker", "Michael J. Henry", "Nathan O. Hodas"], "authorids": ["craig.bakker@pnnl.gov", "michael.j.henry@pnnl.gov", "nathan.hodas@pnnl.gov"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518041287970, "tcdate": 1518041287970, "number": 32, "cdate": 1518041287970, "id": "HylRugKUG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HylRugKUG", "signatures": ["~Craig_Bakker1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "The Outer Product Structure of Neural Network Derivatives", "abstract": "Training methods for neural networks are primarily variants on stochastic gradient descent. Techniques that use (approximate) second-order information are rarely used because of the computational cost and noise associated with those approaches in deep learning contexts. We can show that feedforward and recurrent neural networks exhibit an outer product derivative structure but that convolutional neural networks do not. This structure makes it possible to use higher-order  information without needing approximations or significantly increasing computational cost.", "pdf": "/pdf/12fadc054c0b3188848331beebc44a31ee0812aa.pdf", "TL;DR": "We can show that feedforward and recurrent neural networks exhibit an outer product derivative structure, and this makes it possible to use higher-order information without needing approximations or significantly increasing computational cost.", "paperhash": "bakker|the_outer_product_structure_of_neural_network_derivatives", "keywords": ["Deep Learning", "Training Methods", "Gradient Calculations"], "authors": ["Craig Bakker", "Michael J. Henry", "Nathan O. Hodas"], "authorids": ["craig.bakker@pnnl.gov", "michael.j.henry@pnnl.gov", "nathan.hodas@pnnl.gov"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 4}