{"notes": [{"id": "c5QbJ1zob73", "original": "EVrvHYzSxp", "number": 2351, "cdate": 1601308259130, "ddate": null, "tcdate": 1601308259130, "tmdate": 1614985762435, "tddate": null, "forum": "c5QbJ1zob73", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 30, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Mdz7zs_awPR", "original": null, "number": 1, "cdate": 1610040371362, "ddate": null, "tcdate": 1610040371362, "tmdate": 1610473962879, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents new analysis for self-supervised learning. All reviewers are positive about some new perspectives of the analysis. However, some serious concerns have been raised about the rigorousness and the presentation clarity. The paper would be significantly improved, if the authors could address the concerns."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040371348, "tmdate": 1610473962860, "id": "ICLR.cc/2021/Conference/Paper2351/-/Decision"}}}, {"id": "1bmqY4eOCu", "original": null, "number": 25, "cdate": 1606241410988, "ddate": null, "tcdate": 1606241410988, "tmdate": 1606286813395, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Revision", "comment": "**Summary of updates to paper in response to reviewers**\n\nWe again thank the reviewers for their careful reviews of our work.  We have taken these reviews seriously and made many major updates to our paper, which we have uploaded in our newly submitted revision. Here we summarize the major updates (in addition to these major updates, we have made numerous modifications for clarity which we do not list here).\n\n**First [R1, R3]:** To address the questions about the relation between our paper and the paper \u201cBYOL works even without batch statistics\u201d [1] (which appeared after our paper was submitted), we have derived an entirely new analysis of BYOL for shallow linear architectures without any batch normalization whatsoever.  We find very interesting analytic results that yield conceptual insights into why BYOL can learn useful, non-collapsed representations without having any contrastive terms induced either by BatchNorm or by explicit negative pairs, in what is likely to be the simplest non-trivial setting of shallow linear networks. The basic idea is that collapsed solutions do exist, but they are unstable, while the non-collapsed solutions are stable.  Therefore BYOL, in our simple setting, avoids the former and converges to the latter.  Moreover, we show how the existence of a predictor network is critical for establishing instability of the collapsed-solutions by dramatically modifying the learning dynamics. While for more complicated encoder like ResNet, existing blogpost indeeds shows that BYOL without any normalization still collapses, our analysis provides a non-trivial example showing an important difference with/without the predictor. \n\nAll of this analysis can be found in an entirely new Section F in the appendix, and we now refer to these important results in the abstract, introduction, and results sections.  We hope the reviewers will appreciate our responsiveness to a paper that appeared after our paper was submitted.  \n\nWe furthermore are explicit about the limitations of our new result, stating in the results that \u201cwe leave an analysis of other normalization techniques in nonlinear settings [for BYOL] for future work.\u201d \n\n**Second [R1, R3]:** We note that none of the above results invalidate any of our analytic work showing that BatchNorm does indeed introduce contrastive terms in BYOL.  This statement remains true.  However, of course, we never claimed that other normalization schemes that don\u2019t introduce cross-batch statistics could not also make BYOL work, since we never experimented with such techniques, as [1] did. We have now prominently cited reference [1] in our introduction and clarified that while BN does indeed introduce contrastive terms in BYOL, other normalization methods that don\u2019t can still work.  We have been much more careful with our wording in the revised abstract, introduction and results sections regarding all of these issues, and emphasizing consistency between our results, those of [1], and our new results in Appendix F.\n\n**Third [R1, R2, R4, R5]:** We expected the qualitative conclusions of our results would be relatively robust to the choice of loss function. While our analysis holds rigorously for simple contrastive loss $r_+ - r_-$, here we have bolstered that expectation by providing new theorems (Theorem 4 in the revised version) for the soft-triplet loss and InfoNCE ($H=1$, single negative pair as suggested by R5) **without** the constant gradient assumption. It formally shows the existence of a (weighted) covariance operator for the two losses, and comes with an upper bound estimation of the error term.\n\n**Fourth [R4, R5]:** We have greatly expanded both our motivation for and our description of the Hierarchical Latent Tree Model in Section D.2 where we have added a new Figure (Fig. 6), expanded the table of notation, and added more than a page of text to explain the setting. We also add experiments on the growth of the magnitude of the covariance operator during training in HLTM (Appendix G.2)\n\n**Fifth [R4, R5]:** We extended Sec. 4.2 to multi-dimensional outputs and derive the dynamics without the condition $\\mathrm{Cov}[\\mathbf{u}_j, \\mathbf{u}_k] = 0$ (Eqn. 65) under a new setting ($W_2$ is diagonal), which is much easier to achieve. We also perform experiments (e.g., weight growth, and learning of specific Gaussian components) in Appendix G.1. \n\n**Sixth [R5]:** We have extended Lemma 1 to handle more general case of reversible layers, and added an analysis in the presence of $\\ell_2$ normalization at the topmost layer (Appendix A.2).\n\n**Seventh [R5]:** We have turned the BYOL sections into rigorous statements (Corollary 2) and added EMA analysis  (Theorem 9).\n\n**Eighth [R4]:** We have added detailed experimental setup information in Appendix G.2 (and Fig. 6) and Appendix G.3. \n\nWe hope all of these extensive changes will make reviewers even more positively predisposed to this paper.  Thank you again for a constructive and helpful review process. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "wB_SCkTye7P", "original": null, "number": 18, "cdate": 1605737424663, "ddate": null, "tcdate": 1605737424663, "tmdate": 1605762868032, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "X60tOgCOLI", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your follow-up questions. ", "comment": "We thank R5 for the continued interest in our paper.  We have taken many of your concerns to heart and we are significantly revising our papr. A revised version should be up on the site in a few days. We give detailed responses below, some of which cite revisions that will appear soon to address concerns.\n\n**A1**:  We are encouraged that R5 now partially agrees with us.   We still disagree though with R5\u2019s overly narrow notion that terms of interest in ML are limited to properties like sample complexity, etc.  Indeed, even basic frameworks for understanding SSL have not yet been established.  Furthermore, existing \u201cterms of interest\u201d in ML theory have repeatedly been shown to not accurately describe practice. For example, papers like:\n\n*C. Zhang et al. Understanding deep learning requires rethinking generalization (ICLR 2017 best paper)*\n\n*V. Nagarajan and J. Kolter. Uniform convergence may be unable to explain generalization in deep learning (NeurIPS 2019, Winner of The Outstanding New Directions Paper Award)*\n\nshow that some of very basic concepts like model capacity based bounds on generalization error don\u2019t actually characterize generalization error well, motivating the search for new concepts in supervised learning (e.g. data-dependent sample complexity).  But even more importantly, it is unclear what R5 means by \u201csample complexity\u201d purely within the context of self-supervised learning whose loss function does not explicitly reference any supervised task.  The first step in understanding SSL lies in understanding what features of a data distribution are picked up by an SSL loss function and architecture. This is poorly understood even in the limit of infinite data, and this is the question set out to address.  Showing that the gradient of SIMCLR reduces to a covariance operator that remains positive semidefinite over the entire training process is an important result along this path is an important conceptual result and we highlighted it as a Theorem.  Other reviewers agree with us and disagree with R5 (R4: \u201cthe finding is interesting and novel\u201d, R1: \u201cThe covariance operator sheds light to the black-box learning process of contrastive learning.\u201d  Furthermore we do discuss \u201cproperties of solutions\u201d in illustrative examples in Section 4 for SIMCLR.  Additionally, we will add a completely new section in the appendix discussing properties of solutions of BYOL with linear architectures in the next revision.\n\nWe hope R5 agrees with us that in emerging fields like SSL, where theory is far behind explaining empirical phenomena, one should be very open minded in recognizing interesting and newly discovered \u201cterms of interest\u201d, where traditional ML theory has little to say.\n\n**A2**: The gradient update analysis with $\\ell_2$ normalization on the topmost layer will be updated in the next revision.\n\n**A3**: First, for simple contrastive loss $r_+ - r_-$, the Theorem 2 and the constant negative gradient assumption in Theorem 3 hold exactly.\n\nThe triplet loss $\\max(r_+ - r_- + \\alpha, 0)$ and InfoNCE loss satisfy Theorem 2 at all differentiable points using simple gradient check. Thanks to the questions raised by the reviewers, we recently found that those losses lead to a **weighted covariance operator** under a condition that is much weaker than the constant negative gradient assumed in the current version of the paper. We will update this part in the next revision. As a brief introduction, the weighted covariance operator $\\mathrm{Op}$ is computed as the following:\n\n$$\n\\mathrm{Op}=\\frac{1}{2}\\mathbb{E}_{\\mathbf{x},\\mathbf{x}'\\sim p(\\cdot)}\\left[\\xi(r) (\\bar K_l(\\mathbf{x}) - \\bar K_l(\\mathbf{x}')  (\\bar K_l(\\mathbf{x}) - \\bar K_l(\\mathbf{x}')^\\intercal \\right]\n$$\n\nHere $r := \\frac12\\|\\mathbf{f}_L(\\mathbf{x})-\\mathbf{f}_L(\\mathbf{x}')\\|^2_2$ is the squared distance between the outputs of the two sample $\\mathbf{x}$ and $\\mathbf{x}'$, and $\\xi(r) \\ge 0$ is a weight function. For simple contrastive loss $\\xi(r) \\equiv 1$, for triplet loss $\\xi(r) \\equiv \\mathbb{I}(r \\le \\alpha)$ and for InfoNCE loss $\\xi(r) \\propto \\frac{1}{\\tau}e^{-r/\\tau}$. Note that for triplet loss and for InfoNCE, still some conditions are needed, which are weaker than the constant gradient condition in the current version of the paper. Intuitively, the triplet and InfoNCE loss are basically add different weights to focus on the pair of samples that are close under the current representation.  For simple contrastive loss, $\\xi(r) \\equiv 1$ reduces to $\\mathbb{V}[\\bar K_l(\\mathbf{x})]$.\n\nBoth triple loss and InfoNCE are realistic losses used extensively in many papers (and mentioned by other reviewers). For InfoNCE, we have updated our assumption to be weaker than \u201cconstant negative gradient\u201d which R5 states a preference for. This is the weakest assumption we have come up with so far.  Given this assumption is weaker than what R5 prefers, we hope R5 is more favorably disposed to it than our initial assumption. Thank you for raising this point.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "u13c_Cvk0MO", "original": null, "number": 24, "cdate": 1605762689915, "ddate": null, "tcdate": 1605762689915, "tmdate": 1605762743274, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "LciitZOayd2", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Updated answer for the common question of strong constant gradient condition", "comment": "**Update Answer** to strong constant gradient condition $\\partial L/\\partial r_{k-} = -\\beta / H$. \n\nThanks to the questions raised by the reviewers, after some study, we recently found that both triplets loss and InfoNCE loss can lead to a **weighted covariance operator** under a condition that is much weaker than the constant negative gradient assumed in the current version of the paper. We will update this part in the next revision. As a brief introduction, the weighted covariance operator $\\mathrm{Op}$ is computed as the following:  \n\n$$\n\\mathrm{Op}=\\frac{1}{2}\\mathbb{E}_{\\mathbf{x},\\mathbf{x}'\\sim p(\\cdot)}\\left[\\xi(r) (\\bar K_l(\\mathbf{x}) - \\bar K_l(\\mathbf{x}')  (\\bar K_l(\\mathbf{x}) - \\bar K_l(\\mathbf{x}')^\\intercal \\right]\n$$\n\nHere $r := \\frac12\\|\\mathbf{f}_L(\\mathbf{x})-\\mathbf{f}_L(\\mathbf{x}')\\|^2_2$ is the squared distance between the outputs of the two sample $\\mathbf{x}$ and $\\mathbf{x}'$, and $\\xi(r) \\ge 0$ is a weight function. For simple contrastive loss $\\xi(r) \\equiv 1$, for triplet loss $\\xi(r) \\equiv \\mathbb{I}(r \\le \\alpha)$ and for InfoNCE loss $\\xi(r) \\propto \\frac{1}{\\tau}e^{-r/\\tau}$. Note that for triplet loss and for InfoNCE, still some conditions are needed, which are weaker than the constant gradient condition in the current version of the paper. We will list these conditions in the revision. Intuitively, the triplet and InfoNCE loss are basically add different weights to focus on the pair of samples that are close under the current representation.  For simple contrastive loss, $\\xi(r) \\equiv 1$ reduces to $\\mathbb{V}[\\bar K_l(\\mathbf{x})]$. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "n-Ie5DmQWDQ", "original": null, "number": 23, "cdate": 1605762288354, "ddate": null, "tcdate": 1605762288354, "tmdate": 1605762333255, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "OY5UlRHh1WS", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your follow-up questions. ", "comment": "**A5 and A6**:  As recognized by other reviewers, the main point of this paper is to discover the covariance operator during the training of SSL (Sec. 3) for multiple loss functions, show that it is PSD for all training time (Sec 3),  study its properties under different generative models (Sec. 4), show how to use this covariance operator to describe the solutions found under training on those generative models (Sec. 4), and use it to analyze the mysterious property of BYOL, namely that it works negative pairs (Sec. 5 and new appendix in revised version), and perform experiments to test predictions of the theory (Sec. 6).  \n\nIn particular, we show in an illustrative example, the hierarchical latent tree model (HLTM), how this covariance operator leads to the amplification of features in intermediate layers of a multilayer ReLU network, that correspond to directly to latent variables at a corresponding layer of the HLTM. While the case we analyze is relatively simple (i.e. assuming the top-down Jacobian $J$ satisfies $J^\\intercal J = I$, which we will make it clear in the revised version), we provide a direct example of what R5 claims we do not do: namely we use the covariance operator in an example to show that important latent variables are amplified in the internal representations of a ReLU network, without chaos. Given the above, we strongly disagree with R5 that \u201cthere is no conclusion on SSL in our paper.\u201d Moreover, other reviewers agree with us and disagree with R5; 4 other reviewers all gave higher scores than R5. (R1: \u201cApproximating the data distribution with a hierarchical latent tree model is an interesting technique.\u201d).  \n\nWe believe the time-dependence of the covariance operator is interesting, and we hope that our work will inspire others to study this time dependence in many settings. Indeed addressing what R5 asks for  (e.g,. convergence proofs for SSL with general deep nonlinear networks, as well as a new as of yet completely undefined generalization of the notion of sample complexity from supervised learning to the SSL setting) would require years of work, and the joint efforts of the community. We feel demanding such results in an early paper on understanding SSL is not reasonable.  We hope our paper  can serve as a starting point towards such efforts. And we have already added some new results on properties of solutions found by BYOL in linear architectures, which will appear in the revised version. \n\nHowever, returning to R5\u2019s original Q5 that started this line of thought was a suggestion to take an NTK limit to get rid of the time dependence of the covariance operator.  We responded by saying this would lead to the relatively uninteresting limit of kernel PCA with an NTK kernel. Indeed we already showed in Section 4 that in a linear architecture, where the covariance operator is constant, we show that SIMCLR reduces to PCA within a subspace conserved by data-augmentation.  However, given R5\u2019s belief that the NTK limit would be an interesting direction, we are happy to work this out by the camera ready version; we believe it should be straightforward to do.  But for the revised version which will appear in a few days, we chose to focus on what we feel are more impactful directions, including clarifying the presentation, relaxing assumptions, and understanding BYOL without BN.  However, we will take R5\u2019s concern to heart and include a discussion of SSL in the NTK limit for the camera ready.  We again thank the reviewer for pointing out this direction. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "PtjEzFet3S", "original": null, "number": 22, "cdate": 1605740237574, "ddate": null, "tcdate": 1605740237574, "tmdate": 1605740237574, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "2rOSz7mrnA2", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your follow-up questions", "comment": "**Overall**: We do thank the reviewer for acknowledging that our paper contains \u201cseveral arguments\u201d and that \u201call of them are interesting.\u201d  However,  we find summary dismissals of our paper by R5 as \u201ccontaining no conclusions,\u201d  and \u201chas no main point\u201d to be overly harsh, and far outside the reasonable standards of review (as quantitatively evidenced by the fact that the reviewer\u2019s score is a significantly low outlier amongst 5 scores).\n\nWe hope we have argued convincingly above the many contributions of our paper, which we repeat here. Our fundamental goal is to understand what features are picked up by SSL in different data distributions without having to reference downstream tasks to assess performance. This is a fundamental unanswered question, so we focused on the simplest nontrivial cases.  We demonstrate a covariance operator governs SSL learning dynamics (Sec. 3) for multiple loss functions, show that it is PSD for all training time (Sec 3),  study its properties under different generative models, augmentation procedures, and architectures of increasing complexity (Sec. 4), show how to use this covariance operator to describe the solutions found under training on those generative models (Sec. 4), and use it to analyze the mysterious property of BYOL, namely that it works without negative pairs (Sec. 5 and new appendix in revised version dealing with the case without BatchNorm), and perform experiments to test predictions of the theory (Sec. 6).\n\nAs stated above, demanding convergence proofs for deep ReLU networks trained on complex generative models is an extremely high, and unreasonable bar for publication as this would be a years long effort. Moreover, focusing on concepts like sample complexity in an unsupervised setting also seems like an unreasonable demand given such issues are completely open in many supervised settings.  Finally, as noted above, [AKK+19] is a nice paper but has fundamentally different goals from ours, and is very complementary. Finally we disagree that there is any strict rule that a \u201ctheoretical paper should only focus on one point.\u201d\n\nFinally, we are puzzled by the final statement of the reviewer: \u201cif we can reach a consensus on what a theoretical paper should be like, I would raise my score.\u201d We are not sure what R5 means by this, but we suspect R5 may have an overly rigid view as to what a useful contribution to our conceptual understanding of deep learning in different settings can look like. R5 has often referred to some platonic ideal of a \u201ctheory\u201d paper that is limited to discussing a very specific set of concepts (convergence, sample complexity, etc.) using highly mathematical proofs that make very few assumptions (and in R5\u2019s initial comments these proofs must strictly use mathematical techniques that go beyond calculus). This view is emblematic of the perspective of a pure mathematician.\n\nWe (and evidently the other 4 reviewers) seem to have a more open minded view as to many more ways to obtain conceptual insights into an interdisciplinary field as complex as deep learning that rely on multiple methods, including direct calculations, instructive toy examples, reasonable and perhaps even restrictive assumptions that nevertheless mirror in abstract the structure of real world data, and intuition corroborated by a combination of calculations, proofs and experiments.\n\nWe encourage R5 for example to look at the references to a review article (https://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031119-050745)  that highlights how conceptual insights into the nature of deep learning can be derived by combining many different methods of inquiry, spanning simulations, toy models, and useful calculations, often generated by physicists and applied mathematicians, rather than pure mathematicians.  Indeed pure mathematicians may not agree that such papers are \u201ctheory\u201d papers, whatever that means. But the field as a whole cannot deny that the conceptual insights gained from this body of work are not useful and important (as quantitatively evidenced by citation counts).\n\nWe harp on this point only because R5 has stated \u201cif we can reach a consensus on what a theoretical paper should be like, I would raise my score.\u201d We encourage R5 to be more inclusive and open minded about what constitutes conceptual insights, especially in an emerging field like SSL in which \u201cterms of interest\u201d R5 is used to from other fields are either not useful or are currently far from within reach. And we encourage R5 to be not overly rigid about platonic notions of what constitutes a theory paper. With this open mindedness, the fact that R5 believes our paper contains \u201cseveral arguments\u201d and that \u201call of them are interesting.\u201d should be enough to raise his/her score outside the negative outlier regime amongst 5 reviews.  We hope R5 will like our revised version, which will be enhanced for clarity specific to the concerns of R5 and will contain new results about BYOL without BatchNorm."}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "NIyZXnF3aAf", "original": null, "number": 20, "cdate": 1605739136156, "ddate": null, "tcdate": 1605739136156, "tmdate": 1605739410334, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "bqBfLBiCd9", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your follow-up questions (1/2)", "comment": "**A13**: We are glad R5 now appreciates the motivation behind our HLTM. To mathematically elucidate what features in a data distribution SSL learns, we fundamentally need a model of the data distribution. We made several assumptions of increasing complexity, culminating in the HLTM, which again other reviewers liked (R1: \u201cApproximating the data distribution with a hierarchical latent tree model is an interesting technique\u201d).  Without any assumptions about the data generation process, we might end up with much weaker conclusions which could be more general but might not be useful in practice. This could be regarded as \u201cweakness\u201d but we argue this should be treated as strength since the assumption abstractly reflects aspects of the real-world data distribution (hierarchical structure of objects in different poses) and we care about performance in the real world.  Understanding SSL in even more realistic models of the data constitutes a years long research program, and we hope our work can inspire future progress along these directions. \n\n**A14**: We acknowledge Section 4.3 can be dense and difficult to understand. We are dealing with binary hierarchical generative models, and $v_j(z_\\mu) = \\mathbb{E}_z[f_j | z_\\mu]$ is the expected activation (Table 1), conditioned on the hidden value of $z_\\mu$. Since $z_\\mu$ is a binary variable, we only need to check $v_j(0)$ and $v_j(1)$. If they are far apart, then the node $j$ is strongly correlated with $z_\\mu$. That\u2019s the meaning behind the metric $|v_j^2(0) - v_j^2(1)|$. We take the square because the math expression is easy to compute. We are revising this section to clarify; the revision should appear in a few days. \n\n**A15**: Theorem 6 is a static result at initialization, without any SGD training or optimization. We don\u2019t need to find the lucky node explicitly. As long as the metric $|v_j^2(0) - v_j^2(1)|$ can grow during the SGD training, a good representation emerges during training that is correlated with the latent variable **without** its direct supervision, which is the point we want to show. We are not dealing with downstream tasks in this paper and leave it for future work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "V3tGoFgCRRc", "original": null, "number": 21, "cdate": 1605739400114, "ddate": null, "tcdate": 1605739400114, "tmdate": 1605739400114, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "bqBfLBiCd9", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your follow-up questions (2/2)", "comment": "Regarding the two papers we gave as examples, and which R5 brings up in response, our only reason for bringing them up was to give examples of where making assumptions about the data were useful.  We never intended to initiate a direct and detailed comparison between those papers and ours.  Their goals are fundamentally different from ours, and we believe both those papers and ours are all interesting and complementary. However, since R5 brings those papers up in a more detailed manner, we will respond in a more detailed fashion to emphasize contrasts. \n\n**[AKK+19]** (http://proceedings.mlr.press/v97/saunshi19a/saunshi19a.pdf): \n*Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, Nikunj Saunshi, A Theoretical Analysis of Contrastive Unsupervised Representation Learning, ICML 2019.*\n\nIn our paper, one of the many aspects is to attempt to open the blackbox of the neural network and try to check whether there is a correlation between the intermediate layer of the deep models and the intermediate latent variables. To achieve this level of understanding, we assumed a generative model with intermediate latent variables is natural (and is the only way we could think about). [AKK+19] focused on contrastive loss functions and not BYOL,  and didn\u2019t open the blackbox. In fact, they assumed an abstract function class which might not have anything to do with deep models. Therefore such a \u201ccomplicated\u201d assumption of data generation is not needed. While there are indeed fewer assumptions, it is important to be aware that we also pay the price that insight of what deep models is not revealed as well. Finally this paper focused specifically on downstream tasks, whereas we focus instead on what SSL learns in parametric models of data, without having to refer to downstream tasks.  Hence the papers are quite complementary. \n\nHere we feel that it is necessary to bring the discussion to a higher level. It is not always good to encourage theoretical papers to have very few (and general) assumptions. With very few assumptions, we might get a nice bound, but that bound can also be vacuous and is not relevant to the real problems. The best assumptions are those who connect closely with the real world and those that make or break the algorithm. We would argue that the assumptions we made about data augmentations are precisely like that.  \n\n**[ABG+13]** (https://arxiv.org/pdf/1310.6343.pdf)\n*Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma, Provable bounds for learning some deep representations, ICML 2014*\n\nAlso, we never intended to directly compare to [ABG+13], we only intended to cite it as nice example of assuming a generative model of data. There are many differences between this paper and ours:\n\n1. It uses layer-wise training which is different from SGD/GD we are considering. \n2. It also assumes a random generative model. \n3. More importantly, it assumes that the samples of latent variables at each intermediate layer are known (see Alg. 1 in Sec. 4, and paragraphs before Sec. 6 in https://arxiv.org/pdf/1310.6343.pdf), so that the edges between consecutive layers can be recovered via Graph Recovery algorithms. \n\nWe emphasize that all these are strong assumptions that are not that related to empirical practice in deep learning. The last assumption (3) is particularly strong. One of the main puzzles of the deep models is how back-propagation (i.e., training end-to-end) works and how the intermediate representation emerges **without** the direct supervision of the latent variables. In contrast, our paper made an initial attempt to answer these questions in SSL. Since the problems addressed by [ABG+13] and by our paper are completely different, the detailed comparison of the assumptions doesn\u2019t make too much sense.  But the idea that assumptions about data are required to get interesting theoretical results about the behavior of deep learning is common amongst all 3 papers.   \n\nFrom these two papers, we hope R5 can also understand how difficult it is to show mathematically rigorous results whose assumptions are aligned with empirical settings. It takes years of efforts and a joint effort between researchers working on theory and practice. We hope R5 appreciates our efforts and every small step matters. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "jNZ-jlEKImt", "original": null, "number": 19, "cdate": 1605738802751, "ddate": null, "tcdate": 1605738802751, "tmdate": 1605738802751, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "N6zaHY27Ds", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your follow-up questions. ", "comment": "**A7**:  It is not at all implausible to believe that the data augmentation procedures used in practice (e.g. crop, rotate, color distortion, skew, scaling, etc) do not alter class specific latents, especially variables that correspond directly to class labels. Indeed this is widely taken as common sense in the field of SSL practice, so we are surprised R5 continues to insist that this assumption is unreasonable, even if only limited to simple illustrative examples in Sec. 4 and not assumed in our general theoretical framework.  We encourage R5 to examine Figure 4 of the SIMCLR paper (https://arxiv.org/pdf/2002.05709.pdf) which shows 10 different augmented views of a dog.  Every single augmented view, including the color distortion, is clearly recognizable as a dog.  Thus augmentation procedures used in practice do not destroy or change the class label. Of course it is extremely difficult to formally prove this, because there is as of yet no formal mathematical expression for the presence or absence of a dog in a pixel image.  But we believe Fig. 4 (and many other figures like it in the literature) conclusively demonstrate that our model of data-augmentation as modifying nuisance features but preserving at least class specific latents is not at all unreasonable and aligns well with practice.\n\nOur theory suggests that augmentation procedures that modify class specific latents should only impair performance.  Again, the covariance operator, which, interestingly, R5 continually criticizes as an insignificant contribution, plays a key role in this argument.  The covariance operator is obtained by first averaging all the augmented views of each data point.  Then the covariance of these augmentation averaged data points is computed, and this operator drives learning at any given time.  If the augmentation transforms latents specific to one class, to that of another class, then these two latents will be averaged together, before the variability in the data is computed.  This averaging process will render it more difficult to distinguish the two classes. We hope this example use of the covariance operator provides another illustration of its usefulness in reasoning about SSL.  \n\nFinally, we note that R5 makes a strong assertion that our assumptions about data augmentation (again used for illustrative purposes only in Sec. 4) \u201creally rules out some scenarios of interests. I don't think introducing such assumption is necessary.\u201d  However R5 provides no logical argument for why some form of this assumption is unnecessary, nor provides a concrete example of when all class specific latents are perturbed by data augmentation yet SSL can still work.  We in contrast provide a covariance operator based argument that SSL is unlikely to work in such a setting. However if R5 has a concrete example in mind in which all class specific latents are changed by data augmentation but SSL still works, we are happy to discuss it further.  We do not view intermediate cases in which some class specific latents are perturbed at at least some others are preserved as qualitatively distinct from our assumption, and for illustrative purposes we believe it is more instructive to focus on clear examples where class specific latents are preserved. \n\n**A8**: We again apologize for confusions we may have generated. We will make this clearer in the revised version to appear in a few days. \n\n**A9**: As we have said, Sec. 4 are already illustrative examples used to show how the representation is learned during training, by checking the explicit form of covariance operator under different generative models. They cover different cases and some might discuss the property of ReLU networks, while others don\u2019t, as a contrast.  This paper is about the covariance operator and all cases in Sec. 4 are basically examples demonstrating its explicit form and its roles in different scenarios, and it is natural to have ReLU activation in some of the examples.  We also provide an example in which a ReLU unit can learn but a linear unit cannot. \n\n**A10**: We don't understand \u201cmake the gradient confusion\u201d mentioned by R5. In the revised appendix we provide an example of BYOL with multiple outputs.\n\n**A11**: What part is not rigorous? Please show us. The assumption is used to get a concise weight dynamics and we will show more general cases in the next revision as well as clarify the proof.\n\n**A12**: We never say $A_j$ is constant in the ReLU case. As we have mentioned in the original version of the paper (Page 5, last paragraph. we repeat the exact sentences here for clarity): \u201cNote that if we consider ReLU neurons, $A_j$ changes with $\\mathbf{w}_{1,j}$; while for linear nodes, $A_j$ is a constant, since the gating $\\mathbb{I}(\\mathbf{w}^\\intercal_{1,j} \\mathbf{x} > 0)$ is always $1$\u201d. The hidden layer nodes can be either linear (where the gating is always 1) or has ReLU activation (where the gating changes over time). Here we discuss two situations at the same time. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "2rOSz7mrnA2", "original": null, "number": 17, "cdate": 1605507220695, "ddate": null, "tcdate": 1605507220695, "tmdate": 1605507241253, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "HUGFyjXhpzu", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your reply. I would like to re-emphasize my idea.", "comment": "A17. Thanks for pointing out and I obfuscate the mechanism of batch normalization during the training and testing phase. Sorry for that.\n\nOverall: I would still like to say, if we view this paper as a paper discussing rigorous theoretical properties on SSL, then I cannot find a main point the authors want to argue theoretically and argue rigorously. Probably I have some misunderstanding on part of the arguments, but I cannot see how this paper benefits our understanding on the terms of interests on SSL, e.g. convergence, properties of solutions, sample complexity, etc. This paper indeed contains several arguments, all of them are interesting, but none of this arguments give a clear conclusion on any terms of interests under any kinds of assumptions. In contrast, most theoretical paper would focus on only one points. For example, [AKK+19] argued how SSL will give a provable guarantee on the downstream tasks throughout the paper, with different kinds of assumptions and algorithms. That's the main reason I think this paper is not ready for publication. Also, a theoretical paper should be concise and clear. I'm happy to discuss. And if we can reach a consensus on what a theoretical paper should be like, I would raise my score."}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "bqBfLBiCd9", "original": null, "number": 16, "cdate": 1605505711330, "ddate": null, "tcdate": 1605505711330, "tmdate": 1605505711330, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "Ru2HX_MWT9M", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your reply. I would like to re-emphasize my idea.  ", "comment": "A13. I see the motivation now, but will the learning of neural networks follow this kinds of human-level understanding, especially when we don't know the knowledge of the generative model? From the description I feel the network can only be built with the known generative model.\n\nA14. I would like to say, even the process and network structure are well-known, it's not proper to include only a figure as the description of these terms of interests in a theory paper. It's not concise, and will lead to confusion. The most important thing for a theory paper is accurate, right?\n\nA15. I need to say, due to the lack of necessary explanations of the notations, I may not understand the whole picture in Section 4.3 correctly. I would like to ask what is $v_j(1)$ and $v_j(0)$, and what will their difference measure. I also have questions on Theorem 6:\n(1) Is this a static result, i.e. does not include the optimization? That is to say, even we have lucky nodes, how can we provably find these lucky nodes?\n(2) If we only know the lucky nodes exist, and don't know how to find the lucky nodes, then will the downstream task be influenced by the unlucky nodes? Can we give provable results on the risks of downstream tasks?\n\nAlso, in [AKK+19] there is no generative model assumption and in [ABG+13] the generative model is not as complicate as the authors'. I would like to say, the conclusion in their papers are directly related to the terms of interests. For example [AKK+19] gives the performance of learned representation on downstream tasks, while this paper does not have such results."}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "OY5UlRHh1WS", "original": null, "number": 14, "cdate": 1605502219122, "ddate": null, "tcdate": 1605502219122, "tmdate": 1605503839448, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "ap-YccE5Z4i", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your reply. I would like to re-emphasize my idea.", "comment": "A5. I'm not convinced by these arguments. If the covariance operators change over time, then how do we know what happens during the gradient flow? As the authors does not characterize the solutions we get, how can the authors rule out the case the covariance operator will amplify the signal from different directions at different steps, and thus lead to a chaos? I agree with the arguments that this covariance operator can be useful, but this is not enough. How to use this operator? In other words, what's the main argument of this paper? Introducing such covariance operator does not give any interesting results on any terms of interests.\n\nA6. I would just want to say that what we really want is the conclusion on some terms of interests, e.g. there are existing work on deep ReLU network for separable data that proves to converge to the max-margin solution, which may help us know what deep ReLU work do for separable data. This paper does not provide any kinds of similar results, e.g. what is the SSL loss doing with deep ReLU network? Will SSL loss with deep ReLU network converge to some specific solution that have desirable property? How fast will it converge? How many samples are we needed for find good solutions? I think these are the questions we want to ask about SSL. We may not be restricted to separable data, say if we have a Gaussian mixture model with different SNR, what will SSL do and how will the performance vary with different SNR? That's also interesting. What I would like to say is, there's no clear conclusion on SSL in this paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "N6zaHY27Ds", "original": null, "number": 15, "cdate": 1605503756006, "ddate": null, "tcdate": 1605503756006, "tmdate": 1605503756006, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "vXGGV5KxPFE", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your reply. I would like to re-emphasize my idea.", "comment": "A7. I'm not convinced by the authors' arguments. Here's what I want to ask:\n(1). Can we identify the two components in practice? In other words, how can we demonstrate that the augmentation, e.g. random color distortion, does not change the class/sample-specific latent? Considering we want to ask the texture of some objects, which may be determined by the color, will this change the class/sample-specific latent?\n(2). Will such assumption rule out some scenarios of interests, for example, I mentioned in the review? It's hard to imagine such augmentation that perturb both the class/sample-specific latent and nuisance latent will not work in SSL.\nOverall, my point is the authors introduce this assumption without further validation and demonstration. Meanwhile, although it could be a possible mechanisms on how SSL works, it really rules out some scenarios of interests. I don't think introducing such assumption is necessary. And if the authors want to introduce this assumption, please give more formal description and give strong positive and negative results on this assumption.\n\nA8. I would like the authors make a revision on this part. It's pretty hard to understand.\n\nA9. I'm happy to see illustrative examples in the paper. The main question is, what does the illustrative examples want to show? If this is a paper show the necessarily of the ReLU activation, I think such example is proper. But turns out this is not a paper about how ReLU activation works in SSL, right?\n\nA10. It's interesting to me if multiple output will make the gradient confusion. Can the authors make more discussion on that?\n\nA11. I would like to say that, for a theoretical paper, we need to make every part concise. I prefer every part need to be rigorous is presenting in a formal way, as I said in the 9th question. I don't think make such assumption on the condition convincing. But hope the authors can give more convincing results in the next revision. \n\nA12. I make a mistake on that. I mean in the last of Page 5 rather than Page 7. The issue is why the gate is always $1$, as during the optimization the parameters are changing over time, and at some point it can change from $1$ to $0$, can we always rule out this possibility? What's the meaning of linear nodes, we are talking about ReLU network right? I'm totally lost on that."}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "X60tOgCOLI", "original": null, "number": 13, "cdate": 1605501667937, "ddate": null, "tcdate": 1605501667937, "tmdate": 1605501667937, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "NPL6mMoaz3q", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Thanks for your reply. I would like to re-emphasize my idea.", "comment": "A1: I partially agree with the authors' idea. There are several existing theorems only derived by calculation. But these theorems $\\textbf{state the results on the terms of interest}$, other than give a calculation results of the gradients. The terms of interests can be sample complexity, convergence results, the properties of solutions etc. And calculating the gradient is only one step towards these results on the terms of interests, which we typically call a Lemma (and not theoretical results and contributions).\n\nA2. I didn't notice the results without normalization in BYOL. Sorry for that. But I would like to see the results with normalization. Typically with normalization we need to additionally track the norm of the output, which can be pretty hard. I will not be surprised by there's a similar covariance term in the gradient, but it should make some of the differences for the optimization results, at least the empirical results show without normalization there will be huge performance drop. I would also like to see the differences between the methods w/ and w/o the normalization.\n\nA3. In theoretical analysis we tend to make at least assumption as possible, especially when the assumption can potentially influence the results. For me a more realistic assumption is preferred."}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "LciitZOayd2", "original": null, "number": 7, "cdate": 1605314426183, "ddate": null, "tcdate": 1605314426183, "tmdate": 1605316825494, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Author Response on Common issues (Part 1/2)", "comment": "We thank all our  reviewers for their insightful comments. Here we answer common questions that arose. We also respond specifically to each reviewer about questions that arose specific to each reviewer. \n\n**Q**: Does the recent paper [1] \u201cBYOL works even without batch statistics\u201d invalidate this submission / change its conclusion substantially [R1, R3]?\n\n**A**: We note that [1] was  released on Oct. 20, after the ICLR deadline (Oct. 2), which is the reason why we didn\u2019t cite it or analyze it. Second, the main contribution of our analysis is in the setting of SimCLR, revealing that a time dependent positive definite covariance operator (that depends critically on the data distribution, augmentation distribution, and network architecture) governs SIMCLR learning dynamics.  Thus [1] has no impact on these contributions.  \n\nThe only relevant section in relation to [1] is, Sec. 5, which gives approximate conditions under which BYOL will behave like SimCLR, in that its learning dynamics will also be approximately governed by a positive definite covariance operator as well. Basically we show that when BN is included and when the extra predictor has relative small weights, then a positive definite covariance operator arises for BYOL.   Thus under these conditions, our analysis shows BYOL will work whenever SimCLR works.  However, our conditions are only sufficient and not necessary, in the sense that they do not rule out other possibilities under which BYOL could work,  e.g. by  using Weight Standardization and Group Norm without cross sample statistics as in [1]. \n\nWe acknowledge that our wording (e.g., \u201cBN plays a critical role in BYOL\u201d) in Sec. 5 can be misleading and we will tone down this wording and clarify our discussion to emphasize the consistency with [1].  Namely, this section will be renamed to something like BYOL behaves like SimCLR with BN.  And in the section we will explicitly state that we do not claim that BN is required for BYOL to work, citing the new Ref. [1].  Thus both of these works will be entirely consistent with each other. \n\nFurthermore, within our theoretical framework, we are currently working on theoretical explanations for [1]. The intuition here is that if $E_x[\\bar K(x)] = 0$, then BYOL again reduces to approximate SimCLR no matter whether BN is present or not. And WS+GN might achieve this sufficient condition if some assumption holds. We think R1\u2019s guess about \u201cGroupNorm + weight standardization also provide implicit contrastive objective, similar as BN\u201d is on the right track. \n\n**Q**: Can the analysis extend to other kinds of loss, in addition to InfoNCE (e.g., triple loss)? \n\n**A**: Note that this condition holds exactly for simple contrastive loss like $r_+ - r_-$. Triple loss (https://arxiv.org/abs/1503.03832, https://en.wikipedia.org/wiki/Triplet_loss ) is $\\max(r_+ - r_- + \\alpha, 0)$, which is equivalent to $(r_+ - r_-) * Indicator(r_+ - r_- + \\alpha > 0)$ when taking gradients. So it naturally satisfies all our assumptions (including Theorem 2 and our assumption $\\partial L / \\partial r_- < 0$ and is a constant in Theorem 3). Therefore, Theorem 3 can be applied. The role of the indicator is to change the data distribution dynamically so that the training naturally focuses on the data pairs that are not well trained (i.e., $r_+$ is large compared to $r_-$). \n\n**Q**: Is the condition $\\partial l / \\partial r_{k-} = -\\beta / H$ too strong or unrealistic [R1, R2, R4, R5]:\n\n**A**: As mentioned above, the condition holds true for triple loss and simple contrastive loss. It is an approximate for contrastive loss, used to simplify our technical analysis. Furthermore, we can relax the assumption by assuming\n\n$$\\frac{\\partial l}{\\partial r_{k-}} = - \\sum_j h_j(x)h_j(x_k\u2019)q_j(x_+,x)q_j(x_1,x)q_j(x_{k\u2212},x'_{k})$$\n \nwhere $h_j > 0$ and $q_j > 0$ are arbitrary positive functions (including constant functions), $x$ is un-augmented datapoint used to generate $x_1$ and $x_+$, and $x_k$ used to generate $x_{k-}$. This relaxed assumption leads to a sum of weighted versions of covariance operators and our conclusion still follows. We will put this extension in the next revision. \n\nNote that since we know the sufficient condition that the covariance operator appears, we might find other new loss functions that follow Theorem 2, which we leave for future work. \n\n(to be continued)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "sfuhjTRrxeE", "original": null, "number": 9, "cdate": 1605315649349, "ddate": null, "tcdate": 1605315649349, "tmdate": 1605316719401, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "0qb0RI4aybE", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Author Response to R4", "comment": "We thank R4 for the review. We address common questions above and and address specific questions raised here: \n\n**Q1**: The paper should explain the motivation to choose simCLR for analysis. \n\n**A1**: We choose SimCLR because it achieves close to SoTA performance in SSL, has attracted a great deal of attention in the field, and it has a very clean architecture and fits well with the existing teacher-student theoretical framework. We will edit the paper to clarify this motivation. \n\n**Q2**: The experiment setting should be listed. Such as the learning rate, the training time and the date augmentation used in the experiments. \n\n**A2**: We will list all of these experimental details in the next revision and will open source the code used for experiments.  Thank you for pointing this out. \n\n**Q3**: The paper also needs experiments about the magnitude of the covariance operator and the training time. \n\n**A3**: We will put the relevant experiments in the next revision. In HLTM, we actually compute the covariance operator explicitly and see its norm indeed goes up during training. Moreover, as predicted by the theory, the top-level covariance operator goes up faster and drives the low-level training. \n\n**Q4**: The paper analyzes the activation gap and the weights of different layer for HLTM. The paper needs experiments about the activation gap growing over time and the weight of the top layer and lower layer changing over time.\n\n**A4**: The activation gap indeed grows over time, as demonstrated indirectly in Table 2 where the normalized correlation between the activation of a node and the ground truth latent variable goes up. We will add top-layer/lower-layer experiments in the next revision (we assume R4 refers to the two layer case in Sec. 4.2). \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "6oFZtGQXvPF", "original": null, "number": 12, "cdate": 1605316615768, "ddate": null, "tcdate": 1605316615768, "tmdate": 1605316615768, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "GBbGQ7CYnyJ", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Author Response to R3", "comment": "We thank R3 for the review and appreciate the positive response.\n\nR3 raises two main questions and we answer them below:\n\n**Q1**: it has been proved that BYOL can work without BN in predictor, which seems contradictory the conclusion given in this paper. I recommend the author to compare with this conclusion.\n\n**A1**: Our theoretical analysis that BYOL has implicit contrastive terms when one includes BatchNorm does not contradict the result of the cited reference [1] of R3. We will edit the relevant section to make this clear.  Please see also a detailed response to this point in the \u201cCommon questions\u201d section of the rebuttal. \n\n**Q2**: In Table 5, it seems that reinitializing the predictor will improve the performance a bit. I wonder if the improvements will gradually decrease or even vanish for longer training epochs.\u201d \n\n**A2**: Yes. These performance improvement will gradually decrease/vanish if the training lasts for longer epochs. That\u2019s why we don\u2019t claim that we have a better algorithm than BYOL. Here the the main point is that restarting the predictor during training, doesn\u2019t seem to hurt the performance. We will clarify this further in the next revised version. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "TQfLflPFAVB", "original": null, "number": 11, "cdate": 1605316416114, "ddate": null, "tcdate": 1605316416114, "tmdate": 1605316438613, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "avHyr_g10QK", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Author Response to R1", "comment": "We thank R1 for reviewing our paper and we appreciate the positive enthusiasm.  \n\nWe address the three main points raised here: \n\n**A1**: Please check the \"Common Question\" section above. We do not believe we are in contradiction with the authors of BYOL, and we will edit the paper accordingly to clarify.  In short, our main result is that BYOL behaves like SimCLR when BatchNorm is introduced in that a positive definite covariance operator governs the learning dynamics. Thus BYOL with BN should succeed whenever SimCLR succeeds. We do not wish to claim this is a necessary condition and we will edit this section to reflect that, thereby being consistent with the empirics that BYOL can succeed without BN. \n\n**A2**: Please see the answer to this question in the \u201cCommon Questions\u201d section. \n\n**A3**: Thank you - we will fix that typo. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "sRBWtRivqrY", "original": null, "number": 10, "cdate": 1605316132349, "ddate": null, "tcdate": 1605316132349, "tmdate": 1605316132349, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "iFGgvXfqfS9", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Author Response to R2", "comment": "We thank R2 for reviewing our paper, and appreciate the positive feedback. We address common questions above and and address specific questions raised here:\n\n**Q1**: Below equation (37) in the appendix, the above assumption becomes derivative of L with respect to rk-^2, and H is replaced with n in the constant. It seems that there is some discrepancy with the statement of Theorem 3, and some explanation may be helpful.\n\n**A1**: Thanks for pointing that out. There is a typo in the proof. The letter $n$ should be just $H$.\n \n**Q2**: In the first paragraph of Section 4, it is stated that \"all nuisance latents z' are integrated out in K(z_0)\". This statement seems to be a little vague to me. More rigorous definitions and derivations may be needed here to support the claim.\n\n**A2**: In Section 4, the input x is a function of two kinds of latent variables $z_0$ and $z\u2019$. So we can write $K = K(x) = K(z_0, z\u2019)$. Therefore, we can write \n\n$$\\bar K(z_0) = E_{z'|z_0}[K(z_0,z')] = \\int_{z\u2019} K(z_0, z\u2019) p(z\u2019|z_0) \\mathrm{d} z\u2019$$\n\nThis is what we mean by \u201cintegrated out\u201d. We will make it more clear in the next revision. \n\n**Q3**: How would the augmentation distribution impact the analysis? What would be a good augmentation transform?\n\n**A3**: Section 4 gives a few examples on how different generation processes lead to different covariance operators. We leave a comparable study of different data augmentations using this framework for future work.  But the basic intuition that already arises from our analysis is that data augmentation should scramble or randomize irrelevant / nuisance features, while preserving semantically important latent features that may discriminate between different classes in downstream classification tasks.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "z9D0vBQQ19C", "original": null, "number": 8, "cdate": 1605314511498, "ddate": null, "tcdate": 1605314511498, "tmdate": 1605314530321, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Author Response on Common issues (Part 2/2) ", "comment": "**Q**: Are some assumptions made in the paper unrealistic [R5]?: \n\n**A**: Compared to existing literature on theoretical understanding of DL, our framework doesn\u2019t make strong assumptions throughout the analysis. There is no simple parametric assumption for the data distribution (e.g,. Gaussian, mixture of Gaussian, linear separable, generated by a fixed teacher network) and no strong assumption on the size of the network (e.g., infinite input dimension/width/depth). We study ReLU activation that is extensively used in the empirical work, analyze SimCLR/BYOL that are very recent SoTA empirical methods in SSL, use standard training methods like GD/SGD, and focus on loss functions that are extensively used. The covariance operator exists throughout the training rather than just at the initialization (which is the main focus on many theory papers).  \n\nFor generative models and data augmentation, we made two basic assumptions:\n\n**First**: we assumed the data is generated through a combination of class specific latents and a nuisance latents.  \n**Second**: we assumed the augmentation procedure perturbed the nuisance latents while preserving the class latents.\n\nFirst note that these two assumptions are not necessary for the general theory at all.  We only use them in our illustrative examples to illustrate the general theory in specific cases.  However, we do believe these two assumptions are closely related to what occurs in empirical practice. \n\nFirst, many empirical SSL works have already shown that data augmentation is critical for the performance of SSL methods. E.g., SimCLR shows with different sets of data augmentation, the performance is very different (Fig. 5 in https://arxiv.org/pdf/2002.05709.pdf). Conversely, the recent technical report (https://arxiv.org/pdf/2011.02803.pdf) shows that simply adding semantically or class-specific invariant features in data augmentation immediately kills SSL performance, regardless of the loss function. Therefore, in order to understand why SSL works, we must model the process of data augmentation, and we believe the success of existing data augmentation procedures likes in their ability to scramble irrelevant nuisance features, while preserving semantically important class distinctive features.\n\nR5 questions whether the second assumption is realistic. Here is one example: random color distortions are an excellent way to scramble many low level, likely nuisance features, while preserving high level semantically important features, like shape.  For example, many humans could recognize the class identity of color distorted objects, like animals (e.g. a color distorted horse still is recognizable as a horse).  Thus color distortion provides a clear example of what the reviewer claims is impossible to realize in practice. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "NPL6mMoaz3q", "original": null, "number": 2, "cdate": 1605132958348, "ddate": null, "tcdate": 1605132958348, "tmdate": 1605221554630, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "gHbY_xI60M7", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Rebuttal for R5 (Part 1/5)", "comment": "We thank R5 for all the comments. We do note that this is the most negative outlier review compared to all other reviews, and some subjective opinions stated in this review are in disagreement with very positive opinions stated by other reviewers (i.e. \u201cThe paper is well written, with rigorous mathematical derivations\u201d (R2), \u201cApproximating the data distribution with a hierarchical latent tree model is an interesting technique\u201d (R1), \u201cThe finding is interesting and novel\u201d (R4).\u201d  However, we have taken this negative outlier review to heart and we can address the many questions raised to further improve this paper.  Answers to every question are labelled **A1** through **A18**, in one to one correspondence with the questions **Q1** through **Q18** raised by the reviewer. \n\n**A1**: We strongly disagree that a theorem strictly requires techniques beyond calculus to be called a theorem. Indeed, many mathematical theorems are proven by a pure calculation (e.g., Cauchy's integral theorem forms the fundamental basis in complex analysis.) As long as the sequence of calculational steps is rigorously justified, the final outcome can be called a theorem, or fundamentally a mathematical statement or result that is true.  We collected all of our results that we thought of as having special significance as important intermediate steps, and we stated them as \u201cTheorems\u201d simply to highlight those particular intermediate results, for the ease of the reader.  For example, the fact that in SimCLR, the learning dynamics at any instant of time is governed by a positive semidefinite covariance operator is a non-trivial and previously completely unrecognized property and so we highlighted it in the form of Theorem 3. Other reviewers agreed this was an interesting result.  \n\n**A2**: For empirical evidence, note that BYOL paper already shows that without L2 normalization it still works (See Table 20 in v3 of https://arxiv.org/abs/2006.07733), despite the fact that the activation might blow up during training. We omit the explanation why we choose l2 loss without normalization due to space limitation, and will add it back in the next revision. Besides, we will also provide additional detailed analysis when l2 normalization is present in the next revision. It almost fits into our theoretical framework but with some caveats, and the covariance operator still exists with a slightly different definition.  \n\n**A3**: Note that this assumption holds exactly for simple contrastive loss like $r_+ - r_-$, and is present here largely to simplify our technical analysis. We have already relaxed the assumption by assuming\n \n$$\\frac{\\partial l}{\\partial r_{k-}} = - \\sum_j h_j(x)h_j(x_k\u2019)q_j(x_+,x)q_j(x_1,x)q_j(x_{k\u2212},x'_{k})$$\n \nwhere $h_j > 0$ and $q_j > 0$ are arbitrary positive functions (including constant functions), $x$ is un-augmented datapoint used to generate $x_1$ and $x_+$, and $x_k$ used to generate $x_{k-}$. This relaxed assumption leads to a sum of weighted version of covariance operator and our conclusion still follows. We will put this extension in the next revision. \n\n**A4**: We will fix this omission and define beta when we introduce it. \n\n(to be continued)"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "HUGFyjXhpzu", "original": null, "number": 6, "cdate": 1605134054551, "ddate": null, "tcdate": 1605134054551, "tmdate": 1605134421431, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "gHbY_xI60M7", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Rebuttal for R5 (Part 5/5)", "comment": "**A16**: \u201csym\u201d means that W=W\u2019 (the weight of the online network is the same as that of the target network), defined right before Eqn. 10.  deltaW_l^{BN} is defined right before Eqn. 13, which means the difference between gradient before and after adding BN. \n\n**A17**: We believe R5 has a misunderstanding about the details of BatchNorm. \n\n1. In BatchNorm, the sample mean is back-propagated. R5 can check the original BatchNorm paper https://arxiv.org/pdf/1502.03167.pdf, page 4, in equations right before Sec. 3.1, the expression of $\\partial l / \\partial x_i$ contains both $\\partial l / \\partial \\mu_B$ and $\\partial l / \\partial \\sigma_B$, and thus the gradient with respect to x will backpropagate through both sample mean and standard deviation. Therefore, in terms of zero-mean operations, BN uses \u201cx - x.mean()\u201d rather than \u201cx-x.mean().detach()\u201d as suggested by R5. \n\n2. During training, the mean subtracted by BN is NOT moving average statistics, but precisely the sample mean in the minibatch. R5 could check Alg. 1 block in the original BN paper on page 3. Also In PyTorch implementation, we can clearly see that during training it uses batch statistics rather than running mean/std: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Normalization.cpp#L314-L320\n\nOur analysis on Sec. 5 gives a \u201csufficient condition\u201d under which BYOL will behave like SimCLR, and thus can work. While Theorem 7 is rigorous, in its current version, the remaining statement (at the end of Page 7) is a rough reasoning. We will turn it into a rigorous theorem in the next revision. \n\nBeing a \u201csufficient reasoning\u201d, our argument on BYOL does not rule out other possibilities that enable BYOL to work without BN, e.g, using Weight Standardization and Group Norm without cross sample statistics.  We will rewrite the main message of this section to be that with BN, BYOL behaves like SimCLR in that a PSD covariance operator governs the dynamics. \n\n**A18 and summary**:  We are not sure how to respond to the claim that our presentation is \u201cweird\u201d  especially when other reviewers enjoyed the paper (e.g. R2 says \u201cThe paper is well written, with rigorous mathematical derivations\u201d and \u201cSimulation experiments were provided to justify the theoretical findings.\u201d).   Regarding presentation, R5 focuses on distinctions between observations, intuitions, rigorous theory and empirics.  We believe all four approaches constitute useful avenues for advancing our conceptual understanding and we freely engage in all four approaches, including the use of instructive examples, and making clear when we are making rigorous claims by codifying them in Theorems, and putting intuition outside of these theorems (again we disagree that true mathematical statements derivable from calculus cannot be labelled theorems). \n\nWe hope that our explanations in this comment furthermore justify why we made some of the assumptions we made in our illustrative examples, why they are natural and relevant to practice, and why some other assumptions can be relaxed.  We have taken R5 complaints about lack of clarity to heart also and we will add substantially expanded descriptions of various aspects of this paper in the appendix, especially the HLTM and the motivation behind our illustrative examples, in order to ensure that the confusions that occurred for R5 do not occur for future readers. \n\nOverall, we thank very much R5 for the many comments, as they will significantly help us improve our paper. Given R5 is an outlier negative reviewer, amongst 4 other more positive reviewers, we sincerely hope Rev.5 will consider our comments seriously and consider improving his/her score if he/she finds our comments satisfactory. If not we are very happy to engage further with any remaining concerns. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "Ru2HX_MWT9M", "original": null, "number": 5, "cdate": 1605133837745, "ddate": null, "tcdate": 1605133837745, "tmdate": 1605134414077, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "gHbY_xI60M7", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Rebuttal for R5 (Part 4/5)", "comment": "**A13**: The motivation behind the HLTM model is to have a hierarchical generative process for generating data at multiple levels of hierarchy.  Intuitively, this captures the idea that different objects (top level of hierarchy) are made of parts (lower level) that are themselves made of smaller subparts (even lower level).  The abstract feature vectors we use at different levels could for example reflect 1 hot vectors for different objects (top level), and k-hot vectors for the presence or absence of k parts in a list of parts, conditional on object identity (lower level).    These features could combine with nuisance features (e.g. poses or relative orientations of subparts) to generate pictures of objects, for example.  This was the underlying motivation for the hierarchical tree model.  We believe this picture is very general.   \n\nOf course the actual distributions we chose for the tree were quite simple.  In this simple setting, we wanted to address a fundamental question: when do deep neural networks with multiple layers learn neurons at intermediate layers whose outputs across stimuli correlate with intermediate layers of the generative model  (i.e. can SSL learn part detectors in intermediate layers, while only having access to low level representations of objects in the input layers).  We believe this is a fundamental question, and the HLTM provides the simplest nontrivial setting in which this question can be settled. To the best of our knowledge, no prior work has done such an analysis in SSL settings. \n\nWe note that R1 commented that \u201cApproximating the data distribution with a hierarchical latent tree model is an interesting technique.\u201d  We apologize that our motivation was not clear. We will give an expanded description of the HLTM in the appendix. \n\nBut for now, just for clarity, to more specifically connect to our notation, we let  z_0 represent label of different objects (e.g., cat, human, computer, etc), and z\u2019 to be variables that determine the location and the size of each object part (e.g., the location of a head, the limbs if the input image contains a human, etc). As illustrated in Fig. 2(d) and mentioned in the first paragraph of Sec 4.3, when \u201cgenerating\u201d one object, we first generate its object category, then generate the location z\u2019 of its parts given the object label, and generate the subparts (also z\u2019), until the image is generated as x, which serves as the input of the neural network. In this case, data augmentation on the input image x naturally changes z\u2019 but not z_0 (e.g., a rotation/scaling on the image changes the location/size of each object part, but not the object identity). We consider locally connected rather than FC settings because the analysis is easier.\n\n**A14**: While the description of the HLTM and generative model is logically complete and self-contained (and other reviewers appreciated it), we do sympathize with this reviewer that the description of the HLTM and the neural architecture for performing SSL is quite terse.  We will provide a greatly expanded description of both in the appendix. But at a high level, the HLTM is a branching diffusion process (a well known process) and the neural network is simply a multilayer neural network (again well known).\n\n**A15**: In contrast to R5\u2019s criticism, Theorems in Sec 4.3 indeed give hints on the quality and the meaning of the intermediate representation. Theorem 6 shows that even without training, from an over-parameterized deep network, one can find intermediate nodes that are correlated with the latent variables at the same corresponding layer (and the correlation can be lower bounded). We also explain why this correlation could increase over training. As we mentioned, all statements not in the theorems are not intended to be rigorous but instead intended to give valuable intuition as to how things work. We leave more detailed analysis (e.g., sample complexity and convergence analysis) to the future work.  \n\nFinally, we note that the definition of \u201cquality of the representation\u201d is somewhat up for grabs.  We have defined it to be whether or not the neural network can learn features that correlate with the intermediate latent variables in the hierarchical latent tree model.  There is precedent for this idea in general - where data is generated by important latent variables, and it is asked whether or not neural network training can recover these latent variables. See e.g. https://arxiv.org/pdf/1310.6343.pdf  and https://arxiv.org/abs/1902.09229 . In the paper it is assumed that downstream classification tasks of interest have class labels that depend only on important latent variables, so an important goal of SSL then involves learning explicit representations that are linearly related to the latent variables.   \n\n(to be continued)"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "vXGGV5KxPFE", "original": null, "number": 4, "cdate": 1605133644435, "ddate": null, "tcdate": 1605133644435, "tmdate": 1605134406765, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "gHbY_xI60M7", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Rebuttal for R5 (Part 3/5) ", "comment": "**A7**: We first note that Section 4 is meant to simply illustrate our general theory of SSL in specific settings, to explain when it works and when it fails.  To create such illustrative examples, we made two specific assumptions about the data generation and augmentation process:\n\n**First**: we assumed the data is generated through a combination of class specific latents and a nuisance latents.  \n**Second**: we assumed the augmentation procedure perturbed the nuisance latents while preserving the class latents.  \n\nThese assumptions are not required for our general theory, and are only properties of our specific examples.  However, we strongly disagree that these assumptions are unrealistic and difficult to realize in practice.  Indeed we argue that these assumptions are likely required for SSL to succeed in practice, and therefore exemplify properties of both data and augmentation procedures used in practice when SSL succeeds.  \n\nFor example, many empirical SSL works have already shown that data augmentation is critical for the performance of SSL methods. E.g., SimCLR shows using a correct set of data augmentation is critical for its performance (Fig. 5 in https://arxiv.org/pdf/2002.05709.pdf). Conversely, the recent technical report (https://arxiv.org/pdf/2011.02803.pdf) shows that simply adding semantically or class-specific invariant features in data augmentation immediately kills SSL performance, regardless of the loss function. Both suggest that to understand why SSL works, imposing the right assumption of data augmentation is important, while adding a simple isotropic Gaussian won\u2019t get SSL working.  \n\nR5 questions whether it is possible to only perturb the nuisance latent variable. Here is one example: random color distortions are an excellent way to scramble many low level, likely nuisance features, while preserving high level semantically important features, like shape.  For example, many humans could recognize the class identity of color distorted objects, like animals (e.g. a color distorted horse still is recognizable as a horse).  Thus color distortion provides a clear example of what the reviewer claims is impossible to realize in practice. \n\nTherefore, we believe our assumptions, for illustrative purposes only, that augmentation specifically changes nuisance latents, but not class latents, is highly realistic, and reflects what implicitly occurs in practice whenever SSL succeeds. \n\n**A8**: We apologize for the confusion here. Integrating out z^\\prime is physics language.  It simply refers to the augmentation averaged connection, averaging over z^\\prime, but conditioning on z_0. See the definition of augmentation averaged connection in Theorem 3.  In Sec. 4, the input x = x(z_0,z\u2019) (first paragraph), so we could write bar{K}_l(z_0) after averaging over  z\u2019, which is the only variable changed under data augmentation. As shown in Sec. 4, for different latent variable distributions, we will have different closed forms (or no closed form) for bar{K}_l(z_0). \n\n**A9**:  The entire section 4 is a set of worked examples that demonstrate what the covariance operator looks like under different generative models of data, augmentation procedures and different network architectures, and how the patterns in the dataset are learned in SSL. In the single neuron case and generative models with 1D translation nuisance latent variables, we indeed show that the ReLU activation is important for learning the patterns. \n\n**A10**: can be easily generalized to multiple outputs by replacing a scalar w_{2,j} with a vector. Here we discuss the scalar case for illustration and notational simplicity. \n\n**A11**: As pointed in the paper, the network to be studied is in Fig.2(c). The matrix A changes during training due to the change of w, but remains a PSD matrix since it is a covariance matrix (Eqn. 8). The condition Cov[u_j, u_k] = 0 is a technical condition to allow simple and concise dynamics. Otherwise things can be much complicated and harder to interpret. We will add more general cases without the condition of Cov[u_j, u_k] = 0 in our next revision. Also what does R5 mean by \u201csee 9\u201d?  \n\n**A12**: We are confused. What does R5 mean by the \u201cindicator function\u201d at the end of Page 7 (we assume R5 means the paragraph after Theorem 7)? We didn\u2019t appear to use any indicator function there. \n\nWe guess that R5 means Page 5 instead. If our guess is correct, we precisely say that the indicator function I(w_j . x > 0) can change during optimization, except when the activation is linear and the gating is indeed a constant 1. So we are not sure about the concerns (maybe R5 misses the phrase \u201cwhile for linear node\u201d?). We will expand this analysis to make it more clear in the appendix in our next revision.\n\n(to be continued)"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "ap-YccE5Z4i", "original": null, "number": 3, "cdate": 1605133093115, "ddate": null, "tcdate": 1605133093115, "tmdate": 1605134398952, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "gHbY_xI60M7", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment", "content": {"title": "Rebuttal for R5 (Part 2/5) ", "comment": "**A5**:  We view the dynamical changing of the covariance operator as a key insight into the power of SSL with nonlinear networks.  After all, if the covariance operator were fixed, then SSL would simply reduce to principal components analysis, computing features that align with maximal eigenmodes of the covariance operator. However, the fact that at any instant of time, which weight patterns get amplified are completely controlled by a time-dependent positive semi-definite covariance operator (that remains PSD at any given time), is a fundamental observation about SimCLR that was unknown (and that other reviewers find interesting).  Indeed this result opens up entirely new avenues for understanding what SSL does by analyzing the dominant time-dependent eigenmodes of this covariance operator.  Thus we believe the time-dependence of this PSD operator is of fundamental interest and importance, and elucidating its existence is a fundamental contribution (and other reviewers agree). \n\nWe do not believe that taking the neural tangent kernel (NTK)  limit (infinitely wide limit where learning is slow and the neural tangent kernel remains the same as it was at initialization) would be a way to get an interesting result.  Even in the case of supervised learning, it is unclear that the NTK limit describes well what happens in practice.  For example, recent work (https://arxiv.org/abs/2010.15110) shows that the NTK is not constant, and instead changes rapidly within a few epochs in practice.  Also, the goal of unsupervised learning is feature learning, and the NTK limit corresponds to a lazy training regime where features are not learned (https://arxiv.org/abs/1812.07956). We suspect that performing SSL in the NTK limit with nonlinear networks would simply reduce to kernel PCA with a fixed specific kernel.  In contrast, we show that SSL with finite width networks is much richer.  For example,  as shown in Sec. 4.2, under the covariance operator, dynamics of high-layer weights leads to faster low-layer training.\n\nFinally, note that our covariance operator and the NTK are fundamentally different objects.  The NTK is defined in the sample space and is full-rank if samples are distinct. For super wide networks (compared to the sample size), NTK seldom changes during training and leads to a convex optimization landscape. On the other hand, our covariance operator is defined per layer on any data distribution and network with finite width, and does not grow in size with larger sample size. Finally, while NTK has nothing to do with data augmentation, our covariance operator arises as an emergent property of both the data augmentation and the SSL architecture. Since the covariance operator alone determines learning, its further analysis can aid in the design of both the augmentation procedures and architectures that generate it.  \n\n**A6**:  We are not sure what does the reviewer mean by \u201cseparable data'' in the context of self-supervised learning settings in which data have no labels. Note that \u201cseparable data\u201d is itself a quite strong assumption. \n\n(to be continued)\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c5QbJ1zob73", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2351/Authors|ICLR.cc/2021/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849447, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Comment"}}}, {"id": "iFGgvXfqfS9", "original": null, "number": 1, "cdate": 1603331099639, "ddate": null, "tcdate": 1603331099639, "tmdate": 1605024232147, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review", "content": {"title": "Interesting theoretical analysis of self-supervised learning", "review": "This paper provided in-depth theoretical analysis of self-supervised learning, focusing on two network structures (SimCLR, BYOL). They proved that a covariance operator plays an important role in the SGD learning of self-supervised algorithms. Simulation experiments were provided to justify the theoretical findings. The paper is well written, with rigorous mathematical derivations.\n\nHere are a few comments on the technical details:\n1) In Theorem 3 there is an assumption that the derivative of L with respect to rk- is a constant. Is this a reasonable assumption in real situations? Can we see how this assumption is justified (or violated) in real data? \n\n2) Below equation (37) in the appendix, the above assumption becomes derivative of L with respect to rk-^2, and H is replaced with n in the constant. It seems that there is some discrepancy with the statement of Theorem 3, and some explanation may be helpful.\n\n3) In the first paragraph of Section 4, it is stated that \"all nuisance latents z' are integrated out in K(z_0)\". This statement seems to be a little vague to me. More rigorous definitions and derivations may be needed here to support the claim.\n\n4) A related follow-up question is, how would the augmentation distribution impact the analysis? What would be a good augmentation transform?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098399, "tmdate": 1606915764466, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2351/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review"}}}, {"id": "avHyr_g10QK", "original": null, "number": 2, "cdate": 1603945531242, "ddate": null, "tcdate": 1603945531242, "tmdate": 1605024232063, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review", "content": {"title": "A very solid work", "review": "Summary: This paper does theoretical analysis about self supervised learning (SSL), esp. those methods using contrastive learning. It zooms into one-layer and two-layer networks and proves contrastive learning can converge to weights corresponding to largest eigenvector of a covariance matrix. Experiments on synthetic and real datasets are consistent with theoretical conclusions.\n\nReasons for score: \nThe covariance operator sheds light to the black-box learning process of contrastive learning. Approximating the data distribution with a hierarchical latent tree model is an interesting technique. This work may inspire practical tools to improve SSL. \n\nIssues:\n1. My major concern is the analysis about BYOL. The authors of BYOL disagree with the conclusion in this paper that BatchNorm provides implicit contrastive objective for BYOL, in a newly released paper \"BYOL works even without batch statistics\". I'd ask the authors, does this invalidate your analysis about BYOL? Or does the GroupNorm + weight standardization also provide implicit contrastive objective, similar as BN? \n2. To what degree the validity of the theoretical analysis depends on the type of the loss? Say if I change the InfoNCE loss to some other loss, will that invalidate the whole analysis?\n3. A typo in page 4: \"we setup the following...\" => \"we set up the following...\"\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098399, "tmdate": 1606915764466, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2351/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review"}}}, {"id": "0qb0RI4aybE", "original": null, "number": 3, "cdate": 1604028543007, "ddate": null, "tcdate": 1604028543007, "tmdate": 1605024232003, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review", "content": {"title": "The paper proposes a theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks. The finding is interesting and novel. More experiments are needed.", "review": "The paper proposes a novel theoretical framework to understand self-supervised learning\nmethods that employ dual pairs of deep ReLU networks. The finding is interesting and novel. The experiments support the conclusion. However, there are several issues.\n\nThe paper should explain the motivation to choose simCLR for analysis.\nThe paper defines the covariance operator from contrast loss and the paper declare that the theory is suitable for deep ReLU networks with dual pairs. How to define the covariance operator for other kinds of loss function of deep ReLU networks with dual pairs, such as triplet loss.\nThe experiment setting should be listed. Such as the learning rate, the training time and the date augmentation used in the experiments. The paper also needs experiments about the magnitude of the covariance operator and the training time.\nThe paper analysis the activation gap and the weights of different layer for HLTM. The paper needs experiments about the activation gap grows over time and the weight of the top layer and lower layer change over time.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098399, "tmdate": 1606915764466, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2351/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review"}}}, {"id": "GBbGQ7CYnyJ", "original": null, "number": 4, "cdate": 1604056063900, "ddate": null, "tcdate": 1604056063900, "tmdate": 1605024231943, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review", "content": {"title": "This paper presented a novel theoretical framework to analyze some state-of-the-art self-supervised learning methods.  The authors proved that the weights are updated by a covariance operator that amplifies initial random selectivities that vary across data samples during training and explain how the good representations can be learned in this way. And the authors also analyze the reason why BYOL can work with no negative samples.", "review": "Strengths:\n\uf06c\tThis paper tried to analyze the \u201cBlack-box\u201d of the contrastive learning by inspecting the weight update during contrastive learning. The analysis and conclusion seem promising.\n\uf06c\tThis paper made a theoretically analysis on BYOL and conducted a variety of ablation studies to study the impacts of BN layer and predictor in BYOL. And some interesting experimental results and conclusions are presented, which sheds light on the further explorations on self-supervised learning.\nWeakness:\nWhile I still have some questions:\n\uf06c\tIn [1], it has been proved that BYOL can work without BN in predictor, which seems contradictory the conclusion given in this paper. I recommend the author to compare with this conclusion.\n\uf06c\tIn Table 5, it seems that reinitializing the predictor will improve the performance a bit. I wonder if the improvements will gradually decrease or even vanish for longer training epochs.\n\n[1] BYOL works even without batch statistics. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098399, "tmdate": 1606915764466, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2351/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review"}}}, {"id": "gHbY_xI60M7", "original": null, "number": 5, "cdate": 1604459063037, "ddate": null, "tcdate": 1604459063037, "tmdate": 1605024231874, "tddate": null, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "invitation": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review", "content": {"title": "The observations can be interesting, but they are not rigorous theoretical results, while the presentation is disastrous.", "review": "This paper aimed to understand the self-supervised learning algorithm under a teacher-student network setting. The authors argued that the gradient are specified by a covariance operator, that can amplify extracting the intrinsic features that are invariant to the data augmentation.\n\nI have several concerns on this paper, mainly on the over-simplified setting, unrealistic assumption, and inaccurate claim. Also, the presentation is weird.\n\nThe detailed comments:\n1. I feel most of the theorem are just simple calculus. It\u2019s inappropriate to claim such calculation results as theorem. At least one theorem should contain some information on the property of the terms of interest.\n2. As far as I know, there\u2019s no existing empirical work used the ell_2 loss on the feature as the dissimilarity, though when the features have norm 1 (which cannot simply hold in practice) there are some connections between the ell_2 loss and inner product. I personally would argue that, the authors should first make an empirical justification on this simplification, say such dissimilarity measure can empirically work. I don\u2019t think this is a valid simplification. The authors may argue that methods like simCLR have normalization before computing pairwise similarity, but this will dramatically influence the gradient update. Maybe the remaining conclusion still holds, but at least the authors should include such part.\n3. I would like to say that the assumption for Theorem 3, i.e. the gradient of the negative samples are identical, are too strong and unrealistic. Definitely this will not happen, even though the negative samples are from the same distribution, there should be some variance. I cannot accept such assumption. Even take H=1 is better than this assumption.\n4. Beta is used without any introduction.\n5. As the authors say, the covariance operator are changing over time, it\u2019s hard to claim that this covariance operator let the parameter align with some specific direction that is good to learn a good representation. Take the NTK limit may be one possible way to get some more interested result, however, the authors haven\u2019t done that.\n6. And I would argue that, with only analysis on gradient, it\u2019s rather hard to make some strong claim. There are several existing work on homogenous neural network that characterize the optimization and solution with e.g. separable data, which is much more convincing.\n7. I feel the description starting from Section 4 on two groups of latent variables are so intuitive and also unrealistic. How can we identify such groups of latent variables and how can we make sure in practice we don\u2019t perturb the class/sample-specific latent? On the other hand, what if we perturb the class/sample-specific latent? Imagine we have a Gaussian mixture of two isotropic components, if we augment the data with the isotropic Gaussian, can we make an informative projection onto low dimension with self-supervised learning? I think the current intuitive and unrealistic setting restricted the potential application in practice.\n8. What\u2019s the definition of bar{K}_l(z_0) as the input of this term should be x? What\u2019s the form of the term after integrated out z^\\prime? I feel there are so many ambiguities here and I\u2019m afraid I cannot accept such claim.\n9. I think the arguments in Section 4.1 need to be justified more formally, if this is a `theoretical\u2019 paper. I wonder what does the author want to say in Section 4.1, the importance of non-linear activation? \n10. In Section 4.2, why constrained to 1 output? I would like to ask, as in the derivation in Section 3, there should be no straightforward barrier on considering this more general case?\n11. In Theorem 4, is the A matrix fixed during the dynamics? I\u2019m quite skeptical on that, as the w in the definition of u are changed over time, which can influence the indicator. Also, why we have the covariance is equal to zero? Is that the normal case? Also, there are so many ambiguities here, see 9, can the authors give all the formal description of the neural network we considered, the input data distribution, etc. in a clear way? To be honest, I cannot understand the proof of Theorem 4 as well, due to these ambiguities.\n12. For the description in the last of Page 7, I totally get confused. What does the authors want to say on that? Even the parameter converge to some point, we cannot say during the optimization it keeps the indicator 1? Can the authors stop using such ambiguous description and give some formal description and claim?\n13. What\u2019s the meaning of considering such HLTM? Can it represent the general case of learning? I need to say the authors does not convince me that such setting is general and it\u2019s necessary to consider the multi-layer network with such setting. I would like to say, it\u2019s better first considering the fully-connect network rather than considering the network with local receptive field.\n14. I would like to say the organization of Section 4.3 is really weird. Can the authors give a formal description in the neural network and generative model in Section 4.3, even in the appendix with simpler but clear model? I don\u2019t even know the data generating process of x given z, thus does not the meaning of given a sample x how to resampling z^\\prime. As the notation are only described in Table 1, I cannot get the meaning of each term in the theorem. Give some description on each of term with a formalized mathematical description on the generative model and network, please.\n15. Also, I does not feel the theorem in Section 4.3 give some strong arguments. What we really care in the self-supervised learning is the quality of representation, the sample complexity, the convergence analysis, none of them have been addressed by the author formally. Instead, I feel Theorem 6 presented here is not of particular interest. Also, the discussion is quite intuitive.\n16. What\u2019s the sym subscript in Equation 10? Please introduce the notation before used it. Also, what\u2019s the term deltaW_l^{BN} in Equation 12? Please be self-contained.\n17. The observation of Theorem 7 can be interesting, however, in practice, what we really do during BN is the latter one, i.e. x - x.mean().detach()? And in fact, the mean we subtract in BN is a moving average statistics. I feel it can explain something, but not why BYOL works. The claim that such observation give an analysis of why BYOL work is not accurate. In the contrast, I think it even predict that BYOL will not work or at least need to be fixed.\n18. Overall, what the authors have proposed are better understood as some intuition or observation, not some rigorous theory.\n\nTo sum up, I feel the presentation is weird. The settings are over-simplified and the authors even didn\u2019t introduce the setting with formal description. The assumption can be too strong while some explanations are too intuitive. The authors want to argue several points, however, none of them are strong enough and can be claimed as 'theorem' (I feel they are only 'observations'). Meanwhile, I feel the authors include such different points in the main text with the expense of readability.\n\nIf this is an empirical paper introduced some observation on the self-supervised learning with detailed ablation study and some kinds of theoretical characterization, I\u2019m happy to accept this paper. However, as I don\u2019t feel the authors provide either strong theoretical results or detailed ablation study, meanwhile, the presentation is totally a disaster, I think this paper is not suitable for publishing right now.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2351/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Self-supervised Learning with Dual Deep Networks", "authorids": ["~Yuandong_Tian1", "~Lantao_Yu2", "~Xinlei_Chen1", "~Surya_Ganguli1"], "authors": ["Yuandong Tian", "Lantao Yu", "Xinlei Chen", "Surya Ganguli"], "keywords": ["self-supervised learning", "teacher-student setting", "theoretical analysis", "hierarchical models", "representation learning"], "abstract": "We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \\emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ", "one-sentence_summary": "A theoretical framework for self-supervised learning with deep ReLU networks explaining recent success of SimCLR and BYOL.", "pdf": "/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|understanding_selfsupervised_learning_with_dual_deep_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XhrVv8cI9M", "_bibtex": "@misc{\ntian2021understanding,\ntitle={Understanding Self-supervised Learning with Dual Deep Networks},\nauthor={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},\nyear={2021},\nurl={https://openreview.net/forum?id=c5QbJ1zob73}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c5QbJ1zob73", "replyto": "c5QbJ1zob73", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098399, "tmdate": 1606915764466, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2351/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2351/-/Official_Review"}}}], "count": 31}