{"notes": [{"id": "HygQ7TNtPr", "original": "ryxXqpAIvB", "number": 444, "cdate": 1569439003294, "ddate": null, "tcdate": 1569439003294, "tmdate": 1577168217814, "tddate": null, "forum": "HygQ7TNtPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "g881K34zhI", "original": null, "number": 1, "cdate": 1576798696608, "ddate": null, "tcdate": 1576798696608, "tmdate": 1576800939050, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Decision", "content": {"decision": "Reject", "comment": "The submission proposes methodology for quantizing neural networks.  The reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR.  Concerns included novelty over previous works, comparatively weak baseline comparisons, and overly restrictive assumptions.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717194, "tmdate": 1576800267446, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper444/-/Decision"}}}, {"id": "H1xpabCFjS", "original": null, "number": 7, "cdate": 1573671364976, "ddate": null, "tcdate": 1573671364976, "tmdate": 1573671364976, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment", "content": {"title": "Revised Version Submitted", "comment": "We appreciate the invaluable comments from the reviewers and readers, and we have submitted a revised version of our paper based on these comments. We hope the revised version makes our work clearer."}, "signatures": ["ICLR.cc/2020/Conference/Paper444/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygQ7TNtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper444/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper444/Authors|ICLR.cc/2020/Conference/Paper444/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171407, "tmdate": 1576860560060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment"}}}, {"id": "B1xAC1BbsH", "original": null, "number": 6, "cdate": 1573109717807, "ddate": null, "tcdate": 1573109717807, "tmdate": 1573187115927, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "Bye0CorKOS", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment", "content": {"title": "Reply to the Reader", "comment": "We appreciate the comment from the reader. However, it seems that the reader has some misunderstanding of our results.\n\nFor the first point the reader mentioned, our results do show that weight normalization helps improve the performance of full precision networks, as demonstrated in Figure 2(b) of our paper. Weight normalization improves the performance of full precision networks from the red lines to the blue lines. The reason for worse results than those from the literatures can be that others use full precision results with original weights, while we use clamped weights, which is a more reasonable initial point for quantization (because they resemble the quantized weights more than those without clamping).\n\nGradient calibration is not the most important contribution of our work, but compared with previous results (including the reference the reader provided), we give the detailed derivation of the result (in Appendix D), as well as experimental results on its impact (Figure 3)."}, "signatures": ["ICLR.cc/2020/Conference/Paper444/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygQ7TNtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper444/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper444/Authors|ICLR.cc/2020/Conference/Paper444/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171407, "tmdate": 1576860560060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment"}}}, {"id": "r1gDFAVZjH", "original": null, "number": 2, "cdate": 1573109375013, "ddate": null, "tcdate": 1573109375013, "tmdate": 1573110115693, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HkgNx-OiqB", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment", "content": {"title": "Reply to Review #1", "comment": "We appreciate the kind review from the reviewer, but it seems a bit unfortunate that the reviewer lost some important insights we provided and did not check the results very carefully. As far as we know, the four papers the reviewer listed are all arXiv preprints and none of them are officially published. Nevertheless, below we list detailed comparison between our approach and each of the four references.  It shows that our results outperform all of them (we only list those experiments that overlap between our paper and the reference).\n\nThe first reference (\u2018Learned Step Size Quantization\u2019) is completely orthogonal to our work, and it is difficult to find any common part between our work and their paper.\n\nThe two paper start from different views to analyze the quantization procedure. In the reference, the author proposes a quantizer with trainable step size, and improves training convergence by balancing the magnitude of step size updates with weight updates. The basic reasoning of the reference is that the ratio between update magnitude and parameter magnitude (the original value) should be similar for the learned step size and the learned weight. Meanwhile, to correct the impact of quantization precision, a gradient scale is introduced for the step size loss. In our paper, on the other hand, we start from the beginning by analyzing the training dynamics with mean field theory, which is a method from condensed matter physics but is widely adopted in literature on analysis of weight initialization, to propose some efficient training rules for a generic neural network. Based on our analysis, we investigate the existing quantization algorithms and found some procedure in typical quantization algorithms such as DoReFa violates the efficient training rules which leads to degenerated performance.\n\nThe methods proposed in the two papers are also completely irrelevant. In the LSQ paper, the author focuses on training the step size, and scale the gradients. However, our method focuses on the scale of the weights themselves, through which we manage to control the training dynamics, and influence the gradients indirectly.\n\nThe results of the LSQ also differs from ours drastically. Actually, the only common experiments for the two papers is ResNet-50 on ImageNet, while ours significantly outperform LSQ by as large as 0.8% for 3 bit case, as indicated in the following table.\n\n Actually, all the four references show worse performance than our approach. We are open to include these references in the modified version, but it does not change our claim that we still achieve the state-of-the-art performance on model quantization, especially on challenging mobile-scenario networks."}, "signatures": ["ICLR.cc/2020/Conference/Paper444/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygQ7TNtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper444/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper444/Authors|ICLR.cc/2020/Conference/Paper444/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171407, "tmdate": 1576860560060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment"}}}, {"id": "SkgyCRNWsr", "original": null, "number": 3, "cdate": 1573109446979, "ddate": null, "tcdate": 1573109446979, "tmdate": 1573109757183, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "r1gDFAVZjH", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment", "content": {"title": "Performance Comparison with the References Mentioned by the Reviewer", "comment": "Reference 1: https://arxiv.org/pdf/1902.08153.pdf\n-------------------------------------------------------------------------------------------------\n|      Model      |   Method   |      Top-1 Valid Acc.     |   Top-5 Valid Acc.   |\n-------------------------------------------------------------------------------------------------\n|                        |                    |    2 bit    3 bit    4 bit    |  2 bit   3 bit   4 bit  |\n-------------------------------------------------------------------------------------------------\n|                        |       LSQ      |    73.7    75.8    76.7    |  91.5   92.7   93.2    |\n|   ResNet-50   |                    |                                        |                                |\n|                         | SAT (ours) |   74.1    76.6    76.9     |  91.7   93.1   93.3   |\n-------------------------------------------------------------------------------------------------\n\nReference 2: https://arxiv.org/pdf/1903.08066.pdf\n------------------------------------------------------------------------------------------------------\n|      Model             |   Method   |      Top-1 Valid Acc.     |   Top-5 Valid Acc.   |\n------------------------------------------------------------------------------------------------------\n|                               |       TQT      |           75.4 (8 bit)         |       92.3 (8 bit)       |\n|      ResNet-50      |                    |                                        |                                 |\n|                               | SAT (ours) |          76.9  (4 bit)         |      93.3 (4 bit)        |\n------------------------------------------------------------------------------------------------------\n|                               |       TQT      |           71.1 (8 bit)         |       90.0 (8 bit)       |\n|   MobileNet-V1   |                    |                                        |                                 |\n|                               | SAT (ours) |          72.6  (8 bit)         |      90.7 (8 bit)        |\n------------------------------------------------------------------------------------------------------\n|                               |       TQT      |           71.8 (8 bit)         |       90.6 (8 bit)       |\n|   MobileNet-V2   |                    |                                        |                                 |\n|                               | SAT (ours) |          72.5  (8 bit)         |      90.7 (8 bit)        |\n------------------------------------------------------------------------------------------------------\n\nReference 3: https://arxiv.org/pdf/1905.11452.pdf\n------------------------------------------------------------------------------------------------------\n|      Model             |   Method   |      Top-1 Valid Acc.     |   Top-5 Valid Acc.   |\n------------------------------------------------------------------------------------------------------\n|                               |       DQ       |           70.59 (4 bit)        |              NA              |\n|   MobileNet-V2   |                    |                                        |                                  |\n|                               | SAT (ours) |          71.1  (4 bit)         |      90.0 (4 bit)         |\n-------------------------------------------------------------------------------------------------------\n\nReference 4: https://arxiv.org/pdf/1905.13082.pdf\n------------------------------------------------------------------------------------------------------\n|      Model             |   Method   |      Top-1 Valid Acc.     |   Top-5 Valid Acc.   |\n------------------------------------------------------------------------------------------------------\n|                               |       ref       |           66.46 (4 bit)        |              NA              |\n|   MobileNet-V1   |                    |                                        |                                 |\n|                               | SAT (ours) |          71.3  (4 bit)         |      89.9 (4 bit)        |\n------------------------------------------------------------------------------------------------------"}, "signatures": ["ICLR.cc/2020/Conference/Paper444/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygQ7TNtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper444/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper444/Authors|ICLR.cc/2020/Conference/Paper444/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171407, "tmdate": 1576860560060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment"}}}, {"id": "r1lyVJBZjr", "original": null, "number": 5, "cdate": 1573109543164, "ddate": null, "tcdate": 1573109543164, "tmdate": 1573109685010, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "rklidewTFr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment", "content": {"title": "Reply to Review #3", "comment": "We appreciate the kind review from the reviewer. According to the comments, there might be some misunderstanding to our approach from the reviewer and we are glad to give more detailed explanations.\n\nIn order to provide a semi-quantitative analysis on the key factors of model quantization, we do simplifications of the neural network parameters and also consider the most common configurations. For activation functions, since ReLU is the most generally adopted activation function, we only consider that (the same is applied in the paper https://arxiv.org/abs/1502.01852). For batch normalization weights, empirically it is on the order of O(1), so we can safely omit it without quite being misled. Analysis with details of \\gamma and different activation functions will nevertheless be more complicated and we leave it as future work.\n\nFor (3) and (4), the condition for the law is detailed in the appendix, it holds for most cases except some extreme cases (for general practical case, the approximation effect is incorporated in the parameter of \\kappa\u2019s).\n\nFor CG-PACT, CG is not important for high-precision (for example, 8 bit), as the quantization error is very small. However, for low precision such as 4 bit, the difference can be as large as 0.4% for ImageNet classification, which is not negligible. Lower precision such as 2 bit will further exaggerate the problem as shown in Figure 3 in the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper444/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygQ7TNtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper444/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper444/Authors|ICLR.cc/2020/Conference/Paper444/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171407, "tmdate": 1576860560060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment"}}}, {"id": "rkgpZ1B-jr", "original": null, "number": 4, "cdate": 1573109508822, "ddate": null, "tcdate": 1573109508822, "tmdate": 1573109652827, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HkxtvrkCKr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment", "content": {"title": "Reply to Review #2", "comment": "We appreciate the kind review from the reviewer. According to the comments, There might be some misunderstanding to our approach from the reviewer and we are glad to give more detailed explanations.\n\nFor the analysis in Section 3.2, it is quite generally applicable and there is no restriction on the value of any variables, and no matter they are full-precision or quantized, the analysis applies without any difference.\n\nFor weight initialization, if there is no batch normalization layers followed, Kaiming initialization is the common strategy for correct convergence during training, as already analyzed in detail in the famous work (https://arxiv.org/abs/1502.01852). With batch normalization layers followed, initialization of convolution layers can be arbitrary, and we did not apply our techniques for these cases (as we mentioned, either batch normalization layer is applied, or the variance of effective weights should be proportional to the reciprocal of the number of neurons).\n\nDoReFa is a promising and widely adopted quantization strategy for model quantization, and is the basis for a bunch of modern quantization procedure such as PACT. This is the main reason we adopt this method in our work. Due to the similarity of the quantization procedure of DoReFa with other approaches such as XNORNet, Tenary Quant, HWGQ, etc, we believe our approach can be adopted to improve these algorithms as well.\n\nFor the \u2018scaling factor in Eq. (3)\u2019 before the subsection \u201cEfficient Training Rule II (ETR II)\u201d, we mean the product in front of the variance of weight gradient on the right hand side of the equation."}, "signatures": ["ICLR.cc/2020/Conference/Paper444/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygQ7TNtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper444/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper444/Authors|ICLR.cc/2020/Conference/Paper444/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171407, "tmdate": 1576860560060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment"}}}, {"id": "SJg3kCNZoS", "original": null, "number": 1, "cdate": 1573109220063, "ddate": null, "tcdate": 1573109220063, "tmdate": 1573109220063, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment", "content": {"title": "Novelty and contribution of our work", "comment": "We thank all reviewers for their reviewing effort and constructive comments to our work. All reviewers seem to have some doubts on the novelty of our work, so we reiterate our main contribution here for reference. In this paper, we investigate a problem that has been largely overlooked by most literature on quantization-aware training, that is, whether the accuracy drop of quantized neural networks comes from the reduced capacity, or inefficient training during the quantization procedure. To this end, we first investigate the condition of efficient training of a generic deep neural network, and discover that proper convergence of the network should follow two specific rules of efficient training. It is then demonstrated that deviation from these rules leads to improper training and accuracy reduction, regardless of the quantization level of weights or activations. To deal with this problem, a simple yet effective technique named scale-adjusted training (SAT) is proposed. We apply this method on a popular and typical quantization method PACT/Dorefa to showcase the effectiveness of the approach. The resulted quantized models significantly outperform the original PACT approach and validates our analysis. Violations of training rules also exists in other quantization approaches such as BinaryNet, XNORNet, Ternary Quantization, HWGQ as well as a differentiable quantization approach Darts Quant. We believe our approach can be used to boost the performance of such quantization algorithms as well.\n\nReferences\nBinaryNet: M. Courbariaux, et al., Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. arXiv 2016.\nXNORNet: M. Rastegari, et al., XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV 2016.\nTernary Quantization: C. Zhu, et al., Trained Ternary Quantization. ICLR 2017.\nHWGQ: Zhaowei Cai, et al., Deep Learning with Low Precision by Half-Wave Gaussian Quantization. CVPR 2017.\nDarts Quant: S. Uhlich, et al., Differentiable Quantization of Deep Neural Networks. arXiv 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper444/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygQ7TNtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper444/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper444/Authors|ICLR.cc/2020/Conference/Paper444/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171407, "tmdate": 1576860560060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Comment"}}}, {"id": "rklidewTFr", "original": null, "number": 1, "cdate": 1571807347129, "ddate": null, "tcdate": 1571807347129, "tmdate": 1572972594572, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents network quantization techniques for weight quantization (SAT) and activation quantization (CG-PACT). The authors first formulate efficient training rules (ETR I and ETR II) and propose a normalization scheme for weight quantization based on the rules. Independently, the authors also propose an activation quantization rule by removing the approximated term in PACT. The authors present the effectiveness of their algorithm in MobileNet V1, V2, and PreResNet-50.\n\nOverall, I think the proposed algorithms lack novelty due to the following reasons.\n\n- SAT suggests a normalization scheme under some assumptions (ETR I and ETR 2), however, I am not sure this is a significant contribution compared to DoReFa. In particular, I have some doubt in assumptions which is listed in \u2018Other comments\u2019. \n\n- CG-PACT seems to be a simple variant of PACT. In methodological contribution, I believe that it could only be a marginal improvement with respect to PACT. In terms of empirical contribution, it was hard to see the improvement made by CG-PACT as PACT and PACT+CG exhibit similar performance in Table 4.\n\nOther comments\n- In the derivation of ETR 1, some approximations are not clear for me. For example, the authors drop \\gamma in (17). In addition, approximation in (16d) depends on the activation function and BN parameters but they are also ignored. I believe that incorporating all these would not introduce much computational overhead.\n\n- In (3) and (4), I think that equality only holds under some assumptions (e.g., an infinite number of parameters, i.i.d. weights, etc.) but they are not clearly mentioned. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper444/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834430919, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper444/Reviewers"], "noninvitees": [], "tcdate": 1570237752041, "tmdate": 1575834430931, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Review"}}}, {"id": "HkxtvrkCKr", "original": null, "number": 2, "cdate": 1571841376812, "ddate": null, "tcdate": 1571841376812, "tmdate": 1572972594522, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes two rules for efficient training of quantized networks by investigating the scale of the logit values and gradient flow. The authors claim that accuracy degradation of recent quantization methods results from the violation of these two rules.\n\nOne of my main concerns is that the analysis of the rules for weight and activation quantization are separated. E.g., the analysis of weight quantization in Section 3.2 is based on eq (1a)-(1d) where no activation quantization is considered. In this case, does the analysis still hold when applying weight and activation are quantized simultaneously?\n\nMoreover, the analysis is only suited for a limited range of quantization methods. In the proposed SAT, the authors propose to multiplies the normalized weight with the square root of the reciprocal of the number of neurons in the linear layer, to make up for the variance difference caused by quantization. However, this increase indeed depends on the initialization of the weights. If the weights are not sampled from \"a Gaussian distribution of zero mean and variance proportional to the reciprocal of the number of neurons\" as at the end of page 5, then this recipe may not work any longer. Moreover, the proposed SAT seems to be only suited for the specific quantization function for Dorefa-Net in (5), what about many other recent quantization functions that do not need this kind of clamping?\n\nOthers:\n1. The citation format is wrong.\n2. In the abstract, \"Recent quantization approaches violates ... and results ...\" => \"Recent quantization approaches violate ... and result ...\"\n3. What is the \"scaling factor in Eq. (3)\" before the subsection \"Efficient Training Rule II (ETR II)\"?\n4. Keep the same number of decimal places in the tables."}, "signatures": ["ICLR.cc/2020/Conference/Paper444/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834430919, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper444/Reviewers"], "noninvitees": [], "tcdate": 1570237752041, "tmdate": 1575834430931, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Review"}}}, {"id": "HkgNx-OiqB", "original": null, "number": 3, "cdate": 1572729068225, "ddate": null, "tcdate": 1572729068225, "tmdate": 1572972594479, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Quick Summary: Based on semi-quantitative analysis the paper first proposes two rules for quantization of DNNs, then extends previous methods based on these rules to propose specific technique for quantizing activations and weights. Experimental results are provided on MobileNet V1/VB2 and ResNet50 and compare favourably against baselines (which are old and unfortunately do not represent recent developments in the literature). Overall I found novelty low and also there are several recent papers already published before this submission which provide results which are at least as good or even better\n\nDetails\nSpecifically the rules are: \n1. To prevent logits from entering saturation region of the cross entropy loss, the effective weight in the last fully connected layer should be small.  \n2.To keep the gradient of weights in the same scale across the whole network, either BN layers should be used after linear layers such as convolution and fully- connected layers, or the variance of the effective weights should be on the order of the reciprocal of the number of neurons of the linear layer (n_l).\nThey use DoReFa for weight quantization and PACT for activation quantization based on the above observations/rules. However the contributions were not really very novel in my opinion. See especially the paper \"LEARNED STEP SIZE QUANTIZATION\" I linked below when explaining novelty\n\nThis reviewer feels that the authors were perhaps unaware of several important contributions to the literature this year which are substantially better than the baselines compared against in the paper. I list a few below for the authors to compare against, and to explain their novelty in contributions in the rebuttal phase. Unfortunately this makes me feel the paper should be a clear reject in its current state, unless the authors can convince us otherwise. \n\nhttps://arxiv.org/pdf/1902.08153.pdf\nhttps://arxiv.org/pdf/1903.08066.pdf\nhttps://arxiv.org/pdf/1905.11452.pdf\nhttps://arxiv.org/pdf/1905.13082.pdf\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper444/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper444/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper444/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834430919, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper444/Reviewers"], "noninvitees": [], "tcdate": 1570237752041, "tmdate": 1575834430931, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper444/-/Official_Review"}}}, {"id": "Bye0CorKOS", "original": null, "number": 1, "cdate": 1570491350109, "ddate": null, "tcdate": 1570491350109, "tmdate": 1570491350109, "tddate": null, "forum": "HygQ7TNtPr", "replyto": "HygQ7TNtPr", "invitation": "ICLR.cc/2020/Conference/Paper444/-/Public_Comment", "content": {"comment": "1) Weight normalization methods have been previously shown to improve the performance of full precision networks (https://arxiv.org/abs/1602.07868), therefore it was surprising to see that the scores of full precision networks trained with the author's weight normalization procedure (SAT) were lower than scores reported on the same networks in other publications.  Do the authors have any ideas why this might have been the case?  Is it possible that the training hyperparameters used for the full precision case were not optimal?  For example, for a full precision network, the initial learning rate and weight decay values used seem a bit low.\n\nClarifying this is relevant because an alternative to the author's claim that SAT helps quantized networks outperform their full precision counterparts is that instead this weight normalization method improves the performance of both full precision and quantized networks, while not actually helping close the performance gap between them.  This alternative would also be an interesting result, particularly if it can be shown to out perform existing weight normalization methods.\n\n2) The method by which the activation quantization parameter loss gradient is computed appears to be an adaptation of that used for LSQ in https://arxiv.org/abs/1902.08153.  It would be informative to clarify if any meaningful differences exist compared to this prior work.", "title": "Two points to clarify interpretation of results and novelty of technique"}, "signatures": ["~Steven_K._Esser1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Steven_K._Esser1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Neural Network Quantization", "authors": ["Qing Jin", "Linjie Yang", "Zhenyu Liao"], "authorids": ["jinqingking@gmail.com", "yljatthu@gmail.com", "liaozhenyu2004@gmail.com"], "keywords": ["Deep Learning", "Convolutional Network", "Network Quantization", "Efficient Learning"], "abstract": "Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.", "pdf": "/pdf/23208e1807939f9d05c373591c9328d027ca72ba.pdf", "paperhash": "jin|rethinking_neural_network_quantization", "original_pdf": "/attachment/1646bab658121779aad274b1f5c84d5a578282e2.pdf", "_bibtex": "@misc{\njin2020rethinking,\ntitle={Rethinking Neural Network Quantization},\nauthor={Qing Jin and Linjie Yang and Zhenyu Liao},\nyear={2020},\nurl={https://openreview.net/forum?id=HygQ7TNtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygQ7TNtPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504209021, "tmdate": 1576860593077, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper444/Authors", "ICLR.cc/2020/Conference/Paper444/Reviewers", "ICLR.cc/2020/Conference/Paper444/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper444/-/Public_Comment"}}}], "count": 13}