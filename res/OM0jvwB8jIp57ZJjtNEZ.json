{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457640322723, "tcdate": 1457640322723, "id": "nx924kDvKc7lP3z2iomv", "invitation": "ICLR.cc/2016/workshop/-/paper/107/review/10", "forum": "OM0jvwB8jIp57ZJjtNEZ", "replyto": "OM0jvwB8jIp57ZJjtNEZ", "signatures": ["ICLR.cc/2016/workshop/paper/107/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/107/reviewer/10"], "content": {"title": "Adam with Nesterov Momentum", "rating": "8: Top 50% of accepted papers, clear accept", "review": "- This paper investigates a variant of the Adam optimization algorithm, where the first moment estimate (momentum) is replaced with a Nesterov momentum. The resulting algorithm, coined Nadam, performs well on a nontrivial task (convolutional autoencoder).\n- The paper is well written and easy to read.\n- It should be mentioned that the proposed method is computationally slightly more expensive than Adam. However this is not a big deal for models with weight-sharing (e.g. CNNs and RNNs).\n- The preliminary result looks promising\n\nSome suggestions/questions:\n- Section 4 (experiment) mentions that only the learning rate is tuned for Adam/Nadam, but the hyper-parameter 'mu' is set to the atypical value of .975, which suggests it is (somewhat) tuned to this problem particular. Its value might be arbitrary, but it might be better if this was either set to its Adam default (.9) or tuned with the learning rate.\n- It would be useful to derive an upper bound on the magnitude of the weight update. Adam's simple known upper bound is an advantage and would be nice to have for Nadam as well.\n- Section 3 mentions \"It often helps to gradually increase or decrease mu over time\". Increasing mu_t over time makes intuitive sense (to counter the typically decreasing signal-to-noise ratio of gradients over time), but in which situations would it help to decrease mu over time? Is there published work that investigates particular schedules for mu_t?\n\nOverall a very exciting potential improvement on Adam.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Incorporating Nesterov Momentum into Adam", "abstract": "This work aims to improve upon the recently proposed and rapidly popular-\nized optimization algorithm Adam (Kingma & Ba, 2014). Adam has two main\ncomponents\u2014a momentum component and an adaptive learning rate component.\nHowever, regular momentum can be shown conceptually and empirically to be in-\nferior to a similar algorithm known as Nesterov\u2019s accelerated gradient (NAG). We\nshow how to modify Adam\u2019s momentum component to take advantage of insights\nfrom NAG, and then we present preliminary evidence suggesting that making this\nsubstitution improves the speed of convergence and the quality of the learned mod-\nels.\n", "pdf": "/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf", "paperhash": "dozat|incorporating_nesterov_momentum_into_adam", "conflicts": ["stanford.edu"], "authors": ["Timothy Dozat"], "authorids": ["tdozat@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579953252, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579953252, "id": "ICLR.cc/2016/workshop/-/paper/107/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "OM0jvwB8jIp57ZJjtNEZ", "replyto": "OM0jvwB8jIp57ZJjtNEZ", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/107/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457526791620, "tcdate": 1457526791620, "id": "5Qz2J1LDEsZgXpo7i32z", "invitation": "ICLR.cc/2016/workshop/-/paper/107/review/11", "forum": "OM0jvwB8jIp57ZJjtNEZ", "replyto": "OM0jvwB8jIp57ZJjtNEZ", "signatures": ["ICLR.cc/2016/workshop/paper/107/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/107/reviewer/11"], "content": {"title": "Hyper-parameter selection ", "rating": "7: Good paper, accept", "review": "Inspired by the Nesterov's accelerated gradient, the paper proposes a new variant of Adam algorithm with a modified momentum term. \nThe contribution is incremental but might be of some interest if a broader benchmarking would be provided.\n\nPros:\ni) The proposed modification is easy to implement\nii) Figure 1 shows an acceleration sufficient to consider the algorithm for further investigations\n\nCons:\ni) The empirical validation is rather poor. The algorithm should be tested in situations where the original Adam shows \nstate-of-the-art performance. Then we would better see how much we can gain with Nadam. \nii) Figure 1 can be an artifact of the chosen hyper-parameter values. A more detailed hyper-parameter selection should be provided for Adam and Nadam. One can fix some hyper-parameters and then select two others which seem to be the most important. Then make a contour plot with x-axis: hyper-parameter1 , y-axis: hyperparameter2, z: number of epochs to reach MSE 0.01 of the provided example. Alternatively, one can run hyper-parameter optimization.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Incorporating Nesterov Momentum into Adam", "abstract": "This work aims to improve upon the recently proposed and rapidly popular-\nized optimization algorithm Adam (Kingma & Ba, 2014). Adam has two main\ncomponents\u2014a momentum component and an adaptive learning rate component.\nHowever, regular momentum can be shown conceptually and empirically to be in-\nferior to a similar algorithm known as Nesterov\u2019s accelerated gradient (NAG). We\nshow how to modify Adam\u2019s momentum component to take advantage of insights\nfrom NAG, and then we present preliminary evidence suggesting that making this\nsubstitution improves the speed of convergence and the quality of the learned mod-\nels.\n", "pdf": "/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf", "paperhash": "dozat|incorporating_nesterov_momentum_into_adam", "conflicts": ["stanford.edu"], "authors": ["Timothy Dozat"], "authorids": ["tdozat@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579953019, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579953019, "id": "ICLR.cc/2016/workshop/-/paper/107/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "OM0jvwB8jIp57ZJjtNEZ", "replyto": "OM0jvwB8jIp57ZJjtNEZ", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/107/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455815742113, "tcdate": 1455815742113, "id": "OM0jvwB8jIp57ZJjtNEZ", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "OM0jvwB8jIp57ZJjtNEZ", "signatures": ["~Timothy_Dozat1"], "readers": ["everyone"], "writers": ["~Timothy_Dozat1"], "content": {"CMT_id": "", "title": "Incorporating Nesterov Momentum into Adam", "abstract": "This work aims to improve upon the recently proposed and rapidly popular-\nized optimization algorithm Adam (Kingma & Ba, 2014). Adam has two main\ncomponents\u2014a momentum component and an adaptive learning rate component.\nHowever, regular momentum can be shown conceptually and empirically to be in-\nferior to a similar algorithm known as Nesterov\u2019s accelerated gradient (NAG). We\nshow how to modify Adam\u2019s momentum component to take advantage of insights\nfrom NAG, and then we present preliminary evidence suggesting that making this\nsubstitution improves the speed of convergence and the quality of the learned mod-\nels.\n", "pdf": "/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf", "paperhash": "dozat|incorporating_nesterov_momentum_into_adam", "conflicts": ["stanford.edu"], "authors": ["Timothy Dozat"], "authorids": ["tdozat@stanford.edu"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}