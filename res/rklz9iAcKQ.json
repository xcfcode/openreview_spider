{"notes": [{"id": "rklz9iAcKQ", "original": "SkgHFy_FFX", "number": 513, "cdate": 1538087817649, "ddate": null, "tcdate": 1538087817649, "tmdate": 1545406967706, "tddate": null, "forum": "rklz9iAcKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1eIzPs-eV", "original": null, "number": 1, "cdate": 1544824589808, "ddate": null, "tcdate": 1544824589808, "tmdate": 1545354529552, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Meta_Review", "content": {"metareview": "Because of strong support from two of the reviewers I am recommending accepting this paper. However, I believe reviewer 1's concerns should be taken seriously. Although I disagree with the reviewer that a general \"framework\" method is a bad thing, I agree with them that additional experiments would be valuable.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "borderline paper due to concerns remain about the thoroughness of the experiments"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper513/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353190669, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353190669}}}, {"id": "HkgPLafJkE", "original": null, "number": 13, "cdate": 1543609678777, "ddate": null, "tcdate": 1543609678777, "tmdate": 1544620682475, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "B1giOunS0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Response to Comments", "comment": "Thank you for following the discussion, and giving us further comments. To address them:\n\nFor the optimal discriminator, the GAN and JSD objectives are directly proportional to each other, so in this limit the two discriminators actually optimize the exact same thing (this was pointed out in original GAN paper by Goodfellow et al.). However, the proportionality actually extends to the non-optimal discriminator: plugging in the activation function and convex conjugate into equation (7) in the f-GAN paper for JSD and GAN, makes these two versions of the Fenchel dual proportional as well. We have updated the paper to clarify this.\n\nWhile we do not perform a more exhaustive study of architectures, and our modifications are fairly small compared to common architectures used in other works, we can offer some further insights here based on our experiences. \n\nOverall, we found that using the \u201cwide\u201d architectures with a standard supervised loss quickly and drastically overfits, whereas increasing the depth for a DGI encoder had a less-pronounced effect. For the transductive datasets especially, increased depth could induce a 2-3% drop in accuracy, and the reason for this is likely because deeper GCNs have larger \u201creceptive fields\u201d. This is analogous to the number of random walk steps in other unsupervised approaches, which tend to not benefit from more than two steps. While we cannot say that these trends will hold in general, we now offer some concrete suggestions in the paper: i.e., that with the DGI loss function we often found benefits from employing wider, rather than deeper models."}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "Byl5M6VzkN", "original": null, "number": 14, "cdate": 1543814417624, "ddate": null, "tcdate": 1543814417624, "tmdate": 1543814417624, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rJe0kif1yN", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Regarding sufficient statistics", "comment": "Thank you very much for the revision.\n\nBased on your formulations, the sufficient statistics is any transformation of $X^{(k)}$ which is sufficient to evaluate the likelihood $p(X^{(k)})$. For example, any injective mapping $\\mathcal{R}$ results in a sufficient statistic. However, this condition is not necessary. The information monotonicity bound is tight under transformations corresponding to sufficient statistics: $KL(p(x|theta_1),p(x|theta_2)) \\ge KL(p(f(x)|theta_1),p(f(x)|theta_2))$, where \"=\" holds iff $f(x)$ is a sufficient statistic meaning that under the transformation $f$ there is no loss of information."}, "signatures": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "rJe0kif1yN", "original": null, "number": 12, "cdate": 1543609061855, "ddate": null, "tcdate": 1543609061855, "tmdate": 1543609061855, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "SklVLSaOCX", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Response to Comments", "comment": "Thank you for following the discussion and for your further thoughts.\n\nAs the square function is convex, Jensen's inequality will only give us a lower bound (of 1/2|X|) rather than an upper one. However, we can upper bound the sum of squares of probabilities to 1 without using Jensen's inequality (it's maximised for the one-hot distribution, as for all s_k, p(s_k)^2 <= p(s_k)). Therefore, we can establish that our bound on the error rate lies in the range 1/2|X| <= Err* <= 1/2. Intuitively, in the case of a one-hot distribution, only one summary is ever possible, and therefore anything that belongs to the product of marginals also belongs to the joint---making the classifier's choices a random guess.\n\nWe have checked again, and can confirm that our procedure gives us an upper bound on the error rate. As the error rate is proportional to the sum of eq (2) (the product of marginals over pairs X and R(X)), and the conditional is bounded from above by one, the sum of the marginal squared is an upper bound (including the conditional in the sum will only decrease the value of the sum).\n\nLastly, it is unclear what is meant by a \u201csufficient statistic\u201d in this case: could you provide a concrete example? Given the assumption of the problem (X being i.i.d. and a deterministic encoder), the max-info encoder must be invertible. A simple counter example (two i.i.d. samples mapping to the same feature vector) is trivially suboptimal.\n\nWe have made modifications to the paper in the Conclusions section (as proposed) and added remarks about the bounds on Err*.\n\nThank you once again for your comments!"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "SklVLSaOCX", "original": null, "number": 11, "cdate": 1543193932085, "ddate": null, "tcdate": 1543193932085, "tmdate": 1543193932085, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "HJeiYITZ07", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Comments after Revision", "comment": "Thank you very much for the revision that has improved the paper. Kindly see below some comments:\n\n- Lemma 1\nUse Jesnsen inequality and convexity of x^2 to have some trivial bounds of your bound (especially that it is smaller than 1).\n\n- Lemma 1, please double check if it is an upper bound or lower bound of the error rate\n\n- the stated theorems in section 3.3 is a bit weak because it requires the summarization to be invertible. Could you check whether similar results hold if the summarization vector is a sufficient statistics of X?\n\n- Conclusion, use comma instead of dash (or leave some spaces between dash and words)\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "B1giOunS0Q", "original": null, "number": 10, "cdate": 1542994034529, "ddate": null, "tcdate": 1542994034529, "tmdate": 1542994034529, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rkxb4LT-C7", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Comments after revision", "comment": "I appreciate the authors' thorough response to the concerns (of all reviewers).  Reading the discussions as well as the revised paper, I would like to offer the following comments.\n\n- The theoretical justification on mutual information adds value to the paper, although strictly speaking, the loss function is a GAN-style estimation of the mutual information, rather than the Jensen-Shannon estimation. See Table 2 and eqn (8) of https://arxiv.org/abs/1606.00709\n\n- My main problem remains that the authors separately design encoders (and to a lesser extent, the negative distributions) for different graphs. It is true that state-of-the-art architectures differ as the data sets vary, for the counterpart (semi)supervised setting. But the authors are not using the counterpart architectures, either. For example, for Cora, Citeseer, and Pubmed, the authors use a one-layer GCN. Then, questions naturally arise: Is such an encoder optimal? What if one replaces the one-layer GCN by the usual GCN, or the Planetoid architecture (note that these architectures are used as supervised baselines in the experiments)? Similar questions also arise for the data sets Reddit and PPI. My point is, what is missing in this work is a guidance regarding how should one choose the encoder (and the negative distribution) given a graph. The guidance is important for practitioners who want to apply the framework.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "HJeiYITZ07", "original": null, "number": 8, "cdate": 1542735491293, "ddate": null, "tcdate": 1542735491293, "tmdate": 1542735491293, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "BkeOgwpm37", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Reply to AnonReviewer2", "comment": "Thank you for the very careful review and kind words about our contributions.\n\nRegarding your comment about our information-theoretic contributions, please see our global comment on mutual information and the JSD. We reorganised Sections 3.2 / 3.3 to include more concrete theoretical motivation from mutual information maximisation, and separate this from the specifics that are important for implementation.\n\nYou make an exceptionally good point regarding our method\u2019s reliance on random walks - thanks! It is, in fact, our main claim that combining random-walk *objectives* with GCN-like encoders is potentially unsuitable (given that the GCN already encodes the \u201crandom-walk\u201d information within its structural inductive biases). We have appropriately modified our abstract to reflect this intention.\n\nTo answer your remaining questions:\n\n- The minibatch of 256 nodes for Reddit is randomly selected, and therefore the cost function does not rely on random-walk similarities in this case.\n\n- Regarding the averaging readout: this is a great point, and we expect that the performance on larger graphs will decrease somewhat, especially when using averaging as a readout function, since it is known that the quality of graph-level embeddings degrades as the number of nodes increases when using simple averaging. That said, we expect this problem could be alleviated by applying more sophisticated set2vec and/or pooling approaches for the readout function, and we mention this point in the revised paper---immediately after the averaging is introduced in Section 4.2. Moreover, in this particular case, the smaller performance improvement on the Reddit data is also simply due to the fact that most GCN approaches are already in the 90+% F1 range, and therefore there is limited room for improvement.\n\n- The \u201cDeepWalk+features\u201d baseline directly concatenates the two kinds of features, as was done in all prior work utilising this baseline (e.g. Hamilton et al., NIPS 2017).\n\n- We note that we specifically designed the model with node classification tasks in mind. However, in principle, the generated embeddings could be used for link prediction, as with node2vec embeddings, etc. That said, we expect that strong performance on link prediction could require minor modifications (e.g., tweaking the negative sampling function) and we plan to investigate this in future work.\n\n- Regarding your concern about high variance of the gradient, we haven\u2019t found any issues regarding learning stability---as long as an appropriate choice of learning rate is made.\n\nWe thank you once again for your review, which has definitely helped make our paper\u2019s contributions stronger!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "rkxb4LT-C7", "original": null, "number": 7, "cdate": 1542735400990, "ddate": null, "tcdate": 1542735400990, "tmdate": 1542735400990, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "B1lVxghvnm", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "Firstly, thanks so much for your thorough review!\n\nTowards your comment about Section 3.2 and Equation 1, please see our global comment on mutual information and the JSD. We reorganised Sections 3.2 / 3.3 to include more concrete theoretical motivation from mutual information maximisation, and separate this from the specifics that are important for implementation.\n\nWe agree with your comment that the GCN is in fact used in a semi-supervised setting (as not all nodes are labelled). What we referred to is that the learning objective is fully supervised (solely cross-entropy on the training nodes\u2019 labels), and have revised the paper accordingly.\n\nWe acknowledge your comment about our method having the traits of a framework, but claim that such features are a consequence of the current state of the art in graph neural networks, rather than any limitations of our methodology. Namely:\n\n- Similar to other recently proposed GCN methods, such as the DiffPool algorithm (Ying et al., NIPS 2018), we indeed are agnostic to the choice of the GCN layer. This is because graph convolutional networks are a very active area of research and we don't currently have a \u201ccatch-all\u201d layer for all possible scenarios (e.g. transductive vs. inductive).\n\n- The fact that different high-level architectures are used is normal, and constitutes hyperparameter optimisation and/or relating the work to previous successful architectures.\n\n- Our negative distribution choice is, in fact, mostly uniform. We\u2019d like to use different input graphs as negative examples (as DIM does), but this is only possible (with a limited pool of examples) for PPI. In all other case we use node-wise shuffling, which was demonstrably robust---and we also motivate this robustness with further studies in Appendix C.\n\n\nA randomly initialised graph convolutional network is basically the setting in which we set the number of training epochs to zero---i.e. we start with weights initialised according to Xavier initialisation, and then immediately proceed to use this as our encoder rather than performing any unsupervised training of the encoder.\n\nWe thank you once again for your review, which has definitely helped make our paper\u2019s contributions stronger!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "r1gu9BpZAX", "original": null, "number": 6, "cdate": 1542735247643, "ddate": null, "tcdate": 1542735247643, "tmdate": 1542735247643, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "HkgHlf-qnX", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "Thank you very much for the extremely kind review, and we are very glad you enjoyed the paper! We have made further updates to the paper---some details of which are outlined in our global comment above."}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "HyeIUHpZAm", "original": null, "number": 5, "cdate": 1542735182150, "ddate": null, "tcdate": 1542735182150, "tmdate": 1542735182150, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "[Revision] On mutual information estimation and maximization and the cross-entropy", "comment": "We agree that the discussion on the connections between mutual information and the Jensen-Shannon Divergence / binary cross entropy was insufficient in our paper, and we have added further details in our revision. \n\nWe have added this intuition as motivation to our loss function in Section 3.3.\n\nFor a brief overview:\n\nLet p(X, f(X)) be the joint distribution of the random variable X and f(X), a random variable corresponding to the transformation of X by a deterministic function, f. Also, let p(X) be an empirical distribution specified by a finite set of given samples, and let p(f(X)) be the marginal.\n\nIn our setting, we both train a classifier to distinguish between samples from p(X, f(X)) and from p(X) p(f(X))) and find the function f in F that minimizes the same classifier\u2019s loss. In other words, we are looking for the functions f* in F that satisfy \nf* = argmin_f argmin_c error(p(X, f(X)), p(X) p(f(X)); c), \nand we use the binary cross-entropy (BCE) as a proxy for the classification error.\n\nIt is enough to show that optimal solutions to the above only contain functions f that are invertible, as the mutual information MI(X; f(X)) is invariant over invertible functions of F, and maximized for them. Because f is deterministic, and considering a discrete X, card(X) >= card(f(X)), where with non-invertible functions, this ordering is strict ( i.e., card(X) > card(f(X))). Given a mixture between the joint and the product of marginals, it can be shown that the optimal f (under the classification error) has card(X) = card(f(X)). This and the fact that f is deterministic implies there exists an inverse. Hence the f* that minimizes the classification error between the joint and the product of marginals in this setting also maximizes the mutual information MI(X;f(X)) = MI(X; X) = H(X).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "HJeDwVp-AQ", "original": null, "number": 4, "cdate": 1542734942947, "ddate": null, "tcdate": 1542734942947, "tmdate": 1542734942947, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "Skei4zS7a7", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "On the link to Embedding Propagation (EP)", "comment": "Thank you for your comment pointing out similarities between our work and Embedding Propagation (EP). However, we believe that some of the similarities you mention might have been misattributed. To highlight:\n\nIn DGI, we not only add graph convolutions, we also employ a very different objective function, where the goal is to maximize the mutual information between single node embeddings and a summary embedding of the entire graph. Thus, there are two key points of clarification regarding the fundamental difference between our work and EP: \n\n1) the \u201creadout\u201d function in our work does not simply abstract away the neighborhood aggregation function in EP, since in our work the \u201creadout\u201d function embeds an entire graph; \n\n2) the objective in our work maximizes the mutual information between a single node embedding and the summary graph embedding, whereas in EP they use a \u201cone-step\u201d random walk objective, which maximizes the similarity between a node embedding and the embedding of its local neighborhood.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "B1eoGoSQpX", "original": null, "number": 2, "cdate": 1541786387130, "ddate": null, "tcdate": 1541786387130, "tmdate": 1541860015387, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "BygK5QSXp7", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "More recent results for PPI are reported", "comment": "Hello,\n\nThank you for your comment!\n\nAll reported improvements in PPI results concern solely the fully supervised setup; not the unsupervised one. And, indeed, this is the supervised result we report---namely, the avg. pooling architecture from the GaAN paper (Zhang et al., UAI 2018), which we report, is one example of a supervised result that substantially (30+%) improves on the supervised result reported in the original GraphSAGE paper (of 0.612)."}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "BygK5QSXp7", "original": null, "number": 4, "cdate": 1541784465391, "ddate": null, "tcdate": 1541784465391, "tmdate": 1541784465391, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Public_Comment", "content": {"comment": "It has already been noted that GraphSAGE models can achieve superior results (20-30% improvement) with a different hyperparameter setting on PPI than what was reported in the original paper. It is even mentioned in the Arxiv version of GraphSAGE. It is unfair to report such low scores and I also encourage the authors to update DGI's hyperparam setting correspondingly and report newer numbers.", "title": "Authors should quote more recent results for PPI dataset"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311822709, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rklz9iAcKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311822709}}}, {"id": "Skei4zS7a7", "original": null, "number": 3, "cdate": 1541784115329, "ddate": null, "tcdate": 1541784115329, "tmdate": 1541784115329, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Public_Comment", "content": {"comment": "The work has similarity to Embedding Propagation (Duran et al, NIPS 2017) work, where the node's embedding is made similar to averaged 1-hop neighbors. DGI seems to have extended this idea into a framework. Where the neighbor's summary obtained with an averaging function is abstracted and called as readout function. The embeddings/feature (projection) has been now obtained with a GCN encoder. \n\nThough, there is a mention of this work. I feel enough justice to the earlier work has not been given.", "title": "Needs more explanation on how this is different from Embedding Propagation work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311822709, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rklz9iAcKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311822709}}}, {"id": "BkeOgwpm37", "original": null, "number": 1, "cdate": 1540769520357, "ddate": null, "tcdate": 1540769520357, "tmdate": 1541533931003, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Review", "content": {"title": "Alternative information-theoretic objective for unsupervised graph representation learning", "review": "This paper adapts the Deep Informax (DIM; Hjelm et al. 2018) method, which was used on\nimage data, into the graph domain. The architecture of the neural network and\nthe learning cost function are given by figure 1 and eq.(1), respectively.\n\nThe idea is to maximize the mutual information between a local representation\n(of a \"patch\" defined by graph adjacency) and a global representation (of the entire graph),\nso those different local patches are encouraged to carry some shared\nglobal information.\n\nThis is in contrast to most unsupervised graph encoders, where the objective is\nto fit the random walk similarities (node adjacency on the graph).\n\nIn an unsupervised learning scenario, where the graph structure and node features\nare given, the authors achieved state-of-the-art performance on transductive and\ninductive node classification tasks, in some cases even better than supervised baselines. \n\nThe paper is well written. I recommend acceptance and have the following concerns.\n\nMain comment 1\n\nThe title suggests that there are some information theory contents. \nHowever, section 3 does not include much information theory.\nRather, the author(s) directly give eq.(1) with pointers to references and informal discussions.\nThis is not so helpful. It is not straightforward for\nthe reader to relate eq.(1) with the definition of mutual information.\nIdeally, before eq.(1) there should be one or two equations (with text)\nto introduce the Jesen-Shannon MI estimation and information theoretic bounds etc.\n\nOverall, due to this, the contribution is mainly on adapting the DIM method info the graph domain. Although the experimental results are good, there is not much theoretical insight or \"recreative\" introduction of the DIM method from the authors' perspectives. This is the main reason for that it is not a strong accept.\n\nMain comment 2\n\nA motivation of the proposition is to \"not rely on random walks\", or graph node adjacency.\nNotice that random walks can be intuitively regarded as higher order node adjacency.\nHowever, the encoder, which is based on GCN, does rely on the adjacency matrix,\nas the convolution is done in local neighborhoods (that can also be defined based on\nrandom-walk similarities). The authors are therefore suggested to make it\nclear in related places that, it is the cost function which is not based\non node adjacency, although the neural network structure does rely on it.\n\nAs a related question, in the inductive experiments, in the mini-batch of 256 nodes\nrandomly selected, or selected by a local patch of the graph which is connected or nearby?\nIf it is the latter case, the cost function does rely on random-walk similarities,\nas the summary vector will be a local patch average.\n\nQuestions:\n\n-The summary vector is the average of all node features. On large graphs, the\naverage may carry less information as compared to small graphs. It can be\nobserved that on Pubmed and Reddit, the performance improvement is not as\nhigh as the other small graphs. Could you comment on this?\n\n-In the baseline \"DeepWalk+features\", are the two different types of features directly concatenated?\n\n-Is it straightforward to apply DGI to link prediction tasks?\n\n-It that a concern that the random corruption function will cause a high variance of the gradient?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Review", "cdate": 1542234444184, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335741311, "tmdate": 1552335741311, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgHlf-qnX", "original": null, "number": 3, "cdate": 1541177836916, "ddate": null, "tcdate": 1541177836916, "tmdate": 1541533930756, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Review", "content": {"title": "Solid work, will have high impact", "review": "This paper describes an approach for unsupervised learning of node features on a graph (with known structure), so that learned local representations represent community information that has high mutual info with a graph-level summary. The general idea is they apply InfoMax to graphs via graph convolutional networks (GCN), and report impressive results, including rivaling supervised learning methods for node classification. The 3 experiments are on paper topic classification, social network modeling, and protein classification.\n\nThe idea of using InfoMax with GCNs for unsupervised node learning is clever and timely, the technical contribution is solid, the experiments are executed well, and the paper is clear and easy to read.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Review", "cdate": 1542234444184, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335741311, "tmdate": 1552335741311, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lVxghvnm", "original": null, "number": 2, "cdate": 1541025772383, "ddate": null, "tcdate": 1541025772383, "tmdate": 1541533930553, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Review", "content": {"title": "Idea is interesting; realization is graph-specific", "review": "This paper proposes an unsupervised approach to learning node representations. The basic steps are: (1) use an encoder E to learn node vectors, (2) use a readout function R to summarize node vectors into the graph vector, (3) use a scoring function D to score how much the node vectors are aligned with the graph vector, and (4) maximize the scores for the given graph meanwhile minimize the those from the negative distribution.\n\nI feel that the idea is interesting; however, the paper is less well written and the realization of the idea has drawbacks as well.\n\n1. Presentation of Section 3.2 can be improved. The proposed approach becomes clear only toward the end.\n\n2. Naming and wording is misleading. The title and the whole paper use the wording \"mutual information\", whereas in reality, the loss function is a cross entropy.\n\n3. In equation (1), it is unclear why the authors take expectation with respect to the distribution of graphs before summing the scores for one particular graph. Should the order of the expectation and summation be swapped?\n\n4. The proposal is more like a framework than a specific method. The encoder and the negative distribution need to be separately designed for different graphs.\n\nGood things about the proposal:\n\n5. The downstream classification results are quite comparable to those of supervised methods (except for the PPI data).\n\n6. The learned node representations possess a clear clustering structure (Figure 3).\n\nMinor comments:\n\n7. In the third paragraph of section 4.3, the authors state that \"... for the GCN model in the fully supervised setting\". GCN should be a semi-supervised method rather than a fully-supervised one.\n\n8. In the last paragraph of section 4.3, what is a \"randomly initialized graph convolutional network\" and how is it different from the proposal?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Review", "cdate": 1542234444184, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335741311, "tmdate": 1552335741311, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkxB37p2t7", "original": null, "number": 1, "cdate": 1538212781023, "ddate": null, "tcdate": 1538212781023, "tmdate": 1539878946064, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rJl0fT6itQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "content": {"title": "Provided results", "comment": "Hi Ming,\n\nThanks for your comment and kind feedback!\n\nDGI performs unsupervised learning, so comparisons with supervised methods are inappropriate---the supervised methods we did include were the ones that used similar propagation rules as our encoders. In the transductive case, this was the GCN---as it uses an identical propagation rule (with one extra layer).\n\nWe'll cite GraphSGAN as an indicator of the current supervised state-of-the-art in an updated version of the paper -- thanks!"}, "signatures": ["ICLR.cc/2019/Conference/Paper513/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606939, "tddate": null, "super": null, "final": null, "reply": {"forum": "rklz9iAcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper513/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper513/Authors|ICLR.cc/2019/Conference/Paper513/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606939}}}, {"id": "rJl0fT6itQ", "original": null, "number": 1, "cdate": 1538149653896, "ddate": null, "tcdate": 1538149653896, "tmdate": 1539775952846, "tddate": null, "forum": "rklz9iAcKQ", "replyto": "rklz9iAcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper513/Public_Comment", "content": {"comment": "I think the idea is very interesting. But in the experiments under transductive semi-supervised learning setting, some methods with better performance are missed, for example GAT(Velickovic et al., 2018) and even our GraphSGAN(Ding et al., 2018). I know that this method is actually inductive but you should at least cite and list above results.", "title": "The experiments about transductive learning are incomplete"}, "signatures": ["~Ming_Ding1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper513/Reviewers/Unsubmitted"], "writers": ["~Ming_Ding1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Graph Infomax", "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "keywords": ["Unsupervised Learning", "Graph Neural Networks", "Graph Convolutions", "Mutual Information", "Infomax", "Deep Learning"], "authorids": ["petar.velickovic@cst.cam.ac.uk", "liam.fedus@gmail.com", "wleif@stanford.edu", "pietro.lio@cst.cam.ac.uk", "yoshua.umontreal@gmail.com", "devon.hjelm@microsoft.com"], "authors": ["Petar Veli\u010dkovi\u0107", "William Fedus", "William L. Hamilton", "Pietro Li\u00f2", "Yoshua Bengio", "R Devon Hjelm"], "TL;DR": "A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.", "pdf": "/pdf/67df6b5ffbf0ef252ee5f21442c63f5a1bab1023.pdf", "paperhash": "velikovi|deep_graph_infomax", "_bibtex": "@inproceedings{\nveli\u010dkovi\u01072018deep,\ntitle={Deep Graph Infomax},\nauthor={Petar Veli\u010dkovi\u0107 and William Fedus and William L. Hamilton and Pietro Li\u00f2 and Yoshua Bengio and R Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rklz9iAcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper513/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311822709, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rklz9iAcKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper513/Authors", "ICLR.cc/2019/Conference/Paper513/Reviewers", "ICLR.cc/2019/Conference/Paper513/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311822709}}}], "count": 20}