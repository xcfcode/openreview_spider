{"notes": [{"id": "B1x6BTEKwr", "original": "SyxgvVPDDB", "number": 540, "cdate": 1569439044940, "ddate": null, "tcdate": 1569439044940, "tmdate": 1585283433413, "tddate": null, "forum": "B1x6BTEKwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 24, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "8VS3NxKUPj", "original": null, "number": 1, "cdate": 1576798699224, "ddate": null, "tcdate": 1576798699224, "tmdate": 1576800936619, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Quoting R3: \"This paper studies the theoretical property of neural network's loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima.\"\n\nThere were split reviews, with two reviewers recommending acceptance and one recommending rejection.  During a robust rebuttal and discussion phase, both R2 and R3's appreciation for the work was strengthened.  The authors also provided a robust response to R1, whose main concerns included (i) that the paper's analysis is limited to piecewise linear activation functions, (ii) technical questions about the difficulty of proving theorem 2, which appear to have been answered in the discussion, and (iii) concerns about the strength of the language employed.\n\nOn the balance, the reviewers were positively impressed with the relevance of the theoretical study and its contributions.  Genuine shortcomings and misunderstandings were systematically resolved during the rebuttal process.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715211, "tmdate": 1576800265071, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper540/-/Decision"}}}, {"id": "B1e6XZRcsB", "original": null, "number": 20, "cdate": 1573736740922, "ddate": null, "tcdate": 1573736740922, "tmdate": 1573736740922, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "r1xCmsqKjH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Re: Some questions remain; \"big picture\" somewhat oversold (Part 3 of 3) ", "comment": "Q2.3: Mathematically speaking, I agree there is a difference between deep and shallow. And I do appreciate the effort to really prove it. But the explanation is not for the technical nontriviality of proving \"deep\" instead of shallow.\n\nA2.3: Thank you for the recognization of our effort. We would love to clarify that the technical nontriviality mainly comes from the difficulty of the constructions to force the flow to go through the same linear parts of the activations after the first hidden layer.\n\n\n\nOther discussion:\n\n\n\nQ3.1: For A3(c): \"empirical risk  is convex with respect to a variable mapped from the weights\". \"the local optimality is maintained under the constructed mapping\"\n\nThis is saying \"convexity is kept after mapping\" means \"analogous convexity\". This already exists in the original linear network, and this property was mentioned in, e.g., 1702.08580.  I still think calling \"analogous convexity\" is somewhat misleading. Why just say \"every local min is global min\"?\n\nA3.1: Thanks and revised accordingly. We have updated our manuscript to use \"every local minimum is a global minimum within a cell\".\n\nBesides, the proofs for linear networks cannot be transplanted here. Instead, we develop a novel and non-trivial approach (please see A1.1 and A1.2 for more detail).\n\n\n\nQ3.2: \"we respectfully argue that \u201cnonlinearity\u201d is more common to refer to the difference from linear functions, which has been also used by Yun et al. (2019b).\"\n\nI have to say it is not common. Yun et al. (2019b) analyzed a much bigger class of neurons than this paper. This paper just analyzed piecewise linear. \n\nIn a field with overstatements, I would suggest using more precise title, like \"the loss surface of neural networks with piecewise linear activations\".\n\nA3.2: Thanks and revised accordingly. We have updated our manuscript to use \u201cPiecewise linear activations substantially shape the loss surface of neural network\u201d."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "SJg4hxCcoH", "original": null, "number": 19, "cdate": 1573736619907, "ddate": null, "tcdate": 1573736619907, "tmdate": 1573736619907, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "r1xCmsqKjH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Re: Some questions remain; \"big picture\" somewhat oversold (Part 2 of 3)", "comment": "Q1.3: Overall, I think Theorem 2 exaggerates the contribution of the big picture. The four things are highlighted in the abstract and sounds like a major contribution. But two are trivial; \"every local minimum is good\" in a cell is expected to researchers in this area. The proof is kind of simple, compared to the whole paper, but occupied large space of \"conceptual contribution\".\n\nA1.3: Thanks. We have revised the paper accordingly. The updated abstract is given below.\n\nUnderstanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that the loss surfaces of many neural networks have infinite spurious local minima, which are defined as the local minima with higher empirical risks than the global minima. Our result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice with some mild assumptions. This result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. Essentially, the underlying assumptions for the above result are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear open cells by nondifferentiable boundaries. The constructed spurious local minima are exactly connected with each other in one cell by a continuous path, on which the empirical risk is invariant. We further prove that within every cell of a one-hidden-layer network, local minima are equally good, and also, they are all global minima in the cell.\n\n\n\nQ1b): Another point of exaggeration: the abstract first says \"for any neural network with arbitrary depth\". And later talks about \"big picture\". Only at the end of abstract mentions \"1-hidden layer\" for the second property --which is the most non-trivial one.  I think this might be misleading; readers would think the whole paper is about deep-net, and only a \"minor result\" is for 1-hidden layer. I did not check the paper again, but this needs to be clarified in the paper as well.\n\nA1b): Thanks and revised accordingly. The abstract in the final version has been updated to avoid misunderstanding. We have also relocated the restriction just besides the second property (please refers to A1.3).\n\nMoreover, the restriction of \u201cone hidden layer\u201d for Theorem 2 was stated in the original submission. For more details, please see (1) the paragraph for \u201cEvery local minimum is globally minimal within a cell\u201d in the introduction on p. 2, (2) Theorem 2 on p. 7, and (3) the conclusion on p. 9.\n\n\n\nQ2.1: As for extension. \"training neural networks is increasingly difficult when the networks turn deeper\". This sentence seems to suggest that depth will make problem more difficult, so proving positive result is more difficult. This is not an argument for generating negative result. \n\nA2.1: We respectfully argue that the negative result for one-hidden-layer neural networks does not imply that deep neural networks have spurious local minima. It is correct to use one counterexample to prove a proposition is wrong. However, such a counterexample cannot be straightforwardly applied to a different condition. Large amounts of empirical evidence supports that the loss surface of deep networks could be substantially different from the shallow counterpart. Therefore, it is neither natural nor trivial to show deep networks can inherit a specific property of shallow networks. \n\nBesides, we respectfully note that the full context here is \u201cEmpirical results have overwhelmingly suggested that the increase of the depth of neural networks may substantially improve the performance. Additionally, training neural networks is increasingly difficult when the networks turn deeper.\u201d These two sentences collectively explain the situations of shallow and deep neural networks could be substantially different.\n\n\n\nQ2.2: Thanks for explaining \"Forcing the flow\". The idea is natural (not meaning it is not good), as for piecewise linear one needs to utilize the linear part to create bad local minima. This is why [R1] is nontrivial: since sub-optimal local minima can exist for ReLU due to piecewise linear nature, thus it tried to show such bad cases are rare.\n\nA2.2: We appreciate the significant contributions made by [R1] and will duly cite [R1] in the final version. However, we would love to note that [R1] and this paper support each other to be more convincing, but not undermine the merit of the other side. For details, please refer to A1.2."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "BylzHCT5sS", "original": null, "number": 18, "cdate": 1573735994004, "ddate": null, "tcdate": 1573735994004, "tmdate": 1573736024925, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "r1xCmsqKjH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Re: Some questions remain; \"big picture\" somewhat oversold (Part 1 of 3)", "comment": "Thank you very much for your constructive feedback. All your concerns have been carefully addressed. We will update the final version accordingly. We hope our detailed responses can fully answer your questions and the merit of this paper can be reevaluated. Much appreciated.\n\n\n\nQ1.1: About Theorem 2. I agree that there is a small difference between the conventional linear network and the linear network within each cell. However, due to linearity, it seems just a simple extension. There are quite a few proofs for the linear network. The 2-step proof is used before for analyzing linear networks, e.g. 1702.08580. As for the first step of convexity, it is still a linear transformation of W^hat, thus it is convex (this is essentially the argument from (72) to (78)).  The novelty of this proof is not clear.\n\nA1.1: We respectfully argue that all the existing proofs for linear networks cannot be transplanted here because the geometry within each cell of a nonlinear network is much more complicated than that of the loss surface of any linear network. We provide below a simple example to explain this in detail.\n\nWe consider a simple one-hidden-layer neural network, in which:\n\n(1) the output $h(W_1 X)$ of the hidden layer is a $2 \\times 2$ matrix with elements defined by $h(c_{1, 1})$, $h(c_{1, 2})$, $h(c_{2, 1})$, and $h(c_{2, 2})$, and\n\n(2) the output $W_2 h(W_1 X)$ of the network is another $2 \\times 2$ matrix, with elements defined by $W_2 h(c_{1, 1})$, $W_2 h(c_{1, 2})$, $W_2 h(c_{2, 1})$, and $W_2 h(c_{2, 2})$.\n\nFor a linear activation function $h$ defined by the slope $a$, the output of the hidden layer $h(c_{i, j})$ is $a c_{i, j}$. This means we can use the real number $a$ to express the effect of the activation function $h$, i.e., $h(W_1 X) = a W_1 X$. Therefore, a linear network is a linear model $W\u2019 X$ with respect to the input $X$, where $W\u2019 = W_2 a W_1 X = a W_1 W_2 X$ is the product of a sequence of weight matrices and real numbers. This is the foundation to complete the proof of the existence of spurious local minima for linear networks.\n\nBy contrast, for a piecewise linear activation function $h$ with two slopes $a_1$ and $a_2$, the output of the hidden layer $h(c_{i, j})$ is either $a_1 c_{i, j}$ or $ a_2 c_{i, j}$. This means we can use neither a single real number nor a linear operator to express the effect of the activation $h$ in most cells (in some cell, the output $h(c_{i, j})$ of the hidden layer happens to be $a_1 c_{i, j}$ or $ a_2 c_{i, j}$ for all $(i, j)$). Therefore, all the existing proof techniques for linear networks do not apply here.\n\nThe novelty of this proof is two-fold: (1) in Step 1, we construct a mapping from the weight $W$ to the variable $\\hat W$ (it is the foundation to discuss the convexity of empirical risk $\\hat{\\mathcal R}$ with respect to the variable $\\hat W$); and (2) in Step 2, we prove that the local optimality is retained under the constructed mapping. Although the proof of the convexity in Step 1 is simple and straightforward (due to the linearity of the empirical risk $\\hat{\\mathcal R}$ with respect to $\\hat W$), it is a small portion of the proof of Theorem 2. Therefore, we respectfully argue that the proof for Theorem 2 in general is novel and technically non-trivial. \n\n\n\nQ1.2:  The claim \"our result is stronger and more general than [R1]\" is questionable. This result is stronger in some aspects. But [R1] provides a deeper characterization by computing a certain probability. The proof is much longer than the 2-page proof here. And it is related to hyperplane geometry, not just longer. Again, I agree they are different, but the insight conveyed by [R1] is more nontrivial.\n\nA1.2: Thanks. We agree to remove the claim \u201cour result is stronger and more general than [R1]\u201d and will acknowledge the significant contributions presented in [R1]. \n\nWe appreciate [R1] that helps the community understand deep learning from the perspective of the volume of the spurious local minima, which is demanded. Specifically, [R1] proves that measured by the volume, the ratio of spurious/suboptimal local minima in one-hidden-layer networks approaches $0$ when the training sample size $N$ goes to infinity.\n\nBy contrast, this paper shows that (1) neural networks of arbitrary depth have infinite spurious local minima; and (2) all local minima within a cell are globally minimal. We would love to note that [R1] and this paper support each other to be more convincing, but not undermine the merit of the other side."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "r1xCmsqKjH", "original": null, "number": 17, "cdate": 1573657382225, "ddate": null, "tcdate": 1573657382225, "tmdate": 1573657382225, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "Byg9OHRfjS", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Some questions remain; \"big picture\" somewhat oversold", "comment": "Thanks for the explanation. I appreciate the efforts.   (I modified a previous response)\n\n1) About Theorem 2.  I agree that there is a small difference between the conventional linear network and the linear network within each cell. \n   However, due to linearity, it seems just a simple extension. There are quite a few proofs for the linear network. The 2-step proof is used before for analyzing linear networks, e.g. 1702.08580.  As for the first step of convexity, it is still a linear transformation of W^hat, thus it is convex (this is essentially the argument from (72) to (78)).  The novelty of this proof is not clear.\n      The claim \"our result is stronger and more general than [R1]\" is questionable. This result is stronger in some aspects. But [R1] provides a deeper characterization by computing a certain probability. The proof is much longer than the 2-page proof here. And it is related to hyperplane geometry, not just longer. Again, I agree they are different, but the insight conveyed by [R1] is more nontrivial. \n    Overall, I think Theorem 2 exaggerates the contribution of the big picture. The four things are highlighted in the abstract and sounds like a major contribution. But two are trivial; \"every local minimum is good\" in a cell is expected to researchers in this area. The proof is kind of simple, compared to the whole paper, but occupied large space of \"conceptual contribution\".\n\n1b) Another point of exaggeration: the abstract first says \"for any neural network with arbitrary depth\". And later talks about \"big picture\". Only at the end of abstract mentions \"1-hidden layer\" for the second property --which is the most non-trivial one.  I think this might be misleading; readers would think the whole paper is about deep-net, and only a \"minor result\" is for 1-hidden layer.  \n   I did not check the paper again, but this needs to be clarified in the paper as well. \n\n2) As for extension. \" training neural networks is increasingly difficult when the networks turn deeper\". This sentence seems to suggest that depth will make problem more difficult, so proving positive result is more difficult. This is not an argument for generating negative result.   \n    Thanks for explaining \"Forcing the flow\". The idea is natural (not meaning it is not good), as for piecewise linear one needs to utilize the linear part to create bad local minima. This is why [R1] is nontrivial: since sub-optimal local minima can exist for ReLU due to piecewise linear nature, thus it tried to show such bad cases are rare.  \n     Mathematically speaking, I agree there is a difference between deep and shallow. And I do appreciate the effort to really prove it. But the explanation is not for the technical nontriviality of proving \"deep\" instead of shallow. \n \nOther discussion: \n1) For A3(c): \"empirical risk  is convex with respect to a variable mapped from the weights\". \"the local optimality is maintained under the constructed mapping\"\n   This is saying \"convexity is kept after mapping\" means \"analogous convexity\". This already exists in the original linear network, and this property was mentioned in, e.g., 1702.08580.  I still think calling \"analogous convexity\" is somewhat misleading. Why just say \"every local min is global min\"? \n\n2) \"we respectfully argue that \u201cnonlinearity\u201d is more common to refer to the difference from linear functions, which has been also used by Yun et al. (2019b).\"\n    I have to say it is not common. Yun et al. (2019b) analyzed a much bigger class of neurons than this paper. This paper just analyzed piecewise linear. \n    In a field with overstatements, I would suggest using more precise title, like \"the loss surface of neural networks with piecewise linear activations\". \n    "}, "signatures": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "rkxc9bmFjB", "original": null, "number": 15, "cdate": 1573626257579, "ddate": null, "tcdate": 1573626257579, "tmdate": 1573626257579, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "S1eHva6_ir", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Thank you!", "comment": "Thank you very much for your support!"}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "S1eHva6_ir", "original": null, "number": 14, "cdate": 1573604701183, "ddate": null, "tcdate": 1573604701183, "tmdate": 1573604701183, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "B1gUH9Rfor", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Official Blind Review #2", "comment": "Thank you for the response. After reading the material a second time and considering the amount of time the authors spent on developing the theory, I'm willing to upgrade the score for this paper and bring this study to more readers in the field."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "Bkl1dQB2YH", "original": null, "number": 1, "cdate": 1571734374548, "ddate": null, "tcdate": 1571734374548, "tmdate": 1573603334252, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #2", "review": "This paper focus on how activation functions\u2019 nonlinearities shape the loss surface of neural networks. The authors first show why the loss surface of every neural network has infinite spurious local minima. Secondly, the authors prove one theorem to show four properties of the loss surfaces of nonlinear neural networks.\nAlthough this paper is generally easy to follow, and the motivation about nonlinearities and the loss surface is clear, the insight of this paper is somehow shortcoming. Though this work can prove such properties within different preconditions, whether other works\u2019 conditions are inconvenient or not may remain further discussions. This work is established based on several preconditions, while it is hard to assert that most kinds of neural networks can satisfy them perfectly. For instance, this work mentions \u201cDeep learning without poor local minima (NeurIPS2016)\u201d, which requires full-rank and conditional independence of each node. It could be feasible when training a stacked network with particular limitations. This work requires all hidden layers are wider than the output layer, which may not be suitable for image segmentation, generative tasks or super-resolution, etc. Besides, it is laudable to prove fundamental rules in neural networks, while showing or inspiring researchers about how to implement or approximate such results to improve neural networks might be more helpful.\nSome questions:\n\n1. The authors assert that \u201cthe loss surface of *every* neural network has infinite spurious local minima\u201d in the abstract, while in chapter 3 line 2, authors mention, \u201cWe find that *almost all* practical neural networks have infinitely many spurious local minima.\u201d Which one is correct? Based on the following description in this paper, I guess this result is conditionally tenable.\n\n2. In lemma 3, authors construct the local minima by adding very negative biases and show they are spurious. However, it is less likely to learn such negative biases in the real case. Besides, some networks require biases equal to zero to achieve some specific target. My question is: if biases are conditioned on real-world data distribution, will lemma 3 and 4 still work in this case?\n\n3. This paper mentions \u201cinfinite\u201d many times. Based on the reference, I believe that the \u201cneural network\u201d in this work refers to the \u201cartificial neural network,\u201d which is majorly stored within float tensors. So the number of combinations of parameters is finite. So why use \u201cinfinite\u201d instead of \u201cmany\u201d? Finite means I can train a small scale of networks with fewer precisions and check the global minima with a fixed dataset.\n\nAll in all, I believe this paper can be significantly improved if more details and experiments are provided.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575902028717, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper540/Reviewers"], "noninvitees": [], "tcdate": 1570237750659, "tmdate": 1575902028732, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Review"}}}, {"id": "HkeojKmBjS", "original": null, "number": 13, "cdate": 1573366178986, "ddate": null, "tcdate": 1573366178986, "tmdate": 1573366178986, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "ByxK47GSsH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Thank you!", "comment": "Thank you very much for your support!"}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "ByxK47GSsH", "original": null, "number": 12, "cdate": 1573360432866, "ddate": null, "tcdate": 1573360432866, "tmdate": 1573360432866, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "rklHAmAVsB", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Re: Clarification on cells ", "comment": "Thank you for your responses! My concerns are carefully addressed. I will keep my score and discuss with other reviewers in favor of the acceptance. "}, "signatures": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "B1gUH9Rfor", "original": null, "number": 6, "cdate": 1573214782444, "ddate": null, "tcdate": 1573214782444, "tmdate": 1573352347520, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "Bkl1dQB2YH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We appreciate your thorough review and constructive comments. All your concerns have been duly addressed below. We have also updated the final version accordingly. We sincerely hope that you can take into account the response we have made and reevaluate the merit of this paper. \n\n\n\nQ1: The authors assert that \u201cthe loss surface of *every* neural network has infinite spurious local minima\u201d in the abstract, while in chapter 3 line 2, authors mention, \u201cWe find that *almost all* practical neural networks have infinitely many spurious local minima.\u201d Which one is correct? Based on the following description in this paper, I guess this result is conditionally tenable.\n\nA1: Thanks and revised accordingly. In the final version, we have added the following description.\n\nWe first prove that the loss surfaces of many neural networks have infinite spurious local minima, which are defined as the local minima with higher empirical risks than the global minima. Our result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under many popular loss functions in practice with some mild assumptions.\n\n\n\nQ2: In lemma 3, authors construct the local minima by adding very negative biases and show they are spurious. However, it is less likely to learn such negative biases in the real case. Besides, some networks require biases equal to zero to achieve some specific target. My question is: if biases are conditioned on real-world data distribution, will lemma 3 and 4 still work in this case?\n\nA2: We respectfully argue that the construction of negative bias does not undermine the generality of the obtained results.\n\nUnder a strong restriction that all activations are linear functions, Kawaguchi (2016), Zhou & Liang (2018), and Lu & Kawaguchi (2017) showed that all local minima are global minima, which accounts for the success of deep learning. However, it has been well observed and acknowledged that SGD can converge to points with large training errors, which are apparently not globally optimal. This phenomenon motivates us to study the existence of spurious local minima by relaxing this strong restriction.\n\nTheorem 1 of this paper (based on Lemmas 3 and 4) exactly constructs spurious local minima on the loss surface of a nonlinear neural network (with an arbitrary depth, a differentiable loss and an arbitrary-dimensional output). This counterexample proves that the existing theoretical results cannot be applied to nonlinear networks. Constructing counterexamples is a widely used approach to prove a proposition is wrong. Therefore, our construction does not undermine the generality.\n\n\n\nQ3: This paper mentions \u201cinfinite\u201d many times. Based on the reference, I believe that the \u201cneural network\u201d in this work refers to the \u201cartificial neural network,\u201d which is majorly stored within float tensors. So the number of combinations of parameters is finite. So why use \u201cinfinite\u201d instead of \u201cmany\u201d? Finite means I can train a small scale of networks with fewer precisions and check the global minima with a fixed dataset.\n\nA3: We respectfully argue that it is common yet mild to treat the parameters of neural networks as continuous numbers for theoretical studies, which has been widely used in related studies. Moreover, the constructed local minima are connected with each other by a continuous path, on which every point has the same empirical risk. Therefore, it is impractical to check all the constructed local minima even when they are represented by float tensors, because the number of float tensors on a continuous path is extremely large. For example, there are $2^{52} = 4.5 \\times 10^{15}$ $64$-bit float values between $1$ and $2$ when using the double precision.\n\n\n\nReference\n\nKenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, 2016.\n\nHaihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017. \n\nYi Zhou and Yingbin Liang. Critical points of neural networks: Analytical forms and landscape properties. In International Conference on Learning Representations, 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "rklHAmAVsB", "original": null, "number": 11, "cdate": 1573344204827, "ddate": null, "tcdate": 1573344204827, "tmdate": 1573344204827, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "S1lXLEcNsr", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": " Re: Clarification on cells", "comment": "Thanks for your reply! Yes, you are right. The configuration matrix $A$ changes the game. It makes the geometry within a cell more complicated than the loss surface of linear networks.\n\nIn addition, the current proof rigorously holds for two-layer cases. The two-step strategy is feasible for deep neural networks: (1) we prove that within every cell, the empirical risk $\\hat R$ is convex with respect to a variable $\\hat W$ mapped from the weights $W$; and (2) we further prove that the local optimality is retained under the constructed mapping. However, we need a new concrete construction of the mapping from the weight $W$ to the variable $\\hat W$ for deep neural networks."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "S1lXLEcNsr", "original": null, "number": 10, "cdate": 1573327947367, "ddate": null, "tcdate": 1573327947367, "tmdate": 1573327947367, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "Skx7pZyVjS", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Re: Clarification on cells ", "comment": "Thanks for the clarification. I see the difference now: the linear network has coordinate-wise activation function, but when dealing with the specific configuration, the activation is no longer coordinate-wise and a matrix factor A kicks in, which breaks down the multiplicative \"chain\" structure and makes the analysis non-trivial. I have quickly checked the proof of this part, it seems like the proof only considers two layer cases? Will it be similar to deal with multiple A in the deep cases?"}, "signatures": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "S1xg7CLEiH", "original": null, "number": 9, "cdate": 1573314071803, "ddate": null, "tcdate": 1573314071803, "tmdate": 1573314071803, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "SklDLOHXiS", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "We will discuss the relationship and cite your paper", "comment": "Hi Micah,\n\nThank you very much for bringing your interesting paper to our attention. We acknowledge the similarities shared by our papers. The final version will duly cite your paper. However, there are also substantial differences regarding the relevant parts. We will discuss the relationship between our papers below and the final version will be revised accordingly.\n\nTheorem 1 in [1] constructs some local minima of (nonlinear) multi-layer perceptrons (MLPs) that represent the hypotheses represented by the local minima of linear models. However, [1] does not rigorously prove that these constructed local minima are spurious/suboptimal. By contrast, Theorem 1 in our paper first constructs local minima of nonlinear networks based on linear networks and then proves that they are spurious/suboptimal.\n\nBesides, we really appreciate your experimental results presented in [1]. The result clearly demonstrates that MLPs have spurious/suboptimal local minima, which empirically justifies our theoretical findings.\n\nKind regards,\nThe authors\n\n[1] Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, Tom Goldstein. Truth or backpropaganda? An empirical investigation of deep learning theory. arXiv preprint arXiv:1910.00359, 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "Skx7pZyVjS", "original": null, "number": 8, "cdate": 1573282234652, "ddate": null, "tcdate": 1573282234652, "tmdate": 1573282877627, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "SJlQN4Z7sH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Re: Clarification on cells", "comment": "Thank you very much for your quick reply! We will explain it below. The final version has also been revised accordingly.\n\nPlease let us define some notions first. Suppose $WX$ is a $2 \\times 2$ matrix, whose elements are $c_{1, 1}$, $c_{1, 2}$, $c_{2, 1}$, and $c_{2, 2}$. Then, the effect of the activation $h$ is equivalently multiplying either $a_1$ or $a_2$ to the four elements. For the brevity, let\u2019s express the configuration by a matrix $A$ that collects the multiplied factors. Specifically, if the effect of the activation $h$ on $c_{i, j}$ is multiplying $a_1$, the element $A_{i, j}$ is defined to be $a_1$.\n\nWe agree that a cell is defined with respect to a specific configuration of the activation function. In other words, the matrix $A$ is invariant within a cell. Specifically, when the weight matrix $W$ changes but is still in the cell, all elements $c_{i, j}$ of the matrix $WX$ changes small enough such that the configuration matrix $A$ remains invariant. By contrast, if the weight matrix $W$ crosses the boundaries between two cells, the configuration matrix $A$ changes. Therefore, the interior of a cell is multilinear and smooth, but the boundaries are non-differentiable.\n\nHowever, it is not contradictory to our explanation. Within a fixed cell, the elements of matrix $A$ are not necessarily equal. For example, the elements $A_{1, 1}$, $A_{1, 2}$, $A_{2, 1}$, and $A_{2, 2}$ can be $a_1$, $a_1$, $a_2$, and $a_1$, respectively. Therefore, the effect of the activation $h$ is not equivalently multiplying a single constant (or a single matrix) to the matrix $WX$, which is however the foundation of the proofs for linear networks. Thus, we cannot transplant the case of linear networks here."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "SklDLOHXiS", "original": null, "number": 1, "cdate": 1573242959072, "ddate": null, "tcdate": 1573242959072, "tmdate": 1573242959072, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Public_Comment", "content": {"title": "An Interesting Connection", "comment": "Hi Authors,\nThank you for your interesting paper.  I noticed that your theoretical result concerning the existence of suboptimal local minima is very similar to Theorem 1 in our work [1], which also contains an empirical analysis of suboptimal local minima.  Please consider mentioning the relationship with our work in your next version.  We will cite your work in our next version.\n\n[1] https://arxiv.org/abs/1910.00359"}, "signatures": ["~Micah_Goldblum1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Micah_Goldblum1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504207666, "tmdate": 1576860590587, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Public_Comment"}}}, {"id": "SJlQN4Z7sH", "original": null, "number": 7, "cdate": 1573225515254, "ddate": null, "tcdate": 1573225515254, "tmdate": 1573228200015, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "Syeg-vRziH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Clarification on cells", "comment": "Thank you for the detailed clarification! \nIn my mind, a cell is defined with respect to a specific configurations on the activation function, for example all uses $a_`1$ or half $a_1$ half $a_2$, in which case it becomes multilinear. This seems contradictory to your explanation. A possible reason is that the notion of cell in the paper may be different from this naive definition, in which case, could you provide a  more precise example on what a cell should look like (in a simple case)? That would be helpful for better understanding. \n "}, "signatures": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "Syeg-vRziH", "original": null, "number": 5, "cdate": 1573213944133, "ddate": null, "tcdate": 1573213944133, "tmdate": 1573213944133, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "B1g3dnrRKH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (Part 2 of 2)", "comment": "Q3: Moreover, when considering the local convexity, is it sufficient to say that locally in each cell it is a linear network and then the results on linear network transfers to it locally?\n\nA3: Thanks. It is not sufficient to say that. We have added a detailed explanation to the final version. The significance of our proof is justified below.\n\nTechnically, linear networks can be expressed by the product of a sequence of weight matrices, which guarantees good geometrical properties. Specifically, the effect of every linear activation function is just equivalently multiplying a real constant to the output. However, the loss surface within a cell of a nonlinear neural network does not have this property.\n\nWe take a one-hidden-layer network for example. Suppose the output of the hidden layer is $h(WX))$, where $X$ is the data matrix, $h$ is the activation function, and $W$ is the weight matrix. If $h$ is a linear function $h(x) = ax$, its effect is equivalently multiplying the constant a to every element of the matrix $WX$. However, when the activation h is a two-piece linear function which has slopes $\\{a_1, a_2\\}$, different elements in the matrix $WX$ can be multiplied by either one from $\\{a_1, a_2\\}$. Therefore, we cannot use a single constant to express the effect of this activation, and thus, even within the cell, a nonlinear network cannot be expressed as the product of a sequence of weight matrices. This difference ensures that the proofs of deep linear neural networks cannot be transplanted here.\n\nTo address this issue, we develop a non-trivial approach to prove that all local minima in a cell are globally optimal within the cell. Specifically, we prove this in two steps: (1) we prove that within every cell, the empirical risk $\\hat{\\mathcal R}$ is convex with respect to a variable $\\hat W$ mapped from the weights $W$. Therefore, the local minima with respect to the variable $\\hat W$ are also the global minima in the cell; and then (2) we prove that the local optimality is maintained under the constructed mapping. Specifically, the local minima of the empirical risk $\\hat{\\mathcal R}$ with respect to the parameter $W$ are also the local minima with respect to the corresponding variable $\\hat W$."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "HkgQJvRMoS", "original": null, "number": 4, "cdate": 1573213915033, "ddate": null, "tcdate": 1573213915033, "tmdate": 1573213915033, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "B1g3dnrRKH", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (Part 1 of 2)", "comment": "We appreciate your thorough review and constructive comments. Thank you very much for your kind support. All your concerns have been duly addressed below. We have also updated the final version accordingly.\n\n\n\nQ1: Please be more precise in the abstract that the activation function needs to be piecewise linear. Moreover, if the activation is differentiable, is the claim still hold?\n\nA1: Thanks and revised accordingly. We have stated in the abstract that we proved the cases of piecewise linear activation functions. In addition, the results have not been extended to differentiable activations.\n\n\n\nQ2: How different is the analysis comparing to existing result with [1]? Could you summarize the main difficulty to extend their results to multi-layer cases?\n\nA2: Thanks and revised accordingly. A detailed comparison of the analysis has been added to the final version. We summarise it as follows. \n\nWe first acknowledge that [1] and our paper both employ the following strategy: (a) construct a series of local minima based on a linear classifier; and (b) construct a new point with smaller empirical risk and by this way we prove that the constructed local minima are spurious. However, due to the differences in the loss function and the output dimensions, the exact constructions of local minima are substantially different. Meanwhile, our Stages (2) and (3) employ the transformation operation to force the data flow to go through the same series of the linear parts of the activations. The operations are carefully designed and the whole construction is novel and non-trivial.\n\nBesides, we also made extensions on the loss function and the output dimension. The difficulties are justified below:\n\n1.\tFrom squared loss to arbitrary differentiable loss: Yun et al. (2019b) calculate the analytic formations of derivatives of the loss to construct the local minima and then prove they are spurious. This technique cannot be transplanted to the case of arbitrary differentiable loss functions, because we cannot assume the analytic formation. To prove that the loss surface under an arbitrary differentiable loss has an infinite number of spurious local minima, we employ a new proof technique based on Taylor series and a new separation lemma (see Appendix A.5, Lemma 6, p. 31) to avoid the use of the analytic formulations (see a detailed proof in Appendix A.2, Step (b), pp.14-15).\n\n2.\tFrom one-dimensional output to arbitrary-dimensional output: To prove the loss surface of a neural network with an arbitrary-dimensional output has an infinite number of spurious local minima, we need to deal with the calculus of functions whose domain and codomain are a matrix space and a vector space, respectively. By contrast, when the output dimension is one, the codomain is only the space of real numbers. Therefore, the extension of output dimension significantly mounts the difficulty of the whole proof."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "Byg9OHRfjS", "original": null, "number": 3, "cdate": 1573213554417, "ddate": null, "tcdate": 1573213554417, "tmdate": 1573213554417, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "SJxEpgtGcS", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (Part 3 of 3)", "comment": "Q3(a): While ReLU-type activations are popular, there are still commonly-used activation functions are not piece-wise linear, e.g., tanh, swish. It is not proper to claim that \"this paper presents how nonlinearities in activations substantially shape the loss surface\" and \"almost every practical neural network ....\". I suggest replacing \"nonlinearity\" with \"piecewise linearity\" in both the title and the abstract, and modifying the over-statements. \n\nA3(a): Thanks and revised accordingly. We have carefully improved the final version to avoid misunderstanding. However, we respectfully argue that \u201cnonlinearity\u201d is more common to refer to the difference from linear functions, which has been also used by Yun et al. (2019b).\n\n\n\nQ3(b): In Property 1 of Theorem 2, \u201csmooth and multilinear partition\u201d might be a bit misleading. The loss surface should be fractional in general, where multilinear cells are separated by non-smooth boundaries. \u201cSmooth partition\u201d seems to imply that the boundaries are smooth or the partition method is smooth in some sense.\n\nA3(b): Thanks and revised accordingly. Both \u201csmooth\u201d and \u201cmultilinear\u201d refer to the geometry in the cell, which is defined as an open set that does not include the boundary (see abstract, introduction, and Theorem 2). To avoid misunderstanding, we have added an explanation of the both words: \u201cthe loss surface is partitioned into multiple smooth and multilinear open cells by nondifferentiable boundaries\u201d and formally defined \u201copen set\u201d in the final version: \u201ca set A is open if it does not contains any point in its boundary\u201d (see the definition of boundary in Section 4.1, Definition 3, p. 6).\n\n\n\nQ3(c): The name \u201canalogous convexity\u201d is not appropriate. Analogous convexity is not formally defined in the paper. According to Sec. 4.3 third paragraph, \u201cthe property of analogous convexity that the local minima wherein are equally good\u201d. It seems that \u201canalogous convexity\u201d is just \u201call local minima are good\u201d, which is very different from convexity. It is a weaker property than quasi-convexity, star-convexity, etc, and thus it is better not to call it \u201canalogous convexity\u201d.\n\nA3(c): Thanks and revised accordingly. We have added an explanation of this name in the final version. There are two main reasons to use the name \u201clocal analogous convexity\u201d: (1) within every cell, the empirical risk $\\hat{\\mathcal R}$ is convex with respect to a variable $\\hat W$ mapped from the weights $W$. Therefore, the local minima with respect to the variable $\\hat W$ are also the global minima in the cell; and then (2) the local optimality is maintained under the constructed mapping. Specifically, the local minima of the empirical risk $\\hat{\\mathcal R}$ with respect to the parameter W are also the local minima with respect to the variable $\\hat W$. We will add this discussion to the final version.\n\n\n\nQ3(d): Property 3 of Theorem 2 is very far from \u201cmode connectivity\u201d. The proof of Property 3 relies on a special construction of Theorem 1, and the latter is for two arbitrary global minima.\n\nA3(d): We respectfully argue that the Property 3 of Theorem 2 shed lights to the study of \u201cmode connectivity\u201d, although it is an initial attempt. Specifically, \u201cmode connectivity\u201d says that the minima found by gradient-based methods are connected, while we exactly prove that some local minima are connected."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "S1lvlHCMjB", "original": null, "number": 2, "cdate": 1573213423140, "ddate": null, "tcdate": 1573213423140, "tmdate": 1573213423140, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "SJxEpgtGcS", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (Part 2 of 3)", "comment": "Q2(b): Theorem 2 seems easy to prove. The 1st, 3rd and 4th property are all straightforward. The 2nd property \u201clocal analogous convexity\u201d was given a 2-page proof in the paper. However, I don\u2019t understand why not use the following simple argument: for each region, the network behaves like a deep linear network, thus directly applying existing result shall imply \u201cevery local minimum in the region is the global minimum of the region\u201d, right? If not, what is the difficulty?\n\nA2(b): We agree the 1st and 4th properties can be obtained easily. The 3rd property is derived from Theorem 1, i.e., given Theorem 1, the 3rd property can be obtained easily as well. However, it is not easy to prove Theorem 1. Moreover, it is essential to include all the four properties in Theorem 2, because they collectively draw a picture of the loss surface. Specifically, without highlighting the partition (the 1st property), we have no grounds to study the geometry within the cells (the 2nd property) and we cannot show the constructed spurious local minima are exactly in one single cell (the 3rd property). Besides, the 1st property is essential to derive the 4th property to show linear networks are included as a simplified example of nonlinear networks.\n\nAlso, it is challenging to prove the 2nd property, because the proof techniques for linear networks cannot be transplanted here. Technically, linear networks can be expressed by the product of a sequence of weight matrices, which guarantees good geometrical properties. Specifically, the effect of every linear activation function is just equivalently multiplying a real constant to the output. However, the loss surface within a cell of a nonlinear neural network does not have this property.\n\nWe take a one-hidden-layer network for example. Suppose the output of the hidden layer is $h(WX))$, where $X$ is the data matrix, $h$ is the activation function, and $W$ is the weight matrix. If $h$ is a linear function $h(x) = ax$ (element-wise), its effect is equivalently multiplying the constant a to every element of the matrix $WX$. However, when the activation h is a two-piece linear function which has slopes $\\{a_1, a_2\\}$, different elements in the matrix $WX$ can be multiplied by either one from $\\{a_1, a_2\\}$. Therefore, we cannot use a single constant to express the effect of this activation, and thus, even within the cell, a nonlinear network cannot be expressed as the product of a sequence of weight matrices. This difference ensures that the proofs of deep linear neural networks cannot be transplanted here.\n\nTo address this issue, we develop a novel and non-trivial approach to prove that all local minima in a cell are globally optimal within the cell. Specifically, we prove this in two steps: (1) we prove that within every cell, the empirical risk $\\hat{\\mathcal R}$ is convex with respect to a variable $\\hat W$ mapped from the weights $W$. Therefore, the local minima with respect to the variable $\\hat W$ are also the global minima in the cell; and then (2) we prove that the local optimality is maintained under the constructed mapping. Specifically, the local minima of the empirical risk $\\hat{\\mathcal R}$ with respect to the parameter W are also the local minima with respect to the corresponding variable $\\hat W$.\n\n\n\nQ2(c): The 3rd property says \u201csome local minima are concentrated as a valley in some cell\u201d. What are the formal definitions of \u201cconcentrated\u201d and \u201cvalley\u201d in this sentence?\n\nA2(c): Thanks and revised accordingly. In the final version, we have changed it to: \u201csome local minima are connected within a cell by a continuous path, on which all points have the same empirical risk\u201d.\n\n\n\nQ2(d): The link to Theorem 1 is weak: the link is the 3rd property of Theorem 2 that \u201csome local minima are in a valley\u201d. It is just about some special local minima and weakly related to the other properties on the \u201cglobal view\u201d. In addition, the fact that \u201csome of them are in a valley\u201d may be due to the very special construction, thus it is not surprising and does not reveal anything interesting about the \u201cbig picture\u201d.\n\nA2(d): First, Theorem 1 and Theorem 2 collectively support our argument that the nonlinearities in the activations substantially shape the loss surface. Specifically, Theorem 1 gives a negative result that the loss surfaces of nonlinear networks are substantially different from those of linear networks; and Theorem 2 provides positive results on how the nonlinearities work and how the loss surface looks like.\n\nSecond, we agree that the 3rd property of Theorem 2 is based on special constructions. Therefore, we can say that \u201csome local minima are connected by a continuous path within a cell, on which all points have the same empirical risk\u201d."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "r1xZa7AGiB", "original": null, "number": 1, "cdate": 1573213112603, "ddate": null, "tcdate": 1573213112603, "tmdate": 1573213112603, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "SJxEpgtGcS", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (Part 1 of 3)", "comment": "We appreciate your thorough review and constructive comments. All your concerns have been duly addressed below. We have also updated the final version accordingly. We sincerely hope that you can take into account the response we have made and reevaluate the merit of this paper.\n\n\nQ1: Related results have been studied in a few previous works. In particular, Yun et al. (2019b) prove a similar result for 1-hidden-layer neural-net with ReLU activation. The extension is mathematically nice, but the motivation of this extension is somewhat unclear.\n\nA1: Our paper studies the theoretical foundations of deep learning. We acknowledge the significant contributions made by Yun et al. (2019b), which are however under some restrictions, including one hidden layer, squared loss, and one-dimensional output. Thus, it is not sufficient to comprehensively build the theoretical foundation for deep learning. To date, the theoretical study of deep learning is still in its infancy. Significant efforts are really demanded. The motivation and significance of our studies are justified below:\n\n1.\tFrom one hidden layer to arbitrary depth: Empirical results have overwhelmingly suggested that the increase of the depth of neural networks may substantially improve the performance. Additionally, training neural networks is increasingly difficult when the networks turn deeper. Therefore, the depth would play a significant role in shaping the loss surface of a neural network. To prove that networks with an arbitrary depth have infinite spurious local minima, we develop a novel strategy that employs transformation operations to force data flow through the same linear parts of the activations, in order to construct the spurious local minima (see a summary in Section 3.3, Stages 2 and 3, pp. 5-6; and a detailed proof in Appendix A.3 and A.4, pp. 20-31).\n\n2.\tFrom squared loss to arbitrary differentiable loss: Many other loss functions, such as cross-entropy loss, are widely utilized in deep learning. Only considering squared loss is insufficient. Yun et al. (2019b) calculate the analytic formations of derivatives of the loss to construct the local minima and then prove they are spurious. This technique cannot be transplanted to the case of arbitrary differentiable loss functions, because we cannot assume the analytic formation. To prove that the loss surface under an arbitrary differentiable loss has an infinite number of spurious local minima, we employ a new proof technique based on Taylor series and a new separation lemma (see Appendix A.5, Lemma 6, p. 31) to avoid the use of the analytic formulations (see a detailed proof in Appendix A.2, Step (b), pp.14-15).\n\n3.\tFrom one-dimensional output to arbitrary-dimensional output: Most datasets have high-dimensional labels. For example, the label dimension is 10 in CIFAR-10, 100 in CIFAR-100, and 1,000 in ImageNet.  To prove the loss surface of a neural network with an arbitrary-dimensional output has an infinite number of spurious local minima, we need to deal with the calculus of functions whose domain and codomain are a matrix space and a vector space, respectively. By contrast, when the output dimension is one, the codomain is only the space of real numbers. Therefore, the extension of output dimension significantly mounts the difficulty of the whole proof. \n\n\nQ2(a): The main message of Thm 2 is the partition of the surface into multiple pieces, and each piece has good property. This partition has been studied before, in, e.g., [R1]. Theorem 2 mainly describes the property of each region separately for 2-layer network, which is weaker than [R1].\n\nA2(a): Thank you for bringing the paper [R1] to our attention. We agree that we overlooked this paper in the original submission. We have duly acknowledged this paper in the final version. We noted that Lemma 2 in [R1] is similar to the 2nd property of our Thm 2. However, our proof is completely different from that in [R1] and our result is stronger and more general. Specifically, in Property 2 of Thm 2, we proved that within every cell, all local minima are glocal minimal in the cell. However, the Lemma 2 in [R1] only proves that the local minima in a cell are the same; there would be some point near the boundary has smaller empirical risk and is not locally minimal. Unfortunately, the proof in [R1] cannot exclude this possibility. Furthermore, our proof holds for any convex loss, including squared loss and cross-entropy loss, but [R1] only stands for squared loss."}, "signatures": ["ICLR.cc/2020/Conference/Paper540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1x6BTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper540/Authors|ICLR.cc/2020/Conference/Paper540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169925, "tmdate": 1576860557466, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper540/Authors", "ICLR.cc/2020/Conference/Paper540/Reviewers", "ICLR.cc/2020/Conference/Paper540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Comment"}}}, {"id": "B1g3dnrRKH", "original": null, "number": 2, "cdate": 1571867764473, "ddate": null, "tcdate": 1571867764473, "tmdate": 1572972582664, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the theoretical property of neural network's loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima. Moreover, the paper further characterizes the partition of the local minima. More precisely, the loss surface is partitioned into multiple smooth and multilinear open cells and within each cell, the local minima are equally good. This result can also explain the linear neural network case where there is only one cell, implying that all local minima are global. \n\nOn one hand, I find the paper very clear and the result very clean, which unites a lot of existing results. On the other hand, with a reasonable initialization in practice, we will not attain the local minima constructed in the paper since it requires all the activations to be positive. This limits the plausible implication from this theoretical study. Overall, I am very positive of the paper, the following are some detailed comments. \n\na. Please be more precise in the abstract that the activation function need to be piecewise linear. \nThe current sentence \"the loss surface of every neural network has infinite spurious local minima\" does not include this specification. Moreover, if the activation is differentiable, is the claim still hold? It seems to me from the middle of page 3 that Li et al 2018 shows a non-local minima result in this case.\n\nb. How different is the analysis comparing to existing result?\nI have only go through the skeleton of the proof and have not read into the details. It seems to me the construction of the local minima is very similar to [1], since the main idea is to consider the linear region by activating all the neurons. Could you summarize the main difficulty to extend their results to multi-layer cases? (Maybe it would be good to illustrate with a simple case like 3 layers few neurons per layer)\nMoreover, when considering the local convexity, is it sufficient to say that locally in each cell it is a linear network and then the results on linear network transfers to it locally?\n\n[1] Small nonlinearities in activation functions create bad local minima in neural networks, Yun et al, 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575902028717, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper540/Reviewers"], "noninvitees": [], "tcdate": 1570237750659, "tmdate": 1575902028732, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Review"}}}, {"id": "SJxEpgtGcS", "original": null, "number": 3, "cdate": 1572143292108, "ddate": null, "tcdate": 1572143292108, "tmdate": 1572972582620, "tddate": null, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper540/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper studies the landscape of deep neural networks with piecewise-linear activation functions. The paper showed that under very mild assumptions, the loss surface admits infinite spurious local minima. Further, it is shown that the loss surface is partitioned into many multilinear cells. If the network is two-layer with two-piece linear activations, it is proved that within each cell every local minimum is global. \n\nPros:\n  --Constructed spurious local minima for piecewise linear activations, for a broader setting than previous papers. \n  --The paper is well written, with detailed explanation of proof skeleton. \n\nCons: \nThe significance of the results are not clear. Details are given below. \n\n1.\tThis paper only considers piecewise linear activation, which is a special type of non-linear activation. The major examples are ReLU and leaky ReLU. However, related results for ReLU have been studied for a few previous works mentioned in the last two paragraphs of Sec. 3.2. In particular, Yun et al. 2019b already proves a similar result for 1-hidden-layer neural-net with ReLU activation. I think extending the construction to broader settings (any depth, any piecewise linear and more losses) is mathematically nice, but the motivation of this extension is somewhat unclear to me. One motivation is that this is helpful for the purpose of understanding a \u201cbig picture\u201d of the landscape, which I will discuss next.\n\n2.\tThe second major result is Theorem 2, on the \u201cbig picture\u201d with ReLU-like activations. However, Theorem 2 is somewhat trivial to prove, and the link to Theorem 1 is rather weak.\n   (a) The main message of Thm 2 is the partition of the surface into multiple pieces, and each piece has good property. This partition is somewhat straightforward, and has been studied before, in, e.g., [R1].\n   For a global \u201cbig picture\u201d, partitioning itself is not very interesting. Theorem 2 mainly describes the property of each region separately for 2-layer network, which is weaker than [R1]. \n   (b) Theorem 2 seems easy to prove. The 1st, 3rd and 4th property are all straightforward. The 2nd property \u201clocal analogous convexity\u201d was given a 2-page proof in the paper. However, I don\u2019t understand why not use the following simple argument: for each region, the network behaves like a deep linear network, thus directly applying existing result shall imply \u201cevery local minimum in the region is the global minimum of the region\u201d, right? If not, what is the difficulty?\n   (c) The 3rd property says \u201csome local minima are concentrated as a valley in some cell\u201d. What are the formal definitions of \u201cconcentrated\u201d and \u201cvalley\u201d in this sentence?\n   (d) The link to Theorem 1 is weak: the link is the 3rd property of Theorem 2 that \u201csome local minima are in a valley\u201d. It is just about some special local minima and weakly related to the other properties on the \u201cglobal view\u201d. In addition, the fact that \u201csome of them are in a valley\u201d may be due to the very special construction, thus it is not surprising and does not reveal anything interesting about the \u201cbig picture\u201d. \n\n\n3.\tOther issues:\na) While ReLU-type activations are popular, there are still commonly-used activation functions are not piece-wise linear, e.g., tanh, swish. It is not proper to claim that \"this paper presents how nonlinearities in activations substantially shape the loss surface\" and \"almost every practical neural network ....\". I suggest replacing \"nonlinearity\" with \"piecewise linearity\" in both the title and the abstract, and modifying the over-statements. \n   b) In Property 1 of Theorem 2, \u201csmooth and multilinear partition\u201d might be a bit misleading. The loss surface should be fractional in general, where multilinear cells are separated by non-smooth boundaries. \u201cSmooth partition\u201d seems to imply that the boundaries are smooth or the partition method is smooth in some sense.\n   c) The name \u201canalogous convexity\u201d is not appropriate. Analogous convexity is not formally defined in the paper. According to Sec. 4.3 third paragraph, \u201cthe property of analogous convexity that the local minima wherein are equally good\u201d. It seems that \u201canalogous convexity\u201d is just \u201call local minima are good\u201d, which is very different from convexity. It is a weaker property than quasi-convexity, star-convexity, etc, and thus it is better not to call it \u201canalogous convexity\u201d.\n    d) Property 3 of Theorem 2 is very far from \u201cmode connectivity\u201d. The proof of Property 3 relies on a special construction of Theorem 1, and the latter is for two arbitrary global minima. \n\n\n[R1] Soudry and Hoffer. \"Exponentially vanishing sub-optimal local minima in multilayer neural networks.\"\u00a0arXiv preprint arXiv:1702.05777\u00a0(2017).\n\n\nConclusion:  I think this paper is studying an important and interesting question, and the efforts of constructing local minima and understanding big picture are both interesting to me. However, I\u2019m afraid the current form of the paper does not meet the standard of the conference. That being said, it would be a nice paper if the big picture can be explored deeper, and the link to the spurious local minima can be built stronger. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper540/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["fengxiang.he@sydney.edu.au", "bhwangfy@gmail.com", "dacheng.tao@sydney.edu.au"], "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "authors": ["Fengxiang He", "Bohan Wang", "Dacheng Tao"], "pdf": "/pdf/9fc93c5ae0dcd8dffa1b0e77e1ec74c59f92d8a8.pdf", "TL;DR": "This paper presents how the loss surfaces of nonlinear neural networks are substantially shaped by the nonlinearities in activations.", "abstract": "Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrower than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, we prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.", "keywords": ["neural network", "nonlinear activation", "loss surface", "spurious local minimum"], "paperhash": "he|piecewise_linear_activations_substantially_shape_the_loss_surfaces_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020Piecewise,\ntitle={Piecewise linear activations substantially shape the loss surfaces of neural networks},\nauthor={Fengxiang He and Bohan Wang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1x6BTEKwr}\n}", "original_pdf": "/attachment/8fb1a3676d9862475844b456b120a084dc2671f9.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1x6BTEKwr", "replyto": "B1x6BTEKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575902028717, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper540/Reviewers"], "noninvitees": [], "tcdate": 1570237750659, "tmdate": 1575902028732, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper540/-/Official_Review"}}}], "count": 25}