{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124281438, "tcdate": 1518471274041, "number": 295, "cdate": 1518471274041, "id": "HJMudFkDf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HJMudFkDf", "signatures": ["~Stefan_Falkner1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Practical Hyperparameter Optimization for Deep Learning", "abstract": "Recently, the bandit-based strategy Hyperband (HB) was shown to yield good\nhyperparameter settings of deep neural networks faster than vanilla Bayesian\noptimization (BO). However, for larger budgets, HB is limited by its random search\ncomponent, and BO works better. We propose to combine the benefits of both\napproaches to obtain a new practical state-of-the-art hyperparameter optimization\nmethod, which we show to consistently outperform both HB and BO on a range\nof problem types, including feed-forward neural networks, Bayesian neural networks,\n and deep reinforcement learning. Our method is robust and versatile, while\nat the same time being conceptually simple and easy to implement.", "paperhash": "falkner|practical_hyperparameter_optimization_for_deep_learning", "keywords": ["Hyperparameter optimization", "Bayesian optimization"], "_bibtex": "@misc{\n  falkner2018practical,\n  title={Practical Hyperparameter Optimization for Deep Learning},\n  author={Stefan Falkner and Aaron Klein and Frank Hutter},\n  year={2018},\n  url={https://openreview.net/forum?id=HJMudFkDf}\n}", "authorids": ["sfalkner@informatik.uni-freiburg.de", "kleinaa@informatik.uni-freiburg.de", "fh@informatik.uni-freiburg.de"], "authors": ["Stefan Falkner", "Aaron Klein", "Frank Hutter"], "TL;DR": "We combine Bayesian optimization and Hyperband to obtain a practical hyperparameter optimizer that consistently outperforms both of them.", "pdf": "/pdf/6d8be655ade5d8d04bfcb342252e6f84d8972989.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521583002369, "tcdate": 1519236743233, "number": 1, "cdate": 1519236743233, "id": "HkyqUVoPf", "invitation": "ICLR.cc/2018/Workshop/-/Paper295/Official_Review", "forum": "HJMudFkDf", "replyto": "HJMudFkDf", "signatures": ["ICLR.cc/2018/Workshop/Paper295/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper295/AnonReviewer3"], "content": {"title": "Useful but perhaps somewhat obvious", "rating": "7: Good paper, accept", "review": "This paper proposes combining Bayesian optimization (BO) with the Hyperband algorithm of Li et al 2017.  Specifically, they replace the random configurations of Hyperband with ones chosen by the tree Parzen estimator BO method.  I think this idea is rather obvious making the novelty of the work limited, but the resulting algorithm is clearly useful and has not been explicitly suggested or tested in literature before.  As such, it makes a good fit for a three page short that will be helpful to the community, even if there is little it the way of new ideas.\n\nTo explain more specifically, when I read the original Hyperband paper (https://arxiv.org/abs/1603.06560v1) I was infuriated by their overclaiming and criticism of BO in the context of missing the seemingly obvious point that forcing BO to always carry out a full evaluation is spurious and suboptimal (see quote from private correspondence at the bottom of the review).  I doubt I was the only one to make this assessment at the time and so I am a little surprised that it has taken this long for something to appear in the literature making this seemingly obvious point that there is no reason that Hyperband should choose configurations randomly rather than in principled, BO-based manner.  However, I have not actually seen anything making this point in the literature until the reviewed paper; it is certainly to the best of my knowledge the first paper to the provide numerical results on using Hyperband with principled configuration choice.  The resulting algorithm unsurprisingly outperforms Hyperband and thus provides clear utility to the community.  Consequently, I think the paper should be accepted, even though I also feel it is quite limited in terms of novelty and technical contribution.\n\nEven though the paper is well-written at a low level, well motivated, and easy to follow, I do feel that the relative prioritization of space could be improved.  Namely, I think more space needs dedicating to explaining the specifics of the proposed approached and that this could be found by condensing the other sections.  In particular, the role of b in algorithm 1 should be explained.\n\nPros:\n- Useful algorithm\n- Clear contribution\n- Impactful research area\n- Good experimental results clearly presented\n\nCons:\n- Key idea very straightforward\n- Some key specifics of the proposed approach somewhat glossed over\n\n\nSpecific errata\n- q is not actually used in Alg 1\n- b is not really explained and D_b not defined in Alg 1\n- Should eta in the last line be rho?\n\nQuote from private correspondence on Hyperband paper\n\n\"The most damning part of [the Hyperband paper] is that there is nothing in the existing BO schemes to stop you from also changing the budget spent on each point... Instead one should use partial evaluation of points with revisiting in the same way that they do.  There are theoretical results that show ... one should spend the minimum possible budget on each evaluation and maximize the number of evaluations you make... For GP based BO, the cubically increasing cost of the BO itself is so high that setting this budget remains an open and active research question, but for SMAC and TPE which have negligible overheads, you can comfortably just set it low.\"", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Practical Hyperparameter Optimization for Deep Learning", "abstract": "Recently, the bandit-based strategy Hyperband (HB) was shown to yield good\nhyperparameter settings of deep neural networks faster than vanilla Bayesian\noptimization (BO). However, for larger budgets, HB is limited by its random search\ncomponent, and BO works better. We propose to combine the benefits of both\napproaches to obtain a new practical state-of-the-art hyperparameter optimization\nmethod, which we show to consistently outperform both HB and BO on a range\nof problem types, including feed-forward neural networks, Bayesian neural networks,\n and deep reinforcement learning. Our method is robust and versatile, while\nat the same time being conceptually simple and easy to implement.", "paperhash": "falkner|practical_hyperparameter_optimization_for_deep_learning", "keywords": ["Hyperparameter optimization", "Bayesian optimization"], "_bibtex": "@misc{\n  falkner2018practical,\n  title={Practical Hyperparameter Optimization for Deep Learning},\n  author={Stefan Falkner and Aaron Klein and Frank Hutter},\n  year={2018},\n  url={https://openreview.net/forum?id=HJMudFkDf}\n}", "authorids": ["sfalkner@informatik.uni-freiburg.de", "kleinaa@informatik.uni-freiburg.de", "fh@informatik.uni-freiburg.de"], "authors": ["Stefan Falkner", "Aaron Klein", "Frank Hutter"], "TL;DR": "We combine Bayesian optimization and Hyperband to obtain a practical hyperparameter optimizer that consistently outperforms both of them.", "pdf": "/pdf/6d8be655ade5d8d04bfcb342252e6f84d8972989.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521583002140, "id": "ICLR.cc/2018/Workshop/-/Paper295/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper295/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper295/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper295/AnonReviewer2"], "reply": {"forum": "HJMudFkDf", "replyto": "HJMudFkDf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper295/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521583002140}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582615903, "tcdate": 1520875215895, "number": 2, "cdate": 1520875215895, "id": "SkdALV4FM", "invitation": "ICLR.cc/2018/Workshop/-/Paper295/Official_Review", "forum": "HJMudFkDf", "replyto": "HJMudFkDf", "signatures": ["ICLR.cc/2018/Workshop/Paper295/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper295/AnonReviewer2"], "content": {"title": "Combination of two hyperparameter optimization methods", "rating": "7: Good paper, accept", "review": "The paper proposes to combine Tree Parzen Estimator (TPE) (Bergstra et al., 2011) and Hyperband (Li et al., 2017) for hyperparameter optimization. The proposed method uses TPE as one step of Hyperband that samples hyperparameter configurations. The method is evaluated on three tasks and seem to outperform both TPE and Hyperband.\n\nMore extensive experimental section would be needed for a full paper but this should be good enough as a workshop paper.\n\nSome improvements that could be made:\n- Algorithm 1 is unclear. What is b and D_b? Is the argmax over a family of datasets?\n- The first use of the acronym SH in the last paragraph of Section 3 doesn't contain its long form.\n- Legend of the top row of Figure 1 is difficult to read in the printed version.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Practical Hyperparameter Optimization for Deep Learning", "abstract": "Recently, the bandit-based strategy Hyperband (HB) was shown to yield good\nhyperparameter settings of deep neural networks faster than vanilla Bayesian\noptimization (BO). However, for larger budgets, HB is limited by its random search\ncomponent, and BO works better. We propose to combine the benefits of both\napproaches to obtain a new practical state-of-the-art hyperparameter optimization\nmethod, which we show to consistently outperform both HB and BO on a range\nof problem types, including feed-forward neural networks, Bayesian neural networks,\n and deep reinforcement learning. Our method is robust and versatile, while\nat the same time being conceptually simple and easy to implement.", "paperhash": "falkner|practical_hyperparameter_optimization_for_deep_learning", "keywords": ["Hyperparameter optimization", "Bayesian optimization"], "_bibtex": "@misc{\n  falkner2018practical,\n  title={Practical Hyperparameter Optimization for Deep Learning},\n  author={Stefan Falkner and Aaron Klein and Frank Hutter},\n  year={2018},\n  url={https://openreview.net/forum?id=HJMudFkDf}\n}", "authorids": ["sfalkner@informatik.uni-freiburg.de", "kleinaa@informatik.uni-freiburg.de", "fh@informatik.uni-freiburg.de"], "authors": ["Stefan Falkner", "Aaron Klein", "Frank Hutter"], "TL;DR": "We combine Bayesian optimization and Hyperband to obtain a practical hyperparameter optimizer that consistently outperforms both of them.", "pdf": "/pdf/6d8be655ade5d8d04bfcb342252e6f84d8972989.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521583002140, "id": "ICLR.cc/2018/Workshop/-/Paper295/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper295/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper295/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper295/AnonReviewer2"], "reply": {"forum": "HJMudFkDf", "replyto": "HJMudFkDf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper295/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521583002140}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573553424, "tcdate": 1521573553424, "number": 45, "cdate": 1521573553091, "id": "SyFhCARtG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HJMudFkDf", "replyto": "HJMudFkDf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Practical Hyperparameter Optimization for Deep Learning", "abstract": "Recently, the bandit-based strategy Hyperband (HB) was shown to yield good\nhyperparameter settings of deep neural networks faster than vanilla Bayesian\noptimization (BO). However, for larger budgets, HB is limited by its random search\ncomponent, and BO works better. We propose to combine the benefits of both\napproaches to obtain a new practical state-of-the-art hyperparameter optimization\nmethod, which we show to consistently outperform both HB and BO on a range\nof problem types, including feed-forward neural networks, Bayesian neural networks,\n and deep reinforcement learning. Our method is robust and versatile, while\nat the same time being conceptually simple and easy to implement.", "paperhash": "falkner|practical_hyperparameter_optimization_for_deep_learning", "keywords": ["Hyperparameter optimization", "Bayesian optimization"], "_bibtex": "@misc{\n  falkner2018practical,\n  title={Practical Hyperparameter Optimization for Deep Learning},\n  author={Stefan Falkner and Aaron Klein and Frank Hutter},\n  year={2018},\n  url={https://openreview.net/forum?id=HJMudFkDf}\n}", "authorids": ["sfalkner@informatik.uni-freiburg.de", "kleinaa@informatik.uni-freiburg.de", "fh@informatik.uni-freiburg.de"], "authors": ["Stefan Falkner", "Aaron Klein", "Frank Hutter"], "TL;DR": "We combine Bayesian optimization and Hyperband to obtain a practical hyperparameter optimizer that consistently outperforms both of them.", "pdf": "/pdf/6d8be655ade5d8d04bfcb342252e6f84d8972989.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}