{"notes": [{"id": "HJgpugrKPS", "original": "rJgxU6eKPH", "number": 2415, "cdate": 1569439861020, "ddate": null, "tcdate": 1569439861020, "tmdate": 1591619273419, "tddate": null, "forum": "HJgpugrKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "h1l5jl__g", "original": null, "number": 1, "cdate": 1576798748567, "ddate": null, "tcdate": 1576798748567, "tmdate": 1576800887451, "tddate": null, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This work presents a theory for building scale-equivariant CNNs with steerable filters. The proposed method is compared with some of the related techniques . SOTA is achieved on MNIST-scale dataset and gains on STL-10 is demonstrated. The reviewers had some concern related to the method, clarity, and comparison with related works. The authors have successfully addressed most of these concerns. Overall, the reviewers are positive about this work and appreciate the generality of the presented theory and its good empirical performance. All the reviewers recommend accept.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713313, "tmdate": 1576800262899, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Decision"}}}, {"id": "HJeiPFvjjS", "original": null, "number": 6, "cdate": 1573775714659, "ddate": null, "tcdate": 1573775714659, "tmdate": 1573775714659, "tddate": null, "forum": "HJgpugrKPS", "replyto": "HylDlFPosB", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment", "content": {"title": "Reply to Review #1. Part 2", "comment": "Q: (3.1) In Figure 2, how many layers does the network have which was used to construct the middle plot? \nA: In Figure 2 we represent the equivariance error of a network of just one layer. We added this information to Section 6.1 for better understanding.\n\nQ: (3.2) It would have been useful to include a study of the effect of the range and resolution of the scale space.\nA: The scales and therefore $\\sigma$ are the hyperparameters of the proposed method. The set of values we chose from is tailored by the requirement of the completeness of the obtained basis on the smallest scale when it is projected to the pixel grid.\n\nFor MNIST-scale experiment we used 4 scales with a step of $q=(10/3)^{1/3} \\approx 1.49$. We generate filters for $\\sigma=1.5, 1.5 q, 1.5 q^2, 1.5 q^3$ and store them in an array of spatial extent of $V=13$. We choose $q$ by relying on prior knowledge about the dataset. And the value of 1.5 is chosen from a set of $[1.1 - 1.7]$ with a step of 0.1 by using cross-validation. We choose the value which gives the best accuracy on the validation set. The variation of the accuracy during validation is of 0.1% on the scale of about 2%.\n\nFor STL-10 experiment, we sample 3 bases for $\\sigma = 0.9, 0.9 \\sqrt{2}, 1.8$ and store them in an array of spatial extent of $V= 7$. We chose the maximum number of scales we are able to use on our hardware. Here we use value of $0.9$ as it generated the complete basis on the smallest scale. And the value of $\\sqrt{2}$ is motivated by the assumption that in natural images of cats, cars, horses, etc. the scale variations are usually of factor 2. We did not run cross validation on this dataset."}, "signatures": ["ICLR.cc/2020/Conference/Paper2415/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgpugrKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2415/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2415/Authors|ICLR.cc/2020/Conference/Paper2415/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141709, "tmdate": 1576860538430, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment"}}}, {"id": "HylDlFPosB", "original": null, "number": 5, "cdate": 1573775598850, "ddate": null, "tcdate": 1573775598850, "tmdate": 1573775651026, "tddate": null, "forum": "HJgpugrKPS", "replyto": "SkgxHUDycS", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment", "content": {"title": "Reply to Review #1. Part 1", "comment": "Thank you for your review.\n\n\nQ: (1.1) The SESN-B architecture resembles quite closely the SI-ConvNet architecture of Kanazawa et al. Can you explain the difference? \nA: SI-ConvNet uses image resizing in each convolutional layer of the network. It relies on the interpolation techniques which cause interpolation artifacts and lead to less stable optimization and as a results to a decreased classification accuracy.\n\nQ: (1.2) I would have preferred to see the approach demonstrated on a task which possesses scale equivariance, such as semantic segmentation. \nA: You are right. We are currently working on applying SESN to visual tracking, where scale varies frequently because of the rapidly changing distances between objects and a camera.\n\nQ: (1.3) It would have been more convincing to compare against directly learning the filters at the highest resolution and obtaining the other filters by downsampling. \nA: The approach you describe seems to be close to SiCNN by Xu et al. We compare SESN against this one in our paper and favorably so.\n\nQ: (1.4 a, b, c) It seems that SESN-C should contain SESN-A as a special case. However, SESN-C is worse than SESN-A. \nA: SESN-C has the same number of parameters as SESN-A and SESN-B. Indeed, it is achieved by reducing the number of channels of the model. A wider version of SESN-C with the same number of channels as in SESN-A would have about 12 M parameters and would contain SESN-A as a special case with some weights equal to zero. However, the wider SESN-C would pose a different optimization problem with respect to its weights. \n\nWe used 3 scales for all SESN models in STL-10 experiment. SESN-C is scale-equivariant up to the border effects. This equivariance error, however, causes less generalization comparing to SESN-A and SESN-B.\n\nQ: (1.4 b) What is the number of scales in interaction in this experiment compared to the number of scales in S? And the same question for the plot on the right in Figure 2. \nA: In this experiment we used an interaction of 2 scales. And the total number of scales is equal to 3. In Figure 3, the total number of scales is 5 and the number of scales in interaction is represented on the horizontal axis.\n\nQ: (2.1, 2.4) The explanation of equation 10 is not clear\nA: In the proposed algorithm we represent convHH as a combination of tensor reshapings of different kinds and a convolution. In order to pack Equation 7 into a standard convolution, we expand a convolutional kernel to have a block-diagonal structure in the space of input-output channels. As a result, each scale of the kernel is convolved only with the function defined on the same scale. The obtained representation of the kernel is a sparse matrix. We delegate the further optimization of the proposed algorithm to a well-discovered problem of matrix multiplication. In our implementation we use a convolution routine of PyTorch. We are going to release our code soon.\n\nQ: (2.2) It's not immediately apparent how multiple applications of convHH are used to provide interscale interaction. \nA: In the case of interscale interaction $w$ has shape $[C_\\text{out}, C_\\text{in}, K_S, N_b ]$. we iterate over all scales in interaction, we shift $f$ for each scale and choose a corresponding part of $w$ and apply convHH. We sum the obtained results afterwards. Thank you for pointing this moment of our paper. We fixed it to make easier to understand.\n\nQ: (2.3) The explanation of \"scalar\" and \"vector\" variants in the experimental section was not perfectly clear.\nA: We rephrase this explanation in our text. We call some networks \u201cvector\u201d as they store $N_S$-dimensional vectors in each spatial position in each channel. In contrast, \u201cscalar\u201d networks or just standard CNNs have scalars in each position in each channel.\n\n\nQ: (2.5) It was not immediately obvious how $\\psi(s, t)$ was related to $\\psi(x)$. \nA: We modified it in the text to make it easier for understanding. $\\psi(x) = \\psi(s=1, t=x)$"}, "signatures": ["ICLR.cc/2020/Conference/Paper2415/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgpugrKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2415/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2415/Authors|ICLR.cc/2020/Conference/Paper2415/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141709, "tmdate": 1576860538430, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment"}}}, {"id": "H1xLPrvooS", "original": null, "number": 4, "cdate": 1573774686088, "ddate": null, "tcdate": 1573774686088, "tmdate": 1573774686088, "tddate": null, "forum": "HJgpugrKPS", "replyto": "r1g06sdZir", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment", "content": {"title": "On the related paper", "comment": "Thank you for this useful reference. We added it to our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2415/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgpugrKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2415/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2415/Authors|ICLR.cc/2020/Conference/Paper2415/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141709, "tmdate": 1576860538430, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment"}}}, {"id": "B1xNrEPsjH", "original": null, "number": 3, "cdate": 1573774396092, "ddate": null, "tcdate": 1573774396092, "tmdate": 1573774629017, "tddate": null, "forum": "HJgpugrKPS", "replyto": "H1g-eN-3KS", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment", "content": {"title": "Reply to Review #3", "comment": "Thank you for your review. \n\nWe compared our results on STL-10 to the current state-of-the-art model in the supervised learning setting known as Harm-WRN. Our model SESN-B ourperformes it by more than 1% and achieves new state-of-the-art result on this dataset in the supervised learning setting. We include Harm-WRN  in Table 3 for comparison. \n\nQ: Inter-scale interaction could be elaborated a bit more. I believe this is sequencing over scales in the kernel.\n \nA: Indeed. You are right. In the case of interscale interaction $w$ has shape $[C_\\text{out}, C_\\text{in}, K_S, N_b ]$. We iterate over all scales in interaction, we shift $f$ for each scale and choose a corresponding part of $w$ and apply convHH. We sum the obtained results afterwards. Thank you for this comment. We rephrased this section of our paper to make it easier for understanding.\n\n-------------------------\n\nQ: Which scales were chosen for the fixed basis? How large in spatial extent are the kernels in the basis elements, at each scale? In the implementation, what is the value of V?\n\nA: The scales and therefore $\\sigma$ are the hyperparameters of the proposed method. The set of values we choose from is tailored by the requirement of the completeness of the obtained basis on the smallest scale when it is projected to the pixel grid.\n\nFor MNIST-scale experiment we used 4 scales with a step of $q=(10/3)^{1/3} \\approx 1.49$. We generate filters for $\\sigma=1.5, 1.5 q, 1.5 q^2, 1.5 q^3$ and store them in an array of spatial extent of $V=13$. We choose $q$ by relying on prior knowledge about the dataset. And the value of 1.5 is chosen from a set of $[1.1 - 1.7]$ with a step of 0.1 by using cross-validation. We choose the value which gives the best accuracy on the validation set. The variation of the accuracy during cross-validation is of 0.1% on the scale of about 2%.\n\nFor STL-10 experiment, we sample 3 bases for $\\sigma = 0.9, 0.9 \\sqrt{2}, 1.8$ and store them in an array of spatial extent of $V= 7$. We chose the maximum number of scales we are able to use on our hardware. Here we use value of $0.9$ as it generated the complete basis on the smallest scale. And the value of $\\sqrt{2}$ is motivated by the assumption that in natural images of cats, cars, horses, etc. the scale variations are usually of factor 2. We did not run cross validation on this dataset."}, "signatures": ["ICLR.cc/2020/Conference/Paper2415/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgpugrKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2415/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2415/Authors|ICLR.cc/2020/Conference/Paper2415/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141709, "tmdate": 1576860538430, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment"}}}, {"id": "SyeNvGwsoS", "original": null, "number": 2, "cdate": 1573773916288, "ddate": null, "tcdate": 1573773916288, "tmdate": 1573773916288, "tddate": null, "forum": "HJgpugrKPS", "replyto": "BJgeyM2iYH", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment", "content": {"title": "Reply to Review #2", "comment": "Thank you for your review. \n \nQ: Would you care to make a comparison between these two manuscripts?\n\nA: In \u201cScale-Equivariant Neural Networks with Decomposed Convolutional Filters\u201d the authors propose scale-translation equivariant convolutional layers which are similar to what we propose. In addition, we propose the maximum scale projection which transforms the functions on scale-translations to the functions on just translations. In ScDCFNet paper this step is left unspecified. This difference has consequences for the experimental outcomes."}, "signatures": ["ICLR.cc/2020/Conference/Paper2415/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgpugrKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2415/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2415/Authors|ICLR.cc/2020/Conference/Paper2415/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141709, "tmdate": 1576860538430, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Official_Comment"}}}, {"id": "r1g06sdZir", "original": null, "number": 1, "cdate": 1573125061711, "ddate": null, "tcdate": 1573125061711, "tmdate": 1573125061711, "tddate": null, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Public_Comment", "content": {"title": "A recent reference which also deals with scale.", "comment": "The following reference also deals with scale invariance using coupled-autoencoders. The approach is entirely different, but the idea is to deal with scales under rotations.\n\nSO(2)-equivariance in Neural networks using tensor nonlinearity, Muthuvel Murugan, K V Subrahmanyam, BMVC 2019. "}, "signatures": ["~K_V_Subrahmanyam1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~K_V_Subrahmanyam1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgpugrKPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504180611, "tmdate": 1576860572119, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2415/Authors", "ICLR.cc/2020/Conference/Paper2415/Reviewers", "ICLR.cc/2020/Conference/Paper2415/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Public_Comment"}}}, {"id": "BJgeyM2iYH", "original": null, "number": 1, "cdate": 1571697111660, "ddate": null, "tcdate": 1571697111660, "tmdate": 1572972341203, "tddate": null, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposed scale-equivariant steerable convolutional neural networks that is able to preserve both the translation and scaling symmetry of the data in the representation. To achieve this, the authors developed the scale-convolution blocks in the network, and generalized other common blocks, such as pooling and nonlinearity, to remain scale-equivariant. Extensive experiments have been conducted to show that the proposed scale-equivariant network\n(a) is indeed scale-equivariant even with numerical discretization\n(b) achieves better classification performance when compared to non-scale-equivariant networks as well as previously proposed locally-scale invariant networks.\n\nOverall, this is a very good paper. The paper is well-written and well-organized. The newly proposed scale-convolution is the most general way of achieving scale-equivariant representations. Experiments are convincing and justifies the usage of the proposed architecture in dealing with multi-scale inputs.\n\nOne question to ask that does not effect my rating:\nThere is a very similar paper submitted to this conference:\nhttps://openreview.net/forum?id=rkgCJ64tDB\nWould you care to make a comparison between these two manuscript?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2415/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2415/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139585711, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2415/Reviewers"], "noninvitees": [], "tcdate": 1570237723145, "tmdate": 1576139585730, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Official_Review"}}}, {"id": "H1g-eN-3KS", "original": null, "number": 2, "cdate": 1571718121188, "ddate": null, "tcdate": 1571718121188, "tmdate": 1572972341165, "tddate": null, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper describes a method for integrating scale equivariance into convolutional networks using steerable filters.  After developing the theory using continuous scale and translation space, a discretized implementation using a fixed set of steerable basis elements is described.  Experiments are performed measuring the error from true equivariance, varying number of layers, image scale and scales in scale interactions.  The method is evaluated using MNIST-scale and STL-10, with convincing results on MNIST-scale and bit less convincing but still good results on STL-10.\n\nOverall, I think this is a nice paper with generally good explanations and experiments probing the behavior.  I would have liked to see more probing into the effects of number and distance between scales.  Table 1 and corresponding text say that a significant advantage of the approach is that it can handle arbitrary scale values, but there was no explicit exploration of the effects of using this beyond one set of scales per experiment/dataset.  What scale values can be sampled, which work best, and why?\n\nAlso, while the MNIST-scale experiment seems convincing, I think the STL-10 is a bit less (but still OK):  Although the method outperforms other methods and appropriate baseline models, it's a little disappointing that pooling over scales (which I would would convert the equivariance to invariance) is best, and inter-scale interactions increase error.  (Perhaps this is not too surprising in retrospect, as images may have limited scale variation from camera position in this dataset, but significant within-class viewpoint variation.)\n\nEven so, I still find the method concise and of interest, with the basics evaluated, even if some of its unique advantages may have been better explored.\n\n\nAdditional Questions:\n\n* Inter-scale interaction could be elaborated a bit more.  End of sec 4 says, \"use convHH for each scale sequentially and .. sum\".  I believe this is sequencing over scales in the kernel; explaining a bit better how this is implemented, including the shape of w in this case, would be helpful.\n\n* Which scales were chosen for the fixed basis?  How large in spatial extent are the kernels in the basis elements, at each scale?\n\n* In the implementation, what is the value of V (sampled 2d conv kernel size)?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2415/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2415/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139585711, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2415/Reviewers"], "noninvitees": [], "tcdate": 1570237723145, "tmdate": 1576139585730, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Official_Review"}}}, {"id": "SkgxHUDycS", "original": null, "number": 3, "cdate": 1571939895903, "ddate": null, "tcdate": 1571939895903, "tmdate": 1572972341128, "tddate": null, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "invitation": "ICLR.cc/2020/Conference/Paper2415/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a framework (SESN) for learning deep networks that possess scale equivariance in addition to translation invariance. The formulation is based on group convolution on the scale-translation group. Filters are represented as the coefficients of a set of continuous basis functions, which are sampled (once) at a discrete set of scales. The theoretical formulatioin is clear and interesting. The approach is evaluated in terms of image classification accuracy. The set of baselines is quite exhaustive, including recent papers and papers that are not widely-known.\n\nThe most significant improvement for the STL-10 dataset was obtained by the SESN-B variant. This is interesting, because it applies the same operation independently at multiple scales and periodically performs global pooling over scale.\n\nThe effectiveness of the approach was demonstrated in the low-data regime, where the inductive bias of scale equivariance is more likely to help.\n\nOverall I found the paper to be thought-provoking and well-executed. There are a number of questions that I would still like to see investigated, but nevertheless I feel that this paper already represents a worthwhile contribution.\n\nMost important issues and questions:\n\n(1.1) The SESN-B architecture resembles quite closely the SI-ConvNet architecture of Kanazawa et al. (except that that paper resized the images instead of the filters). While your approach may be more computationally efficient, it's not clear what leads to the improvement in accuracy here? Can you explain the difference?\n\n(1.2) I would have preferred to see the approach demonstrated on a task which possesses scale equivariance, such as semantic segmentation.\n\n(1.3) To argue in favour of the continuous basis, it would have been more convincing to compare against directly learning the filters at the highest resolution and obtaining the other filters by downsampling. This would not represent a runtime cost during inference.\n\n(1.4a) It seems that SESN-C should contain SESN-A as a special case. However, SESN-C is worse than SESN-A. Do you have any idea whether this is due to optimization difficulty or over-fitting? Could you compare the training objectives?\n(1.4b) It is stated that the scale equivariance of SESN-C is worse than SESN-A and -B. However, it should still be scale equivariant, except for boundary effects in scale? What is the parameter N_S in this experiment compared to the number of scales S? And the same question for the plot on the right in Figure 2.\n(1.4c) How does SESN-C have the same number of parameters as SESN-A and SESN-B? I thought that more parameters would be required to compute interscale interaction. Was the number of channels reduced?\n\nIssues with clarity:\n\n(2.1) The explanation of equation 10 is not clear. In particular, the diagonal structure in Figure 1 is not stated anywhere in the text, it is simply explained as an expansion from [C_out, C_in, S, V, V] to [C_out S, C_in S, V, V].\n\n(2.2) It's not immediately apparent how multiple applications of convHH are used to provide interscale interaction. I assume it is achieved by shifting f or psi in the scale dimension for each application of convHH, or equivalently by modifying the base-scale in the basis?\n\n(2.3) The explanation of \"scalar\" and \"vector\" variants in the experimental section was not perfectly clear. It is stated that \"all the layers have now scalar input instead of vector input.\" However, I understood that the max-reduction was only over the scale dimension, not the channel dimension, so that the inputs are still vectors? This is confusing as a reader.\n\n(2.4) The expansion of the filters to a diagonal structure is described in the \"implementation\" section. However, it seems that this would entail wasteful multiplications by zero. Nevertheless, SESN is shown to be highly efficient in the appendix. Do you avoid these pointless operations in the actual implementation?\n\n(2.5) It was not immediately obvious how $\\psi_{\\sigma}(s, t)$ was related to $\\psi_{\\sigma}(x)$.\n\nOther details:\n\n(3.1) In Figure 2, how many layers does the network have which was used to construct the middle plot?\n\n(3.2) It would have been useful to include a study of the effect of the range and resolution of the scale space."}, "signatures": ["ICLR.cc/2020/Conference/Paper2415/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2415/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sosnovikivan@gmail.com", "szmajamichal@gmail.com", "a.w.m.smeulders@uva.nl"], "title": "Scale-Equivariant Steerable Networks", "authors": ["Ivan Sosnovik", "Micha\u0142 Szmaja", "Arnold Smeulders"], "pdf": "/pdf/ce6c00156479706a1ff9e63741b0551427f47911.pdf", "abstract": "The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.", "keywords": ["Scale Equivariance", "Steerable Filters"], "paperhash": "sosnovik|scaleequivariant_steerable_networks", "spotlight_video": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "slides": "https://iclr.cc/virtual_2020/poster_HJgpugrKPS.html", "code": "https://github.com/isosnovik/sesn", "_bibtex": "@inproceedings{\nSosnovik2020Scale-Equivariant,\ntitle={Scale-Equivariant Steerable Networks},\nauthor={Ivan Sosnovik and Micha\u0142 Szmaja and Arnold Smeulders},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgpugrKPS}\n}", "original_pdf": "/attachment/063c866136f90482a841be7fcb7a723d5de4164b.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgpugrKPS", "replyto": "HJgpugrKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2415/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139585711, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2415/Reviewers"], "noninvitees": [], "tcdate": 1570237723145, "tmdate": 1576139585730, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2415/-/Official_Review"}}}], "count": 11}